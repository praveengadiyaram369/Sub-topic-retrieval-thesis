{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "39ee9507",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sri.sai.praveen.gadi\\.virtualenvs\\xxx-eAHl9Gue\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import fasttext\n",
    "import fasttext.util\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_text\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics import precision_recall_fscore_support, classification_report, precision_recall_curve\n",
    "\n",
    "import spacy\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0522dd4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 512)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "array = np.loadtxt(os.getcwd() + '/../dataframes/210705_news_328064.txt', delimiter=',').reshape(1, -1)\n",
    "array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8b6d9872",
   "metadata": {},
   "outputs": [],
   "source": [
    "PRETRAINED_FASTTRACK_MODEL = os.getcwd() + '/../../models/lid.176.bin'\n",
    "LANG_EN = \"__label__en\"\n",
    "LANG_DE = \"__label__de\"\n",
    "\n",
    "fasttext.FastText.eprint = lambda x: None\n",
    "fasttext_model = fasttext.load_model(PRETRAINED_FASTTRACK_MODEL)\n",
    "\n",
    "def detect_language(text):\n",
    "    \n",
    "    lang_label = fasttext_model.predict(text)[0][0].split('__label__')[1]\n",
    "    return lang_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "88fcd4f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ko'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"\"\"미소와 함께 '일상의 변화'를 만들 '미소 메이커스'를 찾습니다  미소는 국내 No.1 O2O 홈서비스 플랫폼 기업입니다. \"Hotel-like service in your home\"  이라는 비전 아래, 고객에게 더욱 행복한 경험을 더욱 많이 제공하고자 합니다. 대표 서비스인 '홈클리닝'을 중심으로, '이사'/'가전청소'/'인테리어'/'펫시팅' 등  70여가지의 서비스 로 사업 범위를 확장하였습니다. 고객 만족 원칙과 데이터 기반 기술로 매년 2배가량 돋보이게 성장하고 있습니다. 2021년 현재 Series A 단계이며, 총 투자규모는 약 130억 원입니다. 한국 O2O 기업으로는 최초로 실리콘밸리의 최대 벤처 투자사 ‘Y Combinator’로부터 31억 원의 투자를 유치했습니다. 누적 매출액은  1,000억 원 , 누적 주문건수  300만 건  및 누적 파트너수  40,000명 을 돌파했습니다.  미소의 일 하는 방식 엿보기 Work hard on the Right Things 미소는 올바른 일에 집중합니다. 미소 팀블로그에서 미소 메이커스의 이야기를 들어 보세요!    예비 미소 메이커스를 위한 참고 사이트   채용과 관련한 모든 문의사항은,  recruit@getmiso.com (People Team) 으로 부탁드립니다.\"\"\"\n",
    "fasttext_model.predict(text)[0][0].split('__label__')[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ed70b5f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:SavedModel saved prior to TF 2.5 detected when loading Keras model. Please ensure that you are saving the model with model.save() or tf.keras.models.save_model(), *NOT* tf.saved_model.save(). To confirm, there should be a file named \"keras_metadata.pb\" in the SavedModel directory.\n"
     ]
    }
   ],
   "source": [
    "tf.saved_model.LoadOptions(\n",
    "    allow_partial_checkpoint=False,\n",
    "    experimental_io_device='/job:localhost',\n",
    "    experimental_skip_checkpoint=False\n",
    ")\n",
    "\n",
    "tf_model = tf.keras.models.load_model(\n",
    "    os.getcwd() + '/../../models/USE_model/'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bd58b5de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_svm_classifier():\n",
    "    return SVC(kernel='rbf', gamma='auto', class_weight='balanced', probability=True, random_state=122)\n",
    "\n",
    "def get_rf_classifier():\n",
    "    return RandomForestClassifier(n_estimators=300, random_state=122)\n",
    "\n",
    "def get_lr_classifier():\n",
    "    return LogisticRegression(class_weight='balanced', random_state=122)\n",
    "\n",
    "def get_three_class_models():\n",
    "    \n",
    "    model_1 = get_svm_classifier()\n",
    "    model_2 = get_svm_classifier()\n",
    "    model_3 = get_lr_classifier()\n",
    "    \n",
    "    return [model_1, model_2, model_3]\n",
    "\n",
    "def get_modified_vectors(vec_data):\n",
    "    \n",
    "    new_data = []\n",
    "    for val in vec_data:\n",
    "        new_data.append(val)\n",
    "    \n",
    "    new_data = np.array(new_data).reshape(-1, 512)\n",
    "    return new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d22d27cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_threshold_output(preds, threshold):\n",
    "    \n",
    "    y_preds = []\n",
    "    for val in preds:\n",
    "        if val >= threshold:\n",
    "            y_preds.append(1)\n",
    "        else:\n",
    "            y_preds.append(0)\n",
    "        \n",
    "    y_preds = np.array(y_preds)\n",
    "    \n",
    "    return y_preds\n",
    "\n",
    "def get_f1_score_binary(y, preds, threshold, print_report=False):\n",
    "    \n",
    "    y_preds = get_threshold_output(preds, threshold)\n",
    "    \n",
    "    metrics = precision_recall_fscore_support(y, y_preds)\n",
    "    f1_score_tech = metrics[2][1]\n",
    "    precision_tech = metrics[0][1]\n",
    "    recall_tech = metrics[1][1]\n",
    "    \n",
    "    if print_report:\n",
    "        print(classification_report(y, y_preds))\n",
    "        return f1_score_tech, precision_tech, recall_tech\n",
    "    \n",
    "    return f1_score_tech\n",
    "\n",
    "def get_best_threshold(y, preds):\n",
    "    \n",
    "    threshold_vals = np.arange(0.1, 1, 0.001)\n",
    "    f1_score_list = []\n",
    "    \n",
    "    for val in threshold_vals:\n",
    "        f1_score_list.append(get_f1_score_binary(y, preds, val))\n",
    "\n",
    "    max_idx = np.nanargmax(f1_score_list)\n",
    "    thre_max = threshold_vals[max_idx]\n",
    "    fscore = f1_score_list[max_idx]\n",
    "    \n",
    "    print(fscore)\n",
    "    print(thre_max)\n",
    "    \n",
    "    return thre_max    \n",
    "\n",
    "def get_trained_model_binary(X, y):\n",
    "    \n",
    "#     skf_f1score = perform_cross_validation(X, y, fold_cnt=5)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, stratify=y, random_state=123)\n",
    "    \n",
    "    model = get_svm_classifier()\n",
    "    clf = make_pipeline(StandardScaler(with_mean=False), model)\n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    pred_probs = clf.predict_proba(X_test)[:,1]\n",
    "    threshold = get_best_threshold(y_test, pred_probs)\n",
    "    \n",
    "    skf_f1score = 0\n",
    "    \n",
    "    return clf, skf_f1score, threshold\n",
    "\n",
    "def get_test_f1score_binary(model, X_test, y_test, threshold):\n",
    "    \n",
    "    preds = model.predict_proba(X_test)\n",
    "    preds = preds[:,1]\n",
    "\n",
    "    f1_score = get_f1_score_binary(y_test, preds, threshold, print_report=True)\n",
    "    \n",
    "    return f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "22cedd9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_multi_class_metrics(y_test, preds, pr_flag=False):\n",
    "    \n",
    "    metrics = precision_recall_fscore_support(y_test, preds)\n",
    "    \n",
    "    precision_tech = metrics[0][1]\n",
    "    precision_milt = metrics[0][2]\n",
    "    \n",
    "    recall_tech = metrics[1][1]\n",
    "    recall_milt = metrics[1][2]\n",
    "    \n",
    "    f1_score_tech = metrics[2][1]\n",
    "    f1_score_milt = metrics[2][2]\n",
    "    \n",
    "#     f1_score_milt = metrics[2][1]\n",
    "    \n",
    "    f1_score = (f1_score_tech+f1_score_milt)/2\n",
    "    \n",
    "    if pr_flag:\n",
    "        print(classification_report(y_test, preds))\n",
    "        return f1_score, preds, f1_score_milt, precision_milt, recall_milt\n",
    "    \n",
    "    return f1_score\n",
    "\n",
    "def perform_cross_validation(X, y, fold_cnt=5):\n",
    "    \n",
    "    skf = StratifiedKFold(n_splits=fold_cnt, shuffle=True, random_state=123)\n",
    "    f1_scores_list = []\n",
    "    \n",
    "    for train_idx, test_idx in skf.split(X, y):\n",
    "        \n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = y[train_idx], y[test_idx]\n",
    "        \n",
    "        model = get_svm_classifier()\n",
    "        clf = make_pipeline(StandardScaler(with_mean=False), model)\n",
    "        \n",
    "        clf.fit(X_train, y_train)\n",
    "        preds = clf.predict(X_test)\n",
    "\n",
    "        f1_score = get_multi_class_metrics(y_test, preds, pr_flag=False)\n",
    "        f1_scores_list.append(f1_score)\n",
    "    \n",
    "    return sum(f1_scores_list)/fold_cnt\n",
    "\n",
    "def get_trained_model(X, y):\n",
    "    \n",
    "    skf_f1score = perform_cross_validation(X, y, fold_cnt=5)\n",
    "    \n",
    "    model = get_svm_classifier()\n",
    "    clf = make_pipeline(StandardScaler(with_mean=False), model)\n",
    "    clf.fit(X, y)\n",
    "    \n",
    "    return clf, skf_f1score\n",
    "\n",
    "def get_test_f1score(model, X_test, y_test):\n",
    "    \n",
    "    preds = model.predict(X_test)\n",
    "    f1_score, preds, f1_score_milt, precision_milt, recall_milt = get_multi_class_metrics(y_test, preds, pr_flag=True)\n",
    "    \n",
    "    return f1_score, preds, f1_score_milt, precision_milt, recall_milt\n",
    "\n",
    "def get_performance_metrics(X_train, y_train, X_test, y_test, binary=False):\n",
    "    \n",
    "    if binary:\n",
    "        model, cv_score, threshold = get_trained_model_binary(X_train, y_train)\n",
    "        test_f1_score, precision_tech, recall_tech = get_test_f1score_binary(model, X_test, y_test,threshold)\n",
    "        \n",
    "        return model, threshold, test_f1_score, precision_tech, recall_tech\n",
    "    else:    \n",
    "        model, test_cv_score = get_trained_model(X_train, y_train)\n",
    "        test_f1_score, preds, f1_score_milt, precision_milt, recall_milt = get_test_f1score(model, X_test, y_test)\n",
    "        \n",
    "        test_df['pred_label'] = preds\n",
    "\n",
    "        return model, test_cv_score, test_f1_score, f1_score_milt, precision_milt, recall_milt\n",
    "\n",
    "    print()\n",
    "    print(f'Training CV f1 score: {cv_score}')\n",
    "    print(f'Test F1-score: {test_f1_score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bff0199c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_pickle(os.getcwd() + '/../dataframes/train_df_features.pkl')  ## train_df\n",
    "test_df = pd.read_pickle(os.getcwd() + '/../dataframes/test_df_features.pkl') ## test_df\n",
    "\n",
    "train_df = train_df.sample(frac=1, random_state=42).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de314f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "unlabeled_df = pd.read_pickle(os.getcwd() + '/../dataframes/unlabeled_data_feature_extracted.pkl') # unlabeled_df\n",
    "unlabeled_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c61f3324",
   "metadata": {},
   "outputs": [],
   "source": [
    "unlabeled_df = pd.read_pickle(os.getcwd() + '/../dataframes/unlabeled_df_features.pkl') # unlabeled_df\n",
    "# unlabeled_df['lang'] = unlabeled_df.apply(lambda x:detect_language(x['text'].revplace(\"\\n\",\" \")), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1770b416",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_use = get_modified_vectors(train_df.doc_vec.values)\n",
    "X_test_use = get_modified_vectors(test_df.doc_vec.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a5f82787",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df['nc_vec'] = train_df.apply(lambda x:get_avg_token_vector(x['text_tokens'][0]), axis=1)\n",
    "# test_df['nc_vec'] = test_df.apply(lambda x:get_avg_token_vector(x['text_tokens'][0]), axis=1)\n",
    "\n",
    "X_train_nc = get_modified_vectors(train_df.nc_vec.values)\n",
    "X_test_nc = get_modified_vectors(test_df.nc_vec.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6ecba6a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train_df.label.values\n",
    "y_test = test_df.label.values\n",
    "\n",
    "y_train_new = np.array([val if val!=3 else 0 for val in y_train]).astype('int32')\n",
    "y_test_new = np.array([val if val!=3 else 0 for val in y_test]).astype('int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b1be2cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_avg_token_vector(token_list):\n",
    "    \n",
    "    avg_token_vec = []\n",
    "    for token in token_list:\n",
    "        avg_token_vec.append(tf_model(token)['outputs'].numpy()[0].reshape(1, -1))\n",
    "        \n",
    "    return np.mean(avg_token_vec, axis=0)\n",
    "\n",
    "def perform_analysis(model_type):\n",
    "    multi_model, test_cv_score, test_f1_score, f1_score_milt, precision_milt, recall_milt = get_performance_metrics(X_train_nc, y_train_new, X_test_nc, y_test_new)\n",
    "\n",
    "    test_df_new = test_df[test_df['pred_label'].isin([0,1,3])]\n",
    "    y_train_tech = np.array([0 if val!=1 else 1 for val in y_train]).astype('int32')\n",
    "\n",
    "    X_test_new_nc = get_modified_vectors(test_df_new.nc_vec.values)\n",
    "    y_test_new_tech = np.array([0 if val!=1 else 1 for val in test_df_new.label.values]).astype('int32')\n",
    "\n",
    "    bin_model, threshold, f1_score_tech, precision_tech, recall_tech = get_performance_metrics(X_train_nc, y_train_tech, X_test_new_nc, y_test_new_tech, binary=True)\n",
    "    \n",
    "    pickle.dump(multi_model, open(os.getcwd()+'/../../models/multi_model_stage_1.pkl', 'wb'))\n",
    "    pickle.dump(bin_model, open(os.getcwd()+'/../../models/bin_model_stage_2.pkl', 'wb'))\n",
    "    \n",
    "    results_list = [test_cv_score, test_f1_score,f1_score_milt,  precision_milt, recall_milt, f1_score_tech, precision_tech, recall_tech]\n",
    "    # filter_unlabeled_data(multi_model, bin_model)\n",
    "\n",
    "    results_list = [str(round(val, 2)) for val in results_list]\n",
    "    results_list.append(model_type)\n",
    "\n",
    "    write_data_to_file(os.getcwd()+'/../csv_data/two_stage_classification_results.txt', '|'.join(results_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "733f5b75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.86      0.88       146\n",
      "           1       0.30      0.19      0.23        16\n",
      "           2       0.53      0.83      0.64        23\n",
      "\n",
      "    accuracy                           0.80       185\n",
      "   macro avg       0.58      0.63      0.59       185\n",
      "weighted avg       0.81      0.80      0.80       185\n",
      "\n",
      "0.25\n",
      "0.10900000000000001\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.80      0.88       137\n",
      "           1       0.24      0.75      0.37        12\n",
      "\n",
      "    accuracy                           0.79       149\n",
      "   macro avg       0.61      0.77      0.62       149\n",
      "weighted avg       0.91      0.79      0.83       149\n",
      "\n"
     ]
    }
   ],
   "source": [
    "perform_analysis(model_type='use')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "97137cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_data_to_file(filepath, data):\n",
    "\n",
    "    with open(filepath, \"a\") as f:\n",
    "        f.write(data+'\\n')\n",
    "\n",
    "def write_document_data(data, filepath):\n",
    "\n",
    "    with open(filepath, 'w') as f:\n",
    "        json.dump(data, f)\n",
    "\n",
    "def filter_unlabeled_data(multi_model, bin_model):\n",
    "    \n",
    "    X_unlabeled = get_modified_vectors(unlabeled_df.nc_vec.values)\n",
    "    unlabeled_df['milt_label'] = multi_model.predict(X_unlabeled)\n",
    "\n",
    "    unlabeled_df_new = unlabeled_df[unlabeled_df['milt_label'].isin([0,1,3])]\n",
    "    X_unlabeled_new = get_modified_vectors(unlabeled_df_new.nc_vec.values)\n",
    "\n",
    "    preds = bin_model.predict_proba(X_unlabeled_new)[:,1]\n",
    "    unlabeled_df_new['tech_label'] = get_threshold_output(preds, threshold)\n",
    "\n",
    "    tech_data_dict = dict()\n",
    "    milt_data_dict = dict()\n",
    "\n",
    "    for idx, row in unlabeled_df_new.iterrows():\n",
    "\n",
    "        if row['tech_label'] == 1:\n",
    "            tech_data_dict[row['id']] = {\n",
    "                'page_id': row['id'],\n",
    "                'text': row['text']\n",
    "            }\n",
    "\n",
    "    for idx, row in unlabeled_df.iterrows():\n",
    "\n",
    "        if row['milt_label'] == 2:\n",
    "            milt_data_dict[row['id']] = {\n",
    "                'page_id': row['id'],\n",
    "                'text': row['text']\n",
    "            }\n",
    "            \n",
    "    tech_len = len(tech_data_dict.keys())    \n",
    "    milt_len = len(milt_data_dict.keys())    \n",
    "    \n",
    "    print(f'Technology documents filtered: {tech_len}')\n",
    "    print(f'Military documents filtered: {milt_len}')\n",
    "\n",
    "    write_document_data(tech_data_dict, os.getcwd()+'/../json_data/technologie_document_data.json')\n",
    "    write_document_data(milt_data_dict, os.getcwd()+'/../json_data/military_document_data.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "id": "eac8e73e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3\n",
      "0.10500000000000001\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.78      0.86       134\n",
      "           1       0.19      0.58      0.29        12\n",
      "\n",
      "    accuracy                           0.76       146\n",
      "   macro avg       0.57      0.68      0.57       146\n",
      "weighted avg       0.89      0.76      0.81       146\n",
      "\n",
      "\n",
      "Training CV f1 score: 0\n",
      "Test F1-score: 0.28571428571428575\n"
     ]
    }
   ],
   "source": [
    "get_performance_metrics(X_train_new_nc, y_train_new_tech, X_test_new_nc, y_test_new_tech, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "id": "513f0d8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3529411764705882\n",
      "0.13100000000000003\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.85      0.88       169\n",
      "           1       0.13      0.25      0.17        16\n",
      "\n",
      "    accuracy                           0.79       185\n",
      "   macro avg       0.53      0.55      0.53       185\n",
      "weighted avg       0.85      0.79      0.82       185\n",
      "\n",
      "\n",
      "Training CV f1 score: 0\n",
      "Test F1-score: 0.1739130434782609\n"
     ]
    }
   ],
   "source": [
    "get_performance_metrics(X_train_topic_use, y_train, X_test_topic_use, y_test_tech, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0240cdf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.73      0.77       113\n",
      "           1       0.43      0.19      0.26        16\n",
      "           2       0.45      0.78      0.57        23\n",
      "           3       0.38      0.42      0.40        33\n",
      "\n",
      "    accuracy                           0.63       185\n",
      "   macro avg       0.52      0.53      0.50       185\n",
      "weighted avg       0.66      0.63      0.63       185\n",
      "\n",
      "\n",
      "Training CV f1 score: 0.41471200104976197\n",
      "Test F1-score: 0.4161490683229813\n"
     ]
    }
   ],
   "source": [
    "get_performance_metrics(X_train_use, y_train, X_test_use, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ba8373",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c6d3c907",
   "metadata": {},
   "source": [
    "### 2. Topic features testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "89f43b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_use_topic_score(doc_vec, topic):\n",
    "    \n",
    "    return cosine_similarity(doc_vec, topic_embeddings_dict[topic])[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "id": "9bf40a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_list = ['Wirtschaft und Finanzen', 'Bildung', 'Politik', 'Tierreich', 'Rechtswissenschaften und Rechtsprechung', 'Gesundheit', 'Automobilbranche', 'Unterhaltung', 'Sport', 'Werbung', 'Technologie', 'Innovation', 'Militär', 'Quantencomputer', 'Swarm', 'Architecture', 'Forschnung', 'Drone', 'Autonomous', 'Modernisierung', 'Prototype', 'efficiency', 'Notebook', 'Angriff', 'Smartphone', 'Corona', 'Hacking', 'Kunden', 'Robot', 'Künstliche Intelligenz', 'smart', 'algorithmus', 'sensor', 'energy', 'digitalen', 'attack']\n",
    "# topic_list = ['Werbung', 'Technologie', 'Innovation', 'Militär', 'Quantencomputer', 'Swarm', 'Architecture', 'Forschnung', 'Drone', 'Autonomous', 'Modernisierung', 'Prototype', 'efficiency', 'Notebook', 'Angriff', 'Smartphone', 'Corona', 'Hacking', 'Kunden', 'Robot', 'Künstliche Intelligenz', 'smart', 'algorithmus', 'sensor', 'energy', 'digitalen', 'attack']\n",
    "\n",
    "topic_embeddings_dict = dict()\n",
    "for topic in topic_list:\n",
    "    topic_embeddings_dict[topic] = tf_model(topic)['outputs'].numpy()[0].reshape(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "id": "f0bd6f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_col_list = []\n",
    "\n",
    "for topic in topic_list:\n",
    "    topic_col_name = topic.lower().replace(' ', '_') + '_sim'\n",
    "    topic_col_list.append(topic_col_name)\n",
    "    \n",
    "    train_df[topic_col_name] = train_df.apply(lambda x:get_use_topic_score(x['doc_vec'], topic), axis=1)\n",
    "    test_df[topic_col_name] = test_df.apply(lambda x:get_use_topic_score(x['doc_vec'], topic), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "id": "53dce200",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_topic_use = train_df[topic_col_list].values\n",
    "X_test_topic_use = test_df[topic_col_list].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "3e9b7486",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.60      0.69       113\n",
      "           1       0.10      0.12      0.11        16\n",
      "           2       0.39      0.61      0.47        23\n",
      "           3       0.30      0.39      0.34        33\n",
      "\n",
      "    accuracy                           0.52       185\n",
      "   macro avg       0.40      0.43      0.40       185\n",
      "weighted avg       0.60      0.52      0.55       185\n",
      "\n",
      "\n",
      "Training CV f1 score: 0.348627069009422\n",
      "Test F1-score: 0.29134218964727443\n"
     ]
    }
   ],
   "source": [
    "get_performance_metrics(X_train_topic_use, y_train, X_test_topic_use, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e76857",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "16530b4b",
   "metadata": {},
   "source": [
    "### 3. Noun-chunks, Verb and Adjective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "e303d716",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_de = spacy.load(\"de_core_news_sm\")\n",
    "nlp_en = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d54e2a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_document_data(text, lang):\n",
    "    \n",
    "    doc = None\n",
    "    \n",
    "    if lang == 'en':\n",
    "        doc = nlp_en(text)\n",
    "    elif lang == 'de':\n",
    "        doc = nlp_de(text)\n",
    "    elif lang == 'ko':\n",
    "        return None\n",
    "    \n",
    "    noun_phrases_list = []\n",
    "    verbs_list = []\n",
    "    adjs_list = []\n",
    "    \n",
    "    for nc in doc.noun_chunks:\n",
    "        noun_phrases_list.append(nc.lemma_) \n",
    "        \n",
    "    for token in doc:\n",
    "        if token.pos_ == \"ADJ\":\n",
    "            adjs_list.append(token.lemma_)\n",
    "        elif token.pos_ == \"VERB\":\n",
    "            verbs_list.append(token.lemma_)\n",
    "            \n",
    "    return (noun_phrases_list, verbs_list, adjs_list)\n",
    "\n",
    "def get_avg_token_vector(token_list):\n",
    "    \n",
    "    avg_token_vec = []\n",
    "    for token in token_list:\n",
    "        avg_token_vec.append(tf_model(token)['outputs'].numpy()[0].reshape(1, -1))\n",
    "        \n",
    "    return np.mean(avg_token_vec, axis=0)\n",
    "\n",
    "def get_mean_vector(vec_list):\n",
    "    return np.mean(vec_list, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "6814545c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "en    1284\n",
       "de     525\n",
       "Name: lang, dtype: int64"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unlabeled_df.lang.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "df495b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "unlabeled_df['text_tokens'] = unlabeled_df.apply(lambda x:get_document_data(x['text'], x['lang']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "59418c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "unlabeled_df = unlabeled_df[unlabeled_df['lang'].isin(['en', 'de'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "ccd5ee67",
   "metadata": {},
   "outputs": [],
   "source": [
    "unlabeled_df.to_pickle(os.getcwd() + '/../dataframes/unlabeled_df_features.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e88521b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df['text_tokens'] = train_df.apply(lambda x:get_document_data(x['text'], x['lang']), axis=1)\n",
    "# test_df['text_tokens'] = test_df.apply(lambda x:get_document_data(x['text'], x['lang']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f72f4e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df['nc_vec'] = train_df.apply(lambda x:get_avg_token_vector(x['text_tokens'][0]), axis=1)\n",
    "# train_df['verb_vec'] = train_df.apply(lambda x:get_avg_token_vector(x['text_tokens'][1]), axis=1)\n",
    "# train_df['adj_vec'] = train_df.apply(lambda x:get_avg_token_vector(x['text_tokens'][2]), axis=1)\n",
    "\n",
    "# test_df['nc_vec'] = test_df.apply(lambda x:get_avg_token_vector(x['text_tokens'][0]), axis=1)\n",
    "# test_df['verb_vec'] = test_df.apply(lambda x:get_avg_token_vector(x['text_tokens'][1]), axis=1)\n",
    "# test_df['adj_vec'] = test_df.apply(lambda x:get_avg_token_vector(x['text_tokens'][2]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "b18f99df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df.to_pickle(os.getcwd() + '/../dataframes/train_df_features.pkl')\n",
    "# test_df.to_pickle(os.getcwd() + '/../dataframes/test_df_features.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ea72ad79",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_nc = get_modified_vectors(train_df.nc_vec.values)\n",
    "X_test_nc = get_modified_vectors(test_df.nc_vec.values)\n",
    "\n",
    "X_train_verb = get_modified_vectors(train_df.verb_vec.values)\n",
    "X_test_verb = get_modified_vectors(test_df.verb_vec.values)\n",
    "\n",
    "X_train_adj = get_modified_vectors(train_df.adj_vec.values)\n",
    "X_test_adj = get_modified_vectors(test_df.adj_vec.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dab8c9ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features_concatenate(vecs_1, vecs_2):\n",
    "    return np.concatenate((vecs_1,vecs_2), axis=1)\n",
    "\n",
    "def get_features_mean(vecs_1, vecs_2):\n",
    "    \n",
    "    return np.mean( (vecs_1,vecs_2), axis=0 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e82793eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_nc_ad = get_features_mean(X_train_nc, X_train_adj)\n",
    "X_test_nc_ad = get_features_mean(X_test_nc, X_test_adj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2714eb08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.63      0.71       113\n",
      "           1       0.30      0.19      0.23        16\n",
      "           2       0.45      0.83      0.58        23\n",
      "           3       0.35      0.48      0.41        33\n",
      "\n",
      "    accuracy                           0.59       185\n",
      "   macro avg       0.48      0.53      0.48       185\n",
      "weighted avg       0.64      0.59      0.60       185\n",
      "\n",
      "\n",
      "Training CV f1 score: 0.42320817864428406\n",
      "Test F1-score: 0.40769230769230763\n"
     ]
    }
   ],
   "source": [
    "get_performance_metrics(X_train_nc_ad, y_train, X_test_nc_ad, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4830e60a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.67      0.73       113\n",
      "           1       0.38      0.19      0.25        16\n",
      "           2       0.49      0.83      0.61        23\n",
      "           3       0.35      0.45      0.39        33\n",
      "\n",
      "    accuracy                           0.61       185\n",
      "   macro avg       0.50      0.54      0.50       185\n",
      "weighted avg       0.64      0.61      0.61       185\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('standardscaler', StandardScaler(with_mean=False)),\n",
       "                ('svc',\n",
       "                 SVC(class_weight='balanced', gamma='auto', probability=True,\n",
       "                     random_state=122))])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_performance_metrics(X_train_nc, y_train, X_test_nc, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1dcf8658",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.76      0.79       113\n",
      "           1       0.40      0.25      0.31        16\n",
      "           2       0.45      0.65      0.54        23\n",
      "           3       0.25      0.27      0.26        33\n",
      "\n",
      "    accuracy                           0.62       185\n",
      "   macro avg       0.48      0.48      0.47       185\n",
      "weighted avg       0.63      0.62      0.62       185\n",
      "\n",
      "\n",
      "Training CV f1 score: 0.2532680007508382\n",
      "Test F1-score: 0.4217032967032967\n"
     ]
    }
   ],
   "source": [
    "get_performance_metrics(X_train_verb, y_train, X_test_verb, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ad71439d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.68      0.71       113\n",
      "           1       0.36      0.31      0.33        16\n",
      "           2       0.48      0.61      0.54        23\n",
      "           3       0.36      0.42      0.39        33\n",
      "\n",
      "    accuracy                           0.59       185\n",
      "   macro avg       0.49      0.51      0.49       185\n",
      "weighted avg       0.61      0.59      0.60       185\n",
      "\n",
      "\n",
      "Training CV f1 score: 0.26448179271708677\n",
      "Test F1-score: 0.4358974358974359\n"
     ]
    }
   ],
   "source": [
    "get_performance_metrics(X_train_adj, y_train, X_test_adj, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ebe563",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fe2bf373",
   "metadata": {},
   "source": [
    "### 4. Combined models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "98222b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_stage_one_models(X_1, X_2, y, model_1, model_2):\n",
    "    \n",
    "    model_1 = make_pipeline(StandardScaler(with_mean=False), model_1)\n",
    "    model_1.fit(X_1, y)\n",
    "    \n",
    "    model_2 = make_pipeline(StandardScaler(with_mean=False), model_2)\n",
    "    model_2.fit(X_2, y)  \n",
    "    \n",
    "    return model_1, model_2\n",
    "\n",
    "def create_stage_two_model(X, y, model_3):\n",
    "    \n",
    "    model_3 = make_pipeline(StandardScaler(with_mean=False), model_3)\n",
    "    model_3.fit(X, y) \n",
    "    \n",
    "    return model_3\n",
    "\n",
    "def get_transformed_features(model_1, model_2, X_1, X_2, y, y_flag=False):\n",
    "    \n",
    "    preds_1 = model_1.predict_proba(X_1)\n",
    "    preds_2 = model_2.predict_proba(X_2)\n",
    "    \n",
    "    combined_features = get_features_concatenate(preds_1, preds_2)\n",
    "    if not y_flag:\n",
    "        return combined_features\n",
    "    \n",
    "    return combined_features, y\n",
    "\n",
    "def get_finalmodel_pipeline(models, X_train_1, X_train_2, y_train, y_flag=False):\n",
    "    \n",
    "    model_1, model_2 = create_stage_one_models(X_train_1, X_train_2, y_train, models[0], models[1])\n",
    "    X, y = get_transformed_features(model_1, model_2, X_train_1, X_train_2, y_train, y_flag=True)\n",
    "    \n",
    "    model_3 = create_stage_two_model(X, y, models[2])\n",
    "    \n",
    "    return [model_1, model_2, model_3]\n",
    "\n",
    "def get_test_f1score_combined(models, X_test_1, X_test_2, y_test):\n",
    "    \n",
    "    X_test = get_transformed_features(models[0], models[1], X_test_1, X_test_2, y_test, y_flag=False)\n",
    "    preds = models[2].predict(X_test)\n",
    "    \n",
    "    f1_score = get_multi_class_metrics(y_test, preds, pr_flag=True)\n",
    "    \n",
    "    return f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "1e296c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_cross_validation_combined(X_1, X_2, y, fold_cnt=5):\n",
    "    \n",
    "    skf = StratifiedKFold(n_splits=fold_cnt, shuffle=True, random_state=123)\n",
    "    f1_scores_list = []\n",
    "    \n",
    "    for train_idx, test_idx in skf.split(X_1, y):\n",
    "        \n",
    "        X_train_1, X_train_2, X_test_1, X_test_2 = X_1[train_idx], X_2[train_idx], X_1[test_idx], X_2[test_idx]\n",
    "        y_train, y_test = y[train_idx], y[test_idx]\n",
    "        \n",
    "        models = get_three_class_models()\n",
    "        models = get_finalmodel_pipeline(models, X_train_1, X_train_2, y_train)\n",
    "        \n",
    "        X_test = get_transformed_features(models[0], models[1], X_test_1, X_test_2, y_test, y_flag=False)\n",
    "        preds = models[2].predict(X_test)\n",
    "\n",
    "        f1_score = get_multi_class_metrics(y_test, preds, pr_flag=False)\n",
    "        f1_scores_list.append(f1_score)\n",
    "    \n",
    "    return sum(f1_scores_list)/fold_cnt\n",
    "\n",
    "def get_trained_model_combined(X_1, X_2, y):\n",
    "    \n",
    "    skf_f1score = perform_cross_validation_combined(X_1, X_2, y, fold_cnt=5)\n",
    "    \n",
    "    models = get_three_class_models()\n",
    "    models = get_finalmodel_pipeline(models, X_1, X_2, y)\n",
    "\n",
    "    return models, skf_f1score\n",
    "    \n",
    "\n",
    "def get_performance_metrics_combined(X_train_1, X_train_2, y_train, X_test_1, X_test_2, y_test):\n",
    "    \n",
    "    models, cv_score = get_trained_model_combined(X_train_1, X_train_2, y_train)\n",
    "    test_f1_score = get_test_f1score_combined(models, X_test_1, X_test_2, y_test)\n",
    "\n",
    "    print()\n",
    "    print(f'Training CV f1 score: {cv_score}')\n",
    "    print(f'Test F1-score: {test_f1_score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "fe62088e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.87      0.77       113\n",
      "           1       1.00      0.12      0.22        16\n",
      "           2       0.44      0.17      0.25        23\n",
      "           3       0.38      0.36      0.37        33\n",
      "\n",
      "    accuracy                           0.63       185\n",
      "   macro avg       0.63      0.38      0.40       185\n",
      "weighted avg       0.63      0.63      0.59       185\n",
      "\n",
      "\n",
      "Training CV f1 score: 0.24217057796005168\n",
      "Test F1-score: 0.2361111111111111\n"
     ]
    }
   ],
   "source": [
    "get_performance_metrics_combined(X_train_nc, X_train_verb, y_train, X_test_nc, X_test_verb, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "58f87814",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.88      0.76       113\n",
      "           1       1.00      0.06      0.12        16\n",
      "           2       0.60      0.26      0.36        23\n",
      "           3       0.33      0.24      0.28        33\n",
      "\n",
      "    accuracy                           0.62       185\n",
      "   macro avg       0.65      0.36      0.38       185\n",
      "weighted avg       0.63      0.62      0.57       185\n",
      "\n",
      "\n",
      "Training CV f1 score: 0.31169002050580996\n",
      "Test F1-score: 0.24064171122994654\n"
     ]
    }
   ],
   "source": [
    "get_performance_metrics_combined(X_train_nc, X_train_adj, y_train, X_test_nc, X_test_adj, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "0b2bb1cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.89      0.78       113\n",
      "           1       1.00      0.12      0.22        16\n",
      "           2       0.50      0.30      0.38        23\n",
      "           3       0.39      0.27      0.32        33\n",
      "\n",
      "    accuracy                           0.64       185\n",
      "   macro avg       0.65      0.40      0.43       185\n",
      "weighted avg       0.64      0.64      0.60       185\n",
      "\n",
      "\n",
      "Training CV f1 score: 0.3365809354044648\n",
      "Test F1-score: 0.3003003003003003\n"
     ]
    }
   ],
   "source": [
    "get_performance_metrics_combined(X_train_nc, X_train_use, y_train, X_test_nc, X_test_use, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
