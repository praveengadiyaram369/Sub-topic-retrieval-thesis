{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fcebc200",
   "metadata": {},
   "source": [
    "### I. Global feature analysis -- query is unknown\n",
    "\n",
    "     1. Document length, no.of tokens, no.of sentences\n",
    "     2. Title length, no. of tokens\n",
    "     3. Pos-tagging distribution -- only NC tested\n",
    "     4. Entity types and distribution\n",
    "\n",
    "### II. Local feature analysis -- query is known\n",
    "\n",
    "     1. Sub-topic distribution in each class\n",
    "     \n",
    "### Assumptions:\n",
    "\n",
    "     1. Best label in case of ambiguity\n",
    "     2. Spearman correlation\n",
    "         Ordinal vs Continuous"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a603968",
   "metadata": {},
   "source": [
    "## I. Global feature analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "18f82459",
   "metadata": {},
   "outputs": [],
   "source": [
    "# _importing required libraries\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import string\n",
    "import numpy as np\n",
    "import hdbscan\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_text\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import spacy\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "import texthero as hero\n",
    "from texthero import preprocessing\n",
    "\n",
    "from fuzzywuzzy import fuzz\n",
    "from fuzzywuzzy import process\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns \n",
    "%matplotlib inline\n",
    "\n",
    "import umap\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "0cc99e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_model = hub.load(os.getcwd()+ '/../../models/USE_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "70e67788",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>page_id</th>\n",
       "      <th>query</th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th>text_len</th>\n",
       "      <th>noun_chunks</th>\n",
       "      <th>mean_nc_vec</th>\n",
       "      <th>title</th>\n",
       "      <th>published_date</th>\n",
       "      <th>source_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>210705_news_167749</td>\n",
       "      <td>Methode Architektur</td>\n",
       "      <td>3</td>\n",
       "      <td>ROME, N.Y. –  U.S. Air Force researchers are a...</td>\n",
       "      <td>467</td>\n",
       "      <td>[N.Y., industry help, big improvement, small, ...</td>\n",
       "      <td>[[-0.010767139494419098, -0.001241824007593095...</td>\n",
       "      <td>SWaP embedded computing artificial intelligenc...</td>\n",
       "      <td>2020-04-27 07:30:41</td>\n",
       "      <td>https://www.militaryaerospace.com/computers/ar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>210705_news_499712</td>\n",
       "      <td>Methode Architektur</td>\n",
       "      <td>3</td>\n",
       "      <td>A recently published 156-page paper from a tea...</td>\n",
       "      <td>758</td>\n",
       "      <td>[a recently publish 156-page paper, a team, Im...</td>\n",
       "      <td>[[-0.008159193210303783, -0.001107345684431493...</td>\n",
       "      <td>An Erlangen Programme to Establish the Geometr...</td>\n",
       "      <td>2021-05-17 11:45:17</td>\n",
       "      <td>https://syncedreview.com/2021/05/05/deepmind-p...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              page_id                query  label  \\\n",
       "0  210705_news_167749  Methode Architektur      3   \n",
       "1  210705_news_499712  Methode Architektur      3   \n",
       "\n",
       "                                                text  text_len  \\\n",
       "0  ROME, N.Y. –  U.S. Air Force researchers are a...       467   \n",
       "1  A recently published 156-page paper from a tea...       758   \n",
       "\n",
       "                                         noun_chunks  \\\n",
       "0  [N.Y., industry help, big improvement, small, ...   \n",
       "1  [a recently publish 156-page paper, a team, Im...   \n",
       "\n",
       "                                         mean_nc_vec  \\\n",
       "0  [[-0.010767139494419098, -0.001241824007593095...   \n",
       "1  [[-0.008159193210303783, -0.001107345684431493...   \n",
       "\n",
       "                                               title      published_date  \\\n",
       "0  SWaP embedded computing artificial intelligenc... 2020-04-27 07:30:41   \n",
       "1  An Erlangen Programme to Establish the Geometr... 2021-05-17 11:45:17   \n",
       "\n",
       "                                          source_url  \n",
       "0  https://www.militaryaerospace.com/computers/ar...  \n",
       "1  https://syncedreview.com/2021/05/05/deepmind-p...  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df = pd.read_pickle(os.getcwd()+'/../dataframes/retrieval_dataset.pkl')\n",
    "final_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4df50920",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_xlm = pd.read_pickle(os.getcwd()+'/final_dataframe.pkl')\n",
    "df_xlm = df_xlm[['id', 'lang']]\n",
    "df_xlm = df_xlm.rename(columns={'id': 'page_id'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "8066858b",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = pd.concat([final_df.set_index('page_id'), df_xlm.set_index('page_id')], axis=1, join='inner').reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c3751c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicated_page_ids = set(list(final_df[final_df['page_id'].duplicated() == True].page_id.values))\n",
    "\n",
    "for page_id in duplicated_page_ids:\n",
    "    final_df.loc[final_df['page_id'] == page_id, 'label'] = min(final_df[final_df['page_id'] == page_id].label.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "923e1e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = final_df.rename(columns={'text_len': 'num_tokens'})\n",
    "final_df = final_df.drop_duplicates(subset=['page_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3669de68",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_df = final_df[['page_id', 'label', 'text', 'num_tokens', 'noun_chunks']]\n",
    "doc_df['doc_len'] = doc_df.apply(lambda x:len(x['text']),axis=1)\n",
    "doc_df['num_sents'] = doc_df.apply(lambda x:len(sent_tokenize(x['text'])),axis=1)\n",
    "doc_df['nc_cnt'] = doc_df.apply(lambda x:len(x['noun_chunks']),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "3a73d556",
   "metadata": {},
   "outputs": [],
   "source": [
    "title_df = final_df[['page_id', 'label', 'title']]\n",
    "title_df['title_len'] = title_df.apply(lambda x:len(x['title']),axis=1)\n",
    "title_df['title_num_tokens'] = title_df.apply(lambda x:len(x['title'].split()),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "b7256bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_title_df = pd.concat([doc_df.set_index('page_id'), title_df.set_index('page_id')], axis=1, join='inner').reset_index()\n",
    "column_names = doc_title_df.columns.values\n",
    "column_names[1] = 'label_duplicate'\n",
    "doc_title_df.columns = column_names\n",
    "\n",
    "doc_title_df.drop('label_duplicate', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "821ea743",
   "metadata": {},
   "source": [
    "## Average document and title feature analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "a443b696",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_len</th>\n",
       "      <th>num_tokens</th>\n",
       "      <th>num_sents</th>\n",
       "      <th>nc_cnt</th>\n",
       "      <th>title_len</th>\n",
       "      <th>title_num_tokens</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4893.706667</td>\n",
       "      <td>695.626667</td>\n",
       "      <td>31.306667</td>\n",
       "      <td>198.493333</td>\n",
       "      <td>59.640000</td>\n",
       "      <td>7.893333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5288.601562</td>\n",
       "      <td>741.601562</td>\n",
       "      <td>34.671875</td>\n",
       "      <td>208.460938</td>\n",
       "      <td>59.898438</td>\n",
       "      <td>7.789062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6098.787072</td>\n",
       "      <td>863.692015</td>\n",
       "      <td>38.722433</td>\n",
       "      <td>239.403042</td>\n",
       "      <td>65.965779</td>\n",
       "      <td>8.714829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6310.435294</td>\n",
       "      <td>882.035294</td>\n",
       "      <td>40.294118</td>\n",
       "      <td>250.729412</td>\n",
       "      <td>66.858824</td>\n",
       "      <td>9.058824</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           doc_len  num_tokens  num_sents      nc_cnt  title_len  \\\n",
       "label                                                              \n",
       "1      4893.706667  695.626667  31.306667  198.493333  59.640000   \n",
       "2      5288.601562  741.601562  34.671875  208.460938  59.898438   \n",
       "3      6098.787072  863.692015  38.722433  239.403042  65.965779   \n",
       "4      6310.435294  882.035294  40.294118  250.729412  66.858824   \n",
       "\n",
       "       title_num_tokens  \n",
       "label                    \n",
       "1              7.893333  \n",
       "2              7.789062  \n",
       "3              8.714829  \n",
       "4              9.058824  "
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_title_df = doc_title_df.groupby(['label']).agg({'doc_len': 'mean',\n",
    "'num_tokens': 'mean',\n",
    "'num_sents': 'mean',\n",
    "'nc_cnt': 'mean',\n",
    "'title_len': 'mean',\n",
    "'title_num_tokens': 'mean',})\n",
    "doc_title_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2db7d812",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_len_sim = doc_df[\"label\"].corr(doc_df[\"doc_len\"], method='spearman')\n",
    "doc_token_sim = doc_df[\"label\"].corr(doc_df[\"num_tokens\"], method='spearman')\n",
    "doc_sent_sim = doc_df[\"label\"].corr(doc_df[\"num_sents\"], method='spearman')\n",
    "doc_nc_sim = doc_df[\"label\"].corr(doc_df[\"nc_cnt\"], method='spearman')\n",
    "\n",
    "title_len_sim = title_df[\"label\"].corr(title_df[\"title_len\"], method='spearman')\n",
    "title_token_sim = title_df[\"label\"].corr(title_df[\"title_num_tokens\"], method='spearman')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c503ba17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document length correlation score: 0.021493587282691552\n",
      "Document token correlation score: 0.02595750949939813\n",
      "Document sentence correlation score: -0.010399220857056942\n",
      "Document noun-chunks correlation score: 0.023655502592754032\n",
      "\n",
      "Title length correlation score: 0.09400850528936555\n",
      "Title token correlation score: 0.10060472213293657\n"
     ]
    }
   ],
   "source": [
    "print(f'Document length correlation score: {doc_len_sim}')\n",
    "print(f'Document token correlation score: {doc_token_sim}')\n",
    "print(f'Document sentence correlation score: {doc_sent_sim}')\n",
    "print(f'Document noun-chunks correlation score: {doc_nc_sim}\\n')\n",
    "\n",
    "print(f'Title length correlation score: {title_len_sim}')\n",
    "print(f'Title token correlation score: {title_token_sim}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "9fd1b1d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_de = spacy.load(\"de_core_news_sm\")\n",
    "nlp_en = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "1263a99c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_document_entities(text, lang):\n",
    "    \n",
    "    doc = None\n",
    "\n",
    "    try:\n",
    "        if len(text) >= 999999:\n",
    "            return None\n",
    "\n",
    "        if lang == 'en':\n",
    "            doc = nlp_en(text)\n",
    "        elif lang == 'de':\n",
    "            doc = nlp_de(text)\n",
    "        else:\n",
    "            return None\n",
    "        \n",
    "        entities = []\n",
    "        \n",
    "        for ent in doc.ents:\n",
    "            entities.append(ent.label_) \n",
    "    except Exception as e:\n",
    "        return None\n",
    "            \n",
    "    return entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "127b0071",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df['entities'] = final_df.apply(lambda x:get_document_entities(x['text'], x['lang']),axis=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "2dffae65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'CARDINAL', 'PERCENT', 'LOC', 'MISC', 'PERSON', 'TIME', 'WORK_OF_ART', 'EVENT', 'PER', 'NORP', 'ORDINAL', 'LANGUAGE', 'LAW', 'GPE', 'PRODUCT', 'DATE', 'QUANTITY', 'ORG', 'MONEY', 'FAC'}\n"
     ]
    }
   ],
   "source": [
    "unique_entities = []\n",
    "\n",
    "for entities in final_df.entities.values:\n",
    "    unique_entities.extend(set(entities))\n",
    "    \n",
    "print(set(unique_entities))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a4c73d7",
   "metadata": {},
   "source": [
    "#### 4. Entity analysis\n",
    "\n",
    "##### Important entities\n",
    "\n",
    "    1. LOC - Location\n",
    "    2. PERSON - People\n",
    "    3. DATE - date\n",
    "    4. EVENT - Named wars, events, sports, hurricanes, \n",
    "    5. NORP - Nationalities, or Religious or Political groups\n",
    "    6. LAW - law related\n",
    "    7. GPE - Countries, states, cities\n",
    "    8. PRODUCT - Objects\n",
    "    9. ORG - Organisations, companies, institutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "329cf7d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_data_list = []\n",
    "\n",
    "for idx, row in final_df.iterrows():\n",
    "    \n",
    "    loc = person = date = event = norp = law = gpe = product = org = None\n",
    "    entities = row['entities']\n",
    "    \n",
    "    entity_data_list.append(\n",
    "    {\n",
    "        'id': row['page_id'],\n",
    "        'label': row['label'],\n",
    "        'LOC': entities.count('LOC'),\n",
    "        'PERSON': entities.count('PERSON'),\n",
    "        'DATE': entities.count('DATE'),\n",
    "        'EVENT': entities.count('EVENT'),\n",
    "        'NORP': entities.count('NORP'),\n",
    "        'LAW': entities.count('LAW'),\n",
    "        'GPE': entities.count('GPE'),\n",
    "        'PRODUCT': entities.count('PRODUCT'),\n",
    "        'ORG': entities.count('ORG'),\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "59cd88ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_df = pd.DataFrame(entity_data_list)\n",
    "entity_df['dummy'] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e7df82f",
   "metadata": {},
   "source": [
    "## Average entity feature analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "b6c8fb39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LOC</th>\n",
       "      <th>PERSON</th>\n",
       "      <th>DATE</th>\n",
       "      <th>EVENT</th>\n",
       "      <th>NORP</th>\n",
       "      <th>LAW</th>\n",
       "      <th>GPE</th>\n",
       "      <th>PRODUCT</th>\n",
       "      <th>ORG</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.413333</td>\n",
       "      <td>4.173333</td>\n",
       "      <td>2.013333</td>\n",
       "      <td>0.026667</td>\n",
       "      <td>1.026667</td>\n",
       "      <td>0.053333</td>\n",
       "      <td>4.546667</td>\n",
       "      <td>0.373333</td>\n",
       "      <td>16.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.273438</td>\n",
       "      <td>3.656250</td>\n",
       "      <td>2.812500</td>\n",
       "      <td>0.109375</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>2.015625</td>\n",
       "      <td>0.398438</td>\n",
       "      <td>18.054688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.091255</td>\n",
       "      <td>4.699620</td>\n",
       "      <td>5.486692</td>\n",
       "      <td>0.277567</td>\n",
       "      <td>1.798479</td>\n",
       "      <td>0.155894</td>\n",
       "      <td>5.642586</td>\n",
       "      <td>0.551331</td>\n",
       "      <td>21.821293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.211765</td>\n",
       "      <td>8.011765</td>\n",
       "      <td>4.035294</td>\n",
       "      <td>0.141176</td>\n",
       "      <td>1.729412</td>\n",
       "      <td>0.211765</td>\n",
       "      <td>6.376471</td>\n",
       "      <td>1.011765</td>\n",
       "      <td>23.764706</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            LOC    PERSON      DATE     EVENT      NORP       LAW       GPE  \\\n",
       "label                                                                         \n",
       "1      3.413333  4.173333  2.013333  0.026667  1.026667  0.053333  4.546667   \n",
       "2      4.273438  3.656250  2.812500  0.109375  0.375000  0.015625  2.015625   \n",
       "3      5.091255  4.699620  5.486692  0.277567  1.798479  0.155894  5.642586   \n",
       "4      4.211765  8.011765  4.035294  0.141176  1.729412  0.211765  6.376471   \n",
       "\n",
       "        PRODUCT        ORG  \n",
       "label                       \n",
       "1      0.373333  16.333333  \n",
       "2      0.398438  18.054688  \n",
       "3      0.551331  21.821293  \n",
       "4      1.011765  23.764706  "
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "group_df = entity_df.groupby(['label']).agg({'LOC': 'mean',\n",
    "'PERSON': 'mean',\n",
    "'DATE': 'mean',\n",
    "'EVENT': 'mean',\n",
    "'NORP': 'mean',\n",
    "'LAW': 'mean',\n",
    "'GPE': 'mean',\n",
    "'PRODUCT': 'mean',\n",
    "'ORG': 'mean'})\n",
    "group_df  # _person und product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "7e637790",
   "metadata": {},
   "outputs": [],
   "source": [
    "loc_entity_sim = entity_df[\"label\"].corr(entity_df[\"LOC\"], method='spearman')\n",
    "person_entity_sim = entity_df[\"label\"].corr(entity_df[\"PERSON\"], method='spearman')\n",
    "date_entity_sim = entity_df[\"label\"].corr(entity_df[\"DATE\"], method='spearman')\n",
    "event_entity_sim = entity_df[\"label\"].corr(entity_df[\"EVENT\"], method='spearman')\n",
    "norp_entity_sim = entity_df[\"label\"].corr(entity_df[\"NORP\"], method='spearman')\n",
    "law_entity_sim = entity_df[\"label\"].corr(entity_df[\"LAW\"], method='spearman')\n",
    "gpe_entity_sim = entity_df[\"label\"].corr(entity_df[\"GPE\"], method='spearman')\n",
    "product_entity_sim = entity_df[\"label\"].corr(entity_df[\"PRODUCT\"], method='spearman')\n",
    "org_entity_sim = entity_df[\"label\"].corr(entity_df[\"ORG\"], method='spearman')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "a31018d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entity LOC correlation score: -0.020544408137479313\n",
      "Entity PERSON correlation score: 0.06299336879161169\n",
      "Entity DATE correlation score: 0.11564571552662409\n",
      "Entity EVENT correlation score: 0.08821053385550824\n",
      "Entity NORP correlation score: 0.04181442232350223\n",
      "Entity LAW correlation score: 0.12179940407770956\n",
      "Entity GPE correlation score: 0.04873733273720441\n",
      "Entity PRODUCT correlation score: -0.016908961987197324\n",
      "Entity ORG correlation score: 0.03417995528927692\n"
     ]
    }
   ],
   "source": [
    "print(f'Entity LOC correlation score: {loc_entity_sim}')\n",
    "print(f'Entity PERSON correlation score: {person_entity_sim}')\n",
    "print(f'Entity DATE correlation score: {date_entity_sim}')\n",
    "print(f'Entity EVENT correlation score: {event_entity_sim}')\n",
    "print(f'Entity NORP correlation score: {norp_entity_sim}')\n",
    "print(f'Entity LAW correlation score: {law_entity_sim}')\n",
    "print(f'Entity GPE correlation score: {gpe_entity_sim}')\n",
    "print(f'Entity PRODUCT correlation score: {product_entity_sim}')\n",
    "print(f'Entity ORG correlation score: {org_entity_sim}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "852f48bf",
   "metadata": {},
   "source": [
    "## II. Local feature analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "6075790f",
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords_df = pd.read_pickle(os.getcwd()+'/final_keywords_dataframe.pkl')\n",
    "keywords_df = keywords_df[['id', 'keywords']]\n",
    "keywords_df = keywords_df.rename(columns={'id': 'page_id'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "600254f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_df = pd.read_pickle(os.getcwd()+'/../dataframes/retrieval_dataset.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "2f2f293e",
   "metadata": {},
   "outputs": [],
   "source": [
    "mr_df = main_df[main_df['query'] == 'Mixed Reality']\n",
    "vs_df = main_df[main_df['query'] == 'Visualisierung']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "c770e56a",
   "metadata": {},
   "outputs": [],
   "source": [
    "mr_df = pd.concat([mr_df.set_index('page_id'), keywords_df.set_index('page_id')], axis=1, join='inner').reset_index()\n",
    "vs_df = pd.concat([vs_df.set_index('page_id'), keywords_df.set_index('page_id')], axis=1, join='inner').reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c87e9966",
   "metadata": {},
   "source": [
    "### 1. keyword extraction\n",
    "### 2. Candidate pool extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "65deb065",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_modified_vectors(vec_data):\n",
    "    \n",
    "    new_data = []\n",
    "    for val in vec_data:\n",
    "        new_data.append(val)\n",
    "    \n",
    "    new_data = np.array(new_data).reshape(-1, 512)\n",
    "    return new_data\n",
    "\n",
    "def get_pool_vec(doc_vec_list, pool):\n",
    "    \n",
    "    doc_vec_list = get_modified_vectors(doc_vec_list)\n",
    "    if pool == 'mean':\n",
    "        return np.nanmean(doc_vec_list, axis=0)\n",
    "    elif pool == 'max':\n",
    "        return np.nanmax(doc_vec_list, axis=0)\n",
    "\n",
    "def get_document_vec(text):\n",
    "    \n",
    "    return tf_model(text)['outputs'].numpy()[0].reshape(1, -1)\n",
    "\n",
    "def get_sent_transformers_keywords_use(keywords, query_vec, max_keyword_cnt = 30):\n",
    "    \n",
    "    keywords = list(dict(keywords).keys())\n",
    "    \n",
    "    candidate_embeddings_keywords = [tf_model(kw)['outputs'].numpy()[0] for kw in keywords]\n",
    "        \n",
    "    query_distances = cosine_similarity([query_vec], candidate_embeddings_keywords)\n",
    "    subtopic_keywords_dict = dict()\n",
    "    for index in query_distances.argsort()[0][-max_keyword_cnt:]: \n",
    "        \n",
    "        subtopic_keywords_dict[keywords[index]] = query_distances[0][index]\n",
    "    \n",
    "    subtopic_keywords_dict = sorted(subtopic_keywords_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    return subtopic_keywords_dict\n",
    "\n",
    "def get_candidate_pool(subtopic_keywords_list):\n",
    "    \n",
    "    candidate_pool = []\n",
    "    \n",
    "    lower_limit = 0.2\n",
    "    upper_limit = 0.4\n",
    "    \n",
    "    for key, value in subtopic_keywords_list:\n",
    "        \n",
    "        if value > 0.2 and value < 0.4:\n",
    "            candidate_pool.append(key)\n",
    "            \n",
    "    return candidate_pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "717404fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_1 = 'Mixed Reality'\n",
    "query_vec_1 = tf_model(query_1)['outputs'].numpy()[0]\n",
    "\n",
    "query_2 = 'Visualisierung'\n",
    "query_vec_2 = tf_model(query_2)['outputs'].numpy()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "384f13cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31.5 s ± 443 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit mr_df['keywords_use'] = mr_df.apply(lambda x:get_sent_transformers_keywords_use(x['keywords'], query_vec_1, max_keyword_cnt = 30), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "b537f766",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32.4 s ± 680 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit vs_df['keywords_use'] = vs_df.apply(lambda x:get_sent_transformers_keywords_use(x['keywords'], query_vec_1, max_keyword_cnt = 30), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "ffc7fb7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.05 ms ± 55 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit mr_df['candidate_pool'] = mr_df.apply(lambda x:get_candidate_pool(x['keywords_use']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "011e19f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.02 ms ± 43.2 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit vs_df['candidate_pool'] = vs_df.apply(lambda x:get_candidate_pool(x['keywords_use']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "4ee9c4af",
   "metadata": {},
   "outputs": [],
   "source": [
    "mr_df.to_pickle(os.getcwd()+'/../dataframes/retrieval_dataset_mr.pkl')\n",
    "vs_df.to_pickle(os.getcwd()+'/../dataframes/retrieval_dataset_vs.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "4884297d",
   "metadata": {},
   "outputs": [],
   "source": [
    "mr_df = pd.read_pickle(os.getcwd()+'/../dataframes/retrieval_dataset_mr.pkl')\n",
    "vs_df = pd.read_pickle(os.getcwd()+'/../dataframes/retrieval_dataset_vs.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c6d157",
   "metadata": {},
   "source": [
    "### 3. Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "51201a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_umap_output(vec_array, dim_size=5):\n",
    "    \n",
    "    umap_obj = umap.UMAP(n_neighbors=40, \n",
    "                        n_components=dim_size, \n",
    "                        min_dist=0.01,\n",
    "                        metric='cosine',\n",
    "                        random_state=123).fit(vec_array) \n",
    "    \n",
    "    umap_output = umap_obj.transform(vec_array) \n",
    "    return umap_output, umap_obj\n",
    "\n",
    "def get_hdbscan_output(data_points, cluster_size=7):\n",
    "    \n",
    "    hdbscan_output = hdbscan.HDBSCAN(\n",
    "        #min_cluster_size=cluster_size,\n",
    "#                                       min_samples=2,\n",
    "                                      metric='euclidean',\n",
    "                                     cluster_selection_method='eom').fit(data_points)\n",
    "    return hdbscan_output\n",
    "\n",
    "def project_on_2Dplane(umap_output, cluster_ids):\n",
    "    \n",
    "    umap_df = pd.DataFrame(np.column_stack((umap_output, cluster_ids)), columns=['x', 'y', 'cluster ids'])\n",
    "    grid = sns.FacetGrid(umap_df, hue='cluster ids', height=7)\n",
    "    grid.map(plt.scatter, 'x', 'y').add_legend()\n",
    "    \n",
    "def get_clustering_analysis(cluster_df, final_candidate_pool_vecs, dimen_size=5, cluster_size=7):\n",
    "    \n",
    "    umap_output_5, umap_5 = get_umap_output(final_candidate_pool_vecs, dim_size=dimen_size)\n",
    "    hdbscan_output = get_hdbscan_output(umap_output_5, cluster_size=cluster_size)\n",
    "    \n",
    "    cluster_df['cluster_id'] = hdbscan_output.labels_\n",
    "    cluster_df.cluster_id.hist(bins=150)\n",
    "    \n",
    "    umap_output_2, umap_2 = get_umap_output(final_candidate_pool_vecs, dim_size=2)\n",
    "    project_on_2Dplane(umap_output_2, cluster_df['cluster_id'])\n",
    "    \n",
    "    return cluster_df\n",
    "\n",
    "def get_nearest_keyword(keywords, keyword_vecs, mean_vec):\n",
    "    \n",
    "    query_distances = cosine_similarity([mean_vec], list(keyword_vecs))\n",
    "    subtopic_keywords_dict = dict()\n",
    "    for index in query_distances.argsort()[0]: \n",
    "        \n",
    "        subtopic_keywords_dict[keywords[index]] = query_distances[0][index]\n",
    "    \n",
    "    subtopic_keywords_dict = sorted(subtopic_keywords_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "    return subtopic_keywords_dict[0][0]\n",
    "\n",
    "def get_topics(cluster_data_df, candidate_pool):\n",
    "    \n",
    "    topic_list = []\n",
    "    \n",
    "    for idx, row in cluster_data_df.iterrows():\n",
    "        \n",
    "        candidate_words = row['candidate_words']\n",
    "        topic = row['topic']\n",
    "        \n",
    "        for cp in candidate_pool:\n",
    "            if cp in candidate_words:\n",
    "                topic_list.append(topic)\n",
    "                break\n",
    "    \n",
    "    return topic_list\n",
    "\n",
    "def get_sub_topic_modelling(query_df):\n",
    "    \n",
    "    final_candidate_pool = []\n",
    "\n",
    "    for idx, row in query_df.iterrows():\n",
    "        final_candidate_pool.extend(row['candidate_pool'])\n",
    "        \n",
    "    final_candidate_pool = list(set(final_candidate_pool))\n",
    "\n",
    "    final_candidate_pool_vecs = [tf_model(nc)['outputs'].numpy()[0] for nc in final_candidate_pool]\n",
    "\n",
    "    df_data = []\n",
    "    for word, vec in zip(final_candidate_pool, final_candidate_pool_vecs):\n",
    "        df_data.append((word, vec))\n",
    "\n",
    "    cluster_df = pd.DataFrame(df_data, columns= ['candidate_words', 'candidate_vecs'])\n",
    "    cluster_df = get_clustering_analysis(cluster_df, final_candidate_pool_vecs, dimen_size=5, cluster_size=8)\n",
    "\n",
    "    cluster_data = []\n",
    "\n",
    "    for cluster_id in set(cluster_df.cluster_id.values):\n",
    "\n",
    "        if cluster_id != -1:\n",
    "            df = cluster_df[cluster_df['cluster_id'] == cluster_id]\n",
    "            cluster_data.append((cluster_id, df.candidate_words.values, df.candidate_vecs.values))\n",
    "\n",
    "    cluster_data_df = pd.DataFrame(cluster_data, columns=['cluster_id', 'candidate_words', 'candidate_vecs'])\n",
    "    cluster_data_df['mean_vec'] = cluster_data_df.apply(lambda x:get_pool_vec(x['candidate_vecs'], 'mean'), axis=1)\n",
    "    cluster_data_df['topic'] = cluster_data_df.apply(lambda x:get_nearest_keyword(x['candidate_words'], x['candidate_vecs'], x['mean_vec']), axis=1)\n",
    "    \n",
    "    query_df['topics'] = query_df.apply(lambda x:get_topics(cluster_data_df, x['candidate_pool']), axis=1)\n",
    "\n",
    "    return query_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "b11ccad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "mr_df = get_sub_topic_modelling(mr_df)\n",
    "vs_df = get_sub_topic_modelling(vs_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "c7981a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "mr_df['topics_cnt'] = mr_df.apply(lambda x:len(x['topics']), axis=1)\n",
    "vs_df['topics_cnt'] = vs_df.apply(lambda x:len(x['topics']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "fe09e117",
   "metadata": {},
   "outputs": [],
   "source": [
    "mr_topic_sim = mr_df[\"label\"].corr(mr_df[\"topics_cnt\"], method='spearman')\n",
    "vs_topic_sim = vs_df[\"label\"].corr(vs_df[\"topics_cnt\"], method='spearman')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71142db3",
   "metadata": {},
   "source": [
    "## Average sub-topic feature analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "6b13c973",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topics_cnt</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.333333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       topics_cnt\n",
       "label            \n",
       "1        4.000000\n",
       "2        2.200000\n",
       "3        3.750000\n",
       "4        2.333333"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mr_group_df = mr_df.groupby(['label']).agg({'topics_cnt': 'mean'})\n",
    "mr_group_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "cf06475d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topics_cnt</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.090909</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       topics_cnt\n",
       "label            \n",
       "1        1.400000\n",
       "2        1.500000\n",
       "3        1.100000\n",
       "4        1.090909"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vs_group_df = vs_df.groupby(['label']).agg({'topics_cnt': 'mean'})\n",
    "vs_group_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "eb21f47f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sub-topics Mixed Reality correlation score: -0.08978520557597833\n",
      "Sub-topics Visualisierung correlation score: -0.27062314559150014\n"
     ]
    }
   ],
   "source": [
    "print(f'Sub-topics Mixed Reality correlation score: {mr_topic_sim}')\n",
    "print(f'Sub-topics Visualisierung correlation score: {vs_topic_sim}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e277be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# _which clusters are useful for identifying positive clusters \n",
    "# _which clusters are useful for identifying negative clusters"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
