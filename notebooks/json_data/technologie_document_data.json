{"210705_news_466583.txt": {"page_id": "210705_news_466583.txt", "text": "Posted by Cassidy Curtis, Visual Designer and David Salesin, Principal Scientist, Google Research \n3D computer animation is a time-consuming and highly technical medium \u2014 to complete even a single animated scene requires numerous steps, like modeling,  rigging  and animating, each of which is itself a sub-discipline that can take years to master. Because of its complexity, 3D animation is generally practiced by teams of skilled specialists and is inaccessible to almost everyone else, despite decades of advances in technology and tools. With the recent development of tools that facilitate  game character creation  and  game balance , a natural question arises: is it possible to democratize the 3D animation process so it\u2019s accessible to everyone?  \n \nTo explore this concept, we start with the observation that most forms of artistic expression have a  casual mode : a classical guitarist might jam without any written music, a trained actor could ad-lib a line or two while rehearsing, and an oil painter can jot down a quick gesture drawing. What these casual modes have in common is that they allow an artist to express a complete thought quickly and intuitively without fear of making a mistake. This turns out to be essential to the  creative process  \u2014 when each sketch is nearly effortless, it is possible to iteratively explore the space of possibilities far more effectively.\n \nIn this post, we describe  Monster Mash , an open source tool  presented  at  SIGGRAPH Asia 2020  that allows experts and amateurs alike to create rich, expressive, deformable 3D models from scratch \u2014 and to animate them \u2014 all in a casual mode, without ever having to leave the 2D plane. With Monster Mash, the user sketches out a character, and the software automatically converts it to a soft, deformable 3D model that the user can immediately animate by grabbing parts of it and moving them around in real time. There is also an  online demo , where you can try it out for yourself. Creating a walk cycle using Monster Mash. Step 1: Draw a character. Step 2: Animate it. Creating a 2D Sketch \nThe insight that makes this casual sketching approach possible is that many  3D models , particularly those of organic forms, can be described by an ordered set of overlapping 2D regions. This abstraction makes the complex task of 3D modeling much easier: the user creates 2D regions by drawing their outlines, then the algorithm creates a 3D model by stitching the regions together and inflating them. The result is a simple and intuitive user interface for sketching 3D figures.\n \nFor example, suppose the user wants to create a 3D model of an elephant. The first step is to draw the body as a closed stroke (a). Then the user adds strokes to depict other body parts such as legs (b). Drawing those additional strokes as open curves provides a hint to the system that they are meant to be smoothly connected with the regions they overlap. The user can also specify that some new parts should go behind the existing ones by drawing them with the right mouse button (c), and mark other parts as symmetrical by double-clicking on them (d). The result is an ordered list of 2D regions.\n Steps in creating a 2D sketch of an elephant. Stitching and Inflation \nTo understand how a 3D model is created from these 2D regions, let\u2019s look more closely at one part of the elephant. First, the system identifies where the leg must be connected to the body (a) by finding the segment (red) that completes the open curve. The system cuts the body\u2019s front surface along that segment, and then stitches the front of the leg together with the body (b). It then inflates the model into 3D by solving a modified form of  Poisson\u2019s equation  to produce a surface with a rounded cross-section (c). The resulting model (d) is smooth and well-shaped, but because all of the 3D parts are rooted in the drawing plane, they may intersect each other, resulting in a somewhat odd-looking \u201celephant\u201d. These intersections will be resolved by the deformation system. Illustration of the details of the stitching and inflation process. The schematic illustrations (b, c) are cross-sections viewed from the elephant\u2019s front. Layered Deformation \nAt this point we just have a static model \u2014 we need to give the user an easy way to pose the model, and also separate the intersecting parts somehow. Monster Mash\u2019s layered deformation system, based on the well-known smooth deformation method  as-rigid-as-possible  (ARAP), solves both of these problems at once.   What\u2019s novel about our  layered  \u201cARAP-L\u201d approach is that it combines deformation and other constraints into a single optimization framework, allowing these processes to run in parallel at interactive speed, so that the user can manipulate the model in real time.\n \nThe framework incorporates a set of  layering and equality constraints,  which move body parts along the  z  axis to prevent them from visibly intersecting each other. These constraints are applied only at the silhouettes of overlapping parts, and are dynamically updated each frame. In steps (d) through (h) above, ARAP-L transforms a model from one with intersecting 3D parts to one with the depth ordering specified by the user. The layering constraints force the leg\u2019s silhouette to stay in front of the body (green), and the body\u2019s silhouette to stay behind the leg (yellow). Equality constraints (red) seal together the loose boundaries between the leg and the body. \nMeanwhile, in a separate thread of the framework, we satisfy  point constraints  to make the model follow user-defined control points (described in the section below) in the  xy -plane. This ARAP-L method allows us to combine modeling, rigging, deformation, and animation all into a single process that is much more approachable to the non-specialist user. The model deforms to match the point constraints (red dots) while the layering constraints prevent the parts from visibly intersecting. Animation \nTo pose the model, the user can create control points anywhere on the model\u2019s surface and move them. The deformation system converges over multiple frames, which gives the model\u2019s movement a soft and floppy quality, allowing the user to intuitively grasp its dynamic properties \u2014 an essential prerequisite for  kinesthetic learning . Because the effect of deformations converges over multiple frames, our system lends 3D models a soft and dynamic quality. \nTo create animation, the system records the user\u2019s movements in real time. The user can animate one control point, then play back that movement while recording additional control points. In this way, the user can build up a complex action like a walk by layering animation, one body part at a time. At every stage of the animation process, the only task required of the user is to move points around in 2D, a low-risk workflow meant to encourage experimentation and play. \n Conclusion \nWe believe this new way of creating animation is intuitive and can thus help democratize the field of computer animation, encouraging novices who would normally be unable to try it on their own as well as experts who  often require fast iteration under tight deadlines . Here you can see a few of the animated characters that have been created using Monster Mash. Most of these were created in a matter of minutes.\n A selection of animated characters created using Monster Mash. The original hand-drawn outline used to create each 3D model is visible as an inset above each character. \nAll of the code for Monster Mash is available as  open source , and you can watch our  presentation  and read  our paper  from SIGGRAPH Asia 2020 to learn more. We hope this software will make creating 3D animations more broadly accessible. Try out the  online demo  and see for yourself!\n Acknowledgements Monster Mash is the result of a collaboration between Google Research, Czech Technical University in Prague, ETH Z\u00fcrich, and the University of Washington. Key contributors include Marek Dvoro\u017e\u0148\u00e1k, Daniel S\u00fdkora, Cassidy Curtis, Brian Curless, Olga Sorkine-Hornung, and David Salesin. We are also grateful to H\u00e9l\u00e8ne Leroux, Neth Nom, David Murphy, Samuel Leather, Pavla S\u00fdkorov\u00e1, and Jakub Javora for participating in the early interactive sessions."}, "210705_news_466586.txt": {"page_id": "210705_news_466586.txt", "text": "vgpu_unlock Unlock vGPU functionality for consumer-grade Nvidia GPUs. Important! This tool is not guarenteed to work out of the box in some cases,\nso use it at your own risk. Description This tool enables the use of Geforce and Quadro GPUs with the NVIDIA vGPU\ngraphics virtualization technology. NVIDIA vGPU normally only supports a\nfew datacenter Teslas and professional Quadro GPUs by design, but not\nconsumer graphics cards through a software limitation. This vgpu_unlock tool\naims to remove this limitation on Linux based systems, thus enabling\nmost Maxwell, Pascal, Volta (untested), and Turing based GPUs to\nuse the vGPU technology. Ampere support is currently a work in progress. A community maintained Wiki written by Krutav Shah with a lot more information\nis  available here. Dependencies: This tool requires Python3 and Python3-pip; the latest version is recommended. The python package \"frida\" is required.  pip3 install frida . The tool requires the NVIDIA GRID vGPU driver. \"dkms\" is required as it simplifies the process of rebuilding the\ndriver alot. Install DKMS with the package manager in your OS. Installation: In the following instructions  <path_to_vgpu_unlock>  need to be replaced with\nthe path to this repository on the target system and  <version>  need to be\nreplaced with the version of the NVIDIA GRID vGPU driver. Install the NVIDIA GRID vGPU driver, make sure to install it as a dkms module. ./nvidia-installer --dkms\n Modify the line begining with  ExecStart=  in  /lib/systemd/system/nvidia-vgpud.service \nand  /lib/systemd/system/nvidia-vgpu-mgr.service  to use  vgpu_unlock  as\nthe executable and pass the original executable as the first argument. Example: ExecStart=<path_to_vgpu_unlock>/vgpu_unlock /usr/bin/nvidia-vgpud\n Reload the systemd daemons: Modify the file  /usr/src/nvidia-<version>/nvidia/os-interface.c  and add the\nfollowing line after the lines begining with  #include  at the beginning of the\nfile. #include \"<path_to_vgpu_unlock>/vgpu_unlock_hooks.c\"\n Modify the file  /usr/src/nvidia-<version>/nvidia/nvidia.Kbuild  and add the\nfollowing line at the bottom of the file. ldflags-y += -T <path_to_vgpu_unlock>/kern.ld\n Remove the nvidia kernel module using dkms: dkms remove -m nvidia -v <version> --all\n Rebuild and reinstall the nvidia kernel module using dkms: dkms install -m nvidia -v <version>\n Reboot. NOTE This script only works with graphics cards in the same generation as their\nprofessional Tesla counterparts. As a result, only Maxwell and newer\ngeneration Nvidia GPUs are supported. It is not designed to be used with\nlow end graphics card models, so not all cards are guarenteed to work\nsmoothly with vGPU. For the best experience, it is recommended to use\ngraphics cards with the same chip model as the Tesla cards.\nThe same applies to the operating system as well, as certain bleeding-edge\nLinux distributions may not work well with vGPU software. How it works vGPU supported? In order to determine if a certain GPU supports the vGPU functionality the\ndriver looks at the PCI device ID. This identifier together with the PCI vendor\nID is unique for each type of PCI device. In order to enable vGPU support we\nneed to tell the driver that the PCI device ID of the installed GPU is one of\nthe device IDs used by a vGPU capable GPU. Userspace script: vgpu_unlock The userspace services nvidia-vgpud and nvidia-vgpu-mgr uses the ioctl syscall\nto communicate with the kernel module. Specifically they read the PCI device ID\nand determines if the installed GPU is vGPU capable. The python script vgpu_unlock intercepts all ioctl syscalls between the\nexecutable specified as the first argument and the kernel. The script then\nmodifies the kernel responses to indicate a PCI device ID with vGPU support\nand a vGPU capable GPU. Kernel module hooks: vgpu_unlock_hooks.c In order to exchange data with the GPU the kernel module maps the physical\naddress space of the PCI bus into its own virtual address space. This is done\nusing the ioremap* kernel functions. The kernel module then reads and writes\ndata into that mapped address space. This is done using the memcpy kernel\nfunction. By including the vgpu_unlock_hooks.c file into the os-interface.c file we can\nuse C preprocessor macros to replace and intercept calls to the iormeap and\nmemcpy functions. Doing this allows us to maintain a view of what is mapped\nwhere and what data that is being accessed. Kernel module linker script: kern.ld This is a modified version of the default linker script provided by gcc. The\nscript is modified to place the .rodata section of nv-kernel.o into .data\nsection instead of .rodata, making it writable. The script also provide the\nsymbols  vgpu_unlock_nv_kern_rodata_beg  and  vgpu_unlock_nv_kern_rodata_end \nto let us know where that section begins and ends. How it all comes together After boot the nvidia-vgpud service queries the kernel for all installed GPUs\nand checks for vGPU capability. This call is intercepted by the vgpu_unlock\npython script and the GPU is made vGPU capable. If a vGPU capable GPU is found\nthen nvidia-vgpu creates an MDEV device and the /sys/class/mdev_bus directory\nis created by the system. vGPU devices can now be created by echoing UUIDs into the  create  files in the\nmdev bus representation. This will create additional structures representing\nthe new vGPU device on the MDEV bus. These devices can then be assigned to VMs,\nand when the VM starts it will open the MDEV device. This causes nvidia-vgpu-mgr\nto start communicating with the kernel using ioctl. Again these calls are\nintercepted by the vgpu_unlock python script and when nvidia-vgpu-mgr asks if\nthe GPU is vGPU capable the answer is changed to yes. After that check it\nattempts to initialize the vGPU device instance. Initialization of the vGPU device is handled by the kernel module and it\nperforms its own check for vGPU capability, this one is a bit more complicated. The kernel module maps the physical PCI address range 0xf0000000-0xf1000000 into\nits virtual address space, it then performs some magical operations which we\ndon't really know what they do. What we do know is that after these operations\nit accesses a 128 bit value at physical address 0xf0029624, which we call the\nmagic value. The kernel module also accessses a 128 bit value at physical\naddress 0xf0029634, which we call the key value. The kernel module then has a couple of lookup tables for the magic value, one\nfor vGPU capable GPUs and one for the others. So the kernel module looks for the\nmagic value in both of these lookup tables, and if it is found that table entry\nalso contains a set of AES-128 encrypted data blocks and a HMAC-SHA256\nsignature. The signature is then validated by using the key value mentioned earlier to\ncalculate the HMAC-SHA256 signature over the encrypted data blocks. If the\nsignature is correct, then the blocks are decrypted using AES-128 and the same\nkey. Inside of the decrypted data is once again the PCI device ID. So in order for the kernel module to accept the GPU as vGPU capable the magic\nvalue will have to be in the table of vGPU capable magic values, the key has\nto generate a valid HMAC-SHA256 signature and the AES-128 decrypted data blocks\nhas to contain a vGPU capable PCI device ID. If any of these checks fail, then\nthe error code 0x56 \"Call not supported\" is returned. In order to make these checks pass the hooks in vgpu_unlock_hooks.c will look\nfor a ioremap call that maps the physical address range that contain the magic\nand key values, recalculate the addresses of those values into the virtual\naddress space of the kernel module, monitor memcpy operations reading at those\naddresses, and if such an operation occurs, keep a copy of the value until both\nare known, locate the lookup tables in the .rodata section of nv-kernel.o, find\nthe signature and data bocks, validate the signature, decrypt the blocks, edit\nthe PCI device ID in the decrypted data, reencrypt the blocks, regenerate the\nsignature and insert the magic, blocks and signature into the table of vGPU\ncapable magic values. And that's what they do."}, "210705_news_466595.txt": {"page_id": "210705_news_466595.txt", "text": "\n    Officials today discussed the National Security Commission on Artificial Intelligence's recent report and the progress for the adoption and implementation of AI across the Defense Department.\n Marine Corps Lt. Gen. Michael S. Groen, director of the Joint Artificial Intelligence Center, and Robert O. Work, vice chair of the NSCAI, spoke to reporters at the Pentagon. Work provided an overview of the report: The United States does not have a strategy, organizational structure and resources to win the competition with China for effective implementation of AI, he said. \"So the first thing is we have got to do is to take this competition seriously, and we need to win it.\" To win, AI must receive the necessary funding, at least 3.4% of the DOD budget. Those funds should then be channeled into priority areas as recommended by a steering committee consisting of the deputy defense secretary, the vice chairman of the Joint Chiefs of Staff and the principal director of national intelligence, he said, noting that a good first step was having the JAIC report directly to the deputy defense secretary. That steering committee would also remove any bureaucratic obstacles and would oversee the development of a technology annex to the  National Defense Strategy , Work said.\u00a0 Also the department should set AI readiness performance goals by the end of this fiscal year, with an eye toward AI-ready implementation by 2025, he said. Work noted that the U.S. is still the world leader in AI, but China is structured to rapidly advance with its strong government support to academia, the private sector and People's Liberation Army. Identifying the right talent for AI, ensuring AI is used in an ethically responsible manner and forming international partnerships are other ingredients for successful AI implementation, Work said. Groen said the department agrees with the report and is already implementing about 100 of recommendations that were given. Other recommendations would be included in feasibility studies. \"AI as a core tenet of defense modernization,\" Groen said. AI efforts will be fundamental to achieving quality networks and data services in a secure manner. \"We've created positive momentum for AI, and we continue to build on that now. But now comes the real critical test in any transformation. The hardest part is institutional change and change management of the workforce and practices and processes that drive a business. This step will not be easy, even within the Department of Defense, but it's foundational to our competitive success, our accountability and our affordability,\" he said. \"We have a generational opportunity here for AI to be our future. We must act now. We need to start putting these pieces into place now.\" AI isn't just important for the DOD's warfighting capabilities, it will also be a powerful driver of the American economy, Groen said. Link to the report:  Final Report ."}, "210705_news_466596.txt": {"page_id": "210705_news_466596.txt", "text": "Today, Secretary of Defense Lloyd J. Austin III issued a memorandum announcing the Immediate Actions to Counter Extremism in the Department and the Establishment of the Countering Extremism Working Group (CEWG). The immediate actions are as follows: Review and Update of DODI 1325.06 Extremism Definition: Office of the Secretary of Defense (Personnel & Readiness) and the Office of the General Counsel (OGC) will review and update DODI 1325.06 to more specifically define what constitutes extremist behavior. Updating the Service Member Transition Checklist: The military departments will add provisions to their service member transition checklists that include training on potential targeting of service members by extremist groups and work with other federal departments agencies to create a mechanism by which veterans have the opportunity to report any potential contact with an extremist group should they chose to do so. Review and Standardization of Screening Questionnaires: All military departments to update and standardize screening questionnaires to solicit specific information about current or previous extremist behavior. Commission of Extremism Study: The Department will commission a study on extremist behavior within our Total Force, to include gaining greater fidelity on the scope of the problem. Led by Bishop Garrison, Senior Advisor to the Secretary of Defense on Human Capital and Diversity, Equity and Inclusion, the CEWG will oversee the implementation of immediate actions as well as the development of mid-term and long-term recommendations for the continued engagement of this issue. The CEWG will report through the Workforce Management Group (WMG) to the Deputy's Workforce Council (DWC). The CEWG will pursue four initial Lines of Effort (LOEs) within subcommittees that receive information from both internal and external Subject Matter Experts as well as coordination and input from the interagency. The Lines of Effort are as follows: Line of Effort 1: Military Justice and Policy. This LOE will focus on the role of military justice and policy. It will evaluate whether seeking to amend the Uniformed Code of Military Justice (UCMJ) or amending related department policy is appropriate in order to address extremism. This LOE will determine if regulations are sufficient or should be expanded. Line of Effort 2: Support and Oversight of Insider Threat Program. This LOE will determine how the Department should facilitate better information collection and sharing among Service Insider Threat Programs, law enforcement organizations, security organizations, and commanders and supervisors. This group will work to strengthen Insider Threat Programs and the Direct Awareness Campaign with the goal of promoting the use of the Insider Threat programs to report concerning behaviors for both military and civilian personnel. Line of Effort 3: Screening Capability. This LOE will discuss the Department's pursuit of scalable and cost-effective capabilities to screen publically available electronic information in accessions and continuous vetting for national security positions. The LOE will make recommendations on further development of such capabilities and incorporating machine learning and natural language processing into social media screening platforms. This LOE will also endeavor to develop policy to expand user activity monitoring of both SIPR and NIPR systems.\u00a0 Line of Effort 4: Education & Training. This LOE will utilize the current extremist and Insider Threat training review being conducted by OUSD(P&R) and OUSD(I&S) and \u00a0examine existing training and make recommendations and updates to those trainings for different leadership levels and separate and discrete targeted audiences, as necessary. It will use lessons learned from the Stand Down to enhance current reviews of training and education. This LOE will also ensure training addresses issues raised by commanders and supervisors on \u201cgray areas\u201d such as reading, following, and liking extremist material and content in social media forums and platforms. The memorandum for the Immediate Actions to Counter Extremism in the Department and the Establishment of the Extremism Working Group  can be found here ."}, "210705_news_466603.txt": {"page_id": "210705_news_466603.txt", "text": "On Thursday, one of Elon Musk's companies, Neuralink,  posted a video  showing a monkey playing Pong using nothing but a brain implant connected wirelessly with the computer hosting the game. While it's a fantastic display of the technology, most of the individual pieces of this feat have been done before\u2014in some cases, over a decade before. But Neuralink has managed to take two important steps: miniaturizing the device and getting it to communicate wirelessly. What has been done Neuralink's goal is to develop easy-to-implant, compact,  wireless brain implants . Initially, these devices will be used for obvious goals, like re-establishing some degree of independence in paralyzed individuals. But Musk has made it very clear he sees the longer term goal as making the implants commonplace and able to do far more mundane things, like providing direct, brain-driven control of electronic devices. For now, however, the early goals are dominating progress. About a year and a half ago, Neuralink showed off hardware implanted in pigs, with the implants providing real-time data about the goings-on within the pigs' brains' sensory regions as the animals explored their surroundings. At the time, Musk suggested that the team was nearly ready for human testing. The video and associated blog post released yesterday show that the company has taken the next intermediate step on the road to human trials: a successful implant in a monkey, which has a brain anatomy that is obviously a bit closer to that of a human. The video demonstrates that the device works wirelessly, and it's compact enough to not be visible in the video. The monkey has been trained to play a couple of on-screen games, including Pong, using a joystick. VIDEO But by recording and analyzing the activity in the motor-control regions of the brain, the research team was able to play the game using the signals coming in from the brain via the implanted Neuralink device. They could unplug the joystick, and the monkey would continue obtaining rewards by successfully playing the game. Not entirely new It's really cool and looks really impressive. But so does the below video, which shows an implant being used by a human to control a robotic arm. Notably, this video dates from 2012. The actual work was probably done earlier, and there were undoubtedly trials in monkeys well before this. To their credit, the people at Neuralink acknowledge this, writing, \"Neuralink's technology builds on decades of research.\" VIDEO So it's worth separating out what represents an actual advance here. The big development (from my perspective, at least) is getting the system to work wirelessly, which is harder than it sounds. One of the big barriers is the fact that even a single neuron produces a  lot  of data. Rather than adopting clean on and off states, neurons tend to have a background level of activity, producing a random string of activity spikes, even when quiet. And when activated, they simply produce an intense burst of spikes rather than switching to a clear \"on\" state. For a research team trying to understand how neurons communicate through these activity spikes, recording all their details is essential, and doing that for more than a handful of neurons requires a high-bandwidth connection to the electrodes in the brain. Neuralink, however, doesn't necessarily need all the details, and the company decided it would compress the data by simply registering how often a spike took place. This aggressive compression of the data was essential to get the stream to where it could transmit data from 1,000 electrodes over a Bluetooth link. It was also a bit of a gamble, given that any two spikes are rarely the same in terms of their magnitude, duration, and so on, and there was no guarantee that omitting some of this information wouldn't omit needed details of the brain's activity. At least for the motor control region, that gamble seems to have worked out. One of the other contributors to the success may be the high density of electrodes in this implant (roughly 1,000)\u2014much higher than the ones used in the earlier human experiments. Getting a larger sampling of the activity in the right area of the brain may help make up for the lack of some details. There are likely to be much higher density electrode arrays developed in the academic labs in the meantime as well, although these haven't been approved for human use yet, either. The lack of a need for external, wired connections (for both data and power, the latter being enabled by battery improvements) is part of the key to the device's compact nature. Advances in electronics over the past decade are also critical. So while the video is striking, it also represents exactly what Neuralink should be doing if it's preparing to submit its designs for human testing. It's not clear when the company will have enough data to be ready to talk to the FDA, and it would be really good if Neuralink would put some details of its system into the peer reviewed literature. But at his earlier presentations on the company, Musk seemed to recognize that this project would be a long haul and indicated he was comfortable with that."}, "210705_news_466611.txt": {"page_id": "210705_news_466611.txt", "text": "\n    Officials today discussed the National Security Commission on Artificial Intelligence's recent report and the progress for the adoption and implementation of AI across the Defense Department.\n Marine Corps Lt. Gen. Michael S. Groen, director of the Joint Artificial Intelligence Center, and Robert O. Work, vice chair of the NSCAI, spoke to reporters at the Pentagon. Work provided an overview of the report: The United States does not have a strategy, organizational structure and resources to win the competition with China for effective implementation of AI, he said. \"So the first thing is we have got to do is to take this competition seriously, and we need to win it.\" To win, AI must receive the necessary funding, at least 3.4% of the DOD budget. Those funds should then be channeled into priority areas as recommended by a steering committee consisting of the deputy defense secretary, the vice chairman of the Joint Chiefs of Staff and the principal director of national intelligence, he said, noting that a good first step was having the JAIC report directly to the deputy defense secretary. That steering committee would also remove any bureaucratic obstacles and would oversee the development of a technology annex to the  National Defense Strategy , Work said.\u00a0 Also the department should set AI readiness performance goals by the end of this fiscal year, with an eye toward AI-ready implementation by 2025, he said. Work noted that the U.S. is still the world leader in AI, but China is structured to rapidly advance with its strong government support to academia, the private sector and People's Liberation Army. Identifying the right talent for AI, ensuring AI is used in an ethically responsible manner and forming international partnerships are other ingredients for successful AI implementation, Work said. Groen said the department agrees with the report and is already implementing about 100 of recommendations that were given. Other recommendations would be included in feasibility studies. \"AI as a core tenet of defense modernization,\" Groen said. AI efforts will be fundamental to achieving quality networks and data services in a secure manner. \"We've created positive momentum for AI, and we continue to build on that now. But now comes the real critical test in any transformation. The hardest part is institutional change and change management of the workforce and practices and processes that drive a business. This step will not be easy, even within the Department of Defense, but it's foundational to our competitive success, our accountability and our affordability,\" he said. \"We have a generational opportunity here for AI to be our future. We must act now. We need to start putting these pieces into place now.\" AI isn't just important for the DOD's warfighting capabilities, it will also be a powerful driver of the American economy, Groen said. Link to the report:  Final Report ."}, "210705_news_466619.txt": {"page_id": "210705_news_466619.txt", "text": "Over the last few months I have been on and off digging into the history of early PC networking products, especially Ethernet-based ones. In that context, it is impossible to miss the classic NE2000 adapter with all its offshoots and clones. Especially in the Linux community, the NE2000 seems to have had rather bad reputation that was in part understandable but in part based on claims that simply make no sense upon closer examination. A genuine Novell NE2000 card (1992) with DP83901 First let\u2019s recap a bit. In late 1986, National Semiconductor introduced the DP8390/91/92 chip set including a complete Ethernet controller, encoder/decoder, and a transceiver. The DP8390 NIC was a relatively simple design, not as advanced as the Intel 82586 or AMD LANCE, but significantly more capable and cheaper than the low-end offering of the era, the  3Com 3C501 EtherLink . National Semiconductor (NS) published a reference design labeled DP839EB (EB for Evaluation Board); Application Note AN-479 described the board (see page 134 in the  1988 databook  PDF). The DP839EB was a short 8-bit ISA card with 8Kx8 SRAM, AUI and BNC (aka Cheapernet) connectors, as well as a RJ-45 connector that required an optional StarLAN daughterboard to work (StarLAN can be thought of as a somewhat different and proprietary forerunner of Twisted Pair Ethernet). NS and Novell worked together to  support the DP8390 in NetWare , and NS encouraged OEMs to build NetWare-compatible cards. The DP839EB reference design could use PC 8237-style DMA to transfer data to and from the NIC, but could also use programmed I/O (PIO) instead if DMA was unavailable or undesirable. In early 1987, two products based on the DP8390 appeared on the market:  Western Digital EtherCard/StarCard Plus  (WD8003E/WD8003S) and Novell NE1000. Both were similar to the DP839EB reference design in that there was not a lot of additional logic surrounding the DP8390 chip, but neither design was exactly the DP839EB. In late 1987 or in 1988, the two boards were joined by a more complex design based on the DP8390, the 3Com EtherLink II (3C503). A 1987 Novell NE1000 (old Assy. #950-054401); note nearly all ICs are NatSemi. The WD8003E used strictly shared memory to move data to and from the card. The entire 8K SRAM was mapped in the host\u2019s address space, and additionally required 32 bytes of I/O port space and an IRQ. The EtherCard Plus list price was initially $399. A 1990 WD8003E EtherCard Plus (with WD83C691A endec). In contrast, the NE1000 did not map its onboard SRAM into the host\u2019s address space, but likewise removed the DMA support, and only supported PIO transfers that utilized the DP8390\u2019s Remote DMA feature. That meant the NE1000 only required 32 bytes of I/O port space and an IRQ; no other resources were needed. The NE1000 was priced at $495 at introduction (3Com\u2019s EtherLink 3C501 cost $595 at the time) but almost immediately dropped to $395 (just below the WD8003). A 1989 3Com EtherLink II (3C503) with DP8390C The comparison between the WD8003 and NE1000 is interesting. WD opted to use shared RAM which is faster but significantly more problematic to configure, and that was especially the case with the coming wave of 386 memory managers. Novell went in the opposite direction, choosing somewhat slower PIO but completely avoiding any configuration issues with shared memory. Both Novell and WD decided to drop DMA support, probably because it avoided yet another source of configuration conflicts  and  because especially on PC/AT class systems, DMA was slower than either PIO or memory anyway. It is important to keep in mind that the existing competition for the WD8003E and NE1000 weren\u2019t fancy adapters like the 3Com 3C505 EtherLink Plus or the Exos 205T but, first and foremost, the cheapest Ethernet option available, the 3Com 3C501 EtherLink. And both the NE1000 and the WD8003E beat the 3C501 hands down, because they had a much bigger packet buffer (8K vs. 2K) and did not have the awful 3C501 limitation of having to switch between mutually exclusive transmit, receive, and host access modes. In 1988, the  NE2000 appeared . It was essentially a 16-bit version of the NE1000 with support for 16-bit AT bus (but still capable of working in 8-bit slots) and two SRAMs in an 8Kx16 configuration. This doubled the onboard memory capacity and enabled both the DP8390\u2019s internal bus and the NE2000\u2019s external ISA bus connection to use 16-bit transfers, significantly improving the speed at which the host could communicate with the adapter. Western Digital released a similarly upgraded WD8013E (EtherCard Plus 16) with a 16-bit ISA interface and 16KB of onboard RAM. On the part of Novell, the motivation was clearly not to make money on hardware but sell more software. In 1991, Novell let Anthem/Eagle take over the network card manufacturing and distribution; after all, the hardware business was something Novell wanted to get out of, not into. The main purpose of the NE1000 and NE2000 was to drive the prices of networking hardware down, and it did just that. For anyone building a LAN in the late 1980s, putting expensive $800 \u201cintelligent\u201d adapters into client machines made zero sense. And choosing between a $500 3C501 EtherLink and a $400 NE1000 really was not much of a choice, and it\u2019s no coincidence that the 3C501 vanished from the market pretty quickly, with the DP8390-based 3C503 taking its place. A 1988 Taiwanese NE1000 clone (CNET LKT-N100E) As a side effect of its low price and reasonable performance, the NE2000 became the mainstay of PC LANs in the late 1980s and early 1990s and was used in many NetWare servers and countless client machines (as evidenced by numerous contemporary  software reviews ). It was supported by just about every PC networking package, and that in turn encouraged a lot of cloning. I believe  that  was the real cause of NE2000 dislike among Linux developers and users. Curious Claims Let\u2019s see if we can break down the mixture of unsourced claims and outright nonsense that made it to everyone\u2019s most reliable source of facts,  Wikipedia : \u201cIn order to create these [NE1000 cards] at minimal R&D, engineering and production costs, Novell simply implemented, almost verbatim, a prototype design created by National Semiconductor using the 8390 Ethernet chip. National Semiconductor, for its part, had no qualms about the use of the design; the use of National Semiconductor chips made the proposal almost pure profit. However, since the design had been intended only as a proof-of-concept prototype, it implemented bare-minimum functionality: PIO was used instead of DMA, no buffering was provided and no provision was made for the use of a transceiver.\u201d The first sentence is more or less accurate, but makes it sound like Novell found a half-baked prototype design lying around, stripped it of anything useful, and started shoving it down the throats of unsuspecting users. The reality is that NS and Novell clearly worked together on software support before even the NE1000 was released, and that DP839EB \u201cprototype design\u201d was closer to what\u2019s nowadays called a \u201creference design\u201d where OEMs are free to make modifications but there\u2019s nothing fundamentally wrong with the reference design as it is. Now let\u2019s take a look at the ostensibly technical part of the claims: \u201c[S]ince the design had been intended only as a proof-of-concept prototype, it implemented bare-minimum functionality: PIO was used instead of DMA, no buffering was provided and no provision was made for the use of a transceiver.\u201d The suggestion is clearly that the DP839EB design was so lame that it didn\u2019t even support DMA, except that\u2019s not even true: Anyone can look at NS\u2019s Application Note AN-479, DP839EB Network Evaluation Board, and see that it  did  support DMA. In fact even the initial revision of the NE1000 supported DMA, or at least had  jumpers to configure it . And yet both Novell and Western Digital took DMA out because it had questionable benefits and made configuration more difficult. Indeed PIO was used instead of DMA\u2026 because no one wanted to deal with DMA. As for the claim that \u201cno buffering was provided\u201d, it is quite mystifying. The NE1000 had 8K of onboard SRAM and the DP8390 had additional on-chip FIFO. If that does not count as buffering, what does? It\u2019s difficult to not classify that claim as pure nonsense. The last bit, \u201cno provision was made for the use of a transceiver\u201d, is similarly suspect. Novell\u2019s  1989 NetWare Installation Supplement  describes the settings of two NE1000 models, Assy. #950-054401 and a newer Assy. #810-160-001, but they both (as well as the NE2000 described in the same manual) have BNC and AUI connectors, which means there\u2019s one onboard transceiver for BNC and a way to connect an external AUI transceiver. The DP839EB likewise had both BNC and AUI connectors. It is theoretically conceivable that there was some completely unknown early AUI-less variant of the NE1000, but it is vastly more likely that the claim is just plain wrong like the other nonsense in the Wikipedia article. What it\u2019s Really About A modified version of the \u201cNE2000 is horrible\u201d claims can be found e.g.  here . Much like the talk about the 3C501 being awful makes sense as soon as one starts pretending that the 3C501 is a design from 1992 and not 1982, the criticisms of the NE2000 make much more sense if one pretends that Novell tried to sell it as a high-performance Ethernet adapter in 1995. Again there are highly dubious claims such as that the NE1000/NE2000 had \u201cno method for selecting a transceiver\u201d, which is only true if the jumper block on the card (the standard method at the time) does not count. The screed also requires one to believe that there were so many Taiwanese NE2000 clones because the NE2000 was the worst design ever and\u2026 that\u2019s why everyone wanted one. I suppose that logic makes some sense in a world where everyone not running Linux is by definition an idiot, because what other reason could there possibly be for not running Linux? But then we get to the real gist of the hate for the NE2000: \u201cProprietary-OS users didn\u2019t care about those incompatibilities, since they used custom driver preloads in their hard drives as delivered by the OEM, or used custom driver diskettes. Linux/BSD users, by contrast, tended to have a rough time since they tended to (rather naively) assume that an NE2000 clone should routinely work with the standard ne.c + 8390.c driver.\u201d That  actually  makes a lot of sense. There were definitely many NE2000 more-or-less compatibles, and many of them used clones of the DP8390 chip rather than the original. And many of those clone chips were different enough that that code written for the DP8390 might break. The first of those clones was probably Western Digital\u2019s  WD83C690  and it already introduced several incompatibilities that happened to not matter to WD\u2019s own drivers. Even if a clone card used a genuine DP8390 chip, its PROM or I/O port behavior could be just different enough that a driver written for the NE2000 might not work. Curiously, even among the Linux folk, there was disagreement on the merit of PCI-based NE2000 clones. While some said that \u201c PCI NE2000 clones are a bad idea \u201c, others considered them \u201c good news \u201c. The first point of view was based on the fact that there were many designs much better suited to PCI than the DP8390, the second argued that unlike ISA-based NE2000 clones, the PCI ones at least were likely to work with Linux. There\u2019s a lot of truth to both of those viewpoints. There are other gems out there, like  this page  which claims that \u201cNe2000 is not technically a card, it is a standard that several implementors follow\u201d. If you look at it like that, then the Sound Blaster wasn\u2019t a card either, and the IBM PC was just a standard that several implementors including IBM happened to follow by sheer coincidence. Which only makes any sense if one decides to completely ignore what what was the cause and what was the effect. There are also other views, such as this one  here : \u201cLike many NatSemi DP8390 based NICs (WD8XXX and many others) [the NE2000] performed decently well with FreeBSD for those times, and it was widely available and quite cheap. Basically, Novell kick-started the PC networking era by throwing that thing out to the masses, essentially at cost. The 8390 and its clones were the Realteks of the ISA era, and did a way better job in that role than Realtek did ever since.\u201d Even though I\u2019m not entirely sure if it\u2019s meant to be praise or criticism, it\u2019s not wrong. Literary Criticism Mostly out of curiosity I started reading the source code of the Linux NE2000 driver. The heart of it is really a driver for the National Semiconductor DP8390 chip which is the shared by drivers for more or less all cards based on the DP8390 and its clones: Novell NE2000, Western Digital WD8003, 3Com 3C503, and numerous others. What I read in the source code was\u2026 not terribly confidence-inspiring. I found several problems that are fairly obvious  if one looks for them  but are also easy enough to overlook. The identified issues all relate to the receive path of the DP8390 driver, which\u2014given the chip architecture\u2014is significantly more tricky to implement than the transmit path. For whatever reason, the Linux driver does not do things by the book, and there  is a book . The Linux driver acknowledges receive interrupts as the last thing it does, which is simply backwards. It is important to keep in mind that the driver code which reads received packets from the DP8390\u2019s ring buffer inevitably races the chip which may be receiving new packets at the same time. For that reason, it is necessary to acknowledge (clear) interrupts first, and  then  remove all received packets. The hardware works in the opposite order and first updates all of its state and writes to memory, and then raises an interrupt. That way, there could be a spurious interrupt for an already-processed packet but nothing will be missed. The Linux driver does things in the wrong order and risks that the hardware receives a new packet and sets the interrupt status register in the window between the Linux driver removing previously received packets and clearing the interrupt status register. If that happens, the receive interrupt will be lost and Linux will not be aware that another packet was received. In many cases, another packet will arrive very soon and \u201cfix\u201d things by triggering another interrupt. But if the overlooked packet happens to be the last in a sequence, it will be stuck in limbo until something like a retransmission causes the driver to notice it. Depending on the upper layer protocols, that might only cause a slight delay or cause significant confusion. TCP/IP is quite sensitive to lost or duplicated packets. There is a related bug in the Linux driver in that it only  receives at most 9 packets  per interrupt and then simply declares that it\u2019s done receiving and clears the receive interrupt status. If more than 9 packets happen to have been queued up and no further packets arrive, the driver might again fail to notice one or more already received packets. It is not at all clear to me what this strange limit on the number of packets processed at a time was meant to solve, and the source code offers no hint either. There was yet another somewhat related bug that did actually get fixed. In  Linux 1.2 , one could find the following code at the end of  ei_receive() :     /* Bug alert!  Reset ENISR_OVER to avoid spurious overruns! */\n    outb_p(ENISR_RX+ENISR_RX_ERR+ENISR_OVER, e8390_base+EN0_ISR); That was changed in Linux 1.3.47 to the following (excerpted from  Linux 5.11 ): /* We used to also ack ENISR_OVER here, but that would sometimes \n   mask a real overrun, leaving the 8390 in a stopped state\n   with rec'vr off. */\n\tei_outb_p(ENISR_RX+ENISR_RX_ERR, e8390_base+EN0_ISR); Well duh\u2014if you acknowledge interrupts without handling them, bad things are bound to happen. If the overflow warning interrupt is acknowledged for no good reason, it is guaranteed that under some conditions the real interrupt will be missed (and since it will prevent further receive interrupts from happening, the receive logic will be stuck). It is also fascinating how the comment went from \u201cReset ENISR_OVER to avoid spurious overruns\u201d to more or less the exact opposite. The issue with clearing interrupts only after working through the receive ring (instead of before) was never fixed and survives in Linux 5.11. Again, the correct recipe was given in the  Writing Drivers for the DP8390 NIC Family of Ethernet Controllers  Application Note, but Linux chose to ignore that. In contrast, for example the  NetBSD 0.9 if_ed  driver (1993) does not have these problems. It is difficult to judge how often the deficiencies in the Linux driver described above caused user-visible problems. I am certain they did cause trouble  sometimes , but perhaps rarely enough to not make a real difference. And they can\u2019t have caused major problems if they went unfixed for so long. But it does make one wonder if perhaps the bad name the NE2000 got among Linux users was caused in part by sub-optimal Linux drivers for it."}, "210705_news_466680.txt": {"page_id": "210705_news_466680.txt", "text": "Vector games 32c3 This is our talk for  32c3  on vector retro-gaming and interfacing MAME with XY displays.  The talk is the result of a collaboration with  Adelle Lin , another  NYC Resistor  member who is a digital designer with a focus on building playful spaces, games and interaction. If you prefer to watch,  the half hour video is online . History of Vector Graphics Analog computers \nThe earliest  analog computers  used oscilloscopes or plotters to output their results.  This  EAI680  is displaying the result of a differential equation modeling a dampened pendulum on its Tektronix XY monitor. The first video game,  Tennis for Two  in 1958, was built with a similar analog computer and displayed on an oscillograph.  The bounce of the ball was implemented as a differential equation, very similar to the one shown on the EAI680. There is no \"source code\" for Tennis for Two since it was implemented entirely in hardware.  There are relays that control the direction of the ball, comparators that detect when it hits the ground or the net, opamps that implement the differential equation, resistors that simulate drag, and so on. Adelle worked on a modern recreation of it for  Silicon City  in collaboration with producer Jeanne Angel, games studio Dozen Eyes and physicist Peter Takacs. Due to the pressure of keeping something running maintenance-free for the five months of the museum exhibit, they used a 4K monitor and emulated the game. During the real demo of Tennis for Two at Brookhaven there were technicians replacing parts as they broke, something that the NYHS wanted to avoid. Digital computers Spacewar!  from 1962 was one of the first digital video games and used the XY display of the PDP-1 to display the players' ships and weapons, as well as the starfield background. The controls were knobs and buttons on the front panel of the system, although many sites built custom controls to avoid wearing out the official ones. Storage tube vector displays were very popular for engineering applications since they could draw very high resolution lines and maintain a stable image.  They are not well suited for games, however, but allowed very low memory computers to draw incredibly complex images over low bandwidth links. Why Vector Games? \u201cIn 1978 when the Digital Vector Generator was developed for Lunar Lander, memory was much too expensive for a frame buffer in a video game.  \u2026  Two frame buffers of 512x512x4 would have required 128 memory devices costing  total of  $614.\u201d  -- Jed Margolin, Atari engineer\n This is why is was worth going through all of this effort to draw vectors when most displays are raster-scan.  The cost of building a raster version of Asteroids with a frame buffer than could hold the screen resolution of 1024x1024 would have been thousands of dollars in hardware. It also requires a display that could draw that many vertical lines: most CRTs were being built for television use and only had a few hundred vertical lines of resolution. Star Wars on the Acorn Electron Star Wars on a vector display Compare the blocky, pixelated bitmap Starwars game to the vector vesion. It's really stunning how much more detail the vector lines can resolve and how well they can represent the circle of the Death Star. Star Wars on the Atari 2600 Star Wars on a vector display Additionally, vectors make it easier to do 3D animation. Here's a comparison of a bitmap game attempting to simulate a 3D world with the Starwars scaled and rotated 3D model of a Tie-Fighter. Which would you rather play? Vector Generating Hardware How vectors are drawn \nIn a vector display the beam doesn't draw horizontal \"raster\" or \"scan lines\" like a conventional CRT. Instead the beam can be steered to any point on the screen and creates a bright dot at that location.  With non-storage vector displays, the longer the beam remains on that point, the brighter it becomes.  By varying the X and Y inputs, it is possible to draw lines or other shapes.  By moving quickly or by varying the brightness (\"Z\") input, disconnected lines can be drawn.  In this case I've disabled the Z input so that we can see the drawing order of the objects on screen. Lunar Lander shown here uses a \"digital vector generator\", which directly drives the beam. \nTempest uses an \"analog vector generator\", which always returns to center since that is the only defined point.  In either case it is a \"hard real time problem\" to generate these vectors -- if the system pauses even for a few nanoseconds there is an appreciable change in the brightness of the line. Analog vs digital vector generation I'm indebited to  Jed Margolin's guide to vector displays  for this section. Digital Vector Generator schematic Analog Vector Generator schematic One way to generate the vectors is with a  Digital-to-analog converter  (DAC) that directly drives the X or Y input to the display.  This is called a \"Digital Vector Generator\" and was used on the early games like  Asteroids  and  Lunar Lander .  They used 10-bit DACs and an opamp configured as a buffer, but required that the CPU spend most of its time updating the inputs to the DAC and also limited the resolution to roughly 1024x768. Another technique was developed for later games like  Starwars .  The \"Analog Vector Generator\" uses an opamp configured as an integrator by the addition of the capacitor on the feedback path.  This means that the input to the DAC is the  slope  of the line to be generated and the analog nature of the opamp ensures that an \"infinitely\" smooth line will be drawn. \nWith modern hardware we can generate 12-bit lines that far exceed the resolution of the small vector displays and we can take advantage of the DMA engines on modern microcontrollers to offload sending the data to the DAC.  This board is a prototype of my opensource hardware  v.st  boards and is easy enough to build on a breadboard for you own hacking. And since we have far more CPU power than when many of these games were created, I've also patched the vector generation code to sort the vectors to reduce the overall travel time.  This makes a big difference for vector displays with lower bandwidth. Vector Display Hardware Oscilloscopes \nModern digital oscilloscopes are not very good for displaying vector data.  The LCD might emulate phosphor persistence, but they typically only have 8-bit ADC and very low resolution.  They are still super useful for general signal measuring, just not in this application! \nInstead go to ebay or craigslist and find a $50 analog CRT two-channel scope with an X-Y mode.  Anything with 5 MHz or more of bandwidth is fine -- the DACs on the v.st boards are less than 2 MHz anyway. Storage displays \nStorage tube displays are great for very detailed static vector images, but won't work well for games with high framerates.  So save your money and don't spend it on a Tek 4014.\n Vectorscopes \nVectorscopes like the  Tek 1720  were used by TV stations to calibrate their colorbars and being phased out as the stations go digital.  As a result these analog displays are plentiful and inexpensive on ebay.  They use very fast electrostatic deflection and typically have small screens, but they tend to be super sharp.  They also have differential inputs, so that you can don't need a negative output on the DAC. Vectrex consoles Vectrex console Vectrex electronics The  Vectrex  home game console has a nice large, sharp B&W screen.  It is fairly easy to disconnect the original motherboard from the analog CRT driver and route it to the outside of the case to allow a  v.st  board to be plugged in. \nThe Vectrex has a somewhat slow magnetic deflection system, which prevents it from drawing as many vectors as the faster electrostatic displays.  It is much larger, however, and also offers a Z-input that allows the brightness to be controller without varying the line drawing speed. Laser projectors \nYou can use a laser projector as well, but the results are really suboptimal. The physical mirrors have quite a bit of inertia and turning the beam on and off requires time.  The refresh rates tend to be low and the images very flickery.  Some specialized games might work with very fast (45-60Kpps) galvos, but most arcade games required too many vectors. \nThe  LazyMame  project appeared to be making good progress, but I can't find any details or source code since 2008.  OpenLase-Mame  had some development in 2013, but nothing for three years. Other people have tried a  custom Unity plugin to create new laser games , with some success.\n MAME MAME  is the Multiple Arcade Machine Emulator and it really lives up to its name: it can emulate thousands of games and cabinets, including nearly every vector arcade game ever made. Emulating Vector Displays Actual vector display highlights Emulated vector bloom The \"bloom\" effect caused by the bright vector lines is one of the features that is most desired by game players.  MAME has support for HLSL shaders that blur the bright lines and creates an approximation of the effect.  On the left you can see a real Vectrex display and on the right MAME's emulation of a similar effect. Patches for head \nUnfortunately  my patch to add support for exporting the vectors  to MAME was closed as \" an unacceptably hacky way to achieve the intended result \".  Instead you'll need to clone my tree and follow my  instructions on building vector MAME .  The majority of the changes are in  src/emu/video/vector.cpp  and take advantage of the clipping windows that MAME is using to limit the vectors to the onscreen area. Patches for Raspberry Pi \nI've also ported the changes to the stripped-down MAME4AllPi build for the Raspberry Pi.  The  instructions for Pi vector MAME  are similar; you'll need to clone my tree and build it on the Pi.  There are quite a few issues that need to be fixed related to joysticks unfortunately. Playing games \nYou can play one of the original arcade games,  Space Wars  from 1977. Asteroids  (1979) is always popular as well.  One of the highest grossing games of its era! Lunar Lander on a vector scope Tempest on a vector scope Or perhaps  Lunar Lander  (1978) and  Tempest  (1980) are more your speed. \nThere are lots of vector games to emulate.  All of the ones in  Atari games  are there.\n \nSo many obscure games! I can't name them all, but have collected the list of  Vectrex games  as well as all of the  vector arcade games  listed on Wikipedia.\n Taking it further Custom input devices Custom two joystick Robotron console Roboton Once you have your vector display hooked up the MAME, you'll probably want to take it to the next level.  It is fairly easy to build custom USB HID joysticks using  Adafruit arcade joysticks ,  buttons  and even  coin acceptors . Tim Bartlett's  Raspberry Pi Asteroids console  looks great and he has published all of the design files if you want to build your own. Juergen Mueller build a marvelous  Asteroids cabinet with Vectrex display  that uses a real Asteroids logicboard, which is just above and beyond. Creative coding with vectors \nIf you're tired of playing games, you can always turn the display into a  Scope Clock .  The Teensy 3 has through-holes for a 32.768KHz clock crystal and backup battery that lets it keep decent time. \nYou can write games that run directly on the Teensy, like  Space Rocks , my Asteroids clone, or art projects like my  Twitter Oscilloscope .\n \nSome folks are really taking it to the next level --  vec9  is the first full-scale vector arcade game in decades and has a custom cabinet with a real tank controller. We've written a simple Processing library to show how to write things that interact with the displays.  Sample code is at  vst/processingDemo  and also available as ProcessingJS, demoed in  Swarm  and  Random Walk . \nIf this is of interest to you we'll be having a class at NYC Resistor in collaboration with Code Liberation and an exhibition at  Baby Castles  in early 2016. More info \nMore info on the boards is at  http://v.st/ v.st ,  MAME ,  Vectrex  and  Tek 1720 .  Thanks for coming to our talk! 2015 Vector display Talks Games CCC \n    \n      Last update:  December 22, 2020"}, "210705_news_466700.txt": {"page_id": "210705_news_466700.txt", "text": "On 8 April 2021, the U.S. Commerce Department\u2019s Bureau of Industry and Security (BIS)  announced  the addition of seven entities to the Entity List, all of which are in China.\u00a0This rule is being published in the Federal Register on 9 April, but is effective on 8 April.\u00a0There is a savings clause for items that were en route aboard a carrier to a port of export, reexport, or transfer (in-country), pursuant to actual orders, on 8 April 8 2021. The rule imposes a license requirement for all items subject to the Export Administration Regulations (EAR), with a presumption of denial, for the following entities: National Supercomputing Center Jinan National Supercomputing Center Shenzhen National Supercomputing Center Wuxi National Supercomputer Center Zhengzhou Shanghai High-Performance Integrated Circuit Design Center Sunway Microelectronics Tianjin Phytium Information Technology According to BIS, these entities are being designated for being \u201cinvolved in activities that support China\u2019s military actors, its destabilizing military modernization efforts, and/or its weapons of mass destruction (WMD) programs.\u201d \u00a0 In addition, Secretary of Commerce Gina Raimondo stated the following in a  press release : \u201cSupercomputing capabilities are vital for the development of many \u2013 perhaps almost all \u2013 modern weapons and national security systems, such as nuclear weapons and hypersonic weapons. The Department of Commerce will use the full extent of its authorities to prevent China from leveraging U.S. technologies to support these destabilizing military modernization efforts.\u201d Press reports by the Washington Post  here  and  here  provide some additional detail behind the Commerce Department\u2019s decision-making: \u201cAll seven are linked to China\u2019s ambition to build the world\u2019s first exascale computer, Commerce Department officials said. An exascale computer \u2014 the next frontier in high- performance computing \u2014 can handle a million trillion calculations per second. That\u2019s the sort of speed necessary to more accurately model the heat and drag on hypersonic vehicles, a field of advanced weapons research in which the Chinese military is already engaged, using its current generation of supercomputers.\u201d Next steps Companies in the semiconductor and other industries will need to carefully assess their activities in China, as the Biden Administration\u2019s actions make clear that it will continue to use export controls aggressively to target China\u2019s policy of civil/military fusion, rather than breaking with the Trump Administration\u2019s approach."}, "210705_news_466704.txt": {"page_id": "210705_news_466704.txt", "text": "Armed combatants recently  overtook  the town of Palma in Mozambique\u2019s Cabo Delgado region, killing local inhabitants as well as foreign workers associated with a natural gas development. Just a week earlier, a dozen U.S. Green Berets had  arrived  to help train Mozambican marines who are fighting the insurgents. And the week before that, the U.S. State Department  designated  ISIS-Mozambique, a reference to the region\u2019s militants, to be a Foreign Terrorist Organization (FTO) and, under a 2001, post-9/11 Executive Order, also designated the group and its leader a Specially Designated Global Terrorist (SDGT). The terrorist designations and U.S. security cooperation are wrong-headed. The U.S. needs to remove the small cadre of Green Berets, not give air to the myth that the Cabo Delgado conflict is global in nature, and avoid any association with the disastrous security intervention occurring there. The Biden administration instead needs to prioritize diplomacy, require accountability, and offer targeted aid to address the underlying issues. It can do that by pressing the Mozambican government and multinational energy companies to provide for basic human security in the region in the form of development and stable governance. For now, Mozambique\u2019s military says it has  retaken Palma  after 10 days of fighting, though that may be more a matter of the combatants\u2019 tendency to  seize and then cede  towns as a show of force. While violence has escalated in recent weeks, it has plagued the province for years, with the current conflict dating back to 2017 and its roots even deeper. That was the year foreign energy companies including France\u2019s Total, U.S.-based Exxon Mobil, Italy\u2019s ENI, and China National Petroleum Corporation (CNPC), stepped up work on the ground to extract liquid natural gas (LNG) from the African continent\u2019s three largest deposits, all in Cabo Delgado. Local populations, who already were living in poverty and have long been neglected by the government in the far-off capital Maputo in the country\u2019s south, were  displaced by the LNG exploration . Nearly  one-third  of Cabo Delgado\u2019s estimated 2.3 million inhabitants have been internally displaced, either by the LNG work or by associated violence. Although northern Mozambique is home to a significant portion of the country\u2019s Muslim minority, the current violence appears to be indiscriminate and based on  local grievance s rather than deliberate targeting of Christians or other groups. In 2019, in response to persistent attacks, the gas companies brought in private security, hiring the infamous Kremlin-linked  Wagner Group , which quickly found itself outmaneuvered in deadly ambushes. Last week in Palma, the South African Dyck Advisory Group, which was brought in on the heels of Wagner\u2019s failures, similarly found itself unable to provide adequate security. The Mozambican military has also failed in its interventions. Now the U.S. is training Mozambican marines, and Portugal, at the request of Mozambique President Filipe Nyusi, is sending a larger contingent, also to train Mozambican armed forces. Amnesty International  reports  the Mozambican military, defense contractors, and insurgents have all killed civilians and committed other atrocities. The increasing deployment of security forces has escalated the conflict, at the cost of civilian lives. Misguided Reading of the Militants U.S. participation and the terrorist designations are premised on a purported affiliation between the insurgents in Mozambique and ISIS. This is misguided. Even as ISIS lost its physical caliphate in Syria and Iraq and has shifted to a non-geographically specific caliphate, northern Mozambique makes a strange choice. The benefits to a transnational terror movement are difficult to discern. Outside of the LNG projects, there is limited Western presence to foment a global narrative. And unlike in Somalia, where  illegal and illicit trade  generates millions of dollars in revenue, there is as yet no way for such a movement to extract wealth from northern Mozambique\u2019s natural resources. With no economic incentives and no demonstrated ability to hold territory, the insurgents are embarking on a scorched-earth campaign that accomplishes nothing for ISIS as a global movement nor for their own interests locally, other than to inflict pain upon those they believe are doing likewise to them. The violence in northern Mozambique is better understood as a highly localized insurgency that has grown in size and ferocity as disaffected populations in proximate provinces have joined the fight. The increasing strength and violence of the insurgency are concerning, but that does not mean they are properly designated as ISIS. In 2013, as I traveled across northern Mozambique, tensions were escalating across the lines that defined Mozambique\u2019s civil war, which had officially ended two decades earlier. Though political negotiations de-escalated that violence, the results ignored underlying local grievances in favor of short-term pacification of leaders. The current insurgency has seized on this dissatisfaction to promote its narratives of government failure and neglect. It also has operated with military precision and strategy,  cutting off military supply lines ,  using seized military uniforms and weapons , and targeting government buildings with the goal of making the region ungovernable. In spite of the claimed affiliation with ISIS, the strategy more closely reflects Mozambique\u2019s Civil War than the pursuit of a global caliphate. During the country\u2019s civil war, the armed rebel movement RENAMO was known for  horrific and terrorizing violence , including burning people alive and forcing new recruits to murder family members. The brutality of the current violence is not new to the region or unique to the present conflict. (Local residents call the combatants \u201cal Shabaab,\u201d but only because it translates as \u201cthe youth;\u201d the fighters don\u2019t appear to have any links with the al-Shabaab militant group that operates out of Somalia in the Horn and East Africa.) Risk of Backfire for US Involvement The March 10 U.S. designation of Mozambique\u2019s local combatants as global terrorists only lends credence to the grievances driving this small but effective and violent insurgency. Ironically, the U.S. attention may actually  attract  external support by affiliating a local conflict with global conflict dynamics. At the same time, the presence of a dozen U.S. Green Berets in Mozambique, while unlikely to make much of a difference in training for the military, only aligns the United States with the LNG developers, their private security firms, and the Mozambican forces, all of whom are viewed with disdain by much of the local population. As Rachel Kleinfeld of the Carnegie Endowment for International Peace  notes , \u201cThere is no correlation between increased SSA [security sector assistance] and stability in fragile states. Studies show that in the absence of minimal state capacity and societal inclusiveness, SSA fails.\u201d To undercut the violent extremist activity in Cabo Delgado, the Mozambican military and private contractors first and foremost need to be held accountable for any human rights abuses. The above-mentioned Amnesty International  report  contains damning allegations against Mozambican security forces of beatings, harassment, extortion, torture, and extrajudicial killings. The Mozambican government appears to be relying on the urgency of global cooperation against ISIS to distract from accountability for these horrific acts. A clear, sensible plan for addressing the causes of the violence and for achieving accountability should be the minimum conditions for the U.S. to consider security cooperation, not to mention  U.S. laws  regulating and even barring such assistance to foreign security forces that have committed human rights abuses. Total, Exxon Mobil, ENI, and CNPC also need to make increased local investment in return for the privilege of extracting the area\u2019s natural resources for profit. The long-neglected population justifiably doubts whether private companies and the government in Maputo will equitably distribute the resultant wealth. A  2016 case study  of such investments described support for a few hundred school children in Pemba, the capital of Cabo Delgado, and training programs in Maputo. In other parts of Africa, such investment has  established health systems  and brought infrastructure development like roads, but the offshore location of the Cabo Delgado deposits has limited the gains experienced by the local population. Emilia Columbo, an analyst working on the region for the Center for Strategic and International Studies,  cites  the potential of the Integrated Development Agency of the North, an initiative of the Mozambican government to address socioeconomic needs in Cabo Delgado and adjacent Niassa and Nampula provinces. The LNG operators could invest in this program, with a specific focus on job creation and infrastructure development that could transform the regional economy. The U.S. government could offer support for this work, while making any security assistance conditional on both development progress and accountability for atrocities committed by Mozambique\u2019s military. All of this would undermine the narratives promoted by the insurgents, while building local resiliency. Mozambique is a case where the U.S. should begin to reject 20 years of widespread conflict escalation that began with the Global War on Terror after 9/11. Instead, the Biden administration should make a fresh start here by digging deeper to the roots of this conflict, and deploy the tools of diplomacy and development to help address the real causes for a more sustainable peace this time. IMAGE: A woman cooks in the community of Ntocota, Metuge District in Pemba, Cabo Delgado Province on Feb. 22, 2021, where thousands of displaced residents have been relocated due to recent attacks by armed insurgents in northern Mozambique. -Tens of thousands of internally displaced people are sheltering in camps in and around Pemba, while many others have moved in with host families. Mozambique is battling to contain an Islamist fundamentalist insurgency which started in October 2017, and has claimed more than 2,500 lives and displaced more than 570,000 people. (Photo by ALFREDO ZUNIGA/AFP via Getty Images) Filed under: Biden administration ,  Counterterrorism ,  development ,  Diplomacy ,  foreign aid ,  Foreign Terrorist Organization ,  ISIS ,  Military ,  Military aid ,  Mozambique ,  Security Assistance ,  terrorism ,  U.S. Military"}, "210705_news_466738.txt": {"page_id": "210705_news_466738.txt", "text": "In a move that will upset some on the left wing of his party,  President Joe Biden is requesting $753 billion for national defense , a 1.7 percent increase over President Donald Trump\u2019s military budget last year. However, as Biden\u2019s Republican critics are bound to complain, he is requesting a 15.9 percent increase in  non-defense  spending. In fact, while Trump\u2019s last budget cut everything but defense, Biden\u2019s first budget gives the Defense Department\u2014in percentage terms\u2014the smallest increase of any federal agency except for the Department of Homeland Security, whose budget remains about the same as last year. In short, Biden will sell the defense budget as a middle-of-the-road compromise\u2014which may mean it satisfies many in Congress but may also mean it satisfies few. Some Republicans have called for a 3 percent increase in the defense budget. Some Democrats have called for a substantial reduction.  Rep. Adam Smith , chairman of the House Armed Services Committee, has said that annual defense spending could be cut by as much as $100 billion, if smartly done. Biden\u2019s budget, released on Friday, contains only \u201ctop-line\u201d numbers (noting only how much he is requesting for each federal department and agency, with little in the way of detail) and only for \u201cdiscretionary\u201d spending (omitting mandatory programs such as Social Security and Medicare). This is normal for a new administration, which, barely two months into its term, has had little time to plunge into every item in the federal budget. The full, detailed request will be submitted to Congress later in the spring. Ideally, a budget debate should be less about how much we spend than what we buy. But in another sense, the theatrics about how much we spend are appropriate. The document does provide a few clues on where Biden wants to take the military. For instance, it cites \u201cthe need to counter the threat from China\u201d as the Pentagon\u2019s \u201ctop challenge.\u201d So U.S. Pacific Command can expect hefty funding for new combat planes, ships, and submarines. The budget also puts a premium on research and development into \u201cbreakthrough technologies\u201d for \u201cnext-generation defense capabilities\u201d\u2014and says these investments will be paid for by retiring old, costly-to-maintain weapons that don\u2019t address emerging threats. Many military analysts foresee a much-transformed battlefield in future wars between major powers\u2014one featuring hypersonic missiles, pilotless planes, cyberattacks, and systems powered by artificial intelligence. Some of these analysts now work in the Biden administration, and, to some degree, the budget reflects their concerns. There is also a hint that Biden will cut spending on nuclear weapons, but not as deeply as some Democrats would prefer. In the words of the budget document, the administration \u201cis reviewing the U.S. nuclear posture,\u201d but it \u201csupports ongoing nuclear modernization programs while ensuring that the efforts are sustainable.\u201d In plain English, this means Biden will fund some new nuclear weapons on the drawing board, but not all of them, as \u00a0many analysts, hawks and doves, have concluded that funding all of them\u2014an array of missiles, bombers, and submarines estimated to cost $300 billion over the next 30 years\u2014is not \u201csustainable.\u201d The budget states that Biden will fund new nuclear-missile submarines, a program that even most pro-disarmament groups support, since submarines\u2014which roam beneath the ocean\u2019s surface, undetectable, and therefore invulnerable to attack\u2014deter adversaries from even contemplating, much less launching, a nuclear first-strike. Even if an attack wiped out our land-based missiles and bombers, the submarine-launched missiles could deliver a devastating counterpunch. Nor will Biden be shutting down the production of nuclear warheads or materials\u2014which the Department of Energy handles. That section of the budget assures there will be money for \u201crecapitalization\u201d of the nuclear weapons labs and other facilities \u201cto ensure the [nuclear] deterrent remains viable.\u201d This is also fairly uncontroversial: Most legislators, as well as all the presidents in the past 75 years, believe that, as long as nuclear weapons exist, they should be kept safe and reliable. (Of Biden\u2019s $753 billion request, $715 billion goes to the Department of Defense; the rest is allocated to the Energy Department\u2019s National Nuclear Security Administration and a few other agencies doing military work.) It is a tossup which part of the budget will spark the most intense debates: the slight increase for the military or the much heftier increases for everything else. For instance, Biden is requesting a 23 percent increase for the Department of Health and Human Services, a 15 percent increase for Housing and Urban Development, a 16 percent increase for Interior, a 14 percent increase for the Department of Labor, a 12 percent increase for the State Department, a 14 percent increase for Transportation, a 26 percent increase for the Environmental Protection Agency, and a 20 percent increase for the National Science Foundation, to name a few. Either way, the budget debate in Congress has always been more theatrics than substance\u2014and this year, with the drastic shifts in spending, the theatrics will be more dramatic than usual. Already, Senate Minority Leader Mitch McConnell has said that Biden has to spend much more on defense in order to confront the threat from China\u2014this, before Biden has revealed how much of the budget is devoted to confronting China. Nor will the debate take much note of the National Intelligence Council\u2019s  Global Trends  report, released on Thursday, which concludes that national security depends at least as much on how well the U.S. adapts to climate change, global migration, the rise of tribalism, and other social, economic, demographic, and environmental phenomena as on precise measurements of the military balance. Ideally, a budget debate should be less about how much we spend than what we buy. But in another sense, the theatrics about how much we spend are appropriate. The top-line budget released on Friday expresses Biden\u2019s priorities\u2014which is what budgets are supposed to do. (Every federal department could make a case that it \u201crequires\u201d more money than it\u2019s getting.) So let\u2019s have a debate on priorities, on what kind of country we want to be\u2014and need to be, to survive and thrive in the decades ahead."}, "210705_news_466869.txt": {"page_id": "210705_news_466869.txt", "text": "NoSQL databases are often compared by various non-functional criteria, such as scalability, performance, and consistency. This aspect of NoSQL is well-studied both in practice and theory because specific non-functional properties are often the main justification for NoSQL usage and\u00a0fundamental results on distributed systems like the\u00a0 CAP theorem \u00a0apply well to NoSQL systems. \u00a0At the same time, NoSQL data modeling is not so well studied and lacks the systematic theory found in relational databases. In this article I provide a short comparison of NoSQL system families from the data modeling point of view and digest several common modeling techniques. I would like to thank\u00a0 Daniel Kirkdorffer \u00a0who reviewed the article and cleaned up the grammar. To \u00a0explore data modeling techniques, we have to start with a more or less systematic view of NoSQL data models that preferably reveals trends and interconnections. The following figure depicts imaginary \u201cevolution\u201d of the major NoSQL system families, namely, Key-Value stores, BigTable-style databases, Document databases, Full Text Search Engines, and Graph databases: NoSQL Data Models First, we should note that SQL and relational model in general were designed long time ago to interact with the end user. This user-oriented nature had vast implications: The end user is often interested in aggregated reporting information, not in separate data items, and SQL pays a lot of attention to this aspect. No one can expect human users to explicitly control concurrency, integrity, consistency, or data type validity. That\u2019s why SQL pays a lot of attention to transactional\u00a0guaranties, schemas, and referential integrity. On the other hand, it turned out that software applications are not so often interested in in-database aggregation and able to control,\u00a0at least in many cases, integrity and validity themselves. Besides this, elimination of these features had an extremely important influence on the performance and scalability of the stores.\u00a0And this was where a new evolution of data models began: Key-Value storage is a very simplistic, but very powerful model. Many techniques that are described below are perfectly applicable to this model. One of the most significant shortcomings of the Key-Value model is a poor applicability to cases that require processing of key ranges. Ordered Key-Value model overcomes this limitation and significantly improves aggregation capabilities. Ordered Key-Value model is very powerful, but it does not provide any\u00a0framework for value modeling. In general, value modeling can be done by an application, but BigTable-style databases go further and model values as a map-of-maps-of-maps, namely, column families, columns, and timestamped versions. Document databases advance the BigTable model offering two\u00a0significant improvements. The first one is\u00a0values with\u00a0schemes\u00a0of arbitrary complexity, not just a map-of-maps. The second one is database-managed indexes, at least in some implementations. Full Text Search Engines can be\u00a0considered a related species in the sense that they also offer flexible schema and automatic indexes. The main difference is that Document database group indexes by field names, as opposed to Search Engines that group indexes by field values. It is also worth noting that some Key-Value stores like Oracle Coherence gradually move towards Document databases via addition of indexes and in-database entry processors. Finally, Graph data models can be considered as a side branch of evolution that origins from the Ordered Key-Value models. Graph databases allow one model business entities very transparently ( this depends on that ), but hierarchical modeling techniques make other data models very competitive in this area too. Graph databases are related to Document databases because many implementations allow one model a value as a map or document. General Notes on NoSQL Data Modeling The rest of this article describes concrete data modeling techniques and patterns. As a preface, I would like to provide a few general notes on NoSQL data modeling: NoSQL data modeling often starts from the application-specific queries as opposed to relational modeling:\n Relational modeling is typically driven by the structure of available data. The main design theme is \u00a0\u201c What answers do I have?\u201d \u00a0 NoSQL data modeling is typically driven by application-specific access patterns, i.e. the types of queries to be supported. The main design theme is \u00a0\u201cWhat questions do I have?\u201d\u00a0 \u00a0 NoSQL data modeling often requires a deeper understanding of data\u00a0structures\u00a0and algorithms than relational database modeling does. In this article I describe several well-known data structures that are not specific for NoSQL, but are very useful in practical NoSQL modeling. Data duplication and denormalization are first-class citizens. Relational databases are not very convenient for hierarchical or graph-like data modeling and processing. Graph databases are obviously a perfect solution for this area, but actually most of NoSQL solutions are surprisingly strong for such problems. That is why the current article devotes a separate section to hierarchical data modeling. Although data modeling techniques are basically\u00a0implementation\u00a0agnostic, this is a list of the particular systems that I had in mind while working on this article: Key-Value Stores: Oracle Coherence, Redis, Kyoto Cabinet BigTable-style Databases: Apache HBase, Apache Cassandra Document Databases: MongoDB, CouchDB Full Text Search Engines: Apache Lucene, Apache Solr Graph Databases: neo4j, FlockDB Conceptual Techniques This section is devoted to the basic principles of NoSQL data modeling. (1) Denormalization Denormalization can be defined as the copying of the same data into multiple documents or tables in order to simplify/optimize query processing or to fit the user\u2019s data into a particular data model.\u00a0Most techniques described in this article leverage denormalization in one or another form. In general, denormalization is helpful for the following trade-offs: Query data volume \u00a0or  IO per query \u00a0VS\u00a0 total data volume . Using denormalization one can group all data that is needed to process a query in one place. This often means that for different query flows the same data will be accessed in different combinations. Hence we need to duplicate data, which increases total data volume. Processing complexity  VS  total data volume . Modeling-time normalization and consequent query-time joins obviously increase complexity of the query processor, especially in distributed systems. Denormalization allow one to store data in a query-friendly structure to simplify query processing. Applicability : Key-Value Stores, Document Databases, BigTable-style Databases (2) Aggregates All major genres of NoSQL provide soft schema capabilities in one way or another: Key-Value Stores and Graph Databases typically do not place constraints on values, so values can be comprised of arbitrary format. It is also possible to vary a number of records for one business entity by using composite keys. For example, a user account can be modeled as a set of entries with composite keys like  UserID_name, UserID_email, UserID_messages \u00a0and so on. If a user has no email or messages then a corresponding entry is not recorded. BigTable models support soft schema via a variable set of columns within a  column family  and a variable number of  versions  for one  cell . Document databases are inherently schema-less, although\u00a0some of them allow one to validate incoming data\u00a0using a user-defined schema. Soft schema allows one to form classes of entities with complex internal structures (nested entities) and to vary the structure of particular entities.This feature provides two major facilities: Minimization of one-to-many relationships by means of nested entities and, consequently, reduction of joins. Masking of \u201ctechnical\u201d differences between business entities and modeling of\u00a0heterogeneous business entities using one collection of documents or one table. These facilities are illustrated in the figure below. This figure depicts modeling of a product entity for an eCommerce business domain. Initially, we can say that all products have an ID, Price, and Description. Next, we discover that different types of products have different attributes like Author for Book or Length for Jeans. Some of these attributes have a one-to-many or many-to-many nature like Tracks in Music Albums. Next, it is possible that some entities can not be modeled using fixed types at all. For example, Jeans attributes are not consistent across brands and specific for each manufacturer. It is possible to overcome all these issues in a relational normalized data model, but solutions are far from elegant. Soft schema allows one to use a single Aggregate (product) that can model all types of products and their attributes: Entity Aggregation Embedding with denormalization can\u00a0greatly\u00a0impact updates both in\u00a0performance\u00a0and consistency, so special\u00a0attention should be paid to update flows. Applicability : Key-Value Stores,\u00a0Document Databases, BigTable-style Databases (3) Application Side Joins Joins are rarely supported in NoSQL solutions. As a consequence of the \u201cquestion-oriented\u201d NoSQL nature, joins are often handled at design time as opposed to relational models where joins are handled at query execution time. Query time joins almost always mean a performance penalty, but in many cases one can avoid joins using Denormalization and Aggregates, i.e. embedding nested entities. Of course, in many cases joins are inevitable and should be handled by an application. The major use cases are: Many to many relationships are often modeled by links and require joins. Aggregates are often inapplicable when entity internals are the subject of frequent modifications. It is usually better to keep a record that something happened and join the records at query time\u00a0as opposed to changing a value . For example, a messaging system can be modeled as a User entity that contains nested Message entities. But if messages are often appended, it may be better to extract Messages as independent entities and join them to the User at query time:\u00a0 Applicability : Key-Value Stores, Document Databases, BigTable-style Databases, Graph Databases General Modeling Techniques In this section we discuss general modeling techniques that applicable to a variety of NoSQL implementations. (4) Atomic Aggregates Many, although not all, NoSQL solutions have limited transaction support. In some cases one can achieve transactional behavior using distributed locks or  application-managed MVCC , but it is common to model data using an Aggregates technique to\u00a0guarantee some of the ACID properties. One of the reasons why powerful transactional machinery is an inevitable part of the relational databases is that normalized data typically require multi-place updates. On the other hand, Aggregates allow one to store a single business entity as one document, row or key-value pair and update it atomically: Atomic Aggregates Of course, Atomic Aggregates as a data modeling technique is not a complete transactional solution, but if the store provides certain\u00a0guaranties of atomicity, locks, or test-and-set instructions then Atomic Aggregates can be\u00a0applicable. Applicability : Key-Value Stores, Document Databases, BigTable-style Databases (5) Enumerable Keys Perhaps the greatest benefit of an unordered Key-Value data model is that entries can be partitioned across multiple servers by just hashing the key. Sorting makes things more complex, but sometimes an application is able to take some advantages of ordered keys even if storage doesn\u2019t offer such a feature. Let\u2019s consider the modeling of email messages as an example: Some NoSQL stores provide atomic counters that allow one to generate sequential IDs. In this case one can store messages using  userID_messageID \u00a0as a composite key. If the latest message ID is known, it is possible to traverse previous messages. It is also possible to traverse\u00a0preceding\u00a0and\u00a0succeeding messages for any given message ID. Messages can be grouped into buckets, for example, daily buckets. This allows one to traverse a mail box backward or forward starting from any specified date or the current date. Applicability : Key-Value Stores (6) Dimensionality Reduction Dimensionality Reduction is a technique that allows one to map\u00a0multidimensional data to a Key-Value model or to other non-multidimensional models. Traditional geographic information systems use some variation of a Quadtree or R-Tree for indexes. These structures need to be updated in-place and are expensive to manipulate when data volumes are large. An alternative approach is to traverse the 2D structure and flatten it into a plain list of entries. One well known example of this technique is a Geohash. A Geohash uses a Z-like scan to fill 2D space and each move is encoded as 0 or 1 depending on direction. Bits for longitude and latitude moves are interleaved as well as moves. The encoding process is illustrated in the figure below, where black and red bits stand for\u00a0longitude and latitude, respectively: Geohash Index An important feature of a Geohash is its ability to estimate distance between regions using bit-wise code proximity, as is shown in the figure. Geohash encoding allows one to store geographical information using plain data models, like sorted key values preserving spatial relationships. The Dimensionality Reduction technique for BigTable was described in [6.1].\u00a0More information about Geohashes and other related techniques can be found in [6.2] and [6.3]. Applicability : Key-Value Stores, Document Databases, BigTable-style Databases (7) Index Table Index Table is a very straightforward technique that allows one to take advantage of indexes in stores that do not support indexes internally. The most important class of such stores is the BigTable-style database. The idea is to create and maintain a special table with keys that follow the access pattern. For example, there is a master table that stores user accounts that can be accessed by user ID. A query that retrieves all users by a specified city can be supported by means of an additional table where city is a key: Index Table Example An Index table can be updated for each update of the master table or in batch mode. Either way, it results in an additional performance penalty and become a consistency issue. Index Table can be considered as an analog of materialized views in relational databases. Applicability : BigTable-style Databases (8) Composite Key Index Composite key is a very generic technique, but it is\u00a0extremely beneficial when a store with ordered\u00a0keys is used. Composite keys in conjunction with secondary sorting allows one to build a kind of multidimensional index which is\u00a0fundamentally similar to the previously described Dimensionality Reduction technique. For example, let\u2019s take a set of records where each record is a user statistic. If we are going to aggregate these statistics by a region the user came from, we can use keys in a format\u00a0 (State:City:UserID) \u00a0that allow us to iterate over records for a particular state or city if that store supports the selection of key ranges by a partial key match (as BigTable-style systems do): \nSELECT Values WHERE state=\"CA:*\"\nSELECT Values WHERE city=\"CA:San Francisco*\"\n Composite Key Index Applicability : BigTable-style Databases (9) Aggregation with Composite Keys Composite keys may be used not only for indexing, but for different types of grouping. Let\u2019s consider an example. There is a huge array of log records with information about internet users and their visits from different sites ( click stream ). The goal is to count the number of unique users for each site. This is similar to the following SQL query: \nSELECT count(distinct(user_id)) FROM clicks GROUP BY site\n We can model this situation using composite keys with a UserID prefix: Counting Unique Users using Composite Keys The idea is to keep all records for one user collocated, so it is possible to fetch such a frame into memory (one user can not produce too many events) and to eliminate site duplicates using hash table or whatever. An alternative technique is to have one entry for one user and append sites to this entry as events arrive. Nevertheless, entry modification is generally less efficient than entry insertion in the majority of\u00a0implementations. Applicability : Ordered Key-Value Stores, BigTable-style Databases (10) Inverted Search \u2013 Direct Aggregation This technique is more a data processing pattern, rather than data modeling. Nevertheless, data models are also impacted by usage of this pattern. The main idea of this technique is to use an index to find data that meets a criteria, but aggregate data using original representation or full scans. Let\u2019s consider an example. There are a number of log records with information about internet users and their visits from different sites ( click stream ). Let assume that each record contains user ID, categories this user belongs to (Men, Women, Bloggers, etc), city this user came from, and visited site. The goal is to describe the audience that meet some criteria (site, city, etc) in terms of unique users for each category that occurs in this audience (i.e. in the set of users that meet the criteria). It is quite clear that a search of users that meet the criteria can be efficiently done using inverted indexes like  {Category -> [user IDs]}  or  {Site -> [user IDs]} . Using such indexes, one can intersect or unify corresponding user IDs (this can be done very efficiently if user IDs are stored as sorted lists or bit sets) and obtain an audience. But describing an audience which is similar to an aggregation query like \nSELECT count(distinct(user_id)) ... GROUP BY category\n cannot be handled\u00a0efficiently using an inverted index if the number of categories is big.\u00a0To cope with this, one can build a direct index of the form  {UserID -> [Categories]}  and iterate over it in order to build a final report. This schema is depicted below: Counting Unique Users using Inverse and Direct Indexes And as a final note, we should take into account that random retrieval of records for each user ID in the audience can be inefficient. One can grapple with this problem by leveraging batch query processing. This means that some number of user sets can be precomputed (for different criteria) and then all reports for this batch of audiences can be computed in one full scan of direct or inverse index. Applicability : Key-Value Stores, BigTable-style Databases, Document Databases Hierarchy Modeling Techniques (11) Tree Aggregation Trees or even\u00a0arbitrary\u00a0graphs (with the aid of denormalization) can be modeled as a single record or document. This techniques is efficient when the tree is accessed at once (for example, an entire tree of blog comments is fetched to show a page with a post). Search and arbitrary access to the entries may be problematic. Updates are\u00a0inefficient in most NoSQL implementations (as compared to independent nodes). Tree Aggregation Applicability : Key-Value Stores, Document Databases \u00a0(12) Adjacency Lists Adjacency Lists are a straightforward way of graph modeling \u2013 each node is modeled as an independent record that contains arrays of direct ancestors or descendants. It allows one to search for nodes by identifiers of their parents or children and, of course, to traverse a graph by doing one hop per query. This approach is usually inefficient for\u00a0getting an entire subtree for a given node, for deep or wide traversals. Applicability : Key-Value Stores, Document Databases (13) Materialized Paths Materialized Paths is a technique that helps to avoid recursive traversals of tree-like\u00a0structures. This technique can be considered as a kind of denormalization. The idea is to attribute each node by identifiers of all its parents or children, so that it is possible to determine all descendants or predecessors of the node without traversal: Materialized Paths for eShop Category Hierarchy This technique is especially helpful for Full Text Search Engines because it allows one to convert hierarchical structures into flat documents. One can see in the figure above that all products or subcategories within the\u00a0 Men\u2019s Shoes  category can be retrieved using a short query which is simply a category name. Materialized Paths can be stored as a set of IDs or as a single string of concatenated IDs. The latter option allows one to search for nodes that meet a certain partial path criteria using regular expressions. This option is illustrated in the figure below (path includes node itself): Query Materialized Paths using RegExp Applicability : Key-Value Stores, Document Databases, Search Engines (14) Nested Sets Nested sets  is a standard technique for modeling tree-like structures. It is widely used in relational databases, but it is perfectly applicable to Key-Value Stores and Document Databases. The idea is to store the leafs of the tree in an array and to map each non-leaf node to a range of leafs using start and end indexes, as is shown in the figure below: Modeling of eCommerce Catalog using Nested Sets This structure is pretty efficient for immutable data because it has a small memory footprint and allows one to fetch all leafs for a given node without traversals. Nevertheless, inserts and updates are quite costly because the addition of one leaf causes an extensive update of indexes. Applicability : Key-Value Stores, Document Databases (15)\u00a0Nested Documents Flattening: Numbered Field Names Search Engines typically work with flat documents, i.e. each document is a flat list of fields and values. The goal of data modeling is to map business entities to plain documents and this can be challenging if the entities\u00a0have a complex internal structure. One typical challenge mapping documents with a hierarchical structure, i.e. documents with nested documents inside. Let\u2019s consider the following example: Nested Documents Problem Each business entity is some kind of resume. It contains a person\u2019s name and a list of his or her skills with a skill level. An obvious way to model such an entity is to create a plain document with  Skill  and  Level  fields. This model allows one to search for a person by skill or by level, but queries that combine both fields are liable to result in false matches, as depicted in the figure above. One way to overcome this issue was suggested in [4.6]. The main idea of this technique is to\u00a0index each skill and corresponding level as a dedicated pair of fields\u00a0 Skill_i \u00a0and\u00a0 Level_i,\u00a0 and to search for all these pairs\u00a0simultaneously (where the number of OR-ed terms in a query is as high as the maximum number of skills for one person): Nested Document Modeling using Numbered Field Names This approach is not really scalable because query complexity grows rapidly as a function of the number of nested structures. Applicability : Search Engines (16) Nested Documents Flattening: Proximity Queries The problem with nested documents can be solved using another technique that were also described in [4.6]. The idea is to use proximity queries that limit the acceptable distance between words in the document. In the figure below, all skills and levels are indexed in one field, namely, SkillAndLevel, and the query indicates that the words \u201cExcellent\u201d and \u201cPoetry\u201d should follow one another: Nested Document Modeling using Proximity Queries [4.3] describes a success story for this\u00a0technique used on top of Solr. Applicability : Search Engines (17) Batch Graph Processing Graph databases like neo4j are exceptionally good for exploring the neighborhood of a given node or exploring relationships between two or a few nodes. Nevertheless, global processing of large\u00a0graphs is not very efficient because general purpose graph databases do not scale well. Distributed graph processing can be done using MapReduce and the Message Passing pattern that was described, for example, in  one of my previous articles . This approach makes\u00a0Key-Value stores, Document databases, and BigTable-style databases suitable for processing large graphs. Applicability : Key-Value Stores, Document Databases, BigTable-style Databases References Finally, I provide a list of useful links related to NoSQL data modeling: Key-Value Stores:\n http://www.devshed.com/c/a/MySQL/Database-Design-Using-KeyValue-Tables/ http://antirez.com/post/Sorting-in-key-value-data-model.htm l http://stackoverflow.com/questions/3554169/difference-between-document-based-and-key-value-based-databases http://dbmsmusings.blogspot.com/2010/03/distinguishing-two-major-types-of_29.html BigTable-style Databases:\n http://www.slideshare.net/ebenhewitt/cassandra-datamodel-4985524 http://www.slideshare.net/mattdennis/cassandra-data-modeling http://nosql.mypopescu.com/post/17419074362/cassandra-data-modeling-examples-with-matthew-f-dennis http://s-expressions.com/2009/03/08/hbase-on-designing-schemas-for-column-oriented-data-stores/ http://jimbojw.com/wiki/index.php?title=Understanding_Hbase_and_BigTable Document Databases:\n http://www.slideshare.net/mongodb/mongodb-schema-design-richard-kreuters-mongo-berlin-preso http://www.michaelhamrah.com/blog/2011/08/data-modeling-at-scale-mongodb-mongoid-callbacks-and-denormalizing-data-for-efficiency/ http://seancribbs.com/tech/2009/09/28/modeling-a-tree-in-a-document-database/ http://www.mongodb.org/display/DOCS/Schema+Design http://www.mongodb.org/display/DOCS/Trees+in+MongoDB http://blog.fiesta.cc/post/11319522700/walkthrough-mongodb-data-modeling Full Text Search Engines:\n http://www.searchworkings.org/blog/-/blogs/query-time-joining-in-lucene http://www.lucidimagination.com/devzone/technical-articles/solr-and-rdbms-basics-designing-your-application-best-both http://blog.griddynamics.com/2011/07/solr-experience-search-parent-child.html http://www.lucidimagination.com/blog/2009/07/18/the-spanquery/ http://blog.mgm-tp.com/2011/03/non-standard-ways-of-using-lucene/ http://www.slideshare.net/MarkHarwood/proposal-for-nested-document-support-in-lucene http://mysolr.com/tips/denormalized-data-structure/ http://sujitpal.blogspot.com/2010/10/denormalizing-maps-with-lucene-payloads.html http://java.dzone.com/articles/hibernate-search-mapping-entit Graph Databases:\n http://docs.neo4j.org/chunked/stable/tutorial-comparing-models.html http://blog.neo4j.org/2010/03/modeling-categories-in-graph-database.html http://skillsmatter.com/podcast/nosql/graph-modelling http://www.umiacs.umd.edu/~jimmylin/publications/Lin_Schatz_MLG2010.pdf Demensionality Reduction:\n http://www.slideshare.net/mmalone/scaling-gis-data-in-nonrelational-data-stores http://blog.notdot.net/2009/11/Damn-Cool-Algorithms-Spatial-indexing-with-Quadtrees-and-Hilbert-Curves http://www.trisis.co.uk/blog/?p=1287 Like this: Like   Loading... Related  "}, "210705_news_466871.txt": {"page_id": "210705_news_466871.txt", "text": "We are looking for a dynamic, experienced customer success professional to ensure we are building strong and enduring relationships with our customers, and that they are receiving the attention and support needed to achieve their goals. About Us Etleap\u2019s mission is to transform the way businesses drive analytics projects. We started Etleap out of frustration with the exorbitant amounts of engineering work required to set up, maintain, and scale data pipelines for analytics. Our vision is to make it simple for analytics teams to quickly and securely connect the data sources that power their businesses. We\u2019re doing this by removing all of the traditional complexities of data integration and creating a new kind of ETL that enables the end user of data to integrate and transform data from any source. Our team is customer-obsessed and values openness and being experimental. We\u2019re growing fast, and we\u2019re backed by First Round Capital, SV Angel, YCombinator, and a number of other top-tier investors. About You - 5+ years of customer success/account management experience\n- Have led onboarding and customer education for a technical enterprise software product\n- Experience building relationships with technically sophisticated customers\n- Exceptional verbal, written, and interpersonal communication skills\n- Self-starter: Able to propose solutions and accomplish tasks with limited direction and a figure-it-out attitude\n- Startup experience is a big plus About the Role - Build relationships and drive engagement with Etleap\u2019s customers via onboarding, periodic business reviews, and other check-ins\n- Champion customers\u2019 growth goals in order to maximize product value and identify upsell opportunities\n- Manage a customer health dashboard to be shared with the executive team\n- Plan and optimize strategy for managing retention and flagging risks regarding renewals\n- Own communication with customers for marketing initiatives, including trade shows, webinars, and case studies You will be working as part of a dynamic team in a fast-paced startup office environment. Our office is near AT&T Park at 2nd & Townsend in San Francisco. We offer catered lunch, a kitchen stocked with great snacks, as well as unlimited PTO, health care, and commuter benefits. How to Apply To apply, send your resume to jobs@etleap.com."}, "210705_news_466896.txt": {"page_id": "210705_news_466896.txt", "text": "One of the biggest tour operators in the UK has postponed all holidays until 23 June, blaming ongoing lack of clarity from the government. Jet2.com and Jet2Holidays is the first tour operator to react to the  Global Travel Taskforce\u2019s framework  by further suspending its flights and holidays. On Friday the government confirmed that  foreign holidays to some destinations will be possible from 17 May . Countries will be  ranked green, amber or red  based on a number of criteria such as vaccinations, infection rates and the prevalence of \u201cvariants of concern\u201d, but the government said it was too early to reveal which countries will be on the list. Jet2 CEO Steve Heapy said he was \u201cextremely disappointed at the lack of clarity and detail\u201d in the proposals outlined by the taskforce. \u201cIn fact, the framework is virtually the same as six months ago,\u201d he said, adding, \u201cFollowing the publication of the framework today, we still do not know when we can start to fly, where we can fly to and the availability and cost of testing. Rather than answering questions, the framework leaves everyone asking more.\u201d Data firm Globaldata warned that Jet2\u2019s decision could further dent already fragile consumer confidence in overseas travel. \u201cThe industry could see a similar move from Jet2.com\u2019s competitors, as confidence in booking for this period could decline if one operator has already suspended operations.\u201d According to a consumer survey carried out by Globaldata on Friday, 30% of respondents are not willing to travel. The travel and aviation sectors have also  warned that the cost of PCR tests  will be a major disincentive for consumers considering a holiday abroad. New research from the Association of British Travel Agents (Abta) and the Airport Operators Association (AOA) shows that the cost of PCR testing for international travel in the UK is double that of testing in some other European countries. It found that individuals travelling from the UK to a green country in Europe under the new traffic light system announced today would have to pay an average of \u00a3306 for testing, as they will need a UK pre-departure test, a pre-departure test in their destination country at the end of their holiday and a post-arrival test once back in the UK.. Abta and AOA are urging the government to reconsider testing requirements. \u201cSmall changes, like requiring a PCR test only if the individual gets a positive result from a lateral flow test, would make international travel more accessible and affordable while still providing an effective mitigation against reimportation of the virus,\u201d said Abta chief executive Mark Tanzer. \u201cThe government should also consider whether those who have been vaccinated can be exempted from testing requirements, should scientific evidence suggest reduced transmissibility.\u201d "}, "210705_news_466899.txt": {"page_id": "210705_news_466899.txt", "text": "To fill some time  during compiling  I tried to get a VNC client running on a coffee machine, specifically to show MS Teams. At work I develop software for these coffee machines in C++, which allows me to do such fun stuff, because from a software point of view, it's just an ARM PC running linux with a framebuffer for graphics. I compiled a few framebuffer VNC clients, fired up an SSH tunnel and used  x11vnc  to share one specific window and after a few attempts, Teams was up and running on my 'new' second monitor.\nThis post contains my little adventure in framebuffer VNC clients, but it's not a comprehensive guide as most of my other articles. Showing you how to set up an Openembedded server with a VariScite specific toolchain is way too much work to cross-compile a simple C program, but since that's my day job, why not use it for fun. It contains some tips for  x11vnc  and shows you two different framebuffer VNC clients,  fbvnc  and  directvnc . Microsoft Teams running on a coffee machine?!?   Consider sponsoring me on Github. It means the world to me if you show your appreciation and you'll help pay the server costs.   You can also sponsor me by getting a Digital Ocean VPS. With this referral link you'll get $100 credit for 60 days.    Important disclaimer: this was a fun side project, neither acknowledged nor endorsed by\nmy employer. Their coffee machines can not run custom software under normal circumstances. The instructions in this post are applicable to any linux framebuffer device.\nIt could be a raspberry pi or even your own machine. Here is another picture with a keyboard attached, input works with fbvnc. The\nmouse is optional since the touchscreen works as well in fbvnc. I did not test\nuser input (mouse/keyboard) much more,  because I intended to use the device\nas a view only screen. Also, my spare board has a broken touch\ncontroller/screen, which is why I can re-use it as a second screen, otherwise\nit would be thrown out. For local development, I sometimes do it the reverse way around, run the\ncoffee machine UI on my  local machine . Manual testing is a rare\noccurrence, since by far a  large part of our codebase is covered by unit\ntests and there is an extensive HTTP API. But still, sometimes there is a need\nfor local testing, we have stubs, can run the QT UI locally and even  the\nflash UI  still runs with some hoops. And of course a bunch of coffee\nmachines to test with, but who likes walking and manually pressing buttons if\nyou can automate that stuff away? This is one of the Frankenstein just-enough-coffee-machines that serves\nas a second screen: Second screen? So why not get an actual second screen? Well, I personally use just one\nscreen, because I try to do one thing at a time. That one thing usually\nrequires all of my focus and attention, so a second screen with all kinds of\ndistractions doesn't really work for me. However, infrequently, I miss a second screen. During chat sessions or video\ncall's where you're pair programming or to have a datasheet open. In such\ncases I would like a small screen which is easy to turn off when it's not\nneeded anymore. A friend of mine is a video editor and he has a separate 5\" screen, for use\nwith Final Cut. That thing was expensive (due to colour correctness) and on\nAliExpress I could only find  these things, still expensive .  Then it occurred, our coffee machines all have 7\" touch screens, they run\nlinux and I have a bunch of Frankenstein just-enough-coffee-machines on my\ndesk for testing. Why not use such a screen for the occasional chat window? The machines run either  gnash  with a flash UI, a  qt  UI or a  headless\nUI for touchless , which uses MQTT and an internal HTTP API. The QT ui also\nuses the HTTP API, flash still runs a custom communication protocol. Both QT and Gnash run on the framebuffer, there is no X server, so a regular\nVNC client will not work. Or the actual teams binary for that matter. x11vnc one window and a reverse SSH port forward I use the following command to start a VNC server on  localhost  only \n(because it runs without a password,  fbvnc  does not support that). The \nspecial thing is that that command allows me to pick one window which is shared\nover VNC, not the entire desktop or a screen region. x11vnc -id pick -viewonly -forever -localhost -geometry 800x600 -scale 800x600 -noxdamage\n The  -geometry  and  -scale  parameters are given because the machine screen\nhas that resolution. It looks less weird that way. Since the VNC server is only accessible via localhost, I use SSH to setup\na secure tunnel to the coffee machine and forward my machine's VNC\nport to the coffee machine: ssh -R 5900:127.0.0.1:5900 user@IP.OF.COFFEE.MACHINE\n The coffee machines normally do not have SSH running or exposed. For local\ndevelopment we can turn it on, but it involves a signed package and UART. Now, as far as the coffee machine is concerned, it has a port on  localhost:5900  \nwhich is also a working VNC  display:0 . Great! Let's continue on to VNC clients. Both VNC clients for the framebuffer were cross-compiled with out specific toolchain,\ninstructions for that are not really applicable elsewhere. On a raspberry pi or \nother ARM board, you can just install  gcc   ( apt install build-essential ) and\nfollow along. directvnc The first client I found was  directvnc . Over 9 years old, seemingly\ndead, but  I could find an  openembedded   recipe  for it which worked\nright away, so that was easy.  Too easy it appeared, because it works partly.\nAs you can see on the image below,  one half of the screen was black  and\nthe colours are a bit off on the other (working) half: I  filed a github issue  but I don't expect it will be fixed, since the\nproject seems dead. The curious thing was that other code using  directfb  did\nwork on the entire  screen, no issues  running the examples .  directvnc \nused  directfb  for the framebuffer rendering. I tried all the possible options in  /etc/directfbrc  and took a quick look at\nthe code, but didn't find anything to make it work. And since it's a side\nproject in my spare time, I decided to move on to the next client I found,\n fbvnc .  If you're compiling yourself, you can clone the repository ( git clone\nhttps://github.com/drinkmilk/directvnc ) and run  autoreconf -vfi   then\n automake , then  ./configure  and finish off with  make . There should be a \nnew binary in that folder. Make sure you have  directfb  installed as\nwell, it's a dependency. On Debian (and Raspbian) you can install  directvnc   as a package , no \ncompilation required: apt install directvnc\n fbvnc fbvnc  is a smaller, more bare-bones tool. The old Debian  webpages are of course\noffline , but archive.org still has copies. However, to my surprise, the \n git repository has recent commits  from its author,  Ali Gholami Rudi - his homepage !  A  git clone  and  make fbvnc  in my cross-compile setup later I have a \nworking binary. The connection failed at first, because I had password\nprotected it. After removing the password, it worked right away. Way easier\ncompilation setup (no  autotools ) and no dependency on  directfb , saves\na lot of work. fbvnc  hard codes the host  127.0.0.1  and the port  5900 , but you\ncan specify others on the command line: fbvnc hostname port\n There is no help or instructions, just a  connection failed  or   init\nfailed . The source is small and readable so figuring out what happens when is\neasy. After you start it,  CTRL+C  to quit doesn't work, to stop it you must \nstop  x11vnc .  Here is a screenshot of my entire KDE desktop running (scaled_ on the coffee\nmachine, via VNC: How does teams run? Performance wise, it's better then I expected. A tad bit laggy, but usable and\neven for video calls, not that much stuttering. Since it's all running on my\nworkstation and only the rendering is on the coffee machine, I didn't expect\nany less. If I have some more spare time, I'll try to make a video sometime\nshowing the  performance. For an occasional second screen, it's just perfect. \nTags:  autotools \n,  blog \n,  development \n,  framebuffer \n,  fun \n,  linux \n,  microsoft \n,  vnc"}, "210705_news_466992.txt": {"page_id": "210705_news_466992.txt", "text": "In mid-February, three travelers were stopped at the airport in Luanda, Angola. Even during the pandemic, the country, a hub for the oil industry, had seen plenty of passengers from Europe and South Africa, where two concerning variants of the virus that causes COVID-19 hold sway. But the strains weren\u2019t yet circulating widely in Angola, so this winter, health officials battened down the hatches. Before any passports get stamped, travelers receive a rapid antigen test and wait 30 minutes for a result. A negative test means self-quarantine, followed by another test a few days later. A positive test means a two-week stay at a quarantine hotel. For the three travelers, it was option two. A few weeks later, samples taken from their noses arrived 2,000 miles south in South Africa, at the lab of Tulio de Oliveira, a geneticist at the University of KwaZulu-Natal. He was in for a surprise. The virus that had infected these three travelers didn\u2019t resemble the strains circulating in most other places, including those labeled as \u201cvariants of concern\u201d for their ability to spread faster and evade certain types of immunity. If those variants are like siblings, this one was more like a forgotten second cousin. It came from a lineage of the virus that emerged in the early days of the pandemic but had disappeared soon thereafter, apparently outcompeted by other variants. And yet here it was, a year later. And it had been busy. The virus had since accumulated dozens of mutations, including many of the same ones that made those other strains worrisome because of increased transmissibility and immune evasion. It had arrived at a similar genetic conclusion all on its own. A scarcity of data The new variant seemed to have stepped out of an epidemiological void. Which, in a way, it had, because the travelers had arrived from a country where the pandemic did not officially exist. Last June, the president of Tanzania, John Magufuli,  declared the country COVID-free , having rid itself of the virus through three days of national prayer. Since then,  reports from doctors and nonprofits  within the country told of a \u201chidden epidemic\u201d raging as fiercely as it was anywhere. But the government\u2019s data had evaporated: no tests or case numbers or genome sequences. With so little information\u2014just three genomes\u2014it was hard to say what this new variant meant. Where had it come from, and where were its closer relatives? Was it spreading widely, or were these cases just a fluke? Were its mutations as worrisome in practice as on paper? De Oliveira and his colleagues are now racing to answer those questions. Such surprises are somewhat common in de Oliveira\u2019s line of work. Since the pandemic began, African labs have uploaded fewer than 12,000 genomes to GISAID, the leading database for viral sequences, compared with 280,000 from North America, a continent with less than half the population. About half of those African genomes come from South Africa, where de Oliveira\u2019s lab is the centerpiece of a national sequencing effort. That means there are plenty of gaps to be explored. \u201cIt\u2019s concerning,\u201d says Emma Hodcroft, a molecular epidemiologist at the University of Bern. \u201cIt\u2019s a huge continent, and we know that there are COVID outbreaks happening. But, apart from South Africa, we don\u2019t have a good idea of what\u2019s happening anywhere else.\u201d In recent months, De Oliveira has been working to change that. In early December, the lab\u2019s genomic sleuthing amidst a surge of cases in South Africa led to the identification of a strain now known as B.1.351. That variant is now spreading globally, causing headaches because it is more resistant to the protection of some vaccines. It was also a wake-up call for South Africa\u2019s neighbors. So earlier this year, de Oliveira\u2019s lab, in partnership with the Africa Centres for Disease Control and Prevention, began receiving weekly or biweekly samples from 10 countries in southern Africa, part of an effort to track the newly uncovered variant and others around the continent. A second lab, Nigeria\u2019s Africa Centre of Excellence for Genomics of Infectious Diseases, or ACEGIP, handles the northern half of the continent. The research from Angola, which was co-led by the country\u2019s health minister, Silvia Lutucuta,  appeared as a preprint Monday  and has not yet been peer reviewed. In the past year, emerging variants have changed the calculus of the pandemic, forcing countries back into lockdowns and to reconsider vaccine strategies. Basically,  it\u2019s now a race : getting shots to more people will help quell the variants\u2019 spread and slow the emergence of new ones. But in Africa, where only a few countries have so far received  a trickle of vaccines , that process is expected to take longer. And as the virus continues to replicate and spread among people, it will also keep changing\u2014with implications for the whole world. \u201cIt\u2019s going to be bumpy,\u201d says Christian Happi, ACEGIP\u2019s director. \u201cWithin the continent, we have found a number of major variants, and there are likely many more.\u201d It\u2019s not unusual for African states to work together to stifle epidemics, he notes. Not every country has access to the sequencing machines that crunch these genomes quickly, and those that do are often relying on a single commercial lab. So governments and labs have learned to collaborate, forming networks that make use of advanced sequencing centers like his and de Oliveira\u2019s to tackle emergent diseases, rather than sending samples overseas. So far in 2021, the initiative has helped double the number of viral genomes sequenced in Africa compared with all of 2020, with a goal of producing 50,000 genomes by year\u2019s end. Even as the capacity to sequence picks up, the process remains challenging, Happi says. A high rate of  asymptomatic cases  and limited health care access means the COVID-19 tests that lead to genome sequencing are limited in some areas. And it\u2019s not easy to gather and store samples from across a country like Somalia and send them to Nigeria, via multiple planes and handlers, while keeping them perfectly intact. From a few hundred samples in a recent delivery from Mogadishu, the lab retrieved complete sequences from only 10 of them. Epidemics within a pandemic One way of thinking about SARS-CoV-2 variants is as a series of epidemics within the pandemic. When variants  first emerge , or when they arrive for the first time in a new place, they\u2019re like embers, ready to catch fire if the opportunity to spread arises and if their mutations make them competitive with other strains. But embers are also easier to extinguish than widespread conflagrations. Variants can be stopped at borders, and outbreaks in hot spots can be identified and quashed\u2014provided variant hunters  move fast and cast a wide net . \u201cWe need consistent and quick turnaround, because these variants tend to dominate quickly,\u201d de Oliveira says. \u201cYou don't want to discover six months late that you have an epidemic of a strain that escapes vaccines.\u201d The type of border checks being done in Angola, a response to surges linked to variants found in nearby countries, is a good example of putting surveillance into action, de Oliveira says. Samples from the airport have turned up not only the new strain, but plenty of examples of B.1.351 and B.1.1.7, the variants of concern first identified in South Africa and the United Kingdom and now circulating worldwide. He thinks catching those kinds of cases early is a crucial part of why Angola didn\u2019t experience the same surge its neighbors did at the beginning of this year. Conducting surveillance at travel hubs also increases overall coverage; the researchers had no way of doing genomic surveillance in Tanzania, for example, until those three travelers happened upon the border check. Even when worrisome variants take hold, the ability to track them has bearing on what public health measures officials can take. \u201cSequencing really helps because you understand the patterns of human migration for a variant,\u201d Happi says. In Nigeria this winter, for example, the government grew concerned  about a surge  of unknown origin. It was impossible at first to tell if the virus was spreading faster, or if human behavior was the cause. Genome sequencing revealed it was driven by B.1.1.7, the variant that was first identified in the United Kingdom, allowing health officials to identify hot spots and, importantly, give the public an explanation for why it was necessary to hunker down. Similarly, when researchers at the Uganda Virus Research Institute  identified a novel variant  circulating there, surveillance led to more testing in prisons and on cross-country trucking routes, where the strain was found to be most densely concentrated. What has shocked researchers about the variant identified in the Tanzanian travelers is that it is so distantly related to other variants of concern. It\u2019s a member of the so-called \u201cA lineage\u201d\u2014sometimes dubbed the \u201c19 lineage\u201d since it appeared in 2019\u2014and is the closest known relative to the virus that  first spilled into humans . \u201cMy postdoc sent me a Slack message saying, \u2018WTF the A lineage??\u2019\u201d says Bill Hanage, an epidemiologist at Harvard University who studies viral evolution. Variants of the A lineage are still picked up from time to time, but by early 2020, most of them had been outcompeted by members of the still-reigning B lineage. The finding underlines the power of human networks in how viruses spread, Hanage adds. B-lineage variants clearly acquired mutations that made them fit to spread across the world, but what if they also got boosted by luck? It\u2019s possible that viruses of the B lineage simply happened to take root early on in densely populated places like New York City and Italy, and from there they took over the world."}, "210705_news_466993.txt": {"page_id": "210705_news_466993.txt", "text": "Blizzard / Sam Machkovech Diablo II: Resurrected  is slated to launch on PC and consoles \"later this year,\" but in the meantime, the remaster is far enough along that its handlers launched an early \"technical alpha\" demo over this weekend. (There's a teensy-tiny chance you can still get in if you sign up right now.) I was invited for the single-player test's first wave\u2014and took the opportunity to stream my initial gameplay via Twitch. Should you watch  that three-hour session , you'll see my largely positive reaction at first blush. (Once some initial online-check stupidity cleared up, at least.) Afterwards, I took a moment to breathe, have a snack, and install the game on other systems in order to do some more full-fledged testing. Even outside that first-look afterglow of seeing  D2:R  running beautifully on a modern PC, the results thus far\u2014of an admittedly unfinished preview version\u2014have charmed me even further than my first session might have led you to believe. The \"Blizzard Classic\" team is currently walking on a long road into hell, and that road just might be paved with redemption. 2D to 3D: Can Blizzard do it? Diablo II: Resurrected  after-and-before gallery, all captured from the April 2021 technical alpha. Updated 4K graphics.                     \n                      Same shot, classic 600p graphics. (The 600p images are all 4:3, if the blockier images weren't obvious enough.)                     \n                                              Blizzard                                           \n                      Updated 4K graphics.                     \n                                              Blizzard                                           \n                      Same shot, classic 600p graphics.                     \n                                              Blizzard                                           \n                      Updated 4K graphics, after tapping the \"F\" button to super-zoom on this dead demon.                     \n                                              Blizzard                                           \n                      Same shot, classic 600p graphics. (Or at this super-zoom, I suppose it's, what, 60p?)                     \n                                              Blizzard                                           \n                      Updated 4K graphics.                     \n                                              Blizzard                                           \n                      Same shot, classic 600p graphics.                     \n                                              Blizzard                                           \n                      Updated 4K graphics.                     \n                                              Blizzard                                           \n                      Same shot, classic 600p graphics.                     \n                                              Blizzard                                           \n                      Updated 4K graphics. Yes, that's Deckard Cain.                     \n                                              Blizzard                                           \n                      Same eager Cain conversation, classic 600p graphics.                     \n                                              Blizzard                                           \n                      Updated 4K graphics.                     \n                                              Blizzard                                           \n                      Same shot, classic 600p graphics.                     \n                                              Blizzard                                           A quick summary: in terms of content,  D2:R  is a gussied-up  Diablo II , paired with its  Lord of Destruction  expansion and updated to run on modern platforms. Every original class, quest, biome, monster, AI routine, dungeon-generation algorithm, song, and more returns here with both single- and multiplayer options. Your muscle memory of the original game should translate perfectly to this remaster. The  D2:R  sales pitch begins with a pledge to bolt all of its new features and polish on top of the 2000 game's original source code. Thanks to some uneven Blizzard Classic history, I didn't know how that would work out.  StarCraft Remastered  pulled the trick off beautifully, since the remastered content was all higher-res 2D sprites, while  WarCraft III: Reforge d's fumbles  included trying to do the same with brand-new 3D content\u2014in unbecoming ways that most players immediately disabled. Diablo II  is an entirely 2D game, but this update includes a ton of 3D assets. Is that a recipe for disaster? In  D2:R 's case, not at all. I became optimistic earlier this year when I learned that Vicarious Visions took over lead development duties on  D2:R . That studio has established a hot streak with game remasters, especially in terms of preserving original artistic intent while adding modern graphical flourishes as appropriate. The same can be said for  D2:R . Your inner glow doesn\u2019t ruin  D2:R \u2019s outer shine At any time, players can tap a keyboard shortcut (by default, bound to \"G\") to switch between the game's original presentation (800x600 resolution, 4:3 ratio) and modern displays (including arbitrary, ultrawide options). It's a fun party trick, and it guarantees that at any time in the game, if you think Vicarious Visions got the game's HD vision wrong, you can take graphical matters into your own hands. What I love thus far with  D2:R 's technical alpha is its clear demonstration that Vicarious Visions\u00a0 deeply  cares for accuracy. Every biome's color tone mapping, in particular, matches almost 1:1 with the original source material\u2014and the devs have somehow nailed this while also including a much more reactive, modern take on dynamic lighting. At any time, regions both above- and underground can be bathed in additional light, whether from your \"inner glow\" or from weather effects.  D2:R  is generally careful to have those color touches appear as mild highlights. Thus, when you compare old graphics to new, you'll sometimes see a redder or bluer tint on a world element, but not in a way that denies those assets their original tone-neutral treatment. If Vicarious Visions had gotten this wrong, I might've switched to the old graphics and never looked back.  Diablo II  was a legendarily good game at establishing a \"dark fantasy\" aesthetic without stripping its world of organic and vibrant color. It hit the sweet spot between the gray-and-brown default of  Diablo  and the too-saturated extremes of  Diablo III \u2014and in previews of  Diablo IV , you can already see that Blizzard art team reverting to what  Diablo II  got so very right in 2000. Vicarious Visions clearly got the same art-direction memo, and thank goodness. Higher-tech visual touches like puddle reflections and rapidly dancing shadows look gorgeous, but they're also grounded in the game's core color-palette philosophy, and that means they don't wear out their splashy welcome. Also, it's interesting to see Vicarious Visions pick and choose how dramatically to change game's most dramatic moments. Maybe an original, massive puddle of blood is now a more understated, red-soaked slab of concrete. Maybe a burning house has been entirely remodeled to make room for more flames. Or maybe a cinematic burst of electricity looks remarkably similar to the 2000 original, in terms of wacky zigzag lines filling the screen. When a scene has been redone with modern rendering techniques, it's usually for the better; walls of flame were originally broken up into separate graphic blocks, but they now combine seamlessly, and I'm fine with that artistic license. But for the most part, the new game recreates original scenes as much as possible, like when light shafts fill a \"purified\" cave or when enemies explode."}, "210705_news_467004.txt": {"page_id": "210705_news_467004.txt", "text": "Et tu, Procter &\u00a0Gamble? Friday, 9 April 2021 Sharon Terlep, Tim Higgins, and Patience Haggin, reporting for The Wall Street Journal,  \u201cP&G Worked With China Trade Group on Tech to Sidestep Apple Privacy Rules\u201d  ( News+ link ): Procter & Gamble Co. helped develop a technique being tested in\nChina to gather iPhone data for targeted ads, a step intended to\ngive companies a way around Apple Inc.\u2019s new privacy tools,\naccording to people familiar with the matter. [\u2026] The company has joined forces with dozens of Chinese trade groups\nand tech firms working with the state-backed China Advertising\nAssociation to develop the new technique, which would use\ntechnology called device fingerprinting, the people said. Dubbed\nCAID, the advertising method is being tested through apps and\ngathers iPhone user data. Through the use of an algorithm, it can\ntrack users for purposes of targeting ads in a way that Apple is\nseeking to prevent. [\u2026] Through apps, CAID collects user device data, such as the device\nstart-up time, model, time zone, country, language and IP address.\nBased on China\u2019s personal information security standards, most of\nthose data aren\u2019t counted as \u201cpersonal information.\u201d But a\nso-called device ID can be generated by algorithm based on these\ndata. That device ID can achieve a similar tracking effect as the\nidentifier that Apple is allowing users to block. Not a good look for a major American company like Procter & Gamble to be in cahoots with a Chinese trade group to circumvent Apple\u2019s new privacy rules. The whack-a-mole 1  aspect of Apple\u2019s new privacy rules is that while Apple can restrict access to the API that provides access to the  IDFA  identifier, clever developers can find (perhaps infinite) other ways to combine things they  do  have access to into a unique, or even just \u201cclose enough to unique to be useful for tracking\u201d, identifier. IP addresses, to name just one example, are a big factor that Apple can\u2019t block would-be-trackers from using. That\u2019s what CAID is, but CAID isn\u2019t some rogue effort on the part of surveillance advertisers alone\u2009\u2014\u2009it has the backing of the Chinese government. Doing this is clearly against Apple\u2019s rules. The questions are: Can Apple detect these techniques? And what is Apple going to do if they do identify apps in China using CAID in flagrant violation of the App Store rules, if those apps have the backing (implicit or explicit) of the Chinese government? Consider just Tencent. What is Apple going to do if  WeChat  is flagged for circumventing the App Store privacy rules, and Tencent says \u201cNo thank you\u201d to Apple\u2019s rules, that they\u2019re going to do it anyway because they have the backing of the PRC? Reading between the lines, I  think  Apple is diplomatically telling the companies involved with CAID that they will pull the apps from the App Store over this. Here\u2019s Apple\u2019s statement to The Journal: Device fingerprinting runs afoul of Apple\u2019s rules, and the tech\ncompany has said it would ban any app that violates its policies. \u201cThe App Store terms and guidelines apply equally to all\ndevelopers around the world, including Apple,\u201d an Apple spokesman\nsaid. \u201cWe believe strongly that users should be asked for their\npermission before being tracked. Apps that are found to disregard\nthe user\u2019s choice will be rejected.\u201d I don\u2019t read diplomat-ese fluently, but that statement seems adamant: \u201call developers around the world, even Apple\u201d. I wonder, though, if Tencent believes they can track users with impunity because Apple wouldn\u2019t dare pull WeChat (etc.) from the Chinese App Store. Basically, IDFA was Apple\u2019s attempt to work with companies to provide a way to offer a sanctioned identifier for advertising tracking that respected user privacy and user control over tracking. It didn\u2019t work\u2009\u2014\u2009these companies have no respect for user privacy or user control, even  with  IDFA. So Apple is taking it to the next level. That\u2019ll only work if Apple backs up its rules with enforcement\u2009\u2014\u2009even in China."}, "210705_news_467036.txt": {"page_id": "210705_news_467036.txt", "text": "The study supplies the latest evidence that Facebook has not resolved its ad discrimination problems since ProPublica first  brought the issue to light in October 2016 . At the time, ProPublica revealed that the platform allowed advertisers of job and housing opportunities to exclude certain audiences characterized by traits like gender and race. Such groups receive special protection under US law, making this practice illegal. It took two and half years and several legal skirmishes for Facebook  to finally remove that feature .   But a few months later, the US Department of Housing and Urban Development (HUD) levied a new lawsuit, alleging that Facebook\u2019s ad-delivery algorithms were still excluding audiences for housing ads without the advertiser specifying the exclusion. A team of independent researchers including Korolova, led by Northeastern University\u2019s Muhammad Ali and Piotr Sapie\u017cy\u0144ski ,  corroborated those allegations a week later . They found, for example, that houses for sale were being shown more often to white users and houses for rent were being shown more often to minority users.   Korolova wanted to revisit the issue with her latest audit because the burden of proof for job discrimination is higher than for housing discrimination. While any skew in the display of ads based on protected characteristics is illegal in the case of housing, US employment law deems it justifiable if the skew is due to legitimate qualification differences. The new methodology controls for this factor.   \u201cThe design of the experiment is very clean,\u201d says Sapie\u017cy\u0144ski, who was not involved in the latest study. While some could argue that car and jewelry sales associates do indeed have different qualifications, he says, the differences between delivering pizza and delivering groceries are negligible. \u201cThese gender differences cannot be explained away by gender differences in qualifications or a lack of qualifications,\u201d he adds. \u201cFacebook can no longer say [this is] defensible by law.\u201d   The release of this audit comes amid heightened scrutiny of Facebook\u2019s AI bias work. In March, MIT Technology Review published the results of  a nine-month investigation  into the company\u2019s Responsible AI team, which found that the team, first formed in 2018, had neglected to work on issues like algorithmic amplification of misinformation and polarization because of its blinkered focus on AI bias. The company  published a blog post  shortly after, emphasizing the importance of that work and saying in particular that Facebook seeks \u201cto better understand potential errors that may affect our ads system, as part of our ongoing and broader work to study algorithmic fairness in ads.\u201d   \"We\u2019ve taken meaningful steps to address issues of discrimination in ads and have teams working on ads fairness today,\" said Facebook spokesperson Joe Osborn in a statement. \"Our system takes into account many signals to try and serve people ads they will be most interested in, but we understand the concerns raised in the report... We're continuing to work closely with the civil rights community, regulators, and academics on these important matters.\u201d   Despite these claims, however, Korolova says she found no noticeable change between the 2019 audit and this one in the way Facebook\u2019s ad-delivery algorithms work. \u201cFrom that perspective, it\u2019s actually really disappointing, because we brought this to their attention two years ago,\u201d she says. She's also offered to work with Facebook on addressing these issues, she says. \"We haven't heard back. At least to me, they haven't reached out.\"   In previous interviews, the company said it was unable to discuss the details of how it was working to mitigate algorithmic discrimination in its ad service because of ongoing litigation. The ads team said its progress has been limited by technical challenges.   Sapie\u017cy\u0144ski, who has now conducted three audits of the platform, says this has nothing to do with the issue. \u201cFacebook still has yet to acknowledge that there is a problem,\u201d he says. While the team works out the technical kinks, he adds, there\u2019s also an easy interim solution: it could turn off algorithmic ad targeting specifically for housing, employment, and lending ads without affecting the rest of its service. It\u2019s really just an issue of political will, he says.   Christo Wilson, another researcher at Northeastern who studies algorithmic bias but didn\u2019t participate in Korolova\u2019s or Sapie\u017cy\u0144ski\u2019s research, agrees: \u201cHow many times do researchers and journalists need to find these problems before we just accept that the whole ad-targeting system is bankrupt?\u201d  "}, "210705_news_467037.txt": {"page_id": "210705_news_467037.txt", "text": "ink version 1.0 release! February, 22, 2021 We're proud to announce that  ink , our open-source scripting language for interactive narrative, has now officially reached  version 1.0 !  What's new in Version 1.0? Version 1.0 is a stable release of \"the story so far\". The core features are well-tested and well-used, and the current integration has powered two full  inkle  releases: 2019's 3D adventure game,  Heaven's Vault , and 2020's procedurally narrated tactics game,  Pendragon .  But there's one big new feature: we've introduced the concept of  parallel, shared-state story-flows  - allowing the game to, say, switch between different simultaneous NPC conversations, while still allowing one conversation to affect the other.  We've also improved error handling and improved the way ink calls game-side functions. Inky now has a  dark mode  (see above!), zoom, a word count and stats menu, and better syntax highlighting. The default web player has new features for links and audio. And the Unity integration now allows live recompilation mid-game. Full details can be found on the release notes page for  ink ,  Inky  and the  ink-Unity-integration plugin . What is ink? ink  is designed from the ground up to be \"Word for interactive fiction\". Open it up, and start writing. Branch when you need to, rejoin the flow seamlessly, track state and vary what's written based on what came before - without any need to plan, layout, or structure in advance. Organise your content when you know what shape it wants to take, not before. It was recently awarded an  Epic Megagrant , and we're otherwise supported by a  Patreon .  A Different Approach To Interactive Writing ink  takes a different approach from other interactive fiction writing software in several ways.  It's entirely script-based, with no diagrams or flow-charts. Instead of being optimised for loops, it allows writers to quickly and robustly create heavily branching flow that runs naturally from beginning to end - as most interactive stories do.  All code and technical information is added as mark-up on top of text, making it easy to scan, proof-read, redraft and edit. It's also easy to see what's been changed in a file when using source-control. Another key concept is global, always-on state tracking: every line the player sees in the course of the game is remembered, automatically, by the engine, without the need to define variables. This allows for fast iteration on game-logic and the easy implementation of cause-and-effect, without the need for \"boilerplate\" code.  Flexible and Powerful But that doesn't mean  ink  is limited: it has variables, functions, maths and logic should you need them, and can also hand off complex decision-making to the game-code itself. ink  is also deliberately layout-agnostic. By handing the UI over to the game, it can be used to make hyperlink games, visual novels, RPGs, chatbots, FMV games, or simply to deliver highly-responsive barks in an first-person action game. Over on the engine side,  ink  comes with a run-time debugger that allows reading and poking of variable state, and a profiling tool to help developers in frame-rate dependent environments to find and fix story-side slowdowns.  Uptake ink has been adopted by game studios and other developers all around the world. It's been used on big indie games such Haven, NeoCab, Over the Alps, Falcon Age, Signs of the Sojourner and others.  For people looking to learn more about using ink, we've got several talks on our approaches, including this on from GDC 2017 on how  Heaven's Vault  drives its 3D world from a text-based script: Development History Here at  inkle ,  ink  has been our bedrock. We've used  ink  on every single title we've released over the last ten years, expanding and developing the feature set of the language over that time from quick mark-up for authoring branching choice-based narratives ( Sorcery! ) to authoring open-world, responsive, go-anywhere-and-do-anything narratives (er,  Sorcery! 3 . Also,  Heaven's Vault .) And we're continuing to find new ways to use the engine, like last year's  experiment in procedurally narrating a chess-like game .  Originally released as an open source beta in 2016,  ink  quickly accrued an editor,  inky , for easily writing and testing content, and a  dedicated Unity plug-in  to assist with integrating and testing stories at run-time.  Community Development Since its release, a wide community of developers and enthusiasts have contributed to the project. There is a full javascript port, built into inky, which allows the editor to produce stand-alone web-playable games. There is a port for the popular Godot engine, and work is progressing on a C port that will ultimately enable Unreal integration.  We've had contributions in the form of bug fixes and features requests to the main ink code base, and too many contributions to ink to list - from Dark Mode, through auto-complete, to an integrated version of the \"Writing With Ink\" documentation and, most recently, an \"open recent project\" menu listing. Looking back, we think these developments have justified our decision to make  ink  fully free and open source: the development around the system that's taken place would never have happened without the efforts of other developers, and we'd like to take this opportunity to thank everyone who's offered contributions, both large and small, over the last five years.  The core meeting point for ink developers has been the  inkle Discord , which is now the go-to place on the internet for assistance with implementing ink features, and contains a wealth of tips and ideas.   Looking forward! As  inkle  continues to develop games, we're continuing to develop and extend both ink and the Unity integration to allow us to tackle new problems. Though we're naturally more cautious with new languages features now the codebase is mature, we have an internal roadmap of issues and features we'd like to address.  The more support we get - both financially, and in terms of bug and community support - the more we can push forwards.  Meanwhile,  ink  will continue to be free to use and available to all for as long as we are able to support it! Happy writing!"}, "210705_news_467051.txt": {"page_id": "210705_news_467051.txt", "text": "Seit Tagen erhalten Nutzer von Smartphones und anderen Handys verst\u00e4rkt Kurznachrichten, die zum Klicken eines Links auffordern. Das Bundesamt f\u00fcr Sicherheit in der Informationstechnik (BSI) hat dahinter eine \"Smishing\"-Welle ausgemacht (SMS-Phishing), \u00fcber die per gef\u00e4lschten Mitteilungen Zugangsdaten ergaunert werden. Aktuell befinde sich in den Handy-Mitteilungen ein Link, hinter dem sich in den meisten aktuell beobachteten F\u00e4llen das Android-Schadprogramm FluBot verberge. Malware-App f\u00fcr Android, Phishing-Websites f\u00fcr iOS Dieser Banking-Trojaner,  der vertrauliche Daten auszusp\u00e4hen und Apps f\u00fcr die Online-Kontenf\u00fchrung oder zur Depotverwaltung anzuzapfen versucht , ist laut dem BSI seit etwa November 2020 im Umlauf. Die T\u00e4ter geben etwa vor, dass die Empf\u00e4nger der SMS bald ein Paket erhalten oder eine Sendung zur\u00fcck an den Absender gehen soll. Android-Nutzer bekommen \u00fcber den Link die sch\u00e4dliche FluBot-App zum Download angeboten, die dann zahlreiche Berechtigungen einfordert. Dabei tarnen die Kriminellen die Malware als eine f\u00fcr die Paketverfolgung angeblich notwendige Anwendung von bekannten Logistikunternehmen wie DHL, Deutsche Post oder FedEx. Auf iPhones oder iPads funktioniert der Download nicht. Nutzer von Ger\u00e4ten mit iOS-Systemen landen in der Regel auf Werbe- oder Phishing-Seiten. Dort lauern Abofallen, Angebote f\u00fcr dubiose Geldanlagen oder andere Schadsoftware. Nicht auf den Link klicken! Beim Erhalt einer verd\u00e4chtigen SMS sei es wichtig, nicht auf den Link zu klicken und die Nachricht umgehend nach Erhalt zu l\u00f6schen,  r\u00e4t das BSI . \"Sollte Ihnen der Absender oder die Absenderin bekannt sein, rufen Sie ihn oder sie zum Beispiel an und fragen Sie nach der Richtigkeit.\" Zugleich sei es empfehlenswert, den Absender \u00fcber das Betriebssystem zu sperren. Generell sollte unter Android die Installation von Apps aus unbekannten Quellen deaktiviert werden. \u00dcber den Mobilfunkanbieter lasse sich zudem eine Drittanbietersperre aktivieren, um unerw\u00fcnschte Abbuchungen zu vermeiden. Nutzer, die auf einen einschl\u00e4gigen Link geklickt oder sogar bereits den Trojaner installiert haben, sollten der Beh\u00f6rde zufolge das Ger\u00e4t in den Flugmodus schalten und so vom Mobilfunknetz trennen. Im Anschluss sei der Provider \u00fcber den Fall zu informieren. Parallel sollten Betroffene ihr Bankkonto und Ihren Zahlungsdienstleister auf nicht selbst veranlasste Abbuchungen \u00fcberpr\u00fcfen. Empfehlung: Strafanzeige erstatten \"Erstatten Sie Strafanzeige bei der \u00f6rtlichen Polizeidienststelle\", empfiehlt das BSI weiter. Dabei sollte das Smartphone f\u00fcr \"Beweissicherungen\" mitgenommen werden. Im Anschluss sollte das Ger\u00e4t auf die Werkseinstellungen zur\u00fcckgesetzt werden. Alle gespeicherten und installierten Daten gingen dabei zwar verloren. Der Schritt sei aber n\u00f6tig, \"um die \u00fcber die aktuellen SMS-Spam-Nachrichten verteilten Android-Schadprogramme vollst\u00e4ndig zu entfernen\". \"Seit den Ostertagen ist in manchen F\u00e4llen sogar eine pers\u00f6nliche Anrede zu beobachten\", verweist das Amt auf neue Tricks der Betr\u00fcger. Das Smishing-Ph\u00e4nomen an sich sei nicht neu. So habe es  das \"B\u00fcrger-CERT\" etwa schon in einem Newsletter Mitte Februar aufgegriffen . Damals sei Betroffenen auf diesem Weg das Android Schadprogramm MoqHao untergejubelt worden. SMS-Flut losgetreten Auch Strafverfolgungsbeh\u00f6rden wie das Polizeipr\u00e4sidium Nordhessen, die Kollegen in Neubrandenburg und das Landeskriminalamt Niedersachsen warnten bereits vor der perfiden Betrugswelle. Die hessischen Ordnungsh\u00fcter mahnten dabei zur besonderen Vorsicht: Aufgrund der hohen Nachfrage im Online-Handel w\u00e4hrend der Corona-Pandemie \"erwarten viele Menschen tats\u00e4chlich ein Paket und klicken auf den folgenschweren Link in der SMS.\" Der Trojaner verbreite sich dann \"wie ein Schneeballsystem unter den gespeicherten Kontaktdaten des Betroffenen\". Dies l\u00f6se eine regelrechte SMS-Flut aus. Die IT-Sicherheitsfirma Eset  wittert hinter der Serie derweil einen Zusammenhang  mit den  j\u00fcngsten Problemen rund um ein massives Datenleck bei Facebook . Die Kampagne nehme in Deutschland rasant Fahrt auf. Der Einsatz solcher gestohlenen Datens\u00e4tze sei nicht un\u00fcblich und beschleunige die Verbreitung der Schad-App enorm. FluBot wird laut dem Unternehmen offenbar in Untergrundforen als Malware-as-Service angeboten zu werden: \"Die T\u00e4ter scheinen die Infrastruktur des Banking-Trojaners lediglich gemietet zu haben.\" Festnahmen vermeintlicher Hinterm\u00e4nner in Spanien h\u00e4tten die Welle nicht abgeschw\u00e4cht. ( tiw )"}, "210705_news_467073.txt": {"page_id": "210705_news_467073.txt", "text": "The Myth of the 'Waterfall' SDLC \n    This is admittedly my interpretation, but I have tried to provide links to all of the source material I have used so that you can do your own research if you don't agree with my conclusions.\n \n\u00a0\n\n Also known as:  Phased Development, Plan-Driven Development, Specification-Driven Development, Cascade Model\n \n\n\u00a0\n\n Introduction \n    This is the 5 th  major version of this wiki article and I thought it might be useful to provide a bit of introduction as to why I have written (and occasionally update) such a long article about a \u2018discredited\u2019 methodology like Waterfall.\n \n    Several years ago my employer started making efforts to formally adopt some Agile processes (Scrum at the time, DAD currently) for some project types. Wanting to educate myself more I started doing Google searches and reading what material I could find. But my reading got hijacked as I came across descriptions of \u2018Waterfall\u2019 over and over again that simply did not match the way we did it at my employer, or the way any other BA I had talked to had described the way they did projects.\n \n    So I got curious. Did my employer adopt some \u2018improved\u2019 version of Waterfall that wasn\u2019t so rigid? Were there actually people out there at some point advocating the highly-rigid form of Waterfall I kept reading about on \u2018Agile\u2019-oriented web sites? If so, what was their logic and what could I learn from it? So I started researching Waterfall. I found out where it supposedly originated from and dug up the original paper (the Royce paper I discuss in depth below) on the internet. And surprisingly, it had almost NOTHING in common with the rigid process I kept reading about on Agile sites.\n \n    So I started researching more, and I was surprised to start coming across research papers that cited the Royce paper, but which completely mis-characterized what it said. These were papers published in academic journals, not some random blog, so you would expect a higher-level of quality. So I researched more and more; and what fascinated me was the dichotomy of what people were consistently referring to as \u2018Waterfall\u2019, and what had actually been published in what were cited as the main \u2018sources\u2019 of the Waterfall methodology.\n \n    So thus was born this article on \u2018Waterfall\u2019.  It is not an \u2018apology\u2019 for or a \u2018defense\u2019 of Waterfall. It is my best effort (so far) to show that as far as I can tell no one has ever seriously advocated for the highly rigid software process that is commonly called  Waterfall  [ with the caveat that I may not have come across the right reference yet ]. And not only had no one ever advocated it, that the people  blamed  for this process called Waterfall had advocated for things very different than what the current understanding is. Lastly, I would argue that if you consider the environment that early software development took place in that there are a number of very valid reasons for a planning-centric, documentation-heavy development process to be used as the best option available.\n Most importantly though, I think it is important to challenge some of the common assumptions and 'common knowledge' that exists in the technology world and question whether that information is true or is presented in certain ways to serve personal or commercial agendas.  I happen to think that 'Waterfall' is a strong example of this sort of thing and that by understanding the background you as a reader may be less inclined to accept statements at face value and do more critical thinking of your own.  This is why all the sources I have used are fully documented and for the majority of them I provide direct links where possible.  It gives you the option to validate my work and form your own opinion. I would also say that it's important to understand the context under which different ideas and approaches arose.  By understanding that context you can understand what may need to re-evaluated as the context changes, and identify similar contexts in your own situation.  This way you may have a better idea of what may work in your current context and why, saving you the trouble of having to learn the hard way. \n    You may not care about any of this. Or the history involved. Or what the \u2018sources\u2019 actually said. You may find all the contextual information tedious and uninteresting. In that case, feel free to read the  What is it?  section immediately below and then skip down past the  Origins  section and ignore everything in between.\n \n    But if you do find it interesting, please read the whole article. If you know of references I haven\u2019t touched on that are appropriate, please let me know. I am completely open to changing this article, but I want to make sure that most of the article is grounded in actual references. People can then form their own opinions based on the evidence.\n \n    My conclusion after reading all that I have so far is the same as it was before I started doing all this research. There is no universally applicable development methodology. You need to tailor your development process to the situation at hand. Sometimes that will look very \u2018Agile\u2019, sometimes it will look very \u2018Waterfall\u2019, and sometimes have parts of both or neither. As has been repeatedly said,  There is no silver bullet .\n \n\n\u00a0\n What is it? \n    Strangely enough, this is probably the most controversial aspect of Waterfall. There seems to be a common \u2018understanding\u2019 of what Waterfall is among a great many people in the technology world, but I would argue that that \u2018understanding\u2019 is not supported by ANY of the references that are named as the \u2018source\u2019 of Waterfall.\n \n    The best initial description that I can come up with based on my reading is that the Waterfall model is a Systems (or Software) Development Life Cycle (SDLC) process model  that is project management and solution-design focused; and which utilizes a highly-planned, specification-driven development process . It was probably the first formalized instance of a SDLC model, and nearly every software development model since has incorporated some of its features. [32] \n    What is now called  Waterfall  is frequently thought to have been first proposed by Winston W. Royce in a 1970 paper, [2]  but aspects of the conceptual process go back to at least a 1956 paper by Herbert D. Benington.  [1, 32] \n    However,  the Waterfall model  is also seen as highly controversial by some in the software development community. Indeed, some have gone so far as to call it a  toxic  concept, [4]  and  the most costly mistake in the world .  [17] \u00a0 Partly, this is due to wide-ranging differences in how people define  the waterfall model . And I think it is partly due to a desire to have  Waterfall  as a useful straw-man to use for promoting personal agendas. These are due to both common misunderstandings of the history of  Waterfall  and because of the changing nature of what has been called  Waterfall  over time.\n \n    In general, the way the  Waterfall model  is described in current times seems to be highly dependent on the knowledge, agenda, and personal biases of the person describing it.\n \n\n\u00a0\n What's in a Name? \n    So why is it called  the Waterfall model ? First, you need to know that Royce never used that term in describing his work and to understand that  the Waterfall model  name was assigned by others.\n \n    Second, it helps to look at a common visual representation of the model such as the one below (which is similar to the one used on Wikipedia):\n Figure A \n\n\u00a0\n\n \n    If you look at that diagram, the common description of \u2018Waterfall\u2019 as  \u2026 each phase cascades down into the next, you know, like a waterfall  begin to make sense. [10] \n    However, the oldest reference to  Waterfall  does not use the \u2018Waterfall\u2019 name in that way, and allows for far more subtlety in interpretation. That earliest reference to \u2018Waterfall\u2019 comes in a 1976 research paper by Bell & Thayer, [12]  6-years after Royce published his paper. The full quote is included below because I think it provides a good example of the way the Royce paper was perceived initially and that the \u2018Waterfall\u2019 nomenclature was not initially meant the way it later came to be interpreted.\n \n    Bell and Thayer say the following:\n The evolution of approaches for the development of software systems has generally paralleled the evolution of ideas for the implementation of code. Over the last ten years more structure and discipline have been adopted, and practitioners have concluded that a top-down approach is superior to the bottom-up approach of the past. The Military Standard set MILSTD 490/483 recognized this newer approach by specifying a system requirements document, a \"design-to\" requirements document that is created in response to the system requirements, and then a \"code-to\" requirements document for each software module in the design. Each of these is at a lower level of detail than the former, so the system developers are led through a top-down process. The same top-down approach to a series of requirements statements is explained, without the specialized military jargon, in an excellent paper by Royce [5]; he introduced the concept of the  Waterfall  of development activities. In this approach software is developed in the disciplined sequence of activities shown in Figure I.  [Figure 1 is a standard Waterfall diagram like the one immediately below]\n         Each of the documents in the early phases of the waterfall can be considered as stating a set of requirements. At each level a set of requirements serve as the input and a design is produced as output. This design then becomes the requirements set for the designer at the next level down. With so many levels of requirements documents, and with so few software projects mapping nicely into the scheme, we must be more specific about what we mean by the term \"software requirements\" as used in our studies.  We do not mean all the various levels of requirements, but only a single one, one that can usually be identified in large software development projects that have ended successfully. At this level the software requirements describe functions that the software must perform, but not how they must be implemented.  For example, they might contain a requirement that a BMD system identify erroneous RADAR returns that have any of five specific characteristics. However, the software to meet this requirement might be spread through twelve subroutines using any of a large number of identification techniques.   [12]  (emphasis mine) \n\u00a0\n\n A Tale of Two Waterfalls \n    The problem with discussing  the Waterfall Model  is that there are at least two significantly different understandings of what it is. The most common interpretation is what I am calling the \u2018Frozen Waterfall\u2019 in this article. The other is what Royce originally described in his paper. Because Royce is most commonly cited as the \u2018source\u2019 of the Waterfall method, I am going to give what he described priority for the use of the term \u2018Waterfall\u2019.\n \n    Thus, for the purposes of this article, I am going to treat  Waterfall  as being the process that Royce fully espoused in his 1970 paper.\u00a0 But first, I think we need to address the \u2018frozen\u2019  [10]  Waterfall interpretation.\n \n\n\u00a0\n \u2018Frozen\u2019 Waterfall \n    Most commonly modeled using a diagram such as Figure A above (which is a recreation of the one on Wikipedia), the \u2018frozen\u2019 Waterfall approach is usually described as having the following characteristics:\n It is a sequential, rigidly-planned process consisting of several phases that must be completed in sequence.  [25, 33, 34, 38] Each phase is a silo, completely separate from the other stages and the results of each phase are frozen once the phase is complete.  [10, 38] Each phase must be completed with 100% certainty before the project team moves on to the next phase.  [5, 10, 33, 34] It  ignores end-user development and end-user involvement outside of requirements specification . [4] \u00a0 Or that if requirements changes are made after coding starts, they are made without involving stakeholders.  [24] That requirements can never be changed once the requirements phase is complete  [5, 10, 34, 36, 37] , and as a result it  assumes human developers are capable of correctly getting the requirements, design and tests correct  on the first try . [4, 36]  (emphasis mine) It  separates analysis from design , forcing developers to  generate a solution, without providing any guidance as to how the solution is generated . [4] \n    There are lots of references to this \u2018frozen\u2019 waterfall methodology, especially from \u2018Agile\u2019 proponents, but I cannot find a single paper that espouses this version of the Waterfall methodology.\u00a0 I can find quite a few who attribute these characteristics to \u2018Waterfall\u2019, while usually referencing the Royce paper, but none that actually espouse this (including Royce).\u00a0 From what I can tell, this \u2018frozen\u2019 Waterfall has never been advocated for by anyone, although it may have existed due to the ignorance of those trying to follow what they thought was the \u2018Waterfall\u2019 process.\n \n    A less opinionated, more academic description of the  problems  with \u2018frozen\u2019 Waterfall can be found in a white paper by QinetiQ, a British defense contractor that was formerly part of a UK government agency. [25] \u00a0 The QinetiQ paper states that  the Waterfall model was based on a number of assumptions :\n that all its stages could be completed in sequence that the costs and benefits of an information system could be calculated in advance that users knew what they wanted that the work needed was known and could be measured that programs once written could be altered that the right answer could be produced first time \n    But even this description seems to be largely unsupportable when you read the various documents that provide the foundation for  Waterfall .\n \n\n\u00a0\n Royce\u2019s Waterfall \n    So if the description above is for the \u2018Frozen\u2019 interpretation of Waterfall, what exactly is the  Waterfall model  as proposed by Royce and modified by others as an idea of \u2018what to do\u2019, rather than \u2018what not to do\u2019?\n \n    Based on my reading of the literature (mostly covered below), I would say that in general the \u2018Waterfall\u2019 model has the following characteristics:\n It is a process that seeks to reduce risk and costs through planning, documentation, and process controls. It includes a significant up-front effort to elicit, analyze, evaluate, and confirm both user and  business  needs before development begins in order to reduce the amount of re-coding that needs to be done later, and to ensure (as much as possible) that the system architecture is sufficient for the current and future needs of the client. It is a process that is focused not just on the software being developed, but strongly factors in the context the software will be used in and supporting the full lifecycle of a software product (development, testing, deployment, maintenance, enhancements, and retirement). The process is comprised of a set of phases that are roughly sequential in nature, in that later phases depend on input from prior phases. However, these phases can overlap, interact, and be revisited. Because of its planning-focused nature, the  overall  cycle is generally completed a small number of times (most commonly 1 to 2 times). If executed more than once, the first cycle is usually for the creation of a prototype. The process is generally documentation heavy, especially compared to  Agile  processes. The process is generally intended for use in large, complex efforts that require careful coordination among multiple teams in order to be successful. Once a certain stage in the overall process has been reached (usually requirements sign-off), a formal change-control process is used to manage change efforts. Quality Assurance is integrated by splitting each phase into two parts: the first part executes the work of the phase and the second part validates it.  [32] \n    Please note that the above description does NOT mean that each phase must be 100% complete before the next phase can be engaged.\u00a0 What it means is that in general, if you want to do analysis, you need to have something to analyze.\u00a0 So the results of a Software Requirements activity (there can be multiple activities) would be the input to an Analysis activity, which would in turn be the input into a Design activity.\n \n\n\u00a0\n Some Historical Context \n    Before discussing this history and details of what  The Waterfall model  are, I think it is critical for readers to first understand the context in which it developed.\n \n\n\u00a0\n\n The Hardware \n    The 1950\u2019s and the 1960\u2019s were the first 2 decades of the computer era. This was largely the era of mainframe computers, with mini-computers starting to appear on the scene in the mid-1960\u2019s. Personal computers such as the IBM PC and Macintosh weren\u2019t even a consideration during this time period, as they did not appear until the early to mid-1980\u2019s.\n \n    Additionally, the capacity of even the best computers was far more limited than modern computers. Much of the software was being developed as parts of integrated systems, where issues of computing speed, memory amounts, and system design all impacted how software could operate and which posed challenges of resource scarcity that modern programmers rarely have to deal with. [1] \n\n\u00a0\n\n The Development Environment \n    Most development was probably being done in assembly, ALGOL (mid-1950\u2019s), Fortran (first compiler in 1957), COMTRAN (released in 1957), FLOW-MATIC (general compiler available in 1959), COBOL (replaced COMTRAN at IBM in 1962), and PL/I (first compiler 1966).\n \n    The C programming language was not developed until the early 1970\u2019s, and not formalized until the late 1970\u2019s. Most object-oriented languages did not receive wide-spread use until the mid-1980\u2019s.\n \n    The languages used were less concise, and thus more subject to coding errors. For example, a simple  hello world  program in a modern language like C can be done in 4 lines of code or less. In an older language like COBOL it took 17 lines of code. [20] \n    Lastly, software development tools were much simpler. While better development tools became more common in the mid-to-late 1960\u2019s, modern tools like code-highlighting, robust Integrated Development Environments (IDE\u2019s), version control systems like Git, and many other tools that make modern development easier and of higher quality did not exist at the time.\n \n\n\u00a0\n\n The Developers \n    While the early software developers of the 1950\u2019s were mostly engineers or mathematicians, by the late-1950\u2019s and early 1960\u2019s the need for programmers vastly exceeded the number of people available with engineering and mathematics backgrounds. The programming field was flooded with people whose training was in the humanities, social sciences, foreign languages, and fine arts. [21] \u00a0 Many of these were reportedly influenced by the  question authority  attitudes of the 1960\u2019s  [21]  and were resistant to attempts to organize and manage the development process.\n \n    Many of these issues continued well into the 1970\u2019s where a 1975 survey found  that the average coder in 14 large organizations had two years of college education and two years of software experience; was familiar with two programming languages and software products; and was generally sloppy, inflexible, 'in over his head', and undermanaged . [21] \u00a0 Of course, how much that statement represents reality and how much it represents bias is impossible to tell 40 years later. But it gives a perspective on the issues of the time.\n \n\n\u00a0\n\n The Development Process \n    Because of the type of Development Environment and Developers that were common at the time, it should be no surprise that the  code and fix  style of programming was the most common process during the 1960\u2019s. The developers were often very creative, but the process of coding, then fixing bugs, then coding more, then fixing more bugs,  tended to result in heavily patched spaghetti code .  [21] \n    The concepts of structured programming such as the use of sub-routines, block structures and loops  [22]  only started to become common in the late-1960\u2019s and even then there was resistance among many programmers to efforts to make code more readable, easy to maintain and fix.\n \n\u00a0\n\n The Cost Centers \n    A critical difference between modern development and what the situation was in the 1950\u2019s, 1960\u2019s, and 1970\u2019s was in the cost centers. In the current environment computing resources are so cheap and plentiful that the human costs of developers, project staff, and related personal far outweigh the cost of any computing resources that are consumed.\n \n    However, the situation was once the exact opposite. As Barry Boehm relates,  On my first day on the job [in the 1950\u2019s], my supervisor showed me a GD ERA 1103 computer, which filled a large room. He said, 'Now listen. We are paying $600 an hour for this computer and $2 an hour for you, and I want you to act accordingly' .\n \n    Benington provides similar information in his paper in which he states that the cost for a full-time engineering man-year (with all overhead) came to roughly $1.50 an hour, whereas computer time was billed at a cost of $500 an hour.  [1] \n    While the cost structure eventually reached its current state of human costs being the vast majority, I would guess that computing resources were more expensive at least into the mid-1980\u2019s.\n \n    What this cost structure meant was that there was an effort to do as much analysis, coding, and quality testing  offline  as possible. Computing time was simply too expensive to be used for things that were done much more cheaply by humans without the use of a computer.\n \n\u00a0\n\n Project Scales \n    The last contextual factor to consider in the development of Waterfall was the project scales that were being undertaken (relative to other work of the era). As Royce states in the first paragraph of his paper, he had spent the previous nine years  mostly concerned with the development of software packages for spacecraft mission planning, commanding and post-flight analysis . Like Barry Boehm and many of other major figures in software development at the time, he had spent most of his time working on major software development projects for the U.S. government. Mainly the Department of Defense and NASA.\n \n    This means nearly all of his work involved the massive complexities of government bidding and contracting, the frequent use of many sub-contractors, and often massive systems integration challenges. Many (or most) of these initiatives would be made up multiple (or a great many) brand new non-computing hardware components (radars, rockets, etc.), integrated with cutting edge computer hardware and software, with dozens or hundreds of sub-systems that had to communicate together properly. It seems not uncommon for there to have been thousands of individual developers working on different aspects of these efforts. In Royce\u2019s case he had the additional complexity of working on systems that would be going into space where the margin for error is essentially zero.\n \n\n\u00a0\n\n Conclusions \n    Given the situation context provided above, it should be no surprise to readers that Royce would discuss a process that was focused on the following:\n Cost Control \u2013 Doing as much without computing resources as possible Quality Management \u2013 Using as many processes as possible to ensure that the solution delivered is of the highest quality possible (thus  documentation, documentation, documentation ). One particularly important aspect of this was the use of documentation as a coordination artifact. Project Management \u2013 As can be seen later in this document, Royce was very concerned with the project management, support, and maintenance aspects of the software lifecycle, not just the actual coding work. Thus, his ideas were focused more on supporting those efforts than the focus of someone who was mostly concerned with development would be. \n\u00a0\n Origins \n    The origins of the Waterfall model are usually ascribed to papers by Herbert D. Benington in 1956  [1]  and Winston W. Royce in 1970  [2]  and the  blame  for the wide-spread adoption of the \u2018frozen\u2019 Waterfall methodology is usually attributed to its adoption by the US Department of Defense with DOD Standard 2167, the MILITARY STANDARD: DEFENSE SYSTEM SOFTWARE DEVELOPMENT, published on June 4, 1985.  [33] \n    However, neither author uses the term  Waterfall  in their papers anywhere, nor are the processes they discuss nearly as rigid as the \u2018frozen\u2019 interpretation of waterfall.\n The Benington Paper \n    Herbert D. Benington presented a paper at a symposium on advanced programming methods sponsored by the Navy Mathematical Computing Advisory Panel in June 1956 in which he described the techniques used to produce the programs for the Semi-Automatic Ground Environment (SAGE) system that was developed at the MIT Lincoln Laboratory.\n \n    His paper was the first that I can find that described a phased development process that is similar to what eventually became the Waterfall model.\u00a0 And Barry Boehm seems to agree when he called Benington\u2019s process the \u2018Stagewise\u2019 model  [26]  and said that it was the first to say that software should be implemented in successive stages.\u00a0 Boehm then went on to say that the \u2018Waterfall\u2019 model was a  highly influential refinement  of the Stagewise model. [26] \u00a0 The following is a re-creation of a diagram Benington provided in his paper that described their overall process.\n \n\n\u00a0\n\n \n    However, before you make the same mistake that others made with Royce\u2019s diagrams and think that Benington was advocating for a rigidly sequential process model, consider the additional information that Benington provides in his paper.\n \n    In describing how they were successful Benington says:  [1] We were all engineers and had been trained to organize our efforts along engineering lines. \u2026 to define a system of documentation so that others would know what was being done; \u2026 to define interfaces and police them carefully; \u2026  to recognize that things would not work well the first, second, or third time \u2026 to keep a record of everything that really went wrong and to see whether it really got fixed \u2026 we undertook the programming  only after we had assembled an experimental prototype  of 35,000 instructions that performed all of the bare-bones functions of air defense . \u2026 producing large computer programs is like raising a family\u2026 you have to start out and do it on your own, learn the unique options you have, see what unexpected problems arise, and, with reasonable luck, perform about as well as those who have been doing it forever . \n    What you see above is the foundations of a structured, planned approach to software development. But you also see that ideas such as building prototypes were being used as far back as the 1950\u2019s. And just because they may not be explicitly shown in diagrams does not mean they were not being done.\n \n    One interesting point of the Benington paper is that it points at why so much effort was placed on documentation by those who successfully worked on large computer programs (at least at that time). He said,\n Finally, there is the problem of documentation. In the early days of programming you could call up the programmer if the machine stopped. You seldom modified another\u2019s program \u2013 you wrote your own. Although present automatic programming technology has done much to make programs more communicable among programmers, there is a long way to go before we can take an integrated program of 100,000 instructions and make it  public property  for the user, the coder, the tester, the evaluator, and the on-site maintenance programmer. The only answer seems to be the documentation of the system on every level from sales brochures for management to instruction listings for maintenance engineers. \n    This emphasis that documentation serves a purpose far beyond use by the coder is an important one that Royce will repeat and which continues to be overlooked in many discussions today.\n \n\n\u00a0\n The Royce Paper \n    The second paper, and the one most frequently cited as the source for the Waterfall methodology, is the paper by Winston W. Royce in 1970.\u00a0 Royce starts the paper off by declaring that it  describes my personal views about managing larger software developments  and that  I have become prejudiced by my experiences and I am going to relate some of these prejudices in this presentation .\n \n    So it should be clear right off the bat that Royce was not saying all programming efforts should be done the way he discusses.\u00a0 He says his discussion is focused on  larger software developments .\u00a0 He also makes clear that he is presenting his  prejudiced  opinions.\n \n\n\u00a0\n The \u2018Common\u2019 Activities of Any Programming Effort \n    Royce starts out by saying that there are two steps common to all programming efforts, regardless of size or complexity.\u00a0 These are the Analysis and Coding steps, which are shown Figure 1 of his paper (recreated below):\n Figure 1 \n    Royce says that these two steps are all that is needed  if the effort is sufficiently small and if the final product is to be operated by those who built it . [2]  However, he then goes on to say that:\n An implementation plan to manufacture larger software systems, and keyed only to these steps, however, is doomed to failure. Many additional development steps are required, none contribute as directly to the final product as analysis and coding, and all drive up the development costs. Customer personnel would rather not pay for them and development personnel would rather not implement them.  The prime function of management is to sell these concepts to both groups and then enforce compliance on the part of development personnel.   [2] [Emphasis mine] \n    Given the statement bolded above, it should be immediately clear that Royce is discussing a software development model that has a [project] management perspective.\u00a0 He\u2019s not trying to optimize developer happiness, customer happiness, or drive-down costs.\u00a0 His focus is on what he considers critical for the success of  large  system development efforts.\n \n    The additional steps Royce feels are necessary for a  large  effort are then shown in Figure 2 of his paper.\u00a0 And this also seems to be where most of the confusion over what  Waterfall  means comes from.\u00a0 That diagram [Figure 2] is recreated [with color added] below:\n Figure 2 \n    Royce is using his figure 2 (above) as an expansion of the simple two-step process he showed in Figure 1 by adding the additional steps he states are necessary. He describes it as:\n A more grandiose approach to software development is illustrated in Figure 2. The analysis and coding steps are still in the picture, but they are preceded by two levels of requirements analysis, are separated by a program design step, and followed by a testing step. These additions are treated separately from analysis and coding because they are distinctly different in the way they are executed. They must be planned and staffed differently for best utilization of program resources. [2] \n    So far this still sounds like the commonly assigned characteristics of  Waterfall .\u00a0 But that idea completely ignores the very next line after the quote above and its accompanying figure.\u00a0 The next paragraph immediately after the quote above is:\n Figure 3 portrays the iterative relationship between successive development phases for this scheme. The ordering of steps is based on the following concept: that as each step progresses and the design is further detailed,  there is an iteration with the preceding and succeeding steps but rarely with the more remote steps in the sequence .\u00a0The virtue of all of this is that as the design proceeds the change process is scoped down to manageable limits. At any point in the design process after the requirements analysis is completed there exists a firm and closeup, moving baseline to which to return in the event of unforeseen design difficulties. What we have is an effective fallback position that tends to maximize the extent of early work that is salvageable and preserved. [2]  [emphasis mine] \n    So Royce was saying that the process he is discussing is an iterative process and, by context, that the first diagram (Figure 2) is just a representation of the overall process flow.\u00a0 He then goes on to show Figure 3 (as indicated in the quote) which does include the iterative nature in the diagram.\u00a0 That diagram (his Figure 3) is recreated below:\n Figure 3 \n\n\u00a0\n Royce\u2019s Criticisms \n    Figure 3 above is probably the closest definition to a \u2018traditional development model\u2019 that I can find, if you consider \u2018traditional development\u2019 to be whatever Royce was building upon.\u00a0 Or if you consider \u2018Waterfall\u2019 to be what Royce was criticizing, Figure 3 above would be the correct model to use, because it is this figure [Figure 3] that forms the baseline process that Royce wants to improve.\n \n    However, it should be made clear that Royce is NOT saying this model [Figure 3] is not a good one to use, or should never be used.\u00a0 This is shown later on the same page of his paper when Royce states:\n However,  I believe the illustrated approach to be fundamentally sound . The remainder of this discussion presents five additional features that must be added to this basic approach to eliminate most of the development risks.   [2]  [emphasis mine] \n    So Royce is stating that the basic process he has described to this point [which is what is shown in Figure 3, NOT what is shown in Figure 2] is fundamentally sound. But that it can be improved by the additional steps he is recommending.\n Royce begins his criticisms by stating:\n\n I believe in this concept, but the implementation described above is risky and invites failure. The problem is illustrated in Figure 4.   [2] I have re-created Figure 4 from the Royce paper below. Figure 4 \n\n\u00a0\n\n After referring to Figure 4, Royce goes on to say: The testing phase which occurs at the end of the development cycle is the first event for which  timing, storage, input/output transfers, etc., are experienced as distinguished from analyzed. These phenomena are not precisely analyzable. They are not the solutions to the standard partial differential equations of mathematical physics for instance. Yet if these phenomena fail to satisfy the various external constraints, then invariably a major redesign is required . A simple octal patch or redo of some isolated code will not fix these kinds of difficulties. The required design changes are likely to be so disruptive that the software requirements upon which the design is based and which provides the rationale for everything are violated. Either the requirements must be modified, or a substantial change in the design is required. In effect the development process has returned to the origin and one can expect up to a lO0-percent overrun in schedule and/or costs.   [2]  [emphasis mine] \n    So Royce is NOT saying that it is impossible to define functional requirements ahead of time, as some have posited.\u00a0 Rather, he is pointing to the difficulties of getting the functional needs of the customer to operate in an acceptable manner within the system constraints as they were originally defined in the system requirements.\u00a0 I think in part this needs to be evaluated based on the more constrained computing resources available at the time and trying to get large and complex software to execute with stability and speed within those constrained system environments.\u00a0 This was no doubt further influenced by the fact that Royce was operating on space systems where issues such as power use, memory, and system reliability were all major constraints.\n This emphasis on the system design requirements is further revealed in yet another quote from page 2 of his paper, in which Royce says: In my experience there are whole departments consumed with the analysis of orbit mechanics, spacecraft attitude determination, mathematical optimization of payload activity and so forth, but when these departments have completed their difficult and complex work, the resultant program steps involve a few lines of serial arithmetic code. If in the execution of their difficult and complex work the analysts have made a mistake, the correction is invariably implemented by a minor change in the code with no disruptive feedback into the other developmental bases.   [2] \n    So while Royce acknowledges that getting the customer (or functional) requirements correct is a  difficult and complex  task (so much so he says it twice), his focus in this aspect is on getting system design requirements correct.\u00a0 I don\u2019t necessarily agree that his statements about the relative ease of implementing (functional) requirements changes are still true, but I am trying to show the context in which he made his statements.\n His concern with system requirements is further shown when he discusses the magnitude of potential disruption caused by missed system requirements by stating: The required design changes are likely to be so disruptive that the software requirements upon which the design is based and which provides the rationale for everything are violated. Either the requirements must be modified, or a substantial change in the design is required. In effect the development process has returned to the origin and one can expect up to a lO0-percent overrun in schedule and/or costs.   [2] \n    So the point he seems to be making is that if the system design (the system requirements) isn\u2019t sufficient to meet the software requirements (the customer or functional requirements AND non-functional requirements), that either the entire system design must be re-architected or the software capabilities must be changed (reduced) so as to enable the system design to run the software.\n \n    While I personally agree that it\u2019s probably not possible to define requirements  perfectly  ahead of time for large software products, using the quote above to criticize the creation of functional requirements misses the point that Royce was raising.\n \n    More broadly, Royce is also making his first major criticism of the process he has described up to this point by pointing out that its scope for iteration is too narrow.\u00a0 With iterations only occurring (generally) between each immediately prior and successive phase, he thinks this is too narrow and risky.\u00a0 He feels this may limit the processes ability to react to issues discovered in one phase may require a larger change than to either the immediately prior or successor phases.\n \n\n\u00a0\n Royce\u2019s Five Fixes #1 \u2013 Program Design Comes First \n    At this point Royce goes about offering his suggestions for how to  fix  the original process through adding 5 additional steps. The first step was  Program Design Comes First , about which he says:\n A preliminary program design phase has been inserted between the software requirements generation phase and the analysis phase. This procedure can be criticized on the basis that the program designer is forced to design in the relative vacuum of initial software requirements without any existing analysis. As a result, his preliminary design may be substantially in error as compared to his design if he were to wait until the analysis was complete. This criticism is correct but it misses the point . By this technique the program designer assures that the software will not fail because of storage, timing, and data flux reasons.  As the analysis proceeds in the succeeding phase the program designer must impose on the analyst the storage, timing, and operational constraints in such a way that he senses the consequence\u2026..  If the total resources to be applied are insufficient or if the embryo operational design is wrong it will be recognized at this earlier stage and the iteration with requirements and preliminary design can be redone before final design, coding and test commences.   [2]  [emphasis mine] He provides Figure 5 in his document (recreated below) to show this change. Figure 5 \n    Again, it is important to note that even with his  fix , Royce is staying with essentially the same  Waterfall  model that he started with.\n \n\n\u00a0\n Royce\u2019s Five Fixes #2 \u2013 Document the Design \n    The second step Royce recommends as part of his  fix  is to  Document the Design . A focus on documentation is often flagged as one of the major  faults  of Waterfall, so it might be useful to understand why Royce was such an ardent fan of documentation.\u00a0 In his words:\n \u201c At this point it is appropriate to raise the issue of \u2013 \u2018how much documentation\u2019? My own view is \u2018quite a lot\u2019, certainly more than most programmers, analysts, or program designers are willing to do if left up to their own devices.  The first rule of managing software development is ruthless enforcement of documentation requirements . \n    \n    Occasionally I am called upon to review the progress of other software design efforts. My first step is to investigate the state of the documentation.  If the documentation is in serious default my first recommendation is simple. Replace project management . Stop all activities not related to documentation. Bring the documentation up to reasonable standards.  Management of software is simply impossible without a very high degree of documentation .\n   [2]  [All emphasis mine] \n    Notice that the emphasis Royce is placing on documentation is not for the purpose of helping the developers. He comes right out and says that the level of documentation that should be present is  more than most programmers, analysts, or program designers are willing to do if left up to their own devices . The purpose of documentation is for the management of the software lifecycle.\n Royce then discusses why so much documentation should be provided and why it is so important during the design phase of a software development effort. As he says: Each designer must communicate with interfacing designers, with his management and possibly with the customer.  A verbal record is too intangible to provide an adequate basis for an interface or management decision . An acceptable written description forces the designer to take an unequivocal position and provide tangible evidence of completion.  It prevents the designer from hiding behind the \u2013 \u2018I am 90 percent finished\u2019 - syndrome month after month .   [2] During the early phase of software development the documentation is the specification and is the design.  Until coding begins these three nouns (documentation, specification, design) denote a single thing. If the documentation is bad, the design is bad. If the documentation does not yet exist there is as yet no design, only people thinking and talking about the design  which is of value, but not much.   [2] Royce also makes the point that documentation serves important purposes far beyond the design stage. \n    Indeed, he says  The real monetary value of good documentation begins downstream of the development process during the testing phase and continues through operations and redesign . The value of documentation can be described in terms of three concrete, tangible situations that every program manager faces.   [2] During the testing phase,  with good documentation the manager can concentrate personnel on the mistakes in the program. Without good documentation, every mistake \u2026 is analyzed by one man who probably made the mistake in the first place  as he is the only man who understands the program area. During the operational phase, with good documentation the manager can use operation-oriented personnel to operate the program and to do a better job, cheaper. Without good documentation the software must be operated by those who built it. \u2026 It should be pointed out that in this connection that in an operational situation,  if there is a hangup the software is always blamed first. In order to either absolve the software or fix the blame, the software documentation must speak clearly . Following initial operations, when system improvements are in order,  good documentation permits effective redesign, updating and retrofitting  in the field. If documentation does not exist, generally the entire existing framework of operating software must be junked, even for relatively modest changes. \n        The key point here is that Royce is looking at documentation from the Program Management perspective, as he is the entire software development lifecycle.\u00a0 The main purpose of documentation is enable management of not just initial development, but also of operations, maintenance, and future development.\n     \n\n\n\u00a0\n\n Royce\u2019s Five Fixes #3 \u2013 Do It Twice \n    Royce\u2019s third  fix  was to  Do It Twice . Or essentially, start off with prototype when building entirely new software. Royce states:\n If the computer program in question is being developed for the first time, arrange matters so that the version finally delivered to the customer for operational deployment is actually the second version  insofar as critical design / operational areas are concerned . \u2026 The point of all of this, as with a simulation, is that questions of timing, storage, etc. which are otherwise matters of judgment, can now be studied with precision. Without this simulation the project manager is at the mercy of human judgment.   [2] \n    Note that even with this change Royce does not seem to be saying that every last detail of the program must be finalized and irrevocable.\u00a0 The purpose of this simulation is to validate what are commonly called  non-functional requirements .\u00a0 Does the designed architecture support the levels of performance necessary?\u00a0 Does the system have enough memory, enough storage, and enough processing power?\u00a0 Can the system design support the operational needs of the customer?\n \n\n\u00a0\n Royce\u2019s Five Fixes #4 \u2013 Plan, Control and Monitor Testing \n    Royce\u2019s fourth  fix  was focused on the testing phase.\u00a0 This was the phase of greatest risk and greatest effort.\u00a0 He states,  Without question the biggest user of project resources, whether it be manpower, computer time, or management judgment, is the test phase .\u00a0 He even acknowledges that the prior three recommendations  are all aimed at uncovering and solving problems before entering the test phase .\n \n    Royce has several suggestions for testing, most of which may no longer be true with modern test automation capabilities.\u00a0 For example, one suggestion Royce made was to review every line of code by human eye because using computer time for that is too expensive.\u00a0 However, he does emphasize the benefits of good documentation in this effort when he says:\n Many parts of the test process are best handled by test specialists who did not necessarily contribute to the original design. \u2026  With good documentation it is feasible to use specialists in software product assurance who will, in my judgment, do a better job of testing than the designer .   [2] \n    So again we see Royce\u2019s perspective as a software program manager who is trying to control costs AND find ways to ensure the software delivered is of high quality as possible.\n \n\n\u00a0\n Royce\u2019s Five Fixes #5 \u2013 Involve the Customer \n    Royce\u2019s fifth  fix  is one that may surprise those who think customers in the  Waterfall  process are only involved in the requirements and operations phases, with some involvement in the testing phase. As Royce says:\n For some reason what a software design is going to do is subject to wide interpretation even after previous agreement. It is important to involve the customer in a formal way so that he has committed himself at earlier points before final delivery.  To give the contractor free rein between requirement definition and operation is inviting trouble .   [2] Royce then lists specific places in the program effort where the customer should be involved,  at a minimum . These are: During the requirements phase, with customer sign-off of the requirements During the preliminary design phase, with customer review and approval of the initial system design During the detailed design phase, with the customer review and approval of the detailed design of every system component During the final software acceptance reviews \n    However, it should also be noted that the caption under the diagram that shows the formal recommendations above [Figure 9 in the Royce paper] says  Involve the customer \u2013 the involvement should be formal, in-depth, and continuing .\n \n\n\u00a0\n Royce\u2019s Recommendations \u2013 The Final Result \n    All of the recommendations above are captured in the final diagram of Royce\u2019s paper [Figure 10], which show a far more complex process than is usually attributed to  Waterfall .\u00a0 This is what I call \u2018Royce\u2019s\u2019 Waterfall when I need to separate it from \u2018Frozen\u2019 Waterfall, and is what Royce actually recommends and proposes in his paper:\n Figure 10 DOD Standard 2167 \n    So if Royce did not describe the \u2018frozen\u2019 Waterfall methodology, where did it come from?\u00a0 The next culprit named is usually the U.S. Department of Defense, which supposedly adopted the Waterfall methodology in DOD Standard 2167 (published 4 June 1985) for the acquisition of  Mission Critical Computer Systems .\u00a0 In doing so, it is argued that the DOD  forced  the \u2018frozen\u2019 Waterfall model upon the rest of the world.  [29] The argument is that the DOD made some slight accommodations to less rigid structure when they adopted the 2167A revision of the standard, but that even that fell short. And if you look at page 2 of the standard document, it sure looks like the military has adopted the  traditional  Waterfall model when you see this diagram: The diagram seems to be implying a linear, sequential process, with no iteration and revisiting prior stages. \n    This potential understanding is not helped if the reader skims through the document looking for the description of the Software Development Cycle, which proscribes the following steps:\n Software Requirements Analysis. \u00a0The purpose of Software Requirements Analysis is to completely define and analyze the requirements for the software. These requirements include the functions the software is required to accomplish as part of the system, segment, or prime item. Additionally, the functional interfaces and the necessary design constraints are defined. During Full Scale Development, and Production and Deployment, this phase typically begins with the release of the SSS [System/Segment Specification], Prime Item Specification(s), Critical Item Specification(s), or Preliminary SRS(s) [Software Requirements Specification] and IRS(s) (Interface Requirements Specification), and terminates with the successful accomplishment of the SSR. During this phase, analyses and trade-off studies are performed, and requirements are made definitive. The results of this phase are documented and approved requirements for the software. At the initiation of Software Requirements Analysis, plans for developing the software are prepared or reviewed (as applicable). Preliminary Design. \u00a0The purpose of Preliminary Design is to develop a design approach which includes mathematical models, functional flows, and data flows. During this phase various design approaches are considered, analysis and trade-off studies are performed, and design approaches selected. Preliminary Design allocates software requirements to TLCSCs, describes the processing that takes place within each TLCSC [Top-Level Computer Software Component], and establishes the interface relationship between TLCSCs. Design of critical lower-level elements of each CSCI may also be performed. The result of this phase is a documented and approved top-level design of the software. The top-level design is reviewed against the requirements prior to initiating the detailed design phase. Detailed Design. \u00a0The purpose of Detailed Design is to refine the design approach so that each TLCSC is decomposed into a complete structure of LLCSCs [Lower-Level Computer Software Components] and Units. The detailed design approach is provided in detailed design documents and reviewed against the requirements and top-level design prior to initiating the coding phase. Coding and Unit Testing. \u00a0The purpose of Coding and Unit Testing is to code and test each Unit of code described in the detailed design documentation. Each Unit of code is reviewed for compliance with the corresponding detailed design description and applicable coding standards prior to establishing internal control of the Unit and releasing it for integration. CSC Integration and Testing.  The purpose of CSC [Computer Software Component] Integration Testing is to integrate and test aggregates of coded Units. Integration tests should be performed based on documented integration test plans, test descriptions, and test procedures. CSC Integration test results, and CSCI test plans, descriptions, and procedures for testing the fully implemented software are reviewed prior to the next phase of testing. CSCI Testing. \u00a0The purpose of CSCI testing is to test the fully implemented CSCI [Computer Software Configuration Item]. Testing during this phase concentrates on showing that the software satisfies its specified requirements. Test results should be reviewed to determine whether the software satisfies its specified requirements. \n\u00a0\n\n \n    However, actually reading through the standard begins to immediately show that this assumption is not correct, as the following statements from just the first 4 pages of the standard indicate:\n This standard is intended to be dynamic and responsive to the rapidly evolving software technology field. As such, this standard should be selectively applied and tailored to fit the unique characteristics of each software acquisition program.  (Page iii, item 2) Software development is usually an iterative process, in which an iteration of the software development cycle occurs one or more times during each of the system life cycle phases [Figure 1].  (Page 1, section 1.2) This standard, or portions thereof, may not apply to small applications which perform a fixed function that is not expected to change for the life of the system.  (Page 1, section 1.2.2) Software shall be developed in accordance with this standard to the extent specified in the contract clauses, SOW, and the Contract Data Requirements List. \u2026 The contracting agency will tailor this standard to require only what is needed for each individual acquisition.  (Page 4, section. 1.3) \n    And as with Royce\u2019s paper, it becomes quickly apparent that the DOD is espousing a [project] management view of the software development process.\u00a0 On page 15 the DOD lays out a contractor\u2019s obligation for the quality of the software, in which they say:\n \n        The contract shall plan and implement the software development project with the objective of building in quality.\u00a0 To achieve this quality, the contractor shall:\n     Establish and maintain a complete set of requirements for the software. These requirements shall serve as the standard against which software quality is evaluated. Establish and implement a complete process, including methodologies and tools, for developing the software and its documentation. The process shall be designed to build quality into the software and its documentation to maintain the level of quality throughout the life cycle of the software. Establish and maintain a process to evaluate the software, associated documentation, and the software development process. The objective of this process shall be to improve the quality of the software and its documentation, by providing feedback and ensuring that necessary corrections are made. \n    Like Royce, the DOD is looking at the full lifecycle of a piece of software.\u00a0 They envisage the need for developers other than the contractor(s) to maintain and enhance the application being developed, and they see documentation as being a critical support tool for that process.\n \n    This is further confirmed later in the document when there are explicit mandates for the contractor to provide standard user instructions (section 5.2.1.8), error identification and diagnostics instructions (section 5.2.1.9), and system life cycle support documentation (5.2.1.10).\n But even in this standard, the DOD is not mandating the use of this approach all of the time, even for critical software. Item 4.8 on page 16 makes this clear when it states: The contractor shall use a top-down approach to design, code, integrate and test all CSCIs [Computer Software Configuration Item],\u00a0 unless specific alternative methodologies have been proposed  in either the SSPM [Software Standards and Procedures Manual] or SDP [Software Development Plan]  and received contracting agency approval . And when it states: The contractor may depart from a top-down approach to: (1) address critical lower-level elements or (2) incorporate commercially available, reusable, and Government furnished software.  (Page 28, item 5.3.1.4) \n    A more detailed examination would also show that the Software Development Cycle is described as being part of a larger System Development Cycle that is not captured in the diagram shown above. The description the System Development Cycle stages is enlightening. It states:\n The system life cycle consists of four phases: Concept Exploration, Demonstration and Validation, Full Scale Development, and Production and Deployment. The software development cycle consists of six phases: Software Requirements, Analysis, Preliminary Design, Detailed Design, Coding and Unit Testing, CSC Integration and Testing, and CSCI Testing.  The total software development cycle or a subset may be performed within each of the system life cycle phases. Successive iterations of software development usually build upon the products of previous iterations . (Page 61, section 20.4) Concept Exploration . The Concept Exploration Phase is the initial planning period when the technical, strategic, and economic bases are established through comprehensive studies, experimental development, and concept evaluation. This initial planning may be directed toward refining proposed solutions or developing alternative concepts to satisfy a required operational capability. During this phase, proposed\u00a0solutions are refined or alternative concepts are developed\u00a0using feasibility assessments, estimates (cost and schedule, intelligence, logistics, etc.), trade-off studies, and analyses. The SSA [Software Support Agency] and user should be involved in these activities. For computer resources,\u00a0the software development cycle should be tailored for use during this phase\u00a0and may result in demonstration of critical algorithms, breadboards, etc. Demonstration and Validation . The Demonstration and Validation Phase is the period  when major system characteristics are refined through studies, system engineering, development of preliminary equipment and prototype computer software, and test and evaluation . The objectives are to validate the choice of alternatives and to provide the basis for determining whether or not to proceed into the next phase. During this phase, system requirements, including requirements for computer resources, are further defined,\u00a0and preferred development methodologies for computer software and data bases are selected. The results of validation activities are used to define the system characteristics (performance, cost, and schedule) and to provide confidence that risks have been resolved or minimized. For computer resources, the software development cycle should be tailored for use during this phase, resulting in prototype software items. Full Scale Development . The Full Scale Development phase is the period when the system, equipment, computer software, facilities, personnel subsystems, training, and the principal equipment and software items necessary for support are designed, fabricated, tested, and evaluated.  It includes one or more major iterations of the software development cycle.  The intended outputs are a system which closely approximates the production item, the documentation necessary to enter the system\u2019s Production and Deployment phase, and the test results that demonstrate that the system to be produced will meet the stated requirements.  During this phase the requirements for additional software items embedded in or associated with the equipment items may be identified . These requirements may encompass firmware, test equipment, environment simulation, mission support, development support, and many other kinds of software. Software requirements analysis is performed in conjunction with system engineering activities related to equipment preliminary design. SRSs and IRSs for each CSCI are completed and authenticated at the SSR, establishing the Allocated Baseline for each CSCI. Requirements for software that is part of an HWCI [Hardware Configuration Item] may be authenticated during HWCI design reviews. The OCD [Operational Concept Document] is completed and reviewed at the SSR as well. A preliminary design effort is accomplished and results in a design approach. For computer software, preliminary design includes the definition of TLCSCs in terms of functions, external and internal interfaces, storage and timing allocation, operating sequences, and data base design. Detailed design of critical lower-level elements of the CSCI may be performed as well. Production and Deployment . The Production and Deployment Phase is the combination of two overlapping periods. The production period is from production approval until the last system item is delivered and accepted. The objective is to efficiently produce and deliver effective and supported systems to the user(s). The deployment period commences with delivery of the first operational system item and terminates when the last system items are removed from the operational inventory. After a system is in operational use, there are a variety of changes that may take place on the hardware items, software items, or both hardware and software items .\u00a0  Changes to software items may be necessary to remove latent errors, enhance operations, further system evolution, adapt to changes in mission requirements, or incorporate knowledge gained from operational use . Based upon complexity and other factors such as system interfaces, constraints, and priorities, control of the changes may vary from on-site management to complex checks and balances with mandatory security keys and access codes. \u2026\u2026 The same six phases of the software development cycle are utilized for each change during the Production and Deployment phase (see Figure 4).\u201d \n\u00a0\n\n \n    Signs that the DOD was not mandating the \u2018frozen\u2019 Waterfall structure are further shown on page 69 of the standard, in section 20.4.5,  Software Development Cycle Application and Documentation . This section includes such statements as:\n The software development cycle may span more than one system life cycle phase, or may occur in any one phase. The phases in the software development cycle may involve iterations back to previous phases. For example, design may reveal problems which lead to the revision of requirements and re-institution of certain analyses; checkout may reveal errors in design, which in turn may lead to redesign or requirements revision; etc. \n    So by reading the specification it would seem that what the DOD actually proscribed was a hybrid of incremental and iterative processes, implemented within a version of the more process-controlled  Waterfall  structure that Royce proposed.\u00a0 It was closer to the  strict  Waterfall methodology in the design and coding of each phase of a software component, but not of the entire solution.\n Note that the DOD standard specifically supports such activities as: Multiple iterations Prototyping User involvement in solution design and specification The discovery of new requirements during and after a software cycle Testing from very early phases And even alternative development methodologies \n    In the end, I suspect a similar issue occurred with the DOD Standard as with the Royce paper.\u00a0 But rather than people just looking at the pictures (or even just the first picture) and not bothering to read the text; you had a lot of DOD personnel and contractors who looked at the pictures, skimmed a bit of the text, and following the tradition of bureaucratic processes everywhere, implemented it in a rigid and detailed manner as possible in order to CYA ( Cover Your Ass  for those unfamiliar with the acronym).\n \n\n\u00a0\n\n\n The \u2018Freezing\u2019 of the Waterfall \n    Given the above, the exact origin of the \u2018frozen\u2019 Waterfall interpretation is a bit of a mystery to me [ I may not have come across the right reference yet, I fully admit ].\u00a0 I see this methodology described in many places, but I can\u2019t find an origin for what is commonly described as  Waterfall  in any of the papers that are cited as the \u2019source\u2019 of the Waterfall concept.\n \n    I would go so far as to say that the highly rigid process commonly described as  traditional development , or what I call \u2018frozen\u2019 waterfall, has probably never been practiced since the 1960\u2019s or before.\u00a0 Even in 1970 Royce was describing the common software development process of the time as being at least a partially iterative process.\u00a0 I don\u2019t know a single Business Analyst who has ever been told to use this exact, highly rigid, methodology [ although again, I will admit there is certainly that possibility ].\n \n    However, I would argue that such rigidness should not be blamed on a methodology that does not seem to exist as more than a theoretical straw man, [9]  and that instead the problem in those cases is the people advocating such rigidness.\n \n    So that raises the question, how did the common perception of the Waterfall Model as being so incredibly rigid and silo\u2019d come about?\u00a0 Unfortunately, I can\u2019t tell you.\u00a0 I do know that by 1988 you can find quotes like this one from academic literature:\n The waterfall model makes the assumption that all activity of a certain type occurs during the phase of that same name and that phases do not overlap. Thus all requirements for a project occur during the requirements phase; all design activity during the design phase.   [28]     And by 1994 you can find quotes like the one below in academic papers: The so-called  Waterfall  model has been, until recently, the most frequently used model for controlling and guiding complex software development projects.  The basic idea underlying this model is that development proceeds in stages. Each stage must or phase must be finished in its entirety before a new phase can start. Just as water in a waterfall cannot flow back, phases that are finished should not be started again.   [27] [Emphasis mine] \n    Unfortunately, these just point to time periods where the perception of \u2018Waterfall\u2019 had shifted to the \u2018Frozen\u2019 interpretation, but not the source or reason for that perception.\u00a0 What\u2019s funny is that neither of the two papers above cites the Royce paper when talking about \u2018Waterfall\u2019.\u00a0 Indeed, neither paper cites ANY source for their statements, which indicates to me that they thought their understanding of Waterfall was so commonly understood and agreed upon that no citation was necessary.\n \n    The best reason I can find for the emergence of the \u2018Frozen\u2019 Waterfall concept comes from a blog post by Professor David Dischave, in which he recites the following story:\n My first encounter with the mythical  Waterfall  methodology was in the early 1980\u2019s. As the director of a systems development department at a fortune 100 company I received a visit from a salesman from Coopers & Lybrand (C&L.) As the director of a multi-million dollar IT department you can imagine how many people wanted to sell me stuff. The C&L sales rep, attempting to sell me a methodology called Summit-D, asked me what systems development method we used. So I shared with him that our shop standard was the Systems Development Lifecycle which entails doing a bunch of tasks and activities and we grouped in five phases: planning, analysis, design, implementation and maintenance & support. I went on to say; you know, we subscribe to Winston Royce\u2019s work. \n     \n    Before I could finish my description, the C&L sales rep quipped right back with 'ahhh, you are using that old obsolete Waterfall model. Oh, by the way who is Winston Royce?' Not waiting for an answer, he added, nobody should use the Waterfall model anymore. 'You see, Dave, once you complete a phase it is frozen.' I asked, what is this  Waterfall  thing that you keep referencing? The C&L sales rep said it is the method almost every company uses, where the phases are worked sequentially i.e. in lock step and no phase can start until the previous one finishes). He went on to say that all deliverables produced in a phase were frozen once that phase ended. He said each phase cascades down into the next, you know, like a waterfall. 'See, it is called waterfall because water just can\u2019t flow up hill.' You can imagine what I must have been thinking.  At that time, I\u2019d been in IT application development for 20 years with five different major corporations and I was introduced to waterfall - by a salesman. I had never heard of a waterfall method. In all of the years and all of the places I worked and all of the conferences I had been to, I didn\u2019t know of any organization that built systems this way.  Yes, I did try to dissuade the sales rep that we were not using any waterfall, watering hole, water table, water can or water cooler methods  but as you can suspect sales folks can\u2019t sell you a solution if you don\u2019t have a problem and he was really trying to create a problem. \u201d  [10] \n    The idea that the common perception of \u2018Waterfall\u2019 as being of the \u2018Frozen\u2019 interpretation could be due to nothing more than sales folks needing a straw-man to compare their \u2018new and improved\u2019 product against doesn\u2019t really seem that outrageous to me.\u00a0 But if anyone can provide alternative explanations please let me know.\n \n\n\u00a0\n The Many Misconceptions of Waterfall IMPORTANT NOTE:  This section attempts to address to some of specific misconceptions about  Waterfall model  that I see out there. Importantly , I equate  the Waterfall model  with what Royce actually described in his paper, with the possibility of expanding that definition to include what is in DOD-2167. \u00a0 At its most restrictive interpretation, you might equate this to Figure 3 in Royce\u2019s paper, or at the wider end you may equate it to Figure 10 in his paper or to the full scope of DOD-2167.\n \n    I am going to assume that you have not read the historical information above, and will try to summarize that information where appropriate.\u00a0 However, in some cases where there is too much information to easily summarize, I will simply refer to the information above.\n \n\n\u00a0\n Royce Only Described  Waterfall  as an Example of What Not to Do \n    One myth about Waterfall that seems arise frequently in one form or another is that Royce discussed \u2018Waterfall\u2019 only as an example of what not to do, or as a straw-man that he could then tear down.  [35]  This myth seems to have multiple ideas behind it. Among the most common quotes from Royce cited for this belief are the following:  [2] \u2026 a more grandiose approach to software development is illustrated in Figure 2 \u2026 the implementation described above is risk and invites failure And then you get people who misunderstand [I think] Royce and who state things such as the following: \u2026 there\u2019s no empirical evidence to back the claim that a linear process works, and actually there\u2019s not even a fluffy \u201ci think so\u201d claim to that effect , but rather Royce considers the idea unworkable .   [31]  [emphasis mine] There are a couple of problems here. \n    The first problem is the misunderstanding that what many people think of as \u2018the Waterfall model\u2019 isn\u2019t what he talked about even in the early part of his paper.\u00a0 It is only an incomplete diagram that is used to provide an overview.\u00a0 This is especially common for people who take the statement  a more grandiose approach to software development is illustrated in Figure 2 , and equate Figure 2 with the Waterfall model that his paper was discussing.\n The second problem is the belief that Royce was describing a linear process at all. Part of the problem is that Figure 2 IS the model that most people think was what Royce was describing in his overall paper. Here is Figure 2 for reference: \n    But that\u2019s not the specific process Royce is discussing for most of the paper. It\u2019s not even the process he criticizes in the paper. It\u2019s just a high-level overview (a \u2018grandiose\u2019 view). This is made clear by the start of the paragraph directly after the quote above, in which he says  Figure 3 portrays the iterative relationship between successive development phases for this scheme . Note that  this scheme  refers to Figure 2, and Figure 3 is showing  the iterative nature  of Figure 2. And here is Figure 3 for reference:\n \n\n\u00a0\n\n \n    It is about Figure 3 that Royce states  I believe in this process, but the implementation described above is risky and invites failure .\u00a0 However, he then goes on to discuss the testing phase exposing issues such as  timing, storage, input/output transfers, etc.   [2] \u00a0 You\u2019ll note that these aren\u2019t standard \u2018software requirements\u2019.\u00a0 Rather they fall into what we commonly call \u2018non-functional requirements\u2019 and which are highly-dependent on the system design.\u00a0 As Royce says,  These phenomena are not precisely analyzable .\n So right there I think we can dismiss the idea that Royce was only describing something he thought would fail or that was  unworkable . This is further supported by the start of the second paragraph after the sentence quoted above, which states: However, I believe the illustrated approach to be fundamentally sound. The reminder of this discussion presents five additional features that must be added to this basic approach to eliminate most of the development risks.   [2] Royce then goes on to suggest 5 enhancements for the basic process that his describes in Figure 3, and which are all documented above. \n\n\u00a0\n Royce / Boehm Was Responsible for Adding Iterative Features to the Base Waterfall Model \n    Another misconception I\u2019ve seen in a few places is that either Royce  [32]  or Boehm added iterative features to the  standard  Waterfall model.\u00a0This misconception usually refers to a figure similar to Figure 3 from Royce\u2019s paper, which is displayed below.\n \n    However, as stated above, Royce says  Figure 3 portrays the iterative relationship between successive development phases for this scheme .\u00a0 Note that  this scheme  refers to Figure 2 which was most people think of as the Waterfall model, and Figure 3 is showing  the iterative nature  of Figure 2.\n So the \u2018Waterfall Model\u2019 included these features from the beginning and they actually pre-date Royce. \n\n\u00a0\n Royce Only Intended the Waterfall Model to Be Used For The Simplest of Efforts \n    This misconception is directly contradicted in the very first line of the Royce paper when he states that he is providing his personal views on  managing large software developments . Indeed, he clearly states that for the simplest of efforts, you only need the Analysis and Coding steps that are shown in Figure 1 of his paper.\u00a0 The whole point of both the Royce and Benington papers was that they were targeted at very complex, very large development efforts.\n \n\n\u00a0\n Waterfall Means Never Working Iteratively \n    As stated in DOD 2167,  Software development is usually an iterative process, in which an iteration of the software development cycle occurs one or more times during each of the system life cycle phases .  [3] \n    The DOD\u2019s system life cycle fully envisions multiple tracks of concurrent and iterative development.\u00a0 Royce\u2019s Waterfall was definitely less explicitly iterative, but its specific inclusion of prototypes and iterations back to prior steps (both the immediately prior stage and larger steps back) would indicate that iterative development was at least minimally supported, if not fully expected.\n \n\n\u00a0\n Waterfall Means Doing All Development at Once \n    Royce\u2019s Waterfall model included the option of iterating back to prior steps in the process.\u00a0 Most commonly the immediately-prior step, but also further back.\u00a0 So while in general most development would be done in one large stage, it allowed for options to do it later.\u00a0 Also, the use of a fully developed prototype by Royce allows for early development to be done, especially in the area of confirming system design options.\u00a0 Indeed, this was the main point of the prototype step envisioned by Royce.\n \n    This was further emphasized by the DOD in standard 2167 when it said,  Software development is usually an iterative process, in which an iteration of the software development cycle occurs one or more times during each of the system life cycle phases .  [3] \n\n\u00a0\n Waterfall Means No Customer Involvement After Requirements \n    A common misconception is something like  Customers don\u2019t provide any input until the solution is delivered or in testing .  [4] \u00a0 This simply isn\u2019t true.\n \n    According to Royce customers  must  provide input to the requirements; sign off on the baseline requirements (which are fully documented); review and approve the preliminary technical design (also fully documented); review and approve the interface design (again, fully documented and which would presumably include wireframes and similar visual models); review and approve the detailed program designs (multiple reviews per Royce\u2019s model) before ANY code is generated; interact with and review a prototype (if created); review the test plans; and finally, test and approve the working code. This is hardly consistent with the idea of  no involvement after requirements .\n \n    But that is just customer involvement in documentation and design plans.\u00a0 Royce espouses the use of an initial prototype in his document.\u00a0 But modern prototyping and simulation tools (such as iRise as an example) didn\u2019t exist then.\u00a0 With tools like those and the use of Model-View-Controller (MVC) design methods, there is nothing in Waterfall that prevents users from being heavily involved in the design and implementation of the user interface long before the rest of the software is \u2018done\u2019.\n \n\n\u00a0\n Waterfall Means Setting Impossible Deadlines \n    This is a myth I\u2019ve found associated to Waterfall that baffles me.\u00a0 Nowhere in any of the \u2018source\u2019 documents for Waterfall is there any discussion of setting deadlines, rigid schedules, or even specifying the exact work that has to be done to implement a software program (the detailed design) early in the process.\u00a0 Not in Benington, not in Royce, not in DOD-2167.\u00a0 But I see references to Waterfall including some assumption of an ability to  reliably predict when we are going to deliver a system   [36]  or something to that effect.\n \n    Yes, Waterfall is a \u2018planned\u2019 methodology.\u00a0 But planning does not mean setting deadlines.\u00a0 All of the planning discussed in the sources I reference above is about planning features, requirements, designs, and system resources.\u00a0 It\u2019s about understanding what you need, as much as possible, and then creating a plan for software that fulfills those needs.\n \n    All three main sources indicate that there will be changes needed even after a software program goes into operations.\u00a0 All three state that there is significant unpredictability in developing software.\n Deadlines  are no more an aspect of Waterfall than they are of \u2018Agile\u2019 or any other development methodology.\u00a0 They are a factor of management and markets.\u00a0 Nothing more.\n \n\n\u00a0\n Waterfall Means that Time from Specification to Delivery is Great \n    Another misconception is that Waterfall is that  it often took a long time between the requirement phase and the user feedback phase ,  [33]  entailing  months or more  of delay from specification until working software is delivered.\u00a0 Or as Scott Ambler put it:\n The period between the time that the requirements are 'finalized' to the time that the system is actually deployed will often span months, if not years. During this timeframe changes will occur in the marketplace, legislation will change, and your organization's strategy will change. All of these changes in turn will motivate true changes to your requirements.   [40] \n    This is an issue of scope, rather than methodology.\u00a0 There is absolutely nothing in the overall Waterfall concept that prevents chunking the business needs into smaller pieces, documenting them as discreet requirements, and delivering those smaller chunks rapidly as multiple, sequential Waterfall efforts.\u00a0 That is literally what the original  iterative  methodology concept was.\u00a0 Essentially using the  Program of multiple Projects  approach to deliver multiple chunks of an overall solution in smaller, more rapid phases.\u00a0 Indeed, this is exactly what DOD-2167 does with its larger System Lifecycle and smaller Software Lifecycle.\n There is also nothing in Waterfall that prevents changes to the requirements or design as the solution moves forward.\u00a0 Discover a new requirement?\u00a0 Evaluate it and if it\u2019s worth adding, start the change management process to make the appropriate changes. \n\n\u00a0\n The Author(s) of DOD-2167 Had Never Heard of Iterative Development \n    A fairly common misconception is that the author(s) of the DOD-2167 standard had never heard of, or weren\u2019t familiar with, Iterative Development.\u00a0 In its more prosaic form this myth gets stated like this:\n But, things change as a busy engineer in the US defense organization is asked to come up with a standard for military grade software projects. He doesn\u2019t know what a good process would be, and he\u2019s told that 'Hey, Royce already came up with the correct method: the waterfall. Use it.' So the waterfall becomes the US military standard DoD-2167.   [15] The main source of this seems to be a book by Craig Larson in which he states that the primary author of the standard told him: He was not familiar with the practice of timeboxed iterative development and evolutionary requirements at the time.   [30] I can\u2019t identify the author(s) from the specification, and Mr. Larson does not identify his source in his book, so I have no way to further research this. However, in looking at the standard itself this seems extremely implausible.\u00a0 Consider that the 3 rd  sentence (and start of the 2 nd  paragraph) of DOD-2167 says the following: Software development is usually an iterative process, in which an iteration of the software development cycle occurs one or more times during each of the system life cycle phases. I will grant that the  time-boxed  part that statement is probably true.\u00a0 And possibly even the  evolutionary  part, if not under that name.\u00a0 However, given the actual text of DOD-2167 it seems less likely that the author(s) were not familiar with the concept of evolutionary development.\u00a0 Although again, perhaps not under that name.\n \n    Add in that it seems unlikely that the DOD would rely on a single person as the  main  author of such a standard when they would presumably have had access to a wealth of highly-knowledgeable resources such Barry Boehm and others who had years of experience (if not decades) in building large systems for government contractors like TRW.  Or that the military (and US Government in general) tends to turn towards think-tanks like Rand or government contractors like IBM to provide model standards for things like software development.\n In the end, I simply can\u2019t find any plausible information that would suggest this myth is true. \n\n\u00a0\n Slow Reaction and High Cost of Change \n    Another misconception is that Waterfall must be slow to react to change and that the cost of change is great.\u00a0 While a formal change control process means that Waterfall may not react to change immediately, it does mean that change is at least evaluated before it is made.\u00a0 Because change can be made at any point, even before coding has begun, I would argue that change is potentially cheaper in Waterfall (in some instances) because change can be made before any development (with its high modern cost) has been done.\n \n    With  agile , change is usually only identified after some coding has been done. While this might make some changes easier to identify (as there is working code), it also means you have already paid development staff to create that software.\u00a0 And in the modern cost structure of software development, development work is usually among the highest-cost activities that are undertaken.\n \n    Yes, the change may be smaller since in theory the sprint is working on a smaller functional base, but that does not mean that a change will not require significant architecture changes that mandate the re-work of major portions of prior sprints.\u00a0 Indeed, in my experience the need for significant architectural change (and its associated delays and costs) is more common among large-scale \u2018agile\u2019 projects than among large-scale \u2018waterfall\u2019 projects.\n \n\n\u00a0\n Is the \u2018Waterfall\u2019 Name Permanently Tainted? \n    Despite all the information above, I\u2019m sure that the term \u2018Waterfall\u2019 will continue to be associated with the \u2018Frozen\u2019 interpretation that has now become the common understanding in software circles of what \u2018Waterfall\u2019 means.\u00a0 This is regrettable because Winston Royce\u2019s name is still commonly associated with that interpretation, despite it having virtually no connection to what his paper described.\n \n    It also seems increasingly anathema to many in the software world to speak positively of planned development methodologies like Royce described, or even later evolutions of the concept.\u00a0 Trying to say anything good about \u2018Waterfall\u2019 on an internet discussion forum will generally make you a target for derision, scorn, or outright hostility.\u00a0 So what do you do if you want to talk about planned development but find the term \u2018Waterfall\u2019 has become too toxic to have meaningful conversations about?\n \n    Some might use the term  Big Development Up Front , [38]  but that seems to have become just a synonym for \u2018Waterfall\u2019 because it assumes  the programs design is to be completed and perfected before that programs implementation is started .  [39] The best option I have found are terms like  Planned Development  which the IIBA seems to have adopted for all non-agile methodologies. \n    But even that will run into those who feel that any attempt to plan is a mistake.\u00a0 I have my own disagreements with that point of view, but they are outside the scope of this article.\u00a0 All I can say is that no matter what the original authors proposed, the term \u2018Waterfall\u2019 seems to be inextricably linked with the \u2018Frozen\u2019 interpretation and so your use of the term should take that into account.\n \n\n\u00a0\n \u2018Modern\u2019 Interpretations \n    So if the term \u2018Waterfall\u2019 is tainted, and you are looking for a more project-management focused  planned development  approach [ setting aside whether you think that is a good idea or not ], what do you do?\n \n    From the reading I have done, it seems likely that at least one reason for the misunderstanding of what Royce was describing was due to the diagrams he chose to use.\u00a0 So the following are attempts to re-configure the diagrams Royce used into something that is more likely (I hope) to show what he was proposing, or which provide a more \u2018modern\u2019 take on what \u2018Waterfall\u2019 or similar planned development approaches might look like.\n \n    The first attempt I made results in the following double-loop design. The primary flows follow the darker lines, with the grey lines being opportunities to \u2018jump\u2019 the flow to other locations.\n \n\n\u00a0\n\n \n    The other idea was to show a version that is closer to what is in DOD-2167 and which is design for the definition of an overall set of requirements, but with the phased implementation of those requirements.\n \n    This model is simplified slightly, but the idea is essentially to leverage concepts from both worlds. The idea is something like this:\n Define the requirements, and especially the entire system architecture to as great a degree as possible first. From that system design and requirements, start carving out functional sub-sets that can be built and implemented on their own. As these are implemented, integrated, and in operation (if possible); feedback is gathered and changes to the requirements are made. Those changes in requirements are incorporated into the ongoing design phase, assigned to one of the development streams, and integrated and deployed to operations when appropriate. \n    This results in an \u2018alternative\u2019 Waterfall that looks like the model below.\n \n\n\u00a0\n\n \n    This is the current end of this article.\u00a0 Any feedback, additional citations, and alternative ideas would be appreciated.\n \n\n\u00a0\n\n\u00a0\n References Research Paper - Production of Large Computer Programs, by Herbert D. Benington. From a presentation at a symposium on advanced programming methods for digital computers sponsored by the Navy Mathematical Computing Advisory Panel and the Office of Naval Research in June 1956. Forward with additional material added in 1987. Research Paper:  Managing the Development of Large Software Systems , by Dr. Winston W. Royce, 1970. Military Standard:  DOD-STD-2167: Defense System Software Development , 4 June 1985. Research Paper:  Toxic Concepts in Systems Analysis and Design: The Systems Development Lifecycle . By Paul Ralph. May 2010. Research Paper:  A Review of Risk Management in Different Software Development Methodologies . By Hijazi, Khdour, and Alarabeyyat. May 2012. <deleted> Wikipedia Entry:  Waterfall Model . On Wikipedia. Various Authors. Accessed on 15 October 2014. Article:  There's no such thing as the Waterfall Approach! (and there never was) . By Conrad Weisert. Information Disciplines, Inc. 8 February 2003. Wikipedia Entry:  Straw man . On Wikipedia. Various Authors. Accessed on 16 October 2014. Article:  A Waterfall Systems Development Methodology \u2026 Seriously?  By David Dischave. On the Global Enterprise Technology web site. 17 September 2012.  Original post no longer online. Link to archived copy via the Internet Archive Wayback Machine. Using this formula: (50 * (236.2 / 27.2)) = 434.19. Calculated based on information found at the The Federal Bank of Minneapolis  here :   Original post no longer online. Link to archived copy via the Internet Archive Wayback Machine. Research Paper:  Software Requirements - Are They Really a Problem?  By T.E. Bell and T. A. Thayer. Proceedings of the 2nd international conference on Software engineering. IEEE Computer Society Press, 1976. Blog Post:  The Waterfall Accident . By Pascal Gugenberger. On his personal web site. 2011.  Original post no longer online. Link to archived copy via the Internet Archive Wayback Machine. Research Paper:  The Impact of DoD-Std-2167A on Iterative Design Methodologies \u2013 Help or Hinder?  By Scott P. Overmyer. ACM SIGSOFT Software Engineering Notes. 1990. 15(5), 50-59. Blog Post:  Don\u2019t draw diagrams of wrong practices \u2013 or Why people still believe in the Waterfall model . By Tarmo Toikkanen. On his personal blog. 9 September 2005. Article:  Department of Defense MIL-STD-2167  Waterfall  is Iterative . By Kenneth Shafer. On the Legacy Guild web site. 27 December 2013. Blog Post:  Waterfall Model Probably the Most Costly Mistake in the World . By Rolf H. On the Value@Work blog. 18 April 2013. Technical Report:  Evolutionary Software Development .\u00a0 By NATO Task Group IST-026. \u00a0 August 2008. Working Paper:\u00a0  Beyond the Waterfall \u2013 Software Development at Microsoft . By Michael A. Cusumano and Stanley Smith. MIT Sloan School of Business. 16 August 1995. Wikipedia Entry:  Source Lines of Code . By various authors. Accessed on November 16, 2014. Research Paper:\u00a0  A View of 20 th  and 21 st  Century Software Engineering . By Barry Boehm. Presented at ICSE\u201906 in Shanghai, China. 2006. Wikipedia Entry:\u00a0  Structured Programming .\u00a0 Various Authors. Accessed on 17 November 2014. Wikipedia Entry:\u00a0  Spaghetti Code . Various Authors. Accessed on 17 November 2014. Blog Post:  There is No Such Thing as Waterfall . By Erik Dietrich. On the DaedTech blog. September 19, 2012. White Paper:  Methodology for Rapid Development of C2 Planning Systems . By Sheena Kelsey and Simon Snell. Of QinetiQ. 2003. Research Paper:  A Spiral Model of Software Development and Enhancement . By Barry Boehm. Computer 21, no. 5 (1988). 61-72. Research Paper:  Constraint\u2010Driven Software Design - An Escape From the Waterfall Model . By R Hoog, T Jong, F Vries. Performance Improvement Quarterly, 1994. Research Paper:  Resource Utilization during Software Development . By Marvin V. Zelkowitz. The Journal of Systems and Software. 1988. Blog Post:  The Rise and Fall of Waterfall Development . By Richard Banks. On his Agile and .Net blog. 8 January 2009. Book:\u00a0 Agile & Iterative Development \u2013 A Manager\u2019s Guide. By Craig Larman. 2003.\u00a0 Pearson Education, Inc. Blog Post:  Failure of the scientific method and the waterfall method (again) . By Tarmo Toikkanen. On his personal blog. 18 October 2007. Research Paper:\u00a0  Software Development Lifecycle Models . By Nayan B. Ruparella. ACM SIGSOFT Software Engineering Notes. May 2010. Blog Post:  The waterfall \u2013 A Historical View to Organizing Software Development . By Patrik Malmquist. 5 September 2011. Article:  The seductive and dangerous V Model . By James Christie. On the Claro Testing web site. An expanded version of a December 2008 article that appeared in Testing Magazine.   Original post no longer online. Link to archived copy via the Internet Archive Wayback Machine. Book Chapter:  Why the Waterfall Model Doesn\u2019t Work . Scaling Software Agility. Research Paper:  Anchoring the Software Process . By Barry Boehm. Software, IEEE 13.4 (1996). Research Paper:  Iterative Software Development \u2013 from Theory to Practice . By Amir Torer, Boaz Shani, and Ely Bonne. RAFAEL, Israel. Wikipedia entry:  Big Design Up Front . Accessed on February 1, 2015. Article:  Examining the \u2018Big Requirements Up Front (BRUF) Approach . By Scott Ambler. On his Agile Modeling web site. Undated."}, "210705_news_467103.txt": {"page_id": "210705_news_467103.txt", "text": "I t\u2019s a truism that it takes a long time for a supertanker to change course; even if the captain spots trouble ahead it may be too late to avoid it. For the last few years, that\u2019s the metaphor that has come to mind whenever  Intel  was mentioned. For as long as most of us can remember, it has been the lumbering supertanker of the silicon chip business because its central processing units (CPUs) powered most computers. But from 2007 onwards, when smartphones, which are really just handheld computers, arrived, that world began to change and dominance in the CPU business passed to the Cambridge company Arm, which designed CPUs that were smaller and consumed far less power than their predecessors. Intel, like its erstwhile partner, Microsoft, seemed lost in this new environment; they were like two supertankers drifting aimlessly while continuing to make comfortable profits from their legacy businesses. Watching the apparently relentless growth of Arm, and Intel\u2019s flailing attempts to make progress (including in areas such as  semiconductor lithography , the physical and chemical processes needed to etch circuits on silicon, which should have been a core competency), it was hard to see a future for it except one of inexorable decline. All of which made the news that Intel had a new CEO who was not behaving like a traditional Intel boss such a surprise. His name is Pat Gelsinger and although he started his career at Intel, for the last 11 years he\u2019s been working in smaller companies. But he\u2019s back with a vengeance, as one began slowly to realise when  watching the talk  he gave after less than 40 days at the helm, outlining the most radical shift in strategy since Andy Grove and Gordon Moore switched the company from making memory chips and into making processors more than 35 years ago. Gelsinger\u2019s ideas look genuinely radical, especially for a company with an organisational culture as powerful as Intel\u2019s. He plans to  outsource  some chip manufacturing, including some of his highest-spec processors, to third-parties. To get a feeling of what a shock that is to the Intel system, it would be like Apple outsourcing iPhone design to Ikea. But at the same time, Intel isn\u2019t abandoning its historical roots of being both a designer and a manufacturer of chips and it will  retain most of its production in-house . And Gelsinger is also proposing to make chips for others companies and targeting customers such as Apple and Qualcomm. The two most interesting things about Gelsinger\u2019s talk (for this viewer anyway) were its frankness and its implicit recognition of the way geopolitics now plays a key role in chip manufacturing. He was remarkably candid about how Intel has screwed up in recent years by backing the wrong technology in lithography. As he talked, one imagined some of his colleagues squirming in embarrassment as the extent of their misjudgments were outlined, even if their new boss went out of his way to seem non-judgmental. If western companies were to lose access to Taiwanese foundries, you might have to wait a long time for your new iPhone But in a way, his most intriguing revelation concerned the nature and location of the massive new manufacturing plants that he proposes to build. Creating a silicon chip involves two processes: the design of the circuits to be etched on to the silicon and manufacturing the designed objects at phenomenal scale. The second process happens in what are \u2013 quaintly \u2013 called silicon \u201cfoundries\u201d, plants that manufacture chips for customers such as Apple, Qualcomm, Samsung et al. Geopolitics intrudes at this point, because at the moment the dominant foundries are all located in Taiwan and, to a lesser extent, South Korea, both states that lie uncomfortably within China\u2019s sphere of interest. Gelsinger\u2019s presentation contained a slide showing that, of the world\u2019s chip-manufacturing capacity, 80% is located in that part of the world, while 15% is located in the US and only 5% in Europe. In recent times, tensions between the US and China have escalated, especially under Trump, and American pressure on chip manufacturers to avoid supplying Huawei has raised the temperature considerably. China doesn\u2019t have an indigenous chip-manufacturing capability and Taiwan is, well, only a few miles away \u2013 and was once a part of China. You don\u2019t have to be a genius to spot a strategic vulnerability: if western companies were to lose access to those Taiwanese foundries, well\u2026 you might have to wait a long time for your new iPhone. Gelsinger has spotted the problem \u2013 and the opportunity. Intel has advanced foundries in Oregon, New Mexico and Arizona, but they make chips only for its own integrated systems. A key part of his turnaround plan is that the company will greatly expand its foundry capacity \u2013 in Europe as well as the US \u2013 and that it will manufacture chips for anyone who wants them. And, funnily enough, earlier this month  the EU budgeted  \u00a3116bn for a drive towards \u201cdigital sovereignty\u201d, aspiring to increase its share of the global semiconductor market from 10% to 20% by 2030. You\u2019d have to be a dumb skipper to ignore a catch like that. What I\u2019ve been reading The real new deal Lessons From the First New Deal for the Next One , Bill Janeway\u2019s essay on the Noahpinion blog, explores the differences between the Roosevelt era and the Biden one. No robot uprising Ted Chiang\u2019s  New Yorker  essay  Why Computers Won\u2019t Make Themselves Smarter  suggests that maybe we shouldn\u2019t worry too much about superintelligent machines. A pick-me-up read There\u2019s a thoughtful, illuminating  blogpost on the Tesla Motors Club site  on what it\u2019s like driving for Uber or Lyft by someone who drives out of choice and curiosity rather than need."}, "210705_news_467149.txt": {"page_id": "210705_news_467149.txt", "text": "In some government circles, the excitement before Britain\u2019s opening-up and return to something like normality is making ministers giddy. Grant Shapps says  everyone may start booking a foreign holiday  now that he is at work revamping last year\u2019s colour-coded map of the world. With a traffic-light system of testing and quarantine rules in place, it is likely that nowhere will be out of bounds for Britons to visit. Inside No 10, last year\u2019s recession is forgotten and the possibilities during the second half of the year are considered to be almost boundless. Just the thought of public gatherings and nights at the theatre has allowed the prime minister to relax back into his pie-and-a-pint bonhomie, setting aside the accusations of bungling and lying so eloquently laid out in Peter Oborne\u2019s  book   The Assault on Truth ,  or the way Brexit seems to be hastening the breakup of Britain\u2019s union of nations. In recent weeks, every forecast of economic growth has acquired a healthier glow. Last week, the International Monetary Fund added its voice to those saying the global economy \u2013 and the UK\u2019s in particular \u2013 would enjoy a broader and faster expansion than previously estimated. The UK is expected to grow by 5.3% in 2021 and by 5.1% in 2022, making it the fastest-growing G7 country at the end of the forecast period. Senior figures at the Bank of England have fuelled the prime minister\u2019s sense that a period of unalloyed jollity, singing and dancing is within sight by using phrases such as \u201ccoiled spring\u201d to describe the strength of the bounce-back. Investment bank economists and business groups paint a similar picture: one where Britain looks like a hive of activity, wheeling and dealing its way to prosperity in a way that boosts the traditional measure of national income \u2013 GDP. Policies masquerade as strategic when the only consistent thing about them is that they are tactical It is clear, though, that all the talk of reinventing the post-Covid economy has so far been just that: talk. Last week\u2019s Halifax house price data showed that traditional markers of success remain paramount inside Downing Street.  House prices  gained 1.1% in March compared with February, taking them 6.5% higher than they were in March 2020 \u2013 well ahead of  February\u2019s average 0.4% inflation rate  and  median annual pay settlements  of 1% in the three months ending 28 February, according to XpertHR. The Treasury has pumped house prices higher with  taxpayer-funded concessions on stamp duty  that mean the \u00a3500,000 tax-free threshold will remain until 30 June and until 30 September at \u00a3250,000, before returning to its original level of \u00a3125,000. The booming shares of the big housebuilders show what we have known for decades: that government subsidies for home purchases are channelled into property-industry profits via higher prices. It is the same old Tory practice of favouring an industry that makes donations to its election war chest, cloaked in PR messages about turning Generation Rent into Generation Buy. Building on flood plains continues apace and housing density, not biodiversity, remains the highest priority of the housing minister, Robert Jenrick.  Meanwhile, the government has scrapped its most ambitious and costly measure to tackle climate change,  the \u00a31.5bn green homes grant , and  halved the science research budget  for universities, leaving a \u00a32bn hole in research funding overall. Councils are kept on a shoestring budget, and grants for \u201clevelling up\u201d are not only shovelled out by the Treasury as short-term fixes, preventing local groups from making plans beyond one year, but have become tainted as yet another way to favour Tory electoral ambitions. Jagjit Chadha, the head of the National Institute for Economic and Social Research, is not the only one to despair at the  lack of a coherent basis on which the government is devising economic policies  and then judging their success, especially against climate goals. Policies masquerade as strategic when the only consistent thing about them is that they are tactical. The Dasgupta review , commissioned by the May government, has called for a form of national accounting that includes the depletion of natural resources, because of the risk that all social and economic gains could be undermined by catastrophic increases in temperature. Its 600 pages could be the welcome, if belated, opportunity for the Johnson administration to rewrite the standard economic rules, handing ministers such as Jenrick a fresh basis for policymaking. Cambridge University\u2019s  Bennett Institute for Public Policy  has put forward a method for remodelling GDP to accommodate such things as the natural environment. Maybe it is too early to judge whether the prime minister will emerge from the pandemic armed with anything more than a smile and some back-slapping bonhomie. Sadly, the omens are not good."}, "210705_news_467156.txt": {"page_id": "210705_news_467156.txt", "text": "A fter 13 years, Google is coming back for patient health records. The tech giant has launched an early user feedback program aimed at exploring how patients might want to see, organize, and share their own medical record data. The work could inform the creation of a consumer-facing medical records tool along the lines of  Apple \u2019s Health Records app. It also follows an early attempt by Google \u2014 later panned by medical experts \u2014 at creating a new version of the electronic medical record in 2008. This time around, timing may be on the company\u2019s side: Its new effort, which is still in the early stages, came on the heels of the introduction of the federal information blocking rule, which lets patients access their medical records through health apps. \n\t\t\t\t\t\tUnlock this article by subscribing to STAT+ and enjoy your first 30 days free!\t\t\t\t\t GET STARTED What is it? \n\t\t\t\t\t\tSTAT+ is STAT's premium subscription service for in-depth biotech, pharma, policy, and life science coverage and analysis.\n\t\t\t\t\t\tOur award-winning team covers news on Wall Street, policy developments in Washington, early science breakthroughs and clinical trial results, and health care disruption in Silicon Valley and beyond.\n\t\t\t\t\t What's included? Daily reporting and analysis The most comprehensive industry coverage from a powerhouse team of reporters Subscriber-only newsletters Daily newsletters to brief you on the most important industry news of the day STAT+ Conversations Weekly opportunities to engage with our reporters and leading industry experts in live video conversations Exclusive industry events Premium access to subscriber-only networking events around the country The best reporters in the industry The most trusted and well-connected newsroom in the health care industry And much more Exclusive interviews with industry leaders, profiles, and premium tools, like our CRISPR Trackr."}, "210705_news_467160.txt": {"page_id": "210705_news_467160.txt", "text": "Or, \u201chow not to GIS\u201d. For the impatient, there\u2019s a\n github repository  for the\n maptrace  program documented here, and it comes complete with example\n inputs  and\n outputs \nfor quick perusal. Introduction From time to time, I moonlight as a\n graphic designer \nand one of the infrequent tasks I take on in this role is helping\npeople clean up geographic maps for presentation and publication. \nOne recent project involved illustrating the locations of speakers of\nvarious languages based on data from\n SIL International . Here\u2019s the resulting map as a PNG file (downsampled 8X from the original size): The  right  way to accomplish this would have been to fire up\n QGIS  or other actual  GIS software , and create a\nset of  shapefiles \ncorresponding to the geographic regions of interest. But since my\n\u201cclient\u201d  was under a time crunch, we settled on tracing over an\nexisting map in Photoshop and filling in regions by hand with the\nPaint Bucket tool. Yes, this was a gross solution but it got the\npublication out on time, and the client is currently working on a\nproper GIS representation. In the meantime, the client wanted to generate a few alternative color\nschemes to highlight particular regions for another presentation (once\nagain on a fairly rapid timetable) and I thought it would be a much\neasier task to accomplish in Illustrator rather than Photoshop, since\nvector graphics are much easier to work with when it comes to\nselecting colors for flat regions. So I wrote a program to take the original map and convert it to a\n scalable vector graphics (SVG) \nfile that can be easily imported into Illustrator. It ended up working\nout really nicely \u2013 here\u2019s the final output (click to enlarge): Let\u2019s take a look at a few details from the two images. For each of\nthese, the left-hand side comes from the original quick-and-dirty\nPhotoshop output, and the right-hand side comes from rasterizing the\nSVG output of the program. You can see lots of tiny fringing artifacts\nfrom using the Paint Bucket tool to flood-fill regions in the\noriginal; they are completely gone in the vector output. The SVG file affords a good deal of compression, too: the original PNG weighs in\nat about 2.5M (reduced to 1.0M when lossily compressed by\n pngquant ). In contrast, the output SVG is\n105K but just 38K when\n gzip-compressed . So not only does it look a lot nicer, the final output is about 25X\n smaller  than the original!   \n(Hmm, reminds me of  my noteshrink project \u2026) Caveats You should carefully consider whether  maptrace  is the right tool\nfor you! First of all, it\u2019s always better to use original\ngeoreferenced data if it\u2019s available. Second, this is an incomplete\nsystem from a GIS standpoint because it doesn\u2019t produce\n georeferenced \noutput. If your input data is georeferenced, you will lose all of this\ninformation when the program is run. Finally, please  do not use this\nsoftware to circumvent copyright restrictions on existing maps!  For\nthe purposes of copyright, you should consider the output of\n maptrace  a derivative work of the original input. So, only publish \n maptrace  output if you are licensed to reproduce the original input. Producing watertight vector maps There already exist tools to automatically convert raster (bitmap)\nimages to vector formats, including Adobe Illustrator\u2019s\n Live Trace / Image Trace \ntools or standalone programs like Peter Selinger\u2019s  potrace , which provides the  comparable functionality in Inkscape . However, all of these tools share a common shortcoming from the\nperspective of mapmaking \u2013 given a simple raster line drawing like\nthis heart: \u2026they actually produce  two  contours, one for the inside, and one for\nthe outside. You can demonstrate this by re-coloring the output of\n potrace  to stroke each contour: What we would prefer is to replace the outline with a single contour that can be stroked\nto any desired width. Here\u2019s the output from my  maptrace  tool: In actuality,  maptrace  also produces two contours surrounding the\nheart (corresponding to the borders of the light and dark regions\nabove); however, they overlap completely, allowing the user to stroke\nthe border to whatever width they please. Borrowing from the 3D modeling community, I\u2019m going to refer to this\nproperty of completely overlapping outlines as a  watertight  map,\nsince the boundary of each map region (i.e. the interior and exterior\nof the heart above) is completely coincident with its neighbor,\nleaving no gaps. Producing watertight maps was my main technical\nobjective in undertaking this project. As I was reading up when getting organized to write this post, I ran across\na couple of related efforts: I\u2019m sure this is an incomplete list, and there\u2019s probably lots more interesting related work out there. How it works Here\u2019s the step-by-step breakdown of how  maptrace  goes from a raster input to an output SVG. 1. Outline detection First, the input image is\n thresholded \nto convert the original grayscale or RGB image into a 1-bpp boolean\nmask indicating where outlines are found. The resulting mask is nonzero\nwherever the  maximum  of the red, green, or blue channels exceeds a\nbrightness threshold, OR wherever the alpha channel falls below some\nopacity threshold (e.g., we assume transparent areas are not map\noutlines). Let\u2019s illustrate on this hastily drawn grayscale yin-yang symbol image: The mask produced by the thresholding operation is zero wherever the\nyin-yang outline falls, and one everywhere else: Next, we can optionally apply\n morphological operators \nsuch as erosion, dilation, opening, or closing to the mask image. This\ncan be useful to dam small gaps in the outlines or join nearby\nregions separated by very small bits of outline. We\u2019re going to omit\nthis step for the yin-yang example, but see the\n README.md in the github repository \nfor examples (look for the  -f  command-line option). 2. Connected component analysis Now we take the mask computed above and perform\n connected-component labeling \nto identify the unique non-outline regions within the image. In our\nyin-yang example, there are five regions, each assigned a random color\nhere (the outline itself is left white): Our next step is to reduce the outline to a zero-thickness boundary. 3. Growing non-outline regions We now obtain the\n Euclidean Distance Transform (EDT) \nof the mask image. The EDT computes the distance from every outline\npixel to the closest non-outline pixel. Here is a visualization\n(brighter pixels are embedded further inside the outline): For our application of the EDT, we don\u2019t care about the distances\nthemselves; we are actually far more interested in the label of the\nclosest non-outline pixel. When each outline pixel is assigned this\nlabel, the non-outline regions grow, and we obtain a map with no gaps\nbetween them at all: Now it\u2019s time to stop working with rasters and begin representing the image as a set of vector outlines. 4. Tracing borders between regions Next, we trace along the outline of each labeled region (as well as\nthe borders of the image) to create a set of nodes and edges. Each\nedge is a sequence of steps along the boundaries of pixels bordering\ntwo distinct regions, and the nodes are their intersection points (or\narbitrary points along closed edges). The SVG figure below shows the nodes as black circles and the edges as\ndotted lines whose colors indicate the labeled regions they border: This tracing operation is by far the slowest part\nof the program, since it\u2019s all done with na\u00efve loops in Python.  It\u2019s\ntheoretically possible to speed this operation up by parallelizing it or\ncalling out to  Cython  but I\u2019d probably rather port\nthe entire program to a compiled langauge eventually instead. 5. Simplification Next, we can optionally merge nearby nodes and then apply the\n Ramer-Douglas-Peucker algorithm \nto simplify each edge, with user-supplied tolerances. This is\nuseful for eliminating the little square jaggies that result from\nfollowing pixel-aligned paths. The results look like this: 6. SVG output Finally, we generate the output SVG. For each labeled region, we loop\nover the edges that border it and concatenate them into one or more\ncontours.  We can assign random colors per region (as we have done so\nfar in this example), pull colors in from the original image, or even\nassign them from another image of the same size and shape. As a command-line option, we can tell the program to stroke the\nlargest region (usually the background or ocean) with a larger\noutline, which often looks quite nice. Here\u2019s the final output SVG for\nour yin-yang symbol example: Here\u2019s another example pulled from the github repository. The Original\ninput image is the 2012 electoral map for Pennsylvania, pulled from\n Wikipedia : \u2026and here\u2019s the  maptrace  output as an SVG file: Cool tricks & future work After connected component labeling but before region growing, we can\noptionally delete very small regions whose areas fall below a certain\nthreshold.  This is nice for getting rid of single-pixel blips, but\ncan also be used to remove text from maps, as in this US Census Bureau\nmap of Pennsylvania counties, also taken from\n Wikipedia : Here\u2019s the resulting SVG file after deleting any enclosed region whose area is under 1000 pixels: Magic, right? One area where  potrace  and its commercial analogues significantly\noutperform  maptrace  is in their ability to recognize and output\ncurved paths. In contrast, every vector output by my program is\npolygonal. They look fine when zoomed out, but don\u2019t look great at low\nresolution. Let\u2019s call that future work. Additionally, since it\u2019s written in Python,  maptrace  is much slower\nthan an optimized implementation in a compiled language would be. Not\nsure when I\u2019ll get around to it, but I\u2019ve been meaning to try out\n Rust  for a while, maybe I\u2019ll take a crack at\nporting it someday\u2026 Anyways, that\u2019s about it for now. Feel free to follow up on twitter\nwith any questions or comments you might have!"}, "210705_news_467222.txt": {"page_id": "210705_news_467222.txt", "text": "I n 2003, Praveen Kumar bought his first mobile phone, as several of his friends already had. But for the 29-year-old schoolteacher from the eastern state of Bihar, one of India\u2019s poorest, to use the device to talk to his friends was inadvisable. \u201cIt was very costly,\u201d Kumar says. Each outgoing minute was charged about eight cents, so a 10-minute call cost about as much as the average daily wage at the time. \u201cWe didn\u2019t have enough money to say everything on the phone.\u201d So Kumar\u2019s friends and family members began ringing each other but hanging up before being charged for a call; the resulting missed-call alerts functioned as a kind of code between them. \u201cIt was decided in advance,\u201d Kumar says. \u201cWe would say, if I\u2019m coming to pick you up, I\u2019ll give you a missed call, and you come out of your house.\u201d Leaving missed calls in this way \u2014 effectively using a mobile phone as a kind of latter-day pager \u2014 was a consumer hack that, in the 2000s, before India\u2019s cheap smartphone and data revolution, grew more popular than texting. The missed call emerged in India as  a critical means of communication  for those who counted every rupee spent on recharge credit. But the practice soon spread,  became trendy , and, even as call rates plunged in the 2000s to among the lowest in the world, evolved into a general tool of convenience: a missed call could mean \u201cI miss you,\u201d \u201cCall me back,\u201d or \u201cI\u2019m here.\u201d The fact that the missed call demanded only basic numeric literacy made them accessible to the third of India\u2019s population that was illiterate. In 2008,  one study  estimated that more than half of Indian phone users were in the habit of calling people with the expectation that they wouldn\u2019t pick up. Then, just as the missed call became ubiquitous \u2014  The Times of India   wrote  in 2009 of Indians\u2019 marked fondness \u201cfor hanging up swiftly\u201d \u2014 a company in Bangalore called ZipDial took the tool and transformed it. With a couple of rings to the appropriate ZipDial hotline, customers received automated texts and callbacks that delivered live cricket scores for a big match, a deal on an affordable shampoo, rudimentary on-demand radio for Bollywood songs, or celebrity tweets \u2014 content supplied by brands that were struggling to reach offline consumers. In exchange, companies learned about their customers\u2019 preferences and created viral offline marketing campaigns for their products. At a time when less than a tenth of India\u2019s population was online \u2014 smartphones were prohibitively expensive, and buying a gigabyte of mobile data, which was glitchy and agonizingly slow outside of major cities, cost the average rural Indian two to three days\u2019 wages \u2014 missed calls made information from an otherwise unreachable digital world available with a single dial. Users needed only a feature phone: the kind with a number pad, preloaded with a game of Snake. \u201cFor many people, ZipDial was the first connection to the internet,\u201d says Sanjay Swamy, one of the company\u2019s founders and board members. With the frantic pace of technological change over the past five years, the internet pay wall in India has largely come down. Budget smartphones and dirt-cheap data rates have ensured that half of Indians are online. And missed calls are just about obsolete, their function better served by WhatsApp, YouTube, Facebook, Twitter, and a welter of e-commerce apps. ZipDial eventually went the way of the missed calls themselves. The company\u2019s founders anticipated from the beginning that the usefulness of their tool would be outpaced by rapid tech advancements in India. But between 2010, when it launched, and 2016, when it ceased operations, it managed to accumulate 60 million unique users and, at its peak, was servicing over 5 million calls a day. The story of ZipDial \u2014 and missed calls \u2014 demonstrates not only the speed of change in India but how businesses can try to balance looking after the needs of today\u2019s customers with the inevitable technological advances of tomorrow. Sanjay Swamy, an entrepreneur, venture capitalist, and one of ZipDial\u2019s founders, at his home in Bangalore. Selvaprakash Lakshmanan for Rest of World The idea of  repurposing missed calls for commercial gain came to Swamy, an entrepreneur and venture capitalist, and his colleague Valerie Wagoner in 2009, during a late-night flight from Delhi to Bangalore. Wagoner, an ambitious ex-eBay employee, had moved to Bangalore from California the previous year to work at mChek, a mobile payments startup headed by Swamy. On the flight, the two were discussing the difficulty of monitoring consumer behavior in India, where over 95% of purchases were made offline and in cash, when they realized there was one method they had overlooked. Many Indian customers were already in the habit of texting \u201cshort codes\u201d to advertised numbers to receive offers and information from brands. \u201cIt struck me that you could do the exact same thing through a missed call,\u201d Swamy said. The two soon involved Amiya Pathak, a software engineer who would become ZipDial\u2019s third co-founder. Within a week, Pathak had built a system for linking callers in the offline world to a back end connected to the internet. It was beguilingly simple: a customer would call a hotline, the phone would ring twice and automatically disconnect, and the system would reply within seconds with dynamic content, such as match scores, through a call or texts. \u201cThe beauty of the missed-call behavior was that it was already pervasive,\u201d said Wagoner, who became the company\u2019s CEO. It had become so commonplace as to vex the country\u2019s telecom operators: missed calls  clogged up  over 30% of phone lines but generated no revenue.\u00a0 Most encouraging for ZipDial\u2019s business prospects was that its potential user base was rapidly expanding. India was in the midst of its first mobile phone boom. In five years, from 2006 to 2011, the number of mobile subscriptions had soared from 166 million to nearly 894 million, according to the World Bank \u2014 over two-thirds of the country\u2019s population at the time. \u201cPeople would have mobile phones before they would have bank accounts,\u201d Wagoner said. It was beguilingly simple: a customer would call a hotline, the phone would ring twice and automatically disconnect, and the system would reply within seconds with dynamic content, such as match scores, through a call or texts. As a test run for the tool, the three co-founders set up a service during the 2010 FIFA World Cup in South Africa to provide updated match scores over text in response to a missed call. It worked smoothly, if inconspicuously, until, one day, the team forgot to update scores, and repeat users flooded them with complaints. \u201cWe were kind of doing it as a hobby until then,\u201d Swamy recalled. \u201cWe said, my goodness, people actually depend on this service!\u201d During the 2011 ICC Cricket World Cup, one of the world\u2019s biggest cricket tournaments, ZipDial broke into the mass market. As in the previous year, Swamy, Wagoner, and Pathak set up a  service  to update callers about match scores. \u201cIt just took off like crazy,\u201d Wagoner says. During the first match, ZipDial ended up processing over 4 million calls in a day. ZipDial soon fashioned itself to conduct lead-generating marketing campaigns, in which customers would dial the missed-call hotline featured alongside a product on a billboard or TV ad. \u201cUltimately, at ZipDial, we were trying to solve the offline-to-online connection,\u201d Wagoner says, \u201cto be able to connect customers to the things that they cared about, be that content or products.\u201d That model found success in 2011 with Gillette, ZipDial\u2019s first big customer, with whom it tracked the recipients of 1 million free razor samples in over 10 cities. Over the next two years, ZipDial accrued more than 400 clients, including a parade of Fortune 500 companies like Pepsi, Procter & Gamble, and Disney. ZipDial\u2019s campaigns easily outperformed social media at the time. By 2013, Gillette  had accumulated  2.4 million subscribers through ZipDial, surpassing its 1.63 million Facebook likes. Disney used the platform to reach 2 million subscribers, over 90% of whom didn\u2019t have a Facebook account or mobile internet; incredibly, these subscribers engaged about 13 times per month on average,  a figure  that would have been unprecedented in developed markets. Missed calls swiftly replaced SMS-only campaigns as the marketing tool of choice for big brands in India. Noting ZipDial\u2019s success, a number of telecommunications firms, such as VivaConnect and Ozonetel, added missed-call campaigns to their own repertoire, starting in 2010. Between mid-2012 and early 2013, ZipDial\u2019s revenue shot up sixfold, and India\u2019s  Economic Times   estimated  that the missed-call industry was worth 5 billion rupees in total \u2014 around $94 million. The missed call proved to be an astonishingly versatile tool. Patents filed by ZipDial in 2012 and 2015 envision a range of scenarios for its use, some of which, such as missed calls to switch credit cards on and off or exchange business cards between two callers, would remain largely aspirational. Others, such as   missed calls to vote for reality show contestants, were realized by ZipDial and its competitors across a number of marketing campaigns. In 2013, Unilever, which was one of ZipDial\u2019s clients, partnered with its rival Ozonetel to establish an innovative use for the medium: a mobile-based on-demand music service called Kan Khajura Tesan, \u201cthe Earworm Station.\u201d To listen to Kan Khajura, residents of Bihar and Jharkhand \u2014 Indian states where daily power outages were common, but 80% of households now owned at least one mobile phone set \u2014 would send a missed call to the station. In return, they would receive a callback with 15 minutes of Bollywood content, mostly banter and songs from films, interspersed with Unilever ads. \u201cAt the time, you couldn\u2019t listen to songs easily,\u201d says Prashant Harsh, a 23-year-old student and regular Kan Khajura subscriber. \u201cYou had to pay to get them from a shop in a memory card.\u201d Kan Khajura\u2019s central appeal was that it could be accessed anywhere and anytime, unlike the radio and TV. Moreover, streaming was free, and the material was always fresh. The station would also adapt on the basis of a caller\u2019s interactions. If a customer ended a call prematurely, algorithms would shuffle the playlist for the subsequent session, so that the station functioned as a kind of cross between terrestrial radio and curated internet-based services such as Spotify or YouTube. Kan Khajura grew to become, for several years, the most accessed  \u201cradio channel\u201d  in Bihar and had around 50 million subscribers across the country. The missed call, in 2013, began bridging the gap between offline users and the expanding world of social media. ZipDial partnered with Twitter to allow users to \u201csubscribe\u201d to celebrities\u2019 feeds \u2014 beginning with that of Shah Rukh Khan, the Bollywood superstar \u2014 through a missed call, with Tweets delivered as texts to users\u2019 phones. It partnered with Facebook, which embedded a missed-call button on its platform so Indian mobile users could interact with advertised brands offline, without incurring extra data charges. However, ZipDial, the industry\u2019s poster child, always maintained that it was \u201cnot just a missed-calls company.\u201d Certainly, to call it that would be to shortchange its vision. The founders wished to position the company, as Wagoner told  one newspaper in 2013 , as \u201cGoogle Analytics for the offline world.\u201d ZipDial aimed to \u201cknow the most about the most number of customers long before they got connected to the internet\u201d \u2014 information that would continue to be useful long after customers did. The company dealt in big data, and in an offline India, the missed call was the ideal tool to gather it. The founders predicted online social media would win out eventually, Wagoner said. \u201cBut there was a good five- or seven-year window where people were not yet going to have the internet on the kind of scale that they could reach those platforms.\u201d Valerie Wagoner, ZipDial\u2019s co-founder and CEO and currently chief growth officer at GoPay, in Singapore. Juliana Tan for Rest of World But how quickly  would India get online? \u201cI had a friend who described ZipDial\u2019s model to me as an ice cube in the sun: many of the use cases would eventually go away,\u201d Swamy says.\u00a0 As early as 2014, it was apparent that Swamy\u2019s ice cube had begun to melt. Anil Kumar, the owner of a matchmaking app based in Chennai, recalled that he was pleasantly surprised when he first noticed young professionals in his office flicking through their new smartphones. \u201cI thought, \u2018Wow, do I pay you enough to afford a smartphone?\u2019\u201d The devices were a sizable fraction of their monthly wage and a sign, he said, that smartphones would come to be seen as an \u201caffordable luxury.\u201d\u00a0 As the mobile adoption rate accelerated, smartphones continued to proliferate, especially as Indian and Chinese manufacturers like Micromax and Xiaomi entered the market in force, selling devices for as little as $80. In 2013, the number of total smartphone users in India had  swelled  to 117 million, a 55% increase from the previous year. \u201cI had a friend who described ZipDial\u2019s model to me as an ice cube in the sun: many of the use cases would eventually go away.\u201d \u201cWe observed and anticipated these changes and built them into how the ZipDial platform evolved,\u201d Wagoner said. So video links were sent alongside callbacks, and WhatsApp messages supplemented SMSes. In 2014, ZipDial\u2019s most prominent campaigns were directed at easing new smartphone owners online. Companies such as Amazon, Flipkart, and Uber ran campaigns that sent callers links to their official apps. \u201cToday, people will say, Go to the App Store or Play store and download my app,\u201d Swamy said. But at the time, \u201cthe discovery was a lot simpler to give them a number to call.\u201d Later that year, Wagoner, Pathak, and Swamy were considering a move to the \u201cnext phase of consumer connectivity.\u201d ZipDial wanted to continue to interface between brands and its 60 million customer base, perhaps through an app. But then, they received a dream offer: Twitter wanted to purchase ZipDial for  a reported $30\u201340 million , in what would be the tech company\u2019s first acquisition in India. Twitter said  in a statement about the deal  that ZipDial\u2019s technology would help \u201cmake great content more accessible to everyone.\u201d Taking the offer made sense for ZipDial as well: it could push the missed call into new markets with the help of Twitter\u2019s money and sales team, albeit in the service of a single client. ZipDial\u2019s founders were ecstatic. \u201cWe had accomplished something extraordinary for the Indian startup ecosystem at that time,\u201d Wagoner said. \u201cZipDial was a shining example for that time in India\u2019s tech innovation.\u201d\u00a0 The welcome screen for ZipDial\u2019s mobile app. Courtesy Valerie Wagoner Over the next year, Twitter expanded the amount of content available via missed calls, adding more Indian celebrities and politicians to its roster. However, in late 2016, just over a year and a half after the sale,  it altered course  to focus on its core product. Several of its subsidiaries were abruptly shuttered, including ZipDial. \u201cOne of the last things I did was lay off all the ZipDial people,\u201d said Wagoner, who was by then Twitter\u2019s senior director for growth. \u201cAnything non-core was \u2018Kill, kill. Sold off, shut down.\u2019\u201d The move effectively terminated ZipDial\u2019s operations. But by 2016, tech in India \u2014 already changing at warp speed as its online population expanded, given steadily falling data rates \u2014 was set to undergo a transformation. \u201cI would not have built that same company again in 2016 that I built in 2009,\u201d Wagoner said. The company\u2019s founders could not have been able to predict just how swiftly India would move past the missed call.\u00a0 In 2015, one year into Prime Minister Narendra Modi\u2019s rule, the government\u2019s \u201cDigital India\u201d campaign vowed to increase internet connectivity across the country. In late 2016, Mukesh Ambani, a close ally of Modi and now India\u2019s richest person, formally launched his new telecom company, Reliance Jio, promising to extend cheap 4G internet to all Indians. Jio\u2019s prices at launch were so ludicrous \u2014 new customers were given four free GB of data per day \u2014 that for weeks afterward, long queues snaked outside its outlets. Jio SIM cards were sold on the black market. Its competitors panicked and slashed their own rates to remain competitive. By the end of the ensuing price war, the cost of one GB of data had   dropped from roughly 226 rupees in 2015 and 76 rupees in 2016 to 19 rupees, and it was still trending downward. The age of mobile data \u201ccame much faster than anticipated, thanks to Jio,\u201d Wagoner says. Ambani brought the very people that ZipDial had been targeting online. Plumbing its parent company\u2019s  deep pockets , Jio increased its number of phone towers in order to provide reliable data coverage in rural areas where its competitors\u2019 networks were apt to fail. By late 2017, it had reached 130 million subscribers. Data use in India consequently exploded. In 2015, the average Indian user consumed about 805 MB of data per month; by 2017, that figure had risen sevenfold, to 5.7 GB. Mobile marketing companies like VivaConnect saw a serious drop in missed-call engagement among millennials, says Vikram Raichura, the company\u2019s founder and CEO. As millions of Indians joined social media platforms like Facebook, brands became reluctant to pay for missed-call campaigns when they could drive traffic more cheaply online. Harsh, the Kan Khajura listener, went from lurking in railway stations and government offices for free Wi-Fi to possessing rapid internet at his fingertips. The days of ringing up Kan Khajura quickly receded into the past. \u201cYou can get everything in the fastest possible way,\u201d he said. \u201cThere was no point in using all of these other services.\u201d Like millions of other Indians, he was able to watch YouTube videos from anywhere, without having to worry about rationing his data usage. Having spent years relying on missed calls to receive cricket updates, \u201cthings changed after Jio\u2019s launch,\u201d he said. \u201cNow, even when you\u2019re outside, you can watch cricket matches and not miss a single ball.\u201d Part of the ZipDial team before its acquisition by Twitter in 2015. Courtesy Valerie Wagoner A large part  of ZipDial\u2019s success flowed from the fact that it was able, during its run, to adapt nimbly to India\u2019s changing tech landscape. \u201cAs a founder, you have to find the strategic balance between what today\u2019s customers need but simultaneously lay the foundations for how you know you will need to innovate in the future,\u201d Wagoner says. \u201cWe were always ready to innovate and respond to market dynamics.\u201d So the missed call began as an alternative to the internet and then became a bridge to it, as Indians moved online, until the profoundly disruptive influence of Jio rendered even that role unnecessary.\u00a0 In retrospect, Wagoner says, ZipDial was a way to close the \u201coffline-to-online gap\u201d that existed between consumers and online services and products in India. It\u2019s a gap that still exists today, Wagoner said, but, with changing needs, is being addressed in a different way: through digital payment systems. There are only a few examples of missed-call models left. Banks still offer missed-call services, for instance, as a way to obtain account statements. And they\u2019re popular in politics, especially with the Bharatiya Janata Party, India\u2019s ruling party. In early 2020, the BJP launched  a missed-call campaign  to drum up support for a controversial bill, the Citizenship Amendment Act, which sought to exclude Muslim refugees from Muslim-majority countries from becoming Indian nationals. The party claimed that it received over 5 million calls of support in favor of the bill.\u00a0 But if missed calls were seen at one point as convenient and egalitarian \u2014 they helped launch a massive anti-corruption campaign in 2011 \u2014 as they have moved away from the mainstream to become a fixture only in the very poorest households, the optics surrounding their use has changed. Missed-call campaigns are seen today as a way to link politicians to \u201cinformation-deprived, gullible voters,\u201d says Keyoor Purani, a marketing professor at the Indian Institute of Management Kozhikode. It\u2019s all but certain that any remaining applications of the missed call will soon become redundant. By 2022, there are expected to be  820 million smartphones in India , and a widespread issue, as Kumar, the schoolteacher, sees it, is Indians being glued to them at all times, enthralled by its offerings. In 2018, the average Indian\u2019s monthly data usage vaulted from  4.13 GB to 7.69 GB per month , one of the highest in the world. \u201cThe missed-call days were less hectic,\u201d he says. \u201cNow that peace is finished.\u201d The internet revolution has brought about vast benefits for India: digital connectivity defines nearly every aspect of Indian life, a trend that has only accelerated during the pandemic. A service like Kan Khajura, which was scrapped in 2019 because of low engagement numbers, seems distant. \u201cIf you compare it to today, it definitely seems silly,\u201d says Harsh. \u201cBut in the past, people used to send letters and telegrams. They used to wait weeks for a reply. At the time, you had to explore how to do the most with the least resources.\u201d"}, "210705_news_467253.txt": {"page_id": "210705_news_467253.txt", "text": "Conventional wisdom holds that passive trading is the rational investing strategy. Active traders are often portrayed as gambling rubes or lucky opportunists who will (eventually) be handily beaten by index funds. But the steady growth of platforms like eToro and Robinhood, coupled with the acceleration of investing communities like WallStreetBets, has tipped the power from institutions back to individuals and created a renewed interest in active trading. The internet has always aggregated fringe behaviors; now it enables individuals to coordinate trading strategies in a way that was previously only possible through institutions. That technological shift, coupled with cultural upheaval\u2014thanks to a combination of loose monetary policy and an uncertain economy,  younger Americans\u2019 path to financial progress is steep \u2014has catalyzed a lean-in mindset around investing, particularly among Gen Z.\u00a0 This is a relatively new phenomenon. Passive investing first became part of the American investors\u2019 portfolio in 1976, when John Bogle, then the CEO of Vanguard, launched a new type of investment vehicle called an index fund. Bogle created the Vanguard 500 Index (VFINX), which would track the S&P 500 and allow retail investors to buy a basket of stocks that represented the market\u2014without buying every single stock. Genius! Though it wasn\u2019t the first vehicle of its kind, what Bogle had invented was a publicly available index fund. He became a major voice in evangelizing this new type of \u201csmarter\u201d investing. Since then, innovations like low-cost mutual funds and the diversification of index funds (to track all types of indexes and baskets) have made passive investing increasingly common among retail investors. In September 2019, passive equity funds  overtook  active investing for the first time.\u00a0\u00a0 (Charts provided herein are for informational purposes and should not be relied upon when making an investment decision.) However, in recent years we\u2019ve seen momentum start to shift back towards active investing. The status-quo\u2014passive-only investing strategies\u2014is being challenged by newer generations of investors entering the market. The reasons behind this shift are part psychological, part structural. First, American psychology and  attitudes around money  are changing. A combination of  illusory superiority bias \u2014the belief that we are more financially savvy than we actually are\u2014and a culture of financial optimism leads most retail traders to believe they have above-average trading ideas and strategies. (This, of course, is contradicted by proponents of passive investing, who point out that active strategies underperform passive ones in returns especially when accounting fees.) In part,  the psychology of American exceptionalism extends to active trading. In addition, Gen Z faces an uncertain path to financial progress. A combination of asset price inflation (QE and COVID stimulus packages) and generally loose monetary policy has driven the transfer of wealth from the asset-poor (young) to the asset-rich (old). A new generation of  Gen Z investors are willing to take risks  to counter a deck that may be stacked against them. Active investing is a natural extension of  hustle culture , in which risk is embraced and failure is accepted (and even celebrated);  YOLO behavior  on WallStreetBets is just one example. Furthermore, Gen Z retail traders have never experienced a market contraction\u2014the oldest of them were 10 in 2007 when the S&P began its 50 percent decline. For many, passive investing is viewed as a strategy for the already rich to *stay* rich, not for those wanting to *get* rich. Beyond the psychological forces driving younger active traders, part of the fuel is structural. Innovations in infrastructure have lowered the barriers to entry for retail investors. No-fee trading was first introduced by Robinhood in 2013; only recently have large brokerages like TD Ameritrade and Charles Schwab eliminated their fees, opening up equities to all investors. Similarly, the rise of fractional investing through companies like Acorns and Stash has enabled investors to participate in the market with as little as $10 or less, which was previously unimaginable. Finally, brokerage-as-a-service platforms like DriveWealth and Alpaca allow existing apps and new investing platforms to offer brokerage capabilities to their users.\u00a0 As a result, we\u2019re seeing of four types of active investing emerge, supported by a growing set of dedicated tools:\u00a0 Distributed hedge funds In the old world, the best investors worked on Wall Street, executing their strategies behind the balance sheets and research departments of hedge funds and banks. Now, individual retail investors can share and coordinate their own research, trading strategies and collective balance sheets to match the sophistication and scale of these hedge funds. These investors can post a trade idea and others can register their approval with their dollars, all verified through proof of trade. Status is conferred by correctly predicting outcomes, and in the future, incentives could align through shared economics (say, 20 percent carry for the community). Trades could even be coordinated in participants\u2019 accounts, so capital wouldn\u2019t have to be pooled; software would facilitate \u201cvoting\u201d on trade ideas and execute those trades. Previously, only hedge funds had the access and capital resources to deal in this type of trading. Through communities like WallStreetBets and execution layers like Numerai, that coordination is becoming more distributed.\u00a0 Individuals as institutional scale asset managers Wall Street has always had star stock pickers. The internet version of this phenomenon is the unbundling of indexes\u2014enabled by platforms like Apex and startups like Doji\u2014and the ability for anyone to follow in the footsteps of  Cathie Wood  and create new ones. A truly efficient market means the best ideas should attract the most capital. In the past, people have mostly flocked to ETFs from name-brand funds like Vanguard Funds and Fidelity Index Funds. But in the future, retail traders will follow individual investors that align with their investing style and return profile. eToro\u2019s copy trade approach in crypto and Doji\u2019s \u201ccreate your own ETF\u201d approach are just two versions of what this might look like. Low-code trading IDEs Just as Github and Figma provide collaborative environments for programmers and designers, there are now a number of emerging collaborative development environments for hobbyist and prosumer quants. With a \u201cGithub for investors,\u201d the best managers in the world could build a public record of successes and participate in the economics of fellow investors that leverage their primitives. Whereas this was previously only happening among quants in banks and hedge funds, now new tools and increased consumer interest are extending this capability to the masses. Composer, Tuned, and Tradologics have started to build early examples of developer-first trading platforms. Bottoms-up communities   In the past, investing conversations were largely fragmented through #fintwit, Discord channels, YouTube, and group chats. But through Stocktwits, WallStreetBets, Commonstock, and more, investing communities are becoming increasingly centralized. The integration of proof of trade and trade execution has also provided much needed transparency, differentiating the current generation of communities from past efforts like Cake Financial. The infrastructure exists to open up trading data and put it at the forefront of new social platforms. Those who chalk up the recent rise in active investing to a bull market miss a number of structural catalysts behind it. The psychology of American exceptionalism, coupled with a challenging path to financial progress for Gen Z, are two of the largest cultural drivers. Those factors, coupled with recent technology shifts, portend that active investing is here to stay. In short, retail portfolio theory\u2014buy and hold for 30 years\u2014is  being replaced  by new investing approaches, including bottoms-up communities and zero-cost passive investing. We believe that in the years to come, active strategies will have a place in every retail investor\u2019s portfolio. \u00a0 * * * The views expressed here are those of the individual AH Capital Management, L.L.C. (\u201ca16z\u201d) personnel quoted and are not the views of a16z or its affiliates. Certain information contained in here has been obtained from third-party sources, including from portfolio companies of funds managed by a16z. While taken from sources believed to be reliable, a16z has not independently verified such information and makes no representations about the enduring accuracy of the information or its appropriateness for a given situation. In addition, this content may include third-party advertisements; a16z has not reviewed such advertisements and does not endorse any advertising content contained therein. This content is provided for informational purposes only, and should not be relied upon as legal, business, investment, or tax advice. You should consult your own advisers as to those matters. References to any securities or digital assets are for illustrative purposes only, and do not constitute an investment recommendation or offer to provide investment advisory services. Furthermore, this content is not directed at nor intended for use by any investors or prospective investors, and may not under any circumstances be relied upon when making a decision to invest in any fund managed by a16z. (An offering to invest in an a16z fund will be made only by the private placement memorandum, subscription agreement, and other relevant documentation of any such fund and should be read in their entirety.) Any investments or portfolio companies mentioned, referred to, or described are not representative of all investments in vehicles managed by a16z, and there can be no assurance that the investments will be profitable or that other investments made in the future will have similar characteristics or results. A list of investments made by funds managed by Andreessen Horowitz (excluding investments for which the issuer has not provided permission for a16z to disclose publicly as well as unannounced investments in publicly traded digital assets) is available at https://a16z.com/investments/. Charts and graphs provided within are for informational purposes solely and should not be relied upon when making any investment decision. Past performance is not indicative of future results. The content speaks only as of the date indicated. Any projections, estimates, forecasts, targets, prospects, and/or opinions expressed in these materials are subject to change without notice and may differ or be contrary to opinions expressed by others. Please see https://a16z.com/disclosures for additional important information."}, "210705_news_467276.txt": {"page_id": "210705_news_467276.txt", "text": "Plans announced by Senegalese-American singer Akon  last August  to build \u201cAkon City,\u201d a futuristic hub in his ancestral homeland, were initially greeted with enthusiasm. But that quickly gave way to confusion: What was \u201cAkoin,\u201d and why was it appearing on billboards, postcards, and candy wrappers across the country advertising the 2,000 acre project? Envisioned as a sustainable smart city, Akon City has been pitched as a residential and commercial hub, complete with resorts, towering condos, recording studios, a stadium, and e-commerce franchises. The city will run on solar and wind energy to circumvent the country\u2019s unreliable power supply. It will also run on  Akoin , a new cryptocurrency currently being piloted by the Grammy Award-nominated singer. \u201cI think that blockchain and crypto could be the savior for Africa in many ways because it brings the power back to the people and brings the security back into the currency system,\u201d\u00a0 Akon  explained at the 2018 Cannes Lion Festival  in France. \u201c[It] also allows the people to utilize it in ways where they can advance themselves, and not allow governments to do those things that are keeping them down.\u201d The gap between the singer\u2019s vision and the response on the ground provides a fascinating snapshot of the role cryptocurrency is playing across the continent. \u201cAkon City would be situated in Senegal and not on an autonomous island. And so the financial laws governing the rest of the country would also apply there,\u201d cautions Paul Diack, a Dakar-based fintech journalist. The Akoin ecosystem Akon is hoping the alternative models proposed by Akon City will help Senegal, a country dealing  with mass protests  in the wake of an economic crisis, tackle poverty and youth unemployment. Construction on the project  is expected  to start this year, and is expected to cost around $6 billion, pooled between Akon and outside investors. The singer this week also announced plans to build  a \u201csister\u201d Akon City in Uganda . Akoin is envisioned not just as a utility token, but as an ecosystem of tools and services designed for entrepreneurs. It is backed by a multi-currency wallet and decentralized exchange that allows users to trade with each other interchangeably or between major cryptocurrencies without the major hurdles or fees of traditional money exchanges. Users can eventually convert their tokens into fiat currencies or purchase mobile phone airtime. Akoin can be purchased via a secure online wallet using major cryptocurrencies such as stellar, bitcoin, ethereum, or with credit and debit cards. The central hope of Akoin is to minimize the barriers to entry faced by many entrepreneurs and small businesses such as the extensive paperwork required by banks. \u201cThe Akoin ecosystem will be the infrastructure that allows African entrepreneurs to take part in the gig economies, and will provide the tools they need to compete in the realm of international business for the first time,\u201d the  project\u2019s website explains . \u201cWith Akoin, African entrepreneurs will have access to financial, health, education, technology, professional, governance and land services, using the Akoin Token as a central medium of exchange.\u201d Children play at a coastal resort town in Senegal. Akon\u2019s cryptocurrency proposal provides a fascinating snapshot of the role cryptocurrency is playing across the continent. But why create an entire new crypto and not use existing ones? According to the singer\u2019s plans, Akoin is envisioned not just as a cryptocurrency, but a means of transaction in a bigger business ecosystem that be a gateway for global brands to access the continent, and benefit Africans specifically. It sounds impressive but unsupportive government regulations could pose a challenge to massive adoption of the coin in Senegal and perhaps beyond. Senegal uses the CFA franc as a currency, as do the eight mostly French-speaking countries in west Africa. The currency is issued and regulated by a Dakar-based central bank known as BCEAO, which has  warned that the adoption and usage of crypto  is illegal with commercial banks operating in the union are forbidden to process crypto transactions. They have also warned those using international crypto platforms or P2P apps as potentially \u201cunsafe investment instruments. \u201d The Akoin platform wants to leverage the existing network of prepaid mobile airtime vendors as a tool to provide financial services to the unbanked population, but that will largely depend on mobile telecommunications companies, whose financial services are also regulated by the central bank. \u201cMobile telecoms firms will face sanctions if they ignore alerts from BCEAO. The only legal tender we have in Senegal is CFA franc. Would merchants accept the Akoin? How do we use it to pay for our children\u2019s school fees or hospital bills?\u201d These are some of the questions about the project being asked by Diack. \u201cBCEAO may likely not give the green light for Akoin this time, because if they do it could disrupt the entire financial system of the region, which is about the most stable in Africa.\u201d The crypto trend in Africa The Akoin Foundation is working to secure partnerships with companies across Africa willing to adopt the token and integrate it into their digital infrastructure, according to its founding documents (the Foundation did not respond to a request for comment.) Established companies and startups in Africa are willing to have a shot at blockchain-based financial platforms, but are concerned about the position of central regulators, most of whom are hostile to crypto adoptions, says Clement Gbegnon, a Togo-based risk analyst formerly with the West African Development Bank. \u201cPayment solutions and investment firms across Africa would like to jump on the crypto bandwagon, but securing a license for legal operations is near impossible presently. Most of those doing crypto-related businesses on the continent are not recognized by their authorities,\u201d Gbegnon says. \u201cAfrica seems to be a fertile ground for cryptocurrencies but the boom will only be achieved if they are allowed to operate legally, not necessarily as legal tenders, but as alternative financial solutions.\u201d The growth of crypto activities in Nigeria, the continent\u2019s largest economy and  the world\u2019s third largest place for cryptocurrency trade in terms of volume,  yielded some hope that the trend could go continent wide. But an unexpected  ban  on cryptocurrency exchanges in the country by Nigeria\u2019s Central Bank last February  sent shock waves  through the African crypto community. Akoin has partnered with Mwale Medical and Technology City, a $2 billion tech hub in Kenya with 35,000 residents and over 2,000 merchants, to be its sole currency and payment processor. The token has  entered its pilot phase  in the metropolis, without any objections from authorities for now, according to  Julius Mwale , the project\u2019s principal investor. The Kenyan government encourages electronic money transactions nationwide but is yet to make its stance on cryptocurrency public. Insaf Nori is one of the directors of  Decred (DCR) , a cryptocurrency popular in Morocco, where cryptos remain prohibited, but which saw transactions soar this year via Paxful and  LocalBitcoins , according to local reports.\u00a0 He believes African governments will eventually yield to the pressure as more crypto projects and adopters emerge. \u201cCentral banks and financial regulators in Africa are currently feeling the heat. They know the aspirations of the young crypto-friendly generation, but are concerned about insecurity due to the anonymity in blockchain transactions,\u201d Nori says. Crypto is not yet popular in Senegal, and while mobile money transactions have become increasingly popular, most daily business is still done in cash. The singer is banking on the coin\u2019s African focus to change that. Yet even in the West, very few vendors and businesses accept crypto for payment which would make the widespread adoption of Akoin unprecedented. Offering more solutions The violent protests that recently paralyzed Senegal following the arrest of an opposition figure were in part an  outcry from the youth  against unemployment and economic hardship. A supporter of opposition leader Ousmane Sonko holds a white flag during clashes with security forces in Dakar, Senegal March 8. Beside providing jobs and boosting business, Akon City has proposed building homes for international visitors and locals to rent or buy, in a bid to deal with Senegal\u2019s massive housing shortage. Its housing ministry already regularly encourages nationals in the diaspora to invest in the sector. Akon\u2019s highly successful  Lighting Africa  project, which has provided scalable solar power solutions to millions of households across 18 countries since launching in 2014, could also serve as credentials for those considering investing in Akon City, and using Akoin. \u201cThere is the general fear that cryptos are run by scammers and that investing in them is tantamount to throwing your money into the sea,\u201d says Adoum Weibigue, a product expert at the Research Laboratory on Institutions and Growth (LINC) of Cheikh Anta DIOP University in Dakar. \u201cBut Akoin was introduced by our own son, Akon, who loves Senegalese and who wants us to prosper.\u201d Akoin\u2019s success will ultimately hinge on its approval, recognition, and uptake in Senegal. But as with elsewhere on the continent, the crypto project puts into perspective a multitude of socioeconomic issues which traditional policies have struggled to address."}, "210705_news_467363.txt": {"page_id": "210705_news_467363.txt", "text": "Alibaba Group has been slapped with a record 18.2 billion yuan ($2.77 billion) fine for breaching China's antitrust regulations and \"abusing [its] market dominance\". The Chinese e-commerce giant says it will \"accept\" the ruling, which marks the culmination of an investigation that began last December.\u00a0  China's State Administration for Market Regulation (SAMR) said in a statement Saturday that Alibaba had been abusing its strong market position since 2015 to prevent merchants from using other online e-commerce platforms. It said such practices impacted the free movement of goods and services, infringing on a merchant's business interests, and in breach of the country's anti-monopoly laws.  According to SAMR, the financial penalty accounted for some 4% of the  e-commerce giant's 2019 revenue  from its home market. The regulator's ruling followed its  investigation into Alibaba  that began last December over alleged anti-competitive practices, including claims it imposed a \"forced exclusivity\" clause requiring merchants to peddle their wares only on one e-commerce platform. \u00a0  SAMR added that the e-commerce operator would have to establish \"comprehensive rectifications\", including adopting fair competition practices, safeguarding merchants on its platforms as well as consumer rights, and boosting its internal controls. Alibaba also would have to provide reports on self-regulation to the Chinese regulator for three years.  In an  open letter , Alibaba said it would \"accept\" the penalty and work to ensure its compliance. It noted that it had \"fully cooperated\" with SAMR's investigation and reviewed its government's policies for online platforms.\u00a0  \"We conducted a self-assessment of, and implemented improvements to, our internal systems while ensuring stable operation of our business,\" it said. \"The penalty issued today served to alert and catalyse companies like ours. It reflects the regulators' thoughtful and normative expectations toward our industry's development. It is an important action to safeguard fair market competition and quality development of internet platform economies.\"  Describing these digital economies as \"new economic structures\", Alibaba said it had been provided opportunities to \"explore and create\" business models such as Taobao and Tmall that lowered the costs of starting and operating businesses as well as enhanced efficiencies and trade flow. Its online platforms also helped millions of small and midsize merchants and consumers, the company said.\u00a0  \"Today, internet platform economies have entered an entirely new phase. They are an integral part of people's everyday life and affect all dimensions of the broader economy,\" Alibaba said. \"It is not lost on us that today's society have new expectations for platform companies, as we must assume more responsibilities as part of the nation's economic and social development.\"  It added that it would continue to introduce measures to lower entry barriers and business costs of operating on its platforms, while ensuring these were \"more open, more equitable, more efficient, and more inclusive\".  Alibaba had faced increasing pressure from its government after the company's founder  Jack Ma criticised China's  rigid regulation and the state's dominance over the banking system. The company's fintech aim Ant Group was later called up by China's central bank to discuss its regulatory compliance, and  its IPO was called off  after regulators said Ant's listing on the Shanghai stock exchange no longer met regulatory and disclosure requirements.  The latest fine dished out by SAMR was more than double that of a previous record 6.1 billion yuan ($928.98 million) that was handed to chipmaker Qualcomm in 2015, according to a  South China Morning Post report .   \tRELATED COVERAGE "}, "210705_news_467372.txt": {"page_id": "210705_news_467372.txt", "text": " Introduction   The VeChainThor is a public blockchain that is designed for mass adoption of blockchain technology by enterprise users of all sizes. VeChainThor is intended to serve as a foundation for a sustainable and scalable enterprise blockchain ecosystem, supported in part by our novel governance and economic models and unique protocol enhancements.   Ethereum has represented the state-of-the-art in public blockchains since 2014. Some of Ethereum\u2019s important innovations included the introduction of an account model that can store information other than balance information; the concept of a smart contract that allows blockchain to describe more complicated objects and activities in the real world; and the consensus-based computations and the invention of the Ethereum Virtual Machine (EVM) that enables smart contracts accordingly.   Despite being a major technological milestone, Ethereum has proven to be unsuitable for hosting large-scale commercial decentralized applications (dApps). One of the main reasons for this is that there hasn\u2019t been an effective governance structure set up from Ethereum\u2019s very beginning that would allow efficient and transparent transitions (upgrades) of the protocol to adapt to new challenges or innovations. Secondly, Ethereum lacks a suitable economic model to allow enterprises to run their dApps with a controllable and predictable cost. Considering the level of volatility of the ether price, it is almost impossible for companies to predict the future price of Ether or the cost of running a dApp based on Ethereum for a given period of time.   The VeChainThor Blockchain is designed to tackle the above problems.  It is not built from scratch; it expands upon some of the essential building blocks of Ethereum (e.g., the account model, the EVM, the modified Patricia tree, and the RLP encoding method) and provides innovative technical solutions that are powered by our novel governance and economic models, which, we believe, will push forward broader blockchain adoption and the creation of new business ecosystems with more efficiency and trust. VeChainThor is packed with technical features that are tailormade for the actual needs of enterprises, individuals, and developers.   Meta-transaction features native to VeChain Thor blockchain\u2019s core protocol, such as multi-party payment, multi-task transaction, controllable transaction lifecycle, transaction dependency, make the development more user-friendly for enterprise adoption.   Controllable Transaction Lifecycle - With BlockRef and Expiration transaction fields, users can set the time when transaction is processed or expired if not being included in a block   Multi-task Transaction (MTT) - Multi-function atomic transactions allow developers to batch payments, add multiple calls to different contract functions into one transaction and determine their sequence.   Multi-party payment (MPP) \u2013 Flexible transaction fee delegation schemes enable a freemium model within a decentralized application to onboard users without friction   Transaction Dependency \u2013 Set dependencies to ensure the execution order meets the business need, transactions that specify a dependency will not be executed until the required transaction is processed.    Proof of Authority (PoA)   Authority Masternode Operators, with the aligned interest in the development of VeChain ecosystem, maintain the VeChainThor protocol according to the Foundation's governance policy. PoA addresses enterprise' common concerns of inefficient upgrade and waste of energy.   Low computation power required to achieve the network security and consensus integrity   Controlled via the built-in smart contract, hard forks can be avoided in case authority mastemodes fail to upgrade   All Authority Masternode Operator identities are strictly verified by the Foundation    Governance   The balance between decentralization and centralization ensures efficiency and transparency. Community-elected Steering Committee, as the governing body of the ecosystem, facilitates decision making and execution supported by the on-chain governance mechanism.   Role-based voting reduces the uncertainty in the platform\u2019s technical and organizational development   On-chain governance mechanism which is divided into three phases - propose, approve, and execute, is designed to support the governance model    Economic Model   The unique two token system (VET+VTHO) significantly helps separate the cost of using blockchain from market speculation. Due to the correlation with the blockchain resource utilization, the cost is more predictable with the monitoring of the VTHO supply and demand. In addition, Foundation's governance mechanism further stabilizes the cost.   VTHO generation from any address holding VET at the predetermined velocity of   per VET per block (10s)   70% of VTHO paid in each transaction is destroyed and the rest is awarded to the Authority Masternode Operator   Adjustment to variables (e.g. gas price, velocity) can be made to maintain the equilibrium of VTHO demand and supply  "}, "210705_news_467393.txt": {"page_id": "210705_news_467393.txt", "text": "The real motivation behind systemd Published on  2018-05-01 . Modified on  2021-02-12 . In this article we're going to take a close look at the real motivation behind the development of systemd, and we're going to take a look at some of the future perspectives for GNU/Linux as an operating system. Table of Contents Introduction Personally I didn't have a problem with systemd in the beginning, when it was mostly just a new init system. However, my problem with systemd today is that it has turned into a kind of  Trojan horse . It is an attempt by  Red Hat  to change the world of Linux in order to better serve their corporate interests. While the Linux kernel, the GNU tools and the different major independent Linus distributions all started out as community driven projects, most of the current development in Linux world is motivated by corporate interests, driven by developers sitting in different key positions in different companies, such as Red Hat, Google, Facebook and several others. Red Hat first disguised their plans by calling systemd an alternative init system. Then the truth was revealed and systemd became \"a suite of software that provides fundamental building blocks for a Linux operating system.\" Red Hat then launched a massive campaign in order to influence all the other major Linux distributions and pressured them to adopt systemd. The effort and work they did seemed rather desperate actually. The systemd developers addressed several third party projects and tried to convince them to make their projects depend upon systemd, such as the attempts made by Lennart Poettering on  the Gnome mailing list , and the attempt made by Red Hat developer \"keszybz\" on the  tmux project . Most of these attempts were disguised as technical issues, however when one read the long email correspondence on the Gnome mailing list and elsewhere, the real intent becomes quite clear. Other tactics deployed by Red Hat was to hire developers from GNOME and other Linux distributions, such as Debian, and then have these people promote systemd. The latest invention by Lennart Poettering called  systemd-homed  is presented as a new way to handle home directories, whereas it really just is a way to get one step closer to eliminating  /etc , which is something Red Hat has dreamed about for a long time. Watch the  FOSSDEM 2020 video  where Poettering presents systemd-homed and notice how he criticizes the way full disk encryption is handled from the point of view that Linux is a multi-user system, yet at the same time he rejects at least five different challenges systemd-homed presents as irrelevant because, well, the laptop is really only used by one single person. The fact is that the main development in Linux world, even in the kernel, has been almost completely hijacked by major companies. It is no longer mainly community driven development. Linux has become really big bucks for many corporations and they really want to control as much of the development as possible. One of the results of all of this has been a huge uproar in the open source Linux community in which Debian Developer Joey Hess, Debian Technical Committee members Russ Allbery and Ian Jackson, and systemd package-maintainer Tollef Fog Heen resigned from their positions. All four justified their decision on the public Debian mailing list and in personal blogs with their exposure to extraordinary stress-levels related to ongoing disputes on systemd integration within the Debian and open source community that rendered regular maintenance virtually impossible. In December 2014 a group calling themselves the \"Veteran Unix Admins\" announced a fork of Debian called Devuan that intends to provide a Debian variant without systemd. Devuan 1.0.0 was released on May 26, 2017. \nWe believe this situation is also the result of a longer process leading to the take-over of Debian by the GNOME project agenda. Considering how far this has propagated today and the importance of Debian as a universal OS and base system in the distribution panorama, what is at stake is the future of GNU/Linux in a scenario of complete homogeneization and lock-in of all base distributions.\n Let's take a look at some indisputable facts. Fact 1: systemd is from Red Hat Lennart Poettering and Kay Sievers who started the systemd project in 2010 are both Red Hat employees. Initially systemd was released as a new init system, but it has slowly grown into what Poettering describes as \"a suite of software that provides fundamental building blocks for a Linux operating system.\" This is by design, not by coincidence. The official reason for the development of systemd was described as: \nThey wanted to improve the software framework for expressing dependencies, to allow more processing to be done concurrently or in parallel during system booting, and to reduce the computational overhead of the shell.\n Fact 2: The primary reason for developing systemd is Red Hat's business interests in embedded devices Red Hats primary business is in embedded devices, and the primary concerns addressed by systemd by design is embedded devices, such as the work towards removing  /etc . In an  interview with Red Had CEO Jim Whitehurst  he states: \nWe partner with the largest embedded vendors in the world, particularly in the telecom and automotive industries where stability and reliability is the number one concern. They easily adapted to systemd.\n Mentor Automotive has released  their slides from a 2015 event  In these slides the many benefits provided by systemd to the embedded automotive market is fairly well explained. The reason why they \"easily adapted to systemd\" is because systemd is specifically designed to suit their needs. The U.S. Military has been Red Hats  biggest customer since 2002  and they have been a major source of motivation behind many of Red Hats decisions. In 2012 Lennart Poettering  changed the systemd license from GPL to LGPL  in order to better suit the embedded market. Fact 3: No, it's not a myth, systemd is truly a huge monolith In his blog post \"The Biggest Myths\", from January 2013, Lennart Poettering argue against calling systemd a \"monolith\", which is what many people consider it to be. Lennart says: \nA package involving 69 individual binaries can hardly be called monolithic. What is different from prior solutions however, is that we ship more components in a single tarball, and maintain them upstream in a single repository with a unified release cycle.\n The fact is however, that many of these so-called individual binaries has functionality that simply will not work without other systemd components. If we take a look at the man page for systemd-networkd it clearly states that if you set the option  UseDNS  as  true  the DNS servers received from the DHCP server will be used and take precedence over any statically configured ones. This corresponds to the  nameserver  option in  /etc/resolv.conf . What the man page neglects to mention is that this setting (and multiple other settings) does not work without systemd-resolved. Other components of systemd are even more tightly integrated. Fact 4: Privacy concerns systemd-resolved has hard coded fallback DNS servers for Cloudflare, Quad9 and Google. Even if you turn these off, a bug might cause these to be used anyway (which actually happened at one point). Fact 5: Red Hat wants to become the next Microsoft Windows This is another major motivation at Red Hat and this is illustrated by Lennart Poetterings slides from  FUDCON + GNOME.Asia Beijing 2014 . Go to page 15 and scroll slowly forward to page 19. Eventually you end up with the project objectives: Turn Linux from a bag of bits into a competitive General Purpose Operating System. Build the Internet's Next Generation OS. Unifying pointless differences between distributions. Bringing innovation back to the core OS. Combined with the next set of slides that display the market Red Hat want to target: Desktop Server Container Embedded Mobile Cloud Cluster Much of the added functionality that the different systemd modules provide has zero benefit in the server industry. It is only added to make desktop systems like GNOME and KDE function like Microsoft Windows. Fact 6: Red Hat needs other major Linux distributions to cooperate If Red Hat was ever going to succeed in their long term plans for developing the \"Internet's Next Generation OS\" they knew they needed to somehow influence the other major Linux distributions. The reason for this is that if a major Linux distribution like Debian was going to reject systemd, Red Hat wouldn't be able to proceed with their plans because too many third party projects simply wouldn't care about how Red Hat would like things to work. This is very important to understand because many open source projects used to develop software with  POSIX  compatibility in mind. As such they try to make sure that their project compiles and works on several Unix-like operating systems. This is something that isn't in the interests of Red Hats. As long as you have to consider other operating systems such as Solaris, FreeBSD, OpenBSD, etc., Linux is \"held back\" when compared to functionality in Microsoft Windows. Functionality such as easy mounting and unmounting, simple privilege escalation, etc. Another problem for Red Hat was that if the other major GNU/Linux distributions had rejected systemd, it would have become much more difficult for them to get systemd relevant changes and code pushed into the kernel. But when the other major distributions also adopted systemd, it became a lot easier. Consequences The main problem with systemd is that its continued development is motivated by a company's economic interests and not the open source Linux community interests. Red Hat cannot be trusted from a security point of view and if the U.S. Military, or some other three letter organization, want Red Hat to put a backdoor into systemd, then this can easily go unnoticed for many years, just like it did with the  Heartbleed bug . And we have already seen several examples of these kind of exploitable bugs in systemd: Whether such bugs are introduced into the code on purpose, disguised as honest mistakes, or they truly are real mistakes, is impossible to tell. But one thing is very clear, Red Had do not have the Linux communities best interests at heart, they only have their own financial interests at heart. Another major problem is the previously mentioned hard coded DNS servers in systemd-resolved. Lennart Poettering  explained  that the hard coded values should be there in case of catastrophic failure of configuration files, and a lack of DHCP on the network (the DNS fallback is changeable but requires a recompile). However, that's the \"embedded developer\" speaking. If a bug is found in the application that makes these DNS servers run even though you have disabled them, or if a  race issue bug is found , you could be facing a serious privacy issue. Furthermore running with Cloudflare, Quad9, and Google DNS servers hard coded into the systemd code is deeply problematic as these companies are not only known for violating peoples privacy, but also because NSA has previously infiltrated Googles data centers, something revealed  by the Snowden documents . Such settings should never be opt-out (where you have to remember to remove them), they should be opt-in, and definitely not the default options. The way these issues are dealt with generally, and the extremely arrogant attitude of Lennart Poettering shows a complete disregard for user privacy and for the interests of the open source Linux community. Final comments It has surprised me that the initial discussion on the Debian mailing list somehow managed to only address SysVinit, Upstart and systemd. Nobody took a serious look at  runit  or  s6 . Not only are these systems much more aligned with the Unix philosophy, but they are also much more secure and easy to understand. Casper Ti. Vectors post on the Gentoo forum,  s6/s6-rc vs systemd, or why you probably do not need systemd , also shows that s6 is a better and in many ways superior solution to systemd. Many people mistakenly think that each and every systemd component is independent, but that's just not true. Take a look at the code and the documentation and see the  tight integration  between many of these so-called modules. Corporate politics, maneuvers and manipulations has no place in the community driven open source projects. And while companies can be allowed to both use open source code, contribute code and also provide financial support from their earnings on these projects, they should never be allowed such massive control as Red Hat and others now have. Thank God for truly independent community driven projects, because without those we're left with crap like Microsoft Windows. On that note, take a look at: If you have any comments or corrections please feel free to email them to me. Also, if you found this content useful consider supporting me on  Patreon"}, "210705_news_467394.txt": {"page_id": "210705_news_467394.txt", "text": "The impact of technological changes on how music is created and consumed is a well covered subject. Music history is full of dramatic changes: electric microphones, stereo LPs, transistor radios, multitrack studios, FM stations, arena-filling sound amplification, the walkman, digital synthesizers, samplers, CDs, MTV, SSL consoles, SoundScan, the iPod, Spotify, bluetooth earphones, voice-activated smart speakers, AirPods etc. In important ways, they all changed everything: the type and variety of music being made, the type of musician, the age/socioeconomic profile of listeners, the amount and frequency of music consumed, the places where it was consumed.\u00a0 One particular detail that greatly amplifies the power of technological changes to alter how music is created and consumed has to with the plasticity of music itself. \u2018Plasticity\u2019 is the quality of \u2018being easily shaped or molded\u2019 and music listening has lots of it. People enjoy\u00a0music in many, many different ways: How much they spent listening. How often. In how many different contexts. Alone or communally. The emotional responde to a particular genre can be particularly intense. The memories attached to a particular song can be overwhelming. Opinions about what is good or bad music are incredibly subjective. Opinions about what is good sounding music as well. People have different preferences in even in music volume! And so on.\u00a0 That means that music demand has tremendous flexibility. And that\u2019s why technological innovations can have a very disproportionate and unpredictable impact. In other words, one can almost say that the music landscape has lived in a constant flow of revolutions. Everything is going fine until a new element or gadget comes along and changes the way music is made/marketed/listened. And it is always unexpected. We never knew we needed Walkmans or iPods until we did. And then in hindsight they all look obvious and \u201cinevitable\u201d: of course the miniaturization of electronic components, together with digital compression algorithms and powerful song library management desktop software (offering also a fast and seamless synchronization feature) would naturally result in a stylish rounded metallic device holding a thousands songs that fits in your pocket! But the obviousness only comes with hindsight. The actual revolutionary innovation had to bring something new. And \u201cnew\u201d can take many forms in this music realm. Anything that explores, illuminates, tweaks or solves one or more of variable dimensions of music interaction can be a \u201ckiller app\u201d. The new idea can change how often we listen or how long. How portable. How instantaneously. How cheaply. How easy it is to search a song. Or remember it. How easy it makes to hide our taste, or share it. How it increases the range of where to listen or with whom etc. etc. And there is so much room for surprises because our connection with music is very deep, broad and variable. We have a strong emotional connection with music. In fact, music is more than entertainment, culture, distraction or escapism: it is an input that our minds absorb and use. Music is brain food. The plasticity of music is sourced on the complexity and neuroplasticity of the brain itself. Like the brain, always rewiring itself as it grows and heals, our relationship with music is always rewiring itself. That\u2019s why changes in technology, hardware, software or interface can have such profound consequences on how we consume music. I can speculate \u2013 what is going to be the next revolution in music: maybe those bone conduction headphones? Or future iterations of those speaker-featuring glasses? 360 audio as Sony dearly hopes? Perhaps AirPods Pro Max Series 5 will work without a watch or phone nearby, and the transparency mode will be so good that you can wear them the whole day? (And Siri evolves in a functional digital companion?) Maybe the audio part will the killer app for AR glasses? Or going into software, the future of music will be intrinsically linked with TikTok and Metaverses? Will AI evolve so far that it can accurately divine our taste? When you go to a party, will your Spotify account register all the songs that you listened (and take note of which made your heartbeat/blood pressure /etc. go up??) Maybe someday music streaming will be fully integrated with your social graph? (2030\u2019s iTunes Ping will be awesome??) Another interesting point is the suspicion that we may have crossed an important threshold in this technological evolution of the music-brain interface: music streaming platforms, with full catalogues served to a global audience, may look in the future like a primitive central nervous system, the ancestor of something much more impressive. Because, unlike other art forms or fields of knowledge, music streaming services are more than databases that index and describe whatever exists in the real world: they are becoming de facto main music layer we interact with. And all future improvements can be built upon this digital base. Nowadays, it is quite feasible to describe music streaming services as low-margin services with limited differentiation. Spotify still struggles to ensure long-term profitability and most of its competitors are tech giants whose music services represent a small fraction of their total revenues. There are several ongoing discussions about user-centric streams models, and how creators are paid, what role exactly each of several \u2018stakeholders\u2019 should have etc. But that\u2019s fine. The evolution of this early digital musical brain we have now is going to be full of surprises and turns. What is more certain is that technology has always powered music plasticity, and the current state of digitalization might very well be an inflection point from which the pace and range of innovation will be increasingly exciting. And of course, once those new fantastic developments eventually arrive, we will later look back and think that they were all obvious and inevitable. April 12 edit: Changed the structure of the first paragraphs for clarity."}, "210705_news_467423.txt": {"page_id": "210705_news_467423.txt", "text": "T he dramatic scale and ambition of Joe Biden\u2019s public spending and tax plans came into sharper focus last week. The emerging picture is breathtaking. As expected, the US president aims to repair the damage done by the pandemic. But huge, longer-term investments in jobs, education and clean energy, and his new insistence on the social responsibilities of big business, point to something far more momentous: a watershed in American economic policymaking. Comparisons abound with Franklin D Roosevelt\u2019s 1930s New Deal. Progressive politicians hail an end to the post-2008 age of austerity. Neoliberalism\u2019s divisive grip is at last  being broken ; free market dogmas are in retreat, they say. Biden is re-legitimising the power of government and the state to equally serve the interests of all its citizens. This revolution, it is claimed, will dent populism\u2019s appeal and may save democracy itself. Such optimism is rare in contemporary politics and is not to be discouraged. The prospect that a leader \u2013 any leader \u2013 can and will achieve a decisive change for the better in ordinary people\u2019s lives is almost a novel idea these days. The absence of such hope and trust accounts for much that has gone wrong within western democracies in recent years. It has encouraged political extremism and the rise, beyond Europe, of authoritarian regimes. Yet Biden has set himself an enormous task, or series of tasks, which he knows will prove difficult to fulfil. Take, for example, his plan for a global minimum corporate tax rate of 21% that could raise an extra  $300bn annually  for governments around the world. Setting such a minimum would help curb tax avoidance and profit-shifting, especially by multinationals, and potentially end the controversies over rival national digital taxes. This bold idea has the backing of tax-fairness campaigners and European members of the  G20 group  of finance ministers. But it is already under attack from corporate lobbyists and Republicans in Washington, who claim it would place American companies at a competitive disadvantage. Countries such as Ireland that benefit from the current system may also object. As with any proposal that requires global adherence, China\u2019s attitude will be crucial. Reversing normal practice, he ran from the centre, yet now he governs from the left Biden already has one big win under his belt: the $1.9tn Covid recovery  stimulus bill  passed by Congress last month. This package by itself is mould-breaking, by recent American standards, in facilitating a vast expansion of the country\u2019s social safety net. It extends federal benefits, allocates funds to tackle child poverty and provides help for states, tribal governments and small businesses damaged by the pandemic. Hot on the heels of that landmark success comes his  $2.3tn initiative  for a longer-term boost for the economy, by creating jobs and repairing and upgrading roads and other infrastructure. Biden calls it a \u201conce in a generation investment in America\u201d. He says the plan will address climate change and pollution through a systemic shift to cleaner energy sources. Beating the climate crisis will henceforth be a \u201cwhole of government\u201d endeavour. Yet more plans are in the offing, including substantial new federal spending on healthcare and early years education, and investment in green technologies and scientific research. Some of these proposals were contained in last week\u2019s 2022 federal budget outline. If agreed \u2013 and that\u2019s a big \u201cif\u201d \u2013 they represent a whopping 16% overall rise in  discretionary government spending . And the huge investments required will be paid for from two sources \u2013 borrowing and higher taxes on the wealthy. Biden argues these and other programmes are essential to reverse a decade of underinvestment in American society. That\u2019s a criticism of Donald Trump, who consistently tried to slash federal spending, but also of Biden\u2019s cautious old boss, Barack Obama, whose record he has begun to eclipse. Republicans, predictably, are opposed, complaining, for example, that military spending is neglected. Yet like many Americans right across the political spectrum, they appear dumbstruck by Biden\u2019s sheer audacity. Over a long career, he was many things but never a radical. Reversing normal practice, he ran from the centre, yet now he governs from the left. Perhaps, at 78, he feels he has little to lose and the nation much to gain. Biden is a man in a hurry and spurring him is not only an older man\u2019s zeal but a crude calculation. The Democrats\u2019 majority in Congress is wafer-thin and the 2022 midterms loom. If Biden pulls off only half of what he plans, it will be a remarkable achievement. Whatever happens, he has already changed the conversation. Economically, the essential, leading role of the state has been forcefully reasserted. This holds true for the US, and also for Britain and Europe, in the transformative age of Covid. Politically, Biden is in the process of demonstrating that liberal democracies, when ably led, can both reform themselves and outperform authoritarian regimes. Positive US global leadership, based on revived prosperity and multilateralism, is returning. More than Trump ever did, Biden is making America great again. Yet even as they cheer him and urge even grander feats, those on the British left, in particular, should take careful note. If you want to \u201cdo a Biden\u201d and enact great change, you must first forge alliances and win an election."}, "210705_news_467437.txt": {"page_id": "210705_news_467437.txt", "text": "Die Sicht auf China hat sich hierzulande in den vergangenen Jahren deutlich gewandelt. Bekamen westliche Manager kurz nach der Jahrtausendwende noch leuchtende Augen, wenn man die \"Werkbank der Welt\" mit ihrem gigantischen Markt in S\u00fcdostasien nur erw\u00e4hnte, sind der Hype und die Euphorie rund um das \"Wirtschaftswunderland\" gr\u00f6\u00dftenteils verflogen. Dies gilt auch f\u00fcr die Technologiepolitik, wo bis vor Kurzem viele deutsche Unternehmen noch Partnerschaften und Joint Ventures mit chinesischen Firmen etwa im Bereich K\u00fcnstliche Intelligenz (KI) massiv vorantrieben. \n          Was fehlt: In der rapiden Technikwelt h\u00e4ufig die Zeit, die vielen News und Hintergr\u00fcnde neu zu sortieren. Am Wochenende wollen wir sie uns nehmen, die Seitenwege abseits des Aktuellen verfolgen, andere Blickwinkel probieren und Zwischent\u00f6ne h\u00f6rbar machen. \n         Ein Beispiel daf\u00fcr ist das Deutsche Forschungszentrum f\u00fcr K\u00fcnstliche Intelligenz (DFKI). Das haupts\u00e4chlich vom Bund und den L\u00e4ndern finanzierte Unternehmen war lange sehr chinafreundlich unterwegs. 2017 unterzeichnete es im Beisein von Bundeskanzlerin Angela Merkel (CDU) und dem chinesischen Ministerpr\u00e4sidenten Keqiang Li  eine Absichtserkl\u00e4rung zu einer Zusammenarbeit im Bereich der visuellen KI  mit der Firma 4DAGE Technology, die 3D-Technologien entwickelt. \"Regelm\u00e4\u00dfiger Informations- und Wissensaustausch\" \"Eckpfeiler sind neben dem regelm\u00e4\u00dfigen Informations- und Wissensaustausch die F\u00f6rderung gemeinsamer Forschungsarbeit, die Planung und Durchf\u00fchrung gemeinsamer Projekte, der Aufbau eines gemeinsamen Technologiemanagements und die Identifikation langfristiger gemeinsamer Forschungsfelder\", hie\u00df es damals. Die gemeinsamen thematischen Schwerpunkte sollten auf Forschungsfeldern wie Augmented Reality und Visual Computing liegen. Die Anwendungsfelder schienen riesig und erstreckten sich dem Plan nach etwa auf Tourismus, virtuelle Museen, Stadtplanung und E-Commerce. 2018 gab es eine \u00e4hnliche Meldung. Die Honoratioren waren diesmal zwar nicht ganz so hoch angesiedelt, aber  immerhin kam der damalige Pekinger Vize-B\u00fcrgermeister Yin Hejun nach Berlin , um der \"feierlichen\" Unterzeichnung eines Vertrags zur erweiterten Kooperation zwischen dem DFKI und dem Artificial Intelligence Technology Center (AITC) beizuwohnen. Das Forschungsunternehmen aus Saarbr\u00fccken erl\u00e4uterte dazu, dass das AITC im April 2017 vom DFKI-Wissenschaftsdirektor Hans Uszkoreit gemeinsam mit Hanyan Zhang gegr\u00fcndet worden sei, \"um KI-Technologien aus den Forschungslaboren und technologischen Vorreiterunternehmen in die Industrie und Gesellschaft zu bringen\". \"Deutsch-chinesisches Joint Venture\" Ein besonderer Fokus des Pekinger Zentrums liege \"auf der Kommerzialisierung von Technologien aus Deutschland in der chinesischen Wirtschaft mit Hilfe von Gesch\u00e4ftsmodellen, die sowohl den Sch\u00f6pfern als auch den Nutzern dieser Technologien zugutekommen\", f\u00fchrte das DFKI aus. Bisherige Forschungsschwerpunkte des AITC seien die Entwicklung von Smart-Service-Systemen und die Kreation einer Big-Data-Plattform. Nun wolle man gemeinsam Technologieentwicklung betreiben. Dazu geh\u00f6rte das Vorhaben, das am DFKI entwickelte DARE-System zur Relationsextraktion aus Texten zu erweitern. Entstehen sollten \"gemeinsame Labs und Testumgebungen zu Themen wie Smarte Produktion oder Industrie 4.0\". Im gleichen Zuge wurde eine Berliner Dependance, das AITC Europe, gegr\u00fcndet. Beim AITC selbst handelt es sich laut dessen  Beschreibung auf der Firmenwebseite  um ein \"deutsch-chinesisches Joint Venture\", das als \"offizielle Vertretung des Deutschen Forschungszentrums f\u00fcr K\u00fcnstliche Intelligenz (DFKI) in China\" fungiere. Es ziele darauf ab, \"modernste Forschungsergebnisse und neueste Technologien zu nutzen, um Startups, reife Unternehmen und aufstrebende Talente zu unterst\u00fctzen\". \"Spin-off\" Das AITC betreibt seit mehreren Jahren als mehr oder weniger offizielle chinesische DFKI-Repr\u00e4sentanz nicht nur ein B\u00fcro in Peking, sondern mit dem weiteren Unternehmen Giance dort auch schon ein \"AITC/DFKI-Spin-off\". Zu den Mitgr\u00fcndern geh\u00f6rt Uszkoreit, der mit seiner chinesischen KI-Forscherkollegin Feiyu Xu verheiratet ist und mittlerweile die meiste Zeit in der chinesischen Hauptstadt lebt. Auf der  Giance-Webseite  ist zu lesen, dass das Start-up auf langj\u00e4hriger Forschung am DFKI, der Universit\u00e4t des Saarlandes und der Universit\u00e4t Peking aufbaue. Die Entwicklung der eigenen Technologie sei vor ihrer Kommerzialisierung \"durch Projektf\u00f6rderungen des Bundesministeriums f\u00fcr Bildung und Forschung, der Deutschen Forschungsgemeinschaft, zwei Google-Forschungspreise, Industriekooperationen des Bundesministeriums f\u00fcr Wirtschaft und Energie, Zusch\u00fcsse der chinesischen Nationalen Forschungsstiftung und F&E-Auftr\u00e4ge von Industrieunternehmen unterst\u00fctzt\" worden, hei\u00dft es dort weiter. Einschl\u00e4gige Forschungsergebnisse w\u00fcrden \"in mehr als 40 internationalen Publikationen dokumentiert\". Auch hier ist von einem \"Berlin-Team\" die Rede, das bereits eine neue Generation eines Produktionssystems mit verbesserten Algorithmen f\u00fcr nat\u00fcrliche Sprach\u00fcbersetzung und Wissensverarbeitung fertiggestellt habe."}, "210705_news_467462.txt": {"page_id": "210705_news_467462.txt", "text": "A trio of countries stand out for the effectiveness of their Covid-19 vaccination programmes:  Israel, Chile  and the UK. All have managed to inoculate an impressively high percentage of their people but each has fared very differently in controlling the disease. Israel has  done so well  it is resuming university lectures, concerts and other mass gatherings and has opened up its restaurants and bars. By contrast, Chile is experiencing soaring levels of Covid cases and faces  new lockdown restrictions . In Britain, deaths and hospital admissions have plummeted but it remains to be seen what will happen when lockdown restrictions are  eased in England  from Monday. (Scotland, Wales and Northern Ireland each have their own timetables for easing.) Will the UK follow the grim example of Chile or the happier precedent of Israel? The nation will soon find out although it should be noted that  Israel  and Chile are not the only ones that provide helpful illustrations of how the fight against Covid-19 should be shaped in coming months. Australia, New Zealand, France, Germany and many others provide key lessons. Nevertheless, it is Chile that supplies the sharpest warning for the UK. Its health workers have delivered first jabs to 37% of the population but daily cases are still rising sharply. Several reasons have been put forward for this unexpected jump: the spread of more virulent  coronavirus strains from Brazil ; increased numbers of Chileans travelling around the country; and reduced adherence to social distancing after the vaccination programme gave people a false sense of security. The importance of this last point was stressed by Prof Lawrence Young, a virologist at Warwick Medical School. \u201cI think that  Chile  shows the danger of being too reliant just on vaccines. Vaccines are fantastic but they\u2019re never going to be a solution on their own and what is happening in Chile provides us with a very clear warning.\u201d People queue for the second dose of their Covid vaccine in Santiago. Despite a successful vaccination programme, Chile has seen sharp rise in cases.  Photograph: Esteban F\u00e9lix/AP Prof Stephen Griffin, of Leeds University School of Medicine, agreed. \u201cYou still need to get cases under control while you\u2019re vaccinating. If you don\u2019t, you will still be in trouble.\u201d Chile therefore reveals the dangers of vaccine hubris. By contrast Israel demonstrates the need for constant planning and preparedness. Since its mighty vaccine rollout began, it has set up a number of initiatives to maintain its progress against Covid. These include a system of green passes that are given to people who have either had both vaccine doses or have recovered from the illness and are therefore deemed unlikely to be infectious. The plan is controversial and many have protested against its imposition. \u201cHowever, for universities, it has helped to get students back into lecture theatres where academics can teach students in person,\u201d said Linda Bauld, professor of public health at Edinburgh University. \u201cThese are the sorts of measures we need to be discussing now so we can be sure we are opening up safely over summer.\u201d Two other Israeli measures were also highlighted by Bauld. Antibody tests \u2013 which will show if a person has Covid antibodies either from a vaccine or a previous infection \u2013 allow international travellers arriving in Israel to avoid quarantine. At the same time, health authorities are also considering giving vaccines to older children once they are approved by regulators. These initiatives show just how far ahead Israel is planning, added Bauld. Other scientists point to the examples of Australia and New Zealand. The former has had just a handful of cases despite launching its vaccination programme only a few weeks ago \u2013 thanks to the rapid closure of its borders last year and its carefully managed hotel quarantine system that has reduced Covid\u2019s spread to minuscule levels. By contrast, Britain\u2019s lamentable test, trace and isolate system remains rickety and unproven \u2013 despite the fact it will be crucial to suppressing new outbreaks of Covid-19 once restrictions are lifted. \u201cTo put it simply, we have not learned just how important isolating infected people is still going to be,\u201d said Griffin. Then there is the issue of vaccinating the world \u2013 for until this happens, Covid-19 will remain a menace and Britain will continue to be under threat. It therefore has a role to play in providing jabs around the globe. Israeli musician Ivri Lider performs for an audience wearing face masks and who had shown a \u2018green pass\u2019 to enter a stadium in Tel Aviv last month.  Photograph: Oded Balilty/AP Scientists estimate that more than 11bn doses of vaccines will be required to provide double  jabs for 70% of the world\u2019s population  \u2013 a number that would, hopefully, achieve some form of global herd immunity. However, recent figures indicate that the richest nations \u2013 who make up a fifth of the world\u2019s population and include the UK \u2013 have already bought 6bn doses, while the remaining poorer nations \u2013 four-fifths of humanity \u2013 have secured only 2.6bn. In the face of this huge vaccine imbalance,  India and South Africa have asked  the World Trade Organization to suspend patent rights on various Covid-19 techniques, vaccines and drugs to help them produce their own treatments to cope with the pandemic. The proposal has now been backed by more than 100 nations. \u201cWe cannot repeat the painful lessons from the early years of the Aids response when wealthier countries got back to health while millions of people in developing countries were left behind,\u201d Winnie Byanyima, executive director of Unaids, the United Nations HIV/Aids agency said in  the journal  Nature  recently . This point was backed last week by Dorothy Guerrero, head of policy at Global Justice Now, an NGO campaigning for equitable vaccine access. She  accused rich countries of hoarding vaccines  at the expense of low- and middle-income countries. \u201cThere is one rapid, sure-fire way to increase global vaccination \u2013 waive patents on Covid-19 vaccines and let countries produce their own jabs. Countries like the UK need to step up.\u201d However, the European Union, the UK and many other western nations, together with major pharmaceutical companies, argue that waiving patent rights would not help. They say making vaccines involves the implementation of a series of careful, quality-controlled steps. Negotiating the distribution of patent rights for these different processes would take far too long. It would be better to increase vaccine production to its highest level and then distribute jabs. Scientists, though, are emphatic that the world will not be safe from Covid-19 until global immunisation has taken place. As the slogan states: no one is safe until everyone is safe. Achieving that goal could take years, however."}, "210705_news_467469.txt": {"page_id": "210705_news_467469.txt", "text": "Bugs in ML code are notoriously hard to fix - they don\u2019t cause compile errors but silently regress accuracy. Once you have endured the pain and fixed one of these, the lesson is forever etched into your brain, right?  Wrong . Recently, an old foe made a comeback - a familiar bug bit me again! As before, the performance improved significantly after fixing it. The bug was subtle and easy to make. How many others has it done damage to? Curious, I downloaded over a hundred thousand repositories from GitHub that import PyTorch, and analysed their source code. I kept projects that define a custom dataset, use NumPy\u2019s random number generator with multi-process data loading, and are more-or-less straightforward to analyse using abstract syntax trees. Out of these, over 95% of the repositories are plagued by this problem. It\u2019s inside PyTorch\u2019s official  tutorial , OpenAI\u2019s  code , and NVIDIA\u2019s  projects . Even Karpathy  admitted  falling prey to it. \n  The bug\n   The canonical way to load, pre-process and augment data in PyTorch is to subclass the  torch.utils.data.Dataset  and overwrite its  __getitem__  method. To apply augmentations, such as random cropping and image flipping, the  __getitem__  method often makes use of NumPy to generate random numbers. The map-styled dataset is then passed to the  DataLoader  to create batches. The training pipeline might be bottlenecked by data pre-processing, and therefore it makes sense to load data in parallel. This can be achieved by increasing the  num_workers  parameter in the  DataLoader  object. The problem:  this workflow results in identical augmentations. \n  A minimal example\n   To make the issue concrete, here\u2019s an example dataset which returns three-element random vectors. We use a batch size of two and four worker processes. import   numpy   as   np from   torch.utils.data   import  Dataset, DataLoader\n\n class   RandomDataset (Dataset):\n     def  __getitem__(self, index):\n         return  np . random . randint( 0 ,  1000 ,  3 )\n\n     def  __len__(self):\n         return   16 \n    \ndataset  =  RandomDataset()\ndataloader  =  DataLoader(dataset, batch_size = 2 , num_workers = 4 )\n for  batch  in  dataloader:\n     print (batch)\n The code returns the following tensors: tensor([[ 116 ,  760 ,  679 ],    # 1st batch, returned by process 0 \n        [ 754 ,  897 ,  764 ]])\ntensor([[ 116 ,  760 ,  679 ],    # 2nd batch, returned by process 1 \n        [ 754 ,  897 ,  764 ]])\ntensor([[ 116 ,  760 ,  679 ],    # 3rd batch, returned by process 2 \n        [ 754 ,  897 ,  764 ]])\ntensor([[ 116 ,  760 ,  679 ],    # 4th batch, returned by process 3 \n        [ 754 ,  897 ,  764 ]])\n\ntensor([[ 866 ,  919 ,  441 ],    # 5th batch, returned by process 0 \n        [  20 ,  727 ,  680 ]])\ntensor([[ 866 ,  919 ,  441 ],    # 6th batch, returned by process 1 \n        [  20 ,  727 ,  680 ]])\ntensor([[ 866 ,  919 ,  441 ],    # 7th batch, returned by process 2 \n        [  20 ,  727 ,  680 ]])\ntensor([[ 866 ,  919 ,  441 ],    # 8th batch, returned by process 3 \n        [  20 ,  727 ,  680 ]])\n The random numbers returned from each process are identical! \n  Why does this happen?\n   PyTorch uses multiprocessing to load data in parallel. The worker processes are created using the  fork   start method . This means each worker process inherits all resources of the parent, including the state of NumPy\u2019s random number generator. \n  The fix\n   The  DataLoader  constructor has an optional  worker_init_fn  parameter. This function is called in each worker process at initialization before any data loading has happened. You can set the seed for NumPy in the  worker_init_fn , for example: def   worker_init_fn (worker_id):                                                          \n    np . random . seed(np . random . get_state()[ 1 ][ 0 ]  +  worker_id)\n\ndataset  =  RandomDataset()\ndataloader  =  DataLoader(dataset, batch_size = 2 , num_workers = 4 , \n                        worker_init_fn = worker_init_fn)\n\n for  batch  in  dataloader:\n     print (batch)\n The code outputs different values for each batch, as one would expect: tensor([[ 282 ,    4 ,  785 ],\n        [  35 ,  581 ,  521 ]])\ntensor([[ 684 ,   17 ,   95 ],\n        [ 774 ,  794 ,  420 ]])\ntensor([[ 939 ,  988 ,   37 ],\n        [ 983 ,  933 ,  821 ]])\ntensor([[ 832 ,   50 ,  453 ],\n        [  37 ,  322 ,  981 ]])\n\ntensor([[ 180 ,  413 ,   50 ],\n        [ 894 ,  318 ,  729 ]])\ntensor([[ 530 ,  594 ,  116 ],\n        [ 636 ,  468 ,  264 ]])\ntensor([[ 142 ,   88 ,  429 ],\n        [ 407 ,  499 ,  422 ]])\ntensor([[  69 ,  965 ,  760 ],\n        [ 360 ,  872 ,   22 ]])\n But wait a minute, when we iterate over epochs, for  epoch  in   range ( 3 ):\n     print (f \"epoch: {epoch}\" )\n     for  batch  in  dataloader:\n         print (batch)\n     print ( \"-\" * 25 )\n this happens: \nepoch:  0 \ntensor([[ 282 ,    4 ,  785 ],\n        [  35 ,  581 ,  521 ]])\ntensor([[ 684 ,   17 ,   95 ],\n        [ 774 ,  794 ,  420 ]])\ntensor([[ 939 ,  988 ,   37 ],\n        [ 983 ,  933 ,  821 ]])\ntensor([[ 832 ,   50 ,  453 ],\n        [  37 ,  322 ,  981 ]])\n ------------------------- \nepoch:  1 \ntensor([[ 282 ,    4 ,  785 ],\n        [  35 ,  581 ,  521 ]])\ntensor([[ 684 ,   17 ,   95 ],\n        [ 774 ,  794 ,  420 ]])\ntensor([[ 939 ,  988 ,   37 ],\n        [ 983 ,  933 ,  821 ]])\ntensor([[ 832 ,   50 ,  453 ],\n        [  37 ,  322 ,  981 ]])\n ------------------------- \nepoch:  2 \ntensor([[ 282 ,    4 ,  785 ],\n        [  35 ,  581 ,  521 ]])\ntensor([[ 684 ,   17 ,   95 ],\n        [ 774 ,  794 ,  420 ]])\ntensor([[ 939 ,  988 ,   37 ],\n        [ 983 ,  933 ,  821 ]])\ntensor([[ 832 ,   50 ,  453 ],\n        [  37 ,  322 ,  981 ]])\n ------------------------- (the dataset length is reduced from 16 to 8 for brevity). Iterating over the dataset three times produces the same random numbers at each epoch. This happens because all changes to random states are local to each worker. By default, the worker processes are killed at the end of each epoch, and all worker resources are lost. At the same time, the random state in the main process hasn\u2019t changed, and it\u2019s used to initialize each worker process again. Therefore you need to change the NumPy\u2019s seed at every epoch, for example by  np.random.seed(initial_seed + epoch) . Moreover, you won\u2019t have these issues if you sample random numbers using PyTorch (for example,  torch.randint ) or Python\u2019s built-in random number generator. PyTorch takes care of these by  setting  the above seeds to  seed + worker_id  automatically. \n  In the wild examples\n   Here I have listed a few projects with the aforementioned bug. \n  Official PyTorch tutorial on custom datasets\n   A go-to tutorial for using a custom dataset in PyTorch is the one listed on their  website . The tutorial demonstrates how to use the  Dataset  and  DataLoader  classes on a face-landmarks dataset. It also mentions the importance of data augmentation, and provides an example of a random crop augmentation. This is implemented using NumPy\u2019s random number generator. top  =  np . random . randint( 0 , h  -  new_h)\nleft  =  np . random . randint( 0 , w  -  new_w)\n Following the tip of speeding up data loading by increasing  num_workers , you get identical crops:   (batch size 8, num_workers 2, random crop augmentation) \n  OpenAI\u2019s EBM\n   In the paper  Implicit Generation and Modeling with Energy-Based Models , an energy-based model is used for generative modeling of images. The dataset\u2019s  __getitem__   method  reads images and labels from disk, corrupts the former, and returns all three: if  FLAGS . datasource  ==   'default' :\n    im_corrupt  =  im  +   0.3   *  np . random . randn(image_size, image_size)\n elif  FLAGS . datasource  ==   'random' :\n    im_corrupt  =   0.5   +   0.5   *  np . random . randn(image_size, image_size)\n\n return  im_corrupt, im, label\n These corruptions, however, are identical:   (the first corrupted image in batch 1 and 2) \n  MelGAN\n   The official code for MelGAN, a model for generative audio synthesis published in the NeurIPS conference,  augments  the loudness of audio files by sampling random scalars using NumPy. data, sampling_rate  =  load(full_path, sr = self . sampling_rate)\ndata  =   0.95   *  normalize(data)\n\n if  self . augment:\n    amplitude  =  np . random . uniform(low = 0.3 , high = 1.0 )\n    data  =  data  *  amplitude\n With  num_workers  set to four, the audio levels are identical between the processes. \n  Conclusion\n   The bug is easy to make. In some cases, it has minimal effect on final performance. In others, the identical augmentations can cause severe degradations. Based on the analysis of open-source PyTorch projects, I am afraid the issue is present in many codebases supporting real products. I hope that better awareness of the trap, and eventually,  better handling  of it in PyTorch, makes these products a little bit better. \u00a0 Thanks to Sten Sootla and Kaspar M\u00e4rtens for their comments and suggestions."}, "210705_news_467470.txt": {"page_id": "210705_news_467470.txt", "text": "Abstract The urban poor in developing countries face challenging living environments, which may interfere with good sleep. Using actigraphy to measure sleep objectively, we find that low-income adults in Chennai, India, sleep only 5.5\u00a0hours a night on average despite spending 8\u00a0hours in bed. Their sleep is highly interrupted, with sleep efficiency\u2014sleep per time in bed\u2014comparable to those with disorders such as sleep apnea or insomnia. A randomized three-week treatment providing information, encouragement, and improvements to home sleep environments increased sleep duration by 27 minutes a night by inducing more time in bed. Contrary to expert predictions and a large body of sleep research, increased nighttime sleep had no detectable effects on cognition, productivity, decision making, or well being, and led to small decreases in labor supply. In contrast, short afternoon naps at the workplace improved an overall index of outcomes by 0.12 standard deviations, with significant increases in productivity, psychological well-being, and cognition, but a decrease in work time.   I. Introduction Understanding the lives of the poor is central to modern development economics. Economists have studied many deprivations associated with poverty, such as lack of access to nutrition, water, education, health care, and clean air. This article considers a previously unexamined challenge faced by the urban poor in developing countries: sleep deprivation. People in these settings face many barriers to a good night\u2019s sleep, such as heat, noise, crowding, physical discomfort, and psychological distress. Sleep could be a crucial input to their productivity, well-being, and cognitive function. Yet we know little about how much and how well people in low-income countries sleep, or the returns to policies that seek to increase sleep. Using state-of-the-art technology to measure sleep objectively, we uncover widespread sleep deprivation in Chennai, India. Our two samples of low-income adults sleep only 5.5\u00a0hours a night on average, far below the minimum level recommended by sleep experts ( Hirshkowitz et\u00a0al. 2015 ;  Watson et\u00a0al. 2015 ). This is not due to a lack of trying. People spend about eight\u00a0hours a night in bed, but their sleep is highly disrupted, with 31 awakenings in a typical night. The implied sleep efficiency\u2014time asleep per time in bed\u2014of 70% is much lower than objective measures from general U.S. populations, and similar to those suffering from disorders such as sleep apnea or insomnia in high-income countries ( Hedner et\u00a0al. 2004 ;  Trauer et\u00a0al. 2015 ). An enormous body of research, mostly conducted in sleep labs in rich countries, documents severe negative effects of sleep deprivation on a range of outcomes, from attention and memory to mood and health ( Banks and Dinges 2007 ;  Lim and Dinges 2010 ). While experimental evidence on the effect of increasing sleep in field settings is scarce, 1  there is a widely held belief among researchers and the public that reducing sleep deprivation would lead to improvements in economic outcomes ( Walker 2017 ). To document these priors, we surveyed 119 experts from sleep science and economics who predicted sizable economic benefits, including a 7% increase in work output, of increasing sleep by half an hour a night from the low levels observed in our setting. To measure the economic effects of increasing sleep in the field, we conducted a randomized controlled trial with 452 adults in Chennai. We employed participants for a one-month data entry job with flexible hours, allowing us to precisely measure productivity and labor supply, as well as physical and psychological well-being, cognition, and time, risk, and social preferences. Two night sleep treatments gave participants (i) items to improve their home-sleep environments, (ii) information and verbal encouragement to increase their night sleep (the encouragement treatment), and (iii) for a subset of participants, additional financial incentives to increase night sleep (the incentives treatment). These treatments were cross-randomized with a nap treatment that offered participants the opportunity for a daily half-hour afternoon nap at their workplace. 2 The night sleep treatments on average increased nighttime sleep by 27 minutes a night (std. err. = 3 minutes), with larger effects for the incentives (33 minutes) than for the encouragement treatment (20 minutes). The increased sleep duration was entirely driven by additional time in bed\u2014on average 38 minutes a night (std. err. = 4 minutes)\u2014rather than higher sleep efficiency. These results demonstrate that people do have substantial ability to adjust their nighttime sleep through changes in time in bed, but may not be able to increase their sleep efficiency. The low sleep efficiency increases the opportunity cost of sleep: raising sleep duration by 1 minute requires 1.4 more minutes in bed. Similarly, the nap treatment increased daytime sleep by about 14 minutes a day on average (std. err. = 0.3 minutes), while slightly crowding out nighttime sleep. Although both types of treatments increased 24-hour sleep, the 27-minute increase from the night sleep treatments was significantly higher than the 8-minute increase from the nap treatment ( p  < .01). We first examine the effects of each combination of treatments on an overall summary index that aggregates all outcomes as in  Anderson (2008) . Each of the night sleep treatments alone had no significant effect on this overall index: 0.00 standard deviations (std. err. = 0.07) and \u22120.05 std. dev. (std. err. = 0.07), respectively, for the encouragement only and incentives only groups. In contrast, naps alone had a positive, marginally significant effect of 0.11 std. dev. (std. err. = 0.07,  p  = .11). Those who received a night sleep treatment in addition to naps had very similar effects to those with naps only. 3  This pattern of results suggests that naps have an overall positive effect on outcomes, whereas increases in night sleep do not. To increase statistical power and streamline the discussion of effects on the individual outcomes, we turn to an analysis that pools the two night sleep treatments and does not allow for an interaction effect of night sleep and nap treatments. The effects of each treatment described below should thus be interpreted as conditional on the distribution of the other treatment ( Muralidharan, Romero, and W\u00fcthrich 2019 ). In the pooled analysis, we find no significant effect of increased night sleep on the overall index (\u22120.01 std. dev., std. err. = 0.04), or on summary indices corresponding to four families of outcomes: work, well-being, cognition, and economic preferences. In fact, the pooled night sleep treatment had no significant positive effect on any outcome other than sleep itself. It did not significantly increase productivity at the data entry job, a relatively cognitively demanding task intended to be sensitive to sleep deprivation. Instead, increased sleep came at the cost of lowering labor supply by nine minutes a day, leading to a small (but not statistically significant) decrease in earnings. We reject the median expert prediction of a 7% increase in output ( p  < .001). Similarly, we find no significant effects on detailed measures of physical and psychological well-being or standard measures of cognition and social, risk, and time preferences. Why does increased night sleep not have benefits in our setting, contrary to expert predictions and a large body of lab studies? One possibility is that the large effects from lab experiments, which typically dramatically reduce sleep for up to a few nights, do not generalize to marginal, policy-relevant increases in sleep in the field. Another possibility is that the low quality of sleep observed in our setting\u2014as proxied by low efficiency and frequent awakenings\u2014explains the lack of benefits of increased nighttime sleep. Returns to increased sleep could be higher in typical rich-country settings. We cannot adjudicate these reasons, but our results highlight the importance of studying sleep in the field, where outcomes have real stakes and sleep is a choice variable with opportunity costs. They also caution against extrapolating sleep science findings across diverse contexts. In contrast to night sleep, naps significantly improved a range of outcomes. The nap treatment significantly increased the overall summary index by 0.12 std. dev. (std. err. = 0.04,  p  = .00), as well as the index variables corresponding to well-being (0.08 std. dev.,  p  = .03), and cognition (0.10 std. dev.,  p  = .08). The effect of naps on work outcomes depends on the comparison. Compared with taking a break, naps increased earnings\u2014the summary variable for the work outcomes\u2014by 0.05 std. dev. ( p  = .05). However, driven by a decrease in labor supply, naps reduced earnings compared to working during the nap time by 0.10 std. dev. ( p  = .00), highlighting the importance of taking into account the opportunity costs of sleep. Considering the individual outcome variables one by one and adjusting for multiple comparisons, naps have significant positive effects on productivity (0.04 std. dev.,  p  = .06), psychological well-being (0.12 std. dev.,  p  = .04), lab measures of cognitive function (0.08 std. dev.,  p  = .06), and attention at work (0.20 std. dev.,  p  = .07). The nap and night sleep treatments have statistically different effects on the overall index ( p  = .02). We are less powered to detect differences for the family-level indices. Point estimates are larger for the nap treatment for well-being, cognition, and preferences, but the differences are not statistically significant. At the level of individual outcomes, we find statistically significant differences in effects only for psychological well-being ( p  = .04). Estimating the per minute effects of the two types of sleep in an instrumental variable analysis, however, we can reject equal per minute effects of naps and nighttime sleep on the summary index ( p  < .01), on the family indices for cognition ( p  = .08) and preferences ( p  = .08), and on some individual outcomes, including labor supply ( p  = .04) and psychological well-being ( p  = .05). In every case but labor supply, the effects are more positive for naps. One possible reason for the different effects of the nap and night sleep treatments is that the timing of sleep may matter. Contrary to hypotheses and some evidence in sleep science (e.g.,  Nicholson et\u00a0al. 1985 ;  Mollicone, Van\u00a0Dongen, and Dinges 2007 ;  Mollicone et\u00a0al. 2008 ), naps and night sleep may simply not be close substitutes. An alternative explanation is that sleep quality may play a role, since naps in our study occurred in a more comfortable office environment. We cannot separate these explanations, but hope that future work in similar settings may help answer this question. Our article makes the following contributions. First, it contributes to a better understanding of the living conditions faced by the poor in developing countries by providing objective measures of sleep. We discover surprisingly low levels of sleep duration and efficiency among the urban poor in Chennai. These findings are consistent with two papers measuring sleep objectively in smaller samples in Sri Lanka and Haiti ( Castro et\u00a0al. 2013 ;  Schokman et\u00a0al. 2018 ) and contrast with self-reported measures of sleep, which may fail to capture the low sleep efficiency and its effect on total sleep  (Stranges et\u00a0al. 2012 ;  Gildner et\u00a0al. 2014 ;  Simonelli et\u00a0al. 2018 ). Second, we build on a recent literature that estimates the causal effect of sleep outside of sleep laboratories. The lack of effects of nighttime sleep we find using a field experiment contrasts with an economics literature that uses natural experiments in rich countries to demonstrate that sleep can have sizable effects on wages  (Gibson and Shrader 2018 ;  Giuntella and Mazzonna 2019 ), hospitalizations ( Jin and Ziebarth 2020 ), accidents ( Smith 2016 ), and civic behaviors ( Holbein, Schafer, and Dickinson 2019 ). 4  We speculate that the stark difference in sleep efficiency in our setting compared to rich-country populations contributes to this difference. It could also be that increased night sleep simply does not have high benefits at the margin in the field, as in another recent field experiment ( Avery, Giuntella, and Jiao 2019 ). Additional field studies of sleep, including interventions that improve sleep efficiency, may help reconcile these findings. Third, we show that afternoon naps in a comfortable office environment have positive effects on a range of outcomes, including productivity, well-being, and cognition. Naps are a common feature of life around the world and are particularly prevalent in tropical countries ( Dinges 1992 ). Naps have been studied in sleep labs, but we have little causal evidence on the effects of naps on worker productivity and other real-world outcomes or consideration of the opportunity cost of naps  (Lovato and Lack 2010 ;  Ficca et\u00a0al. 2010 ). Our work takes a step toward filling this gap, and shows that naps may be an effective way to combat sleep deprivation. The decline of naps as employment in developing countries shifts toward Western schedules could therefore be costly. Finally, recent research in behavioral and development economics argues that people in developing countries often underinvest in high-return investments such as preventive health, agricultural inputs, or capital investments ( Kremer, Rao, and Schilbach 2019 ). At first glance, the low levels of sleep we discovered appear to tell the same story, and experts predicted substantial effects of increased sleep. Instead, our evidence suggests that\u2014in this context\u2014people do not underinvest in sleep duration given the environmental constraints that they face. The returns to increasing night sleep in their home environments are low and possibly even negative. To paraphrase  Schultz (1964) , low-income people in Chennai may be poor but efficiently tired.   II. Measuring Sleep in Chennai II.A. Measuring Sleep Outside the Lab The gold standard for objectively measuring sleep in labs is polysomnography (PSG), by recording brain waves, blood oxygen levels, eye movements, and body movements to determine sleep/wake cycles and stages of sleep ( Marino et\u00a0al. 2013 ). Although highly accurate, this bulky technology is impractical for field studies and may interfere with natural sleep patterns at people\u2019s homes, thus making measuring sleep outside of sleep labs challenging. Self-reported measures are unreliable and correlate only moderately with objective measures because people tend to report time in bed rather than hours asleep, leading to overreporting of sleep duration ( Lauderdale et\u00a0al. 2008 ;  Schokman et\u00a0al. 2018 ). Actigraphs, which resemble wristwatches and infer sleep/wake states from body movement, recently emerged as a viable alternative for field studies. These devices allow researchers to objectively measure sleep in participants\u2019 home environments without interfering with sleep, as these devices are portable, comfortable, and unobtrusive. Validation studies show that actigraphs reliably measure sleep duration. Comparisons between actigraphy and PSG measures show high degrees of accuracy in sleep-wake detection, with 90% minute-by-minute agreement between the two ( Sadeh et\u00a0al. 1995 ;  Marino et\u00a0al. 2013 ). Actigraphs have been found to provide valid and clinically useful measures of sleep duration even among people with sleep disorders  (Kushida et\u00a0al. 2001 ;  Smith et\u00a0al. 2018 ) and reliably capture treatment effects of various interventions on sleep ( Sadeh 2011 ). Actigraphs also measure sleep efficiency, defined as time asleep divided by time in bed. This measure is available since\u2014in addition to number of hours asleep\u2014actigraphs also detect when an individual is in bed but not asleep. Sleep efficiency is perhaps the most commonly used proxy for sleep quality in sleep science ( Ohayon et\u00a0al. 2017 ). Disruptions to sleep, such as brief awakenings during the night, drive down sleep efficiency. In addition, sleep efficiency affects the opportunity cost of sleep, since it indicates the time in bed needed to achieve an hour of actual sleep.   II.B. Sleep Deprivation around the World While sleep scientists recommend seven to nine\u00a0hours of sleep a night  (Hirshkowitz et\u00a0al. 2015 ;  Watson et\u00a0al. 2015 ), numerous studies show that people in high-income countries sleep less than this ( Walker 2017 ). 5  For instance,  Lauderdale et\u00a0al. (2008)  measure sleep via actigraphy among a large, diverse population of healthy young adults in Chicago, and report an average sleep duration of 6.1\u00a0hours a night, well below the recommended range. In contrast, there is scant evidence on sleep patterns in developing countries. Sleep deprivation may be widespread and even more severe in the rapidly growing cities of the developing world, where residential structures are often of low quality and people are exposed to excessive heat, noise, crowding, and pollution\u2014all conditions likely to hinder sleep. Even self-reports\u2014which typically overestimate sleep\u2014suggest a substantial share of people in developing countries sleep less than the recommended seven to nine\u00a0hours. For example, the 4,500 rural, older Indian adults surveyed in  Gildner et\u00a0al. (2014)  self-report 7.1\u00a0hours of sleep on average, with about 30% of these individuals reporting 6 or fewer hours a night ( Selvamani et\u00a0al. 2018 ). Two recent studies in low-income countries measured sleep using actigraphs and identify significant fractions of the population as sleep deprived. In particular,  Schokman et\u00a0al. (2018)  finds an average of only six\u00a0hours a night among 175 adults from urban Sri Lanka.  Knutson (2014)  finds that 58 adults in Haiti sleep on average seven\u00a0hours a night in a rural population without electricity.   II.C. Sleep(less) in Chennai Our first measure of sleep in Chennai comes from the RCT sample of 452 adults recruited for a full-time data entry job for one month. To capture reliable and objective measures of sleep beyond self-reports, all participants wore actigraphs continuously throughout the study. 6  Below, we describe sleep during the baseline period (before treatment) in this sample. We then report very similar patterns of sleep in a broader sample in Chennai, which wore actigraphs for three nights.   1. A Typical Night in Chennai We first provide an example to highlight key features of participants\u2019 sleep patterns.  Figure\u00a0I , Panel A illustrates a typical night for a study participant, using minute-by-minute actigraph measures of sleep (light gray) and wake (dark red) status. This night closely matches the average time in bed, sleep duration, and sleep efficiency in the RCT sample. The participant spends about 8\u00a0hours in bed during this night but only achieves 5.6\u00a0hours of highly fragmented and interrupted sleep, involving over 30 awakenings. For comparison, we show a less interrupted night with 90% efficiency in  Figure\u00a0I , Panel B. While this night is unusual in Chennai\u2014only 1% of nights in our sample feature such high sleep\u2014it resembles nights of healthy adults in high-income countries who typically enjoy sleep efficiency of 85%\u201395% ( Cole et\u00a0al. 1992 ;  Carrier et\u00a0al. 2001 ;  Walker 2017 ).   Figure\u00a0I Typical Sleep in Chennai This figure represents actigraph-measured sleep-wake patterns of two particular nights of two selected study participants. Light gray areas indicate one-minute periods in which the participant was asleep, and dark red areas (color version available online) indicate periods in which the participant was awake according to the actigraph. The gray dashed lines indicate when the participant got into or out of bed. In Panel A we show a typical night in our sample, represented by average levels of time in bed, time asleep, and sleep efficiency. During this particular night, the participant stayed in bed for 7\u00a0hours, 45 minutes but slept for only 5\u00a0hours, 20 minutes, resulting in a sleep efficiency of 69%, corresponding to the 41st, 40th, and 43rd percentile of the control group, respectively. The participant awoke 31\u00a0times during this night, and the longest sleep episode lasted 45 minutes. Panel B depicts a good night of sleep, with sleep patterns similar to those found in the United States and other rich countries: the participant stayed in bed for 7\u00a0hours, 53 minutes and slept for 7\u00a0hours, 8 minutes, resulting in a sleep efficiency of 90%, corresponding to the 46th, 91st, and 99th percentile of the control group, respectively. In this night, the participant only awoke nine\u00a0times, and the longest sleep episode lasted 223 minutes. Figure\u00a0I Typical Sleep in Chennai This figure represents actigraph-measured sleep-wake patterns of two particular nights of two selected study participants. Light gray areas indicate one-minute periods in which the participant was asleep, and dark red areas (color version available online) indicate periods in which the participant was awake according to the actigraph. The gray dashed lines indicate when the participant got into or out of bed. In Panel A we show a typical night in our sample, represented by average levels of time in bed, time asleep, and sleep efficiency. During this particular night, the participant stayed in bed for 7\u00a0hours, 45 minutes but slept for only 5\u00a0hours, 20 minutes, resulting in a sleep efficiency of 69%, corresponding to the 41st, 40th, and 43rd percentile of the control group, respectively. The participant awoke 31\u00a0times during this night, and the longest sleep episode lasted 45 minutes. Panel B depicts a good night of sleep, with sleep patterns similar to those found in the United States and other rich countries: the participant stayed in bed for 7\u00a0hours, 53 minutes and slept for 7\u00a0hours, 8 minutes, resulting in a sleep efficiency of 90%, corresponding to the 46th, 91st, and 99th percentile of the control group, respectively. In this night, the participant only awoke nine\u00a0times, and the longest sleep episode lasted 223 minutes.   2. Time in Bed versus Time Asleep The RCT sample spends roughly eight\u00a0hours a night in bed before treatment begins, with strong congruence between actigraph measures ( Figure\u00a0II , Panel A) and self-reports ( Figure\u00a0II , Panel B). Time in bed in Chennai is quite similar to that found in U.S. samples. 7  Despite this significant time in bed, study participants only enjoy 5.6\u00a0hours asleep per night ( Table\u00a0I  and  Figure\u00a0II , Panel C), significantly below time in bed and the recommended seven to nine\u00a0hours. Ninety-five percent of participants slept less than seven\u00a0hours a night, and 71% slept less than six\u00a0hours a night on average. In high-income countries, such low average time asleep is typical in populations with disorders such as sleep apnea  (Cole et\u00a0al. 1992 ;  Kushida et\u00a0al. 2001 ;  Gershon et\u00a0al. 2012 ).   Figure\u00a0II Baseline Distributions of Sleep-Related Variables (RCT Sample) This figure shows the distribution of the sleep-related variables averaged at the participant-level over the baseline period (seven nights) in the RCT sample ( N  = 452). The left three panels show distributions of actigraph-measured sleep patterns and the right three panels show the corresponding distributions based on self-reports. Panels A and B show hours in bed as measured by actigraphy and by self-reports, respectively. Panels C and D show night sleep duration in hours as measured by actigraphy and by self-reports, respectively. Panels E and F show sleep efficiency (night sleep duration/time in bed) as measured by actigraphy and by self-reports, respectively. Figure\u00a0II Baseline Distributions of Sleep-Related Variables (RCT Sample) This figure shows the distribution of the sleep-related variables averaged at the participant-level over the baseline period (seven nights) in the RCT sample ( N  = 452). The left three panels show distributions of actigraph-measured sleep patterns and the right three panels show the corresponding distributions based on self-reports. Panels A and B show hours in bed as measured by actigraphy and by self-reports, respectively. Panels C and D show night sleep duration in hours as measured by actigraphy and by self-reports, respectively. Panels E and F show sleep efficiency (night sleep duration/time in bed) as measured by actigraphy and by self-reports, respectively.   TABLE I Sleep Statistics in Two Samples in Chennai   \n            .\u00a0 RCT sample \n            .\u00a0 Broader sample \n            .\u00a0 \n            .\u00a0 (pretreatment) \n            .\u00a0 \n            .\u00a0 \n            .\u00a0 (1) \n            .\u00a0 (2) \n            .\u00a0 Panel A: Night sleep\u00a0 \u00a0 \u00a0 \u2003Hours in bed\u00a0 8.03\u00a0 7.68\u00a0 \u00a0 (0.97)\u00a0 (1.23)\u00a0 \u2003Hours asleep\u00a0 5.58\u00a0 5.46\u00a0 \u00a0 (0.87)\u00a0 (1.15)\u00a0 \u2003Sleep efficiency\u00a0 0.70\u00a0 0.71\u00a0 \u00a0 (0.08)\u00a0 (0.10)\u00a0 \u2003Number of awakenings\u00a0 31.95\u00a0 27.4\u00a0 \u00a0 (7.95)\u00a0 (10.14)\u00a0 \u2003Fraction sleeping less than 7\u00a0hours\u00a0 \u00a0 \u00a0 \u2003\u2003Participant-level\u00a0 0.95\u00a0 0.93\u00a0 \u00a0 (0.22)\u00a0 (0.26)\u00a0 \u2003\u2003Participant-day-level\u00a0 0.89\u00a0 0.87\u00a0 \u00a0 (0.31)\u00a0 (0.33)\u00a0 \u2003Fraction sleeping less than 6\u00a0hours\u00a0 \u00a0 \u00a0 \u2003\u2003Participant-level\u00a0 0.71\u00a0 0.69\u00a0 \u00a0 (0.46)\u00a0 (0.46)\u00a0 \u2003\u2003Participant-day-level\u00a0 0.65\u00a0 0.64\u00a0 \u00a0 (0.48)\u00a0 (0.48)\u00a0 \u2003Self-reported hours asleep\u00a0 7.20\u00a0 6.42\u00a0 \u00a0 (0.94)\u00a0 (1.49)\u00a0 Panel B: Nap sleep\u00a0 \u00a0 \u00a0 \u2003Percent napping on a given day\u00a0 N/A\u00a0 0.25\u00a0 \u00a0 \u00a0 (0.43)\u00a0 \u2003Duration of naps (conditional on napping)\u00a0 N/A\u00a0 0.85\u00a0 \u00a0 \u00a0 (0.61)\u00a0 Panel C: Total sleep\u00a0 \u00a0 \u00a0 \u2003Hours asleep\u00a0 5.58\u00a0 5.69\u00a0 \u00a0 (0.87)\u00a0 (1.15)\u00a0 Participant-nights\u00a0 3,080\u00a0 1,367\u00a0 Participants\u00a0 452\u00a0 439\u00a0 \n            .\u00a0 RCT sample \n            .\u00a0 Broader sample \n            .\u00a0 \n            .\u00a0 (pretreatment) \n            .\u00a0 \n            .\u00a0 \n            .\u00a0 (1) \n            .\u00a0 (2) \n            .\u00a0 Panel A: Night sleep\u00a0 \u00a0 \u00a0 \u2003Hours in bed\u00a0 8.03\u00a0 7.68\u00a0 \u00a0 (0.97)\u00a0 (1.23)\u00a0 \u2003Hours asleep\u00a0 5.58\u00a0 5.46\u00a0 \u00a0 (0.87)\u00a0 (1.15)\u00a0 \u2003Sleep efficiency\u00a0 0.70\u00a0 0.71\u00a0 \u00a0 (0.08)\u00a0 (0.10)\u00a0 \u2003Number of awakenings\u00a0 31.95\u00a0 27.4\u00a0 \u00a0 (7.95)\u00a0 (10.14)\u00a0 \u2003Fraction sleeping less than 7\u00a0hours\u00a0 \u00a0 \u00a0 \u2003\u2003Participant-level\u00a0 0.95\u00a0 0.93\u00a0 \u00a0 (0.22)\u00a0 (0.26)\u00a0 \u2003\u2003Participant-day-level\u00a0 0.89\u00a0 0.87\u00a0 \u00a0 (0.31)\u00a0 (0.33)\u00a0 \u2003Fraction sleeping less than 6\u00a0hours\u00a0 \u00a0 \u00a0 \u2003\u2003Participant-level\u00a0 0.71\u00a0 0.69\u00a0 \u00a0 (0.46)\u00a0 (0.46)\u00a0 \u2003\u2003Participant-day-level\u00a0 0.65\u00a0 0.64\u00a0 \u00a0 (0.48)\u00a0 (0.48)\u00a0 \u2003Self-reported hours asleep\u00a0 7.20\u00a0 6.42\u00a0 \u00a0 (0.94)\u00a0 (1.49)\u00a0 Panel B: Nap sleep\u00a0 \u00a0 \u00a0 \u2003Percent napping on a given day\u00a0 N/A\u00a0 0.25\u00a0 \u00a0 \u00a0 (0.43)\u00a0 \u2003Duration of naps (conditional on napping)\u00a0 N/A\u00a0 0.85\u00a0 \u00a0 \u00a0 (0.61)\u00a0 Panel C: Total sleep\u00a0 \u00a0 \u00a0 \u2003Hours asleep\u00a0 5.58\u00a0 5.69\u00a0 \u00a0 (0.87)\u00a0 (1.15)\u00a0 Participant-nights\u00a0 3,080\u00a0 1,367\u00a0 Participants\u00a0 452\u00a0 439\u00a0 TABLE I Sleep Statistics in Two Samples in Chennai   \n            .\u00a0 RCT sample \n            .\u00a0 Broader sample \n            .\u00a0 \n            .\u00a0 (pretreatment) \n            .\u00a0 \n            .\u00a0 \n            .\u00a0 (1) \n            .\u00a0 (2) \n            .\u00a0 Panel A: Night sleep\u00a0 \u00a0 \u00a0 \u2003Hours in bed\u00a0 8.03\u00a0 7.68\u00a0 \u00a0 (0.97)\u00a0 (1.23)\u00a0 \u2003Hours asleep\u00a0 5.58\u00a0 5.46\u00a0 \u00a0 (0.87)\u00a0 (1.15)\u00a0 \u2003Sleep efficiency\u00a0 0.70\u00a0 0.71\u00a0 \u00a0 (0.08)\u00a0 (0.10)\u00a0 \u2003Number of awakenings\u00a0 31.95\u00a0 27.4\u00a0 \u00a0 (7.95)\u00a0 (10.14)\u00a0 \u2003Fraction sleeping less than 7\u00a0hours\u00a0 \u00a0 \u00a0 \u2003\u2003Participant-level\u00a0 0.95\u00a0 0.93\u00a0 \u00a0 (0.22)\u00a0 (0.26)\u00a0 \u2003\u2003Participant-day-level\u00a0 0.89\u00a0 0.87\u00a0 \u00a0 (0.31)\u00a0 (0.33)\u00a0 \u2003Fraction sleeping less than 6\u00a0hours\u00a0 \u00a0 \u00a0 \u2003\u2003Participant-level\u00a0 0.71\u00a0 0.69\u00a0 \u00a0 (0.46)\u00a0 (0.46)\u00a0 \u2003\u2003Participant-day-level\u00a0 0.65\u00a0 0.64\u00a0 \u00a0 (0.48)\u00a0 (0.48)\u00a0 \u2003Self-reported hours asleep\u00a0 7.20\u00a0 6.42\u00a0 \u00a0 (0.94)\u00a0 (1.49)\u00a0 Panel B: Nap sleep\u00a0 \u00a0 \u00a0 \u2003Percent napping on a given day\u00a0 N/A\u00a0 0.25\u00a0 \u00a0 \u00a0 (0.43)\u00a0 \u2003Duration of naps (conditional on napping)\u00a0 N/A\u00a0 0.85\u00a0 \u00a0 \u00a0 (0.61)\u00a0 Panel C: Total sleep\u00a0 \u00a0 \u00a0 \u2003Hours asleep\u00a0 5.58\u00a0 5.69\u00a0 \u00a0 (0.87)\u00a0 (1.15)\u00a0 Participant-nights\u00a0 3,080\u00a0 1,367\u00a0 Participants\u00a0 452\u00a0 439\u00a0 \n            .\u00a0 RCT sample \n            .\u00a0 Broader sample \n            .\u00a0 \n            .\u00a0 (pretreatment) \n            .\u00a0 \n            .\u00a0 \n            .\u00a0 (1) \n            .\u00a0 (2) \n            .\u00a0 Panel A: Night sleep\u00a0 \u00a0 \u00a0 \u2003Hours in bed\u00a0 8.03\u00a0 7.68\u00a0 \u00a0 (0.97)\u00a0 (1.23)\u00a0 \u2003Hours asleep\u00a0 5.58\u00a0 5.46\u00a0 \u00a0 (0.87)\u00a0 (1.15)\u00a0 \u2003Sleep efficiency\u00a0 0.70\u00a0 0.71\u00a0 \u00a0 (0.08)\u00a0 (0.10)\u00a0 \u2003Number of awakenings\u00a0 31.95\u00a0 27.4\u00a0 \u00a0 (7.95)\u00a0 (10.14)\u00a0 \u2003Fraction sleeping less than 7\u00a0hours\u00a0 \u00a0 \u00a0 \u2003\u2003Participant-level\u00a0 0.95\u00a0 0.93\u00a0 \u00a0 (0.22)\u00a0 (0.26)\u00a0 \u2003\u2003Participant-day-level\u00a0 0.89\u00a0 0.87\u00a0 \u00a0 (0.31)\u00a0 (0.33)\u00a0 \u2003Fraction sleeping less than 6\u00a0hours\u00a0 \u00a0 \u00a0 \u2003\u2003Participant-level\u00a0 0.71\u00a0 0.69\u00a0 \u00a0 (0.46)\u00a0 (0.46)\u00a0 \u2003\u2003Participant-day-level\u00a0 0.65\u00a0 0.64\u00a0 \u00a0 (0.48)\u00a0 (0.48)\u00a0 \u2003Self-reported hours asleep\u00a0 7.20\u00a0 6.42\u00a0 \u00a0 (0.94)\u00a0 (1.49)\u00a0 Panel B: Nap sleep\u00a0 \u00a0 \u00a0 \u2003Percent napping on a given day\u00a0 N/A\u00a0 0.25\u00a0 \u00a0 \u00a0 (0.43)\u00a0 \u2003Duration of naps (conditional on napping)\u00a0 N/A\u00a0 0.85\u00a0 \u00a0 \u00a0 (0.61)\u00a0 Panel C: Total sleep\u00a0 \u00a0 \u00a0 \u2003Hours asleep\u00a0 5.58\u00a0 5.69\u00a0 \u00a0 (0.87)\u00a0 (1.15)\u00a0 Participant-nights\u00a0 3,080\u00a0 1,367\u00a0 Participants\u00a0 452\u00a0 439\u00a0   3. Sleep Efficiency Average sleep efficiency in our sample is 70% ( Figure\u00a0II , Panel E), far below recommended levels by sleep scientists who found that a minimum of 85% is needed to indicate \u201chigh-quality\u201d sleep ( Ohayon et\u00a0al. 2017 ). Like sleep duration, sleep efficiency is much lower than typically found in high-income countries, and instead resembles U.S.-based patients suffering from sleep disorders such as sleep apnea ( Roure et\u00a0al. 2008 ) or insomnia ( Trauer et\u00a0al. 2015 ). Sleep efficiency is low throughout the night, remaining around 70% between 1 to 5 a.m. (when almost everyone is in bed), consistent with interrupted sleep throughout the night ( Online Appendix Figure\u00a0A.I ). Participants experience about 32 awakenings on an average night ( Table\u00a0I , column (1)), again comparable to insomniacs in the United States ( Lichstein et\u00a0al. 2006 ). 8   4. Barriers to Sleep Why is sleep so inefficient? Survey responses highlight the importance of mental and physical distress (e.g., worries, stress, pain, or hunger) as well as environmental factors ( Online Appendix, Figure\u00a0A.IVa ). Over 50% of study participants indicate that their sleep is disrupted by heat, noise, and/or light, which the night sleep treatments described below were intended to address.   5. Napping Naps are relatively common in this population. Seventy-three percent of participants in our study reported taking at least one nap in the week before enrolling in the study. Conditional on napping, the median time reported for a nap is about one hour. The frequency and length of naps in U.S. populations is similar:  Dinges (1992)  finds that across a broad population of U.S. adults, 61% reported napping at least once a week with an average nap duration of 73 minutes, while in  Pilcher, Michalowski, and Carrigan (2001) , 74% of healthy adults report napping during a seven-day period.   6. Self-Reported Sleep Self-reports significantly overestimate time asleep, relative to the objective actigraph measures, consistent with findings in the United States ( Lauderdale et\u00a0al. 2008 ;  Avery, Giuntella, and Jiao 2019 ). 9  Average baseline self-reported sleep duration in our study is 7.2\u00a0hours ( Figure\u00a0II , Panel D), quite similar to the average of 7.1\u00a0hours found in a representative survey of older adults in rural India described in  Gildner et\u00a0al. (2014) . In comparison, average self-reported sleep duration in the United States ranges from 6.8 to 7.9\u00a0hours a night ( Lauderdale et\u00a0al. 2008 ;  Watson et\u00a0al. 2015 ;  Jackson et\u00a0al. 2018 ).   7. Broader Population To investigate the representativeness of our RCT sample, we conducted a supplementary sleep survey with 3,833 individuals across randomly sampled neighborhoods in Chennai. 10  A subset ( N  = 439) completed three nights of actigraph measurements. Despite not using any of the RCT screening criteria for this survey, the nighttime sleep duration and efficiency in this broader sample are similar to that of RCT participants, with an average of 5.5\u00a0hours of sleep a night and 71% sleep efficiency ( Table\u00a0I , column (2)). As in the RCT sample, napping is common, with 25% of participants napping on any given day and an average duration of those naps of roughly 50 minutes.   III. Experimental Design We designed our experiment with three broad goals in mind. First, we aimed to estimate the effects of increased sleep over a few weeks in relatively natural sleep environments (as opposed to depriving people of sleep in lab settings). Second, we wanted to precisely measure labor supply, productivity, and earnings, and thus we employed study participants full-time in a realistic but closely controlled data entry job. Finally, to provide a broad view of the effects of increased sleep, we collected a range of additional outcomes, including cognition, preferences, and well-being. Figure\u00a0III  provides an overview of the experimental design and timeline of the study. Four hundred fifty-two participants worked for 28\u00a0days in an office in Chennai, spending most of their workdays doing paid data entry work. Enrollment took place on a rolling basis. The office contained computer work stations for data entry, a break room, booths for surveys and experimental tasks, and nap stations on a separate floor.   Figure\u00a0III RCT Timeline and Experimental Design This figure presents an overview of the timeline and experimental design of the study. After eight baseline days, the 452 participants were divided into three night sleep treatment groups: control, sleep devices + encouragement, and sleep devices + incentives. Participants in each of these groups were further randomized into a nap group, which was allowed and encouraged to use a nap station in the office in the early afternoon, and a no-nap group. Participants in the no-nap group were further randomized on a daily level (i.e., within individual) either to being allowed to work during the nap period or to take a mandatory pause during that time, with equal probability. The nap treatment ends at day 27, and the participants return the sleep devices on day 28. Endline surveys occur on day 28 or shortly thereafter. Figure\u00a0III RCT Timeline and Experimental Design This figure presents an overview of the timeline and experimental design of the study. After eight baseline days, the 452 participants were divided into three night sleep treatment groups: control, sleep devices + encouragement, and sleep devices + incentives. Participants in each of these groups were further randomized into a nap group, which was allowed and encouraged to use a nap station in the office in the early afternoon, and a no-nap group. Participants in the no-nap group were further randomized on a daily level (i.e., within individual) either to being allowed to work during the nap period or to take a mandatory pause during that time, with equal probability. The nap treatment ends at day 27, and the participants return the sleep devices on day 28. Endline surveys occur on day 28 or shortly thereafter.   III.A. Interventions to Increase Sleep For their first eight days in the study, participants remained in a control condition, allowing us to collect rich baseline data. Then, we cross-randomized participants to two night sleep treatments and a nap treatment, stratified by baseline sleep and earnings.   1. Night Sleep Treatments Each participant was randomly assigned to one of two night sleep treatment groups (encouragement or incentives) or to a control group in equal proportions. Devices + Encouragement:  This treatment involved a bundled intervention to increase night sleep. Individuals were offered (a) information about the benefits of sleep (in particular, generic health benefits) and tips to improve their sleep (such as going to bed at the same time every day, avoiding caffeine after 4 p.m., and avoiding screens before bed), (b) encouragement to increase their sleep as well as daily feedback on their night sleep duration as measured by the actigraph, and (c) loaned devices to improve their sleep environment. The offered devices included eye shades, earplugs, a cot, a mattress, sheets, pillows, and a fan (see  Online Appendix Figure\u00a0A.IIb ). 11 Devices + Incentives:  This group received the same bundled intervention as the Devices + Encouragement group plus financial incentives to increase their actigraph-measured sleep during the treatment period. Each day, participants were paid Rs. 1 per minute of increased sleep for up to two hours of increased sleep (Rs. 120, about $1.70), relative to their baseline period sleep. There was no penalty for sleeping less than in the baseline period. 12  To control for any income effects due to the sleep incentive payments, participants in the control and encouragement groups were randomly and anonymously matched to participants in the incentives group and received the same stream of payments, independent of their own sleep. Control:  This group did not receive any of the above treatments. To deal with the concern that loaning items might generate reciprocity effects or affect reported well-being directly, we offered placebo household goods, unrelated to sleep, to a subset of control participants. The total value of these goods was roughly the same as that of the sleep devices and included items such as small kitchen devices, a chair, decorative figurines, and a flashlight. These goods were also returned at the end of the study. 13 Given the difficulty of increasing sleep in the field, we took a bundled approach in designing our treatments, working to increase sleep through multiple channels. Participants could respond to the encouragement and incentives by spending more time in bed or by taking steps to increase their sleep efficiency. The tips to improve sleep, such as avoiding caffeine in the evening, turning off the television, or putting away one\u2019s cellphone at night, could plausibly increase sleep efficiency. Finally, the loaned devices could increase sleep efficiency and time in bed, if the devices made it easier to fall asleep or reduced awakenings or if they made time spent in bed more enjoyable.   2. Nap Treatment Motivated by existing lab evidence that naps can be effective in boosting cognitive function ( Lovato and Lack 2010 ) and can make up for limited nighttime sleep ( Mollicone et\u00a0al. 2008 ), we cross-randomized the night sleep treatments with a nap intervention. Starting on day 9 of the study, a random subset of individuals were given the opportunity to take a short afternoon nap every day between 1:30 and 2 p.m. Located in a quiet and gender-separated part of the study office, the 25 private nap spaces included a bed, blanket, pillow, table fan, ear plugs, and eye shades (see  Online Appendix Figure\u00a0A.IIc ). The actigraphs show that roughly 90% of study participants did sleep during their allotted nap time. Those who did not want to nap were asked to sit quietly or rest in their nap area; they did not have the option to work during this time. The remaining (no-nap) participants were randomized each day with equal probability to either a work day, on which we allowed them to work through the nap period, or a break day, on which we enforced a half-hour break from data entry during the same period. Break day participants were allowed to engage in any leisure activity they chose, including sitting in a comfortable office break room. By comparing nap and break participants, we isolate the effect of a nap relative to a break of the same length. By instead comparing the nap and work participants, we can estimate the net effect of naps on work output, including the lost work time.   III.B. Outcome Measures Sleep and work are the two key sets of outcomes of this study. We measured each of them daily using actigraphs and the data entry platform, respectively. Study participants also completed a series of short surveys and experimental tasks throughout the study (see  Online Appendix Table\u00a0A.III ). Described in greater detail in  Online Appendix C , these measures fall into three broad categories: (i) physical and psychological well-being, (ii) cognition, and (iii) economic preferences.   1. Measures of Sleep We measure night and nap sleep\u2014sleep duration, time in bed, efficiency, and interruptions\u2014using actigraphs, as described in  Section II.A . Ninety-four percent of participants wore their actigraph on any given day, balanced across treatments. We complement these measures with daily self-reports of time in bed, time asleep, and number of awakenings during the night.   2. Work-Related Outcomes Participants were engaged each day in data entry work. We designed a software interface that presented participants with images containing alphanumeric text and asked them to transcribe the data by typing into text boxes (see  Online Appendix Figure\u00a0A.III ). The task was designed to mimic a real-world data entry job. 14  Participants were paid for time spent typing as well as the amount of data entered, as described below. This design allows us to precisely measure labor supply, productivity, and earnings. Labor Supply.  Our preregistered measure of labor supply is the active typing time as automatically measured by the data entry software. As in many workplaces, participants were not forced to arrive or leave the office at precise times. On most days, participants could arrive or depart from the office as they chose between 9:30 a.m. and 8 p.m. When in the office, participants could take breaks from work. We can precisely measure even short breaks: if a participant spent two consecutive minutes without typing, the software automatically paused and the break period did not count toward the labor-supply measure. Thus, participants had a fair amount of control over their labor supply, except for time slots set aside for surveys, experimental tasks, and the lunch break. Earnings.  Earnings in the data entry work is our preregistered measure of performance at work and was used as our summary measure of work because it combines labor supply and productivity. It has two components: an \u201cattendance pay\u201d per hour of active typing (one-third of work earnings) and a \u201cperformance pay,\u201d a piece rate for each correct character and a penalty per mistake (two-thirds of data entry earnings). Each half hour, piece rates were randomly varied between a low value (Rs. 5 per 1,000 correct characters) and high value (four times as large) with equal probability. The penalty rate remained constant throughout at Rs. 1 per 10 mistakes. The variation in piece rates allows us to benchmark any productivity effects of the sleep treatments against monetary incentives. The participants were paid daily, just before leaving the office for the day. 15 Productivity . Our preregistered measure of worker productivity is output divided by active typing hours. Output is the number of correct entries minus (a weighted) number of mistakes. The weight on mistakes was defined as the ratio of the average piece rate and the penalty rate.   3. Well-Being We collected a wide range of measures of psychological and physical well-being. As preregistered, we examine these variables both as indices and individually. The preregistered measures of mental well-being are happiness, sense of life possibilities (Cantril Scale), life satisfaction, stress, and depression. The measures of physical well-being are performance in a stationary biking task, reported days of illness, self-reported pain, activities of daily living, and blood pressure. 16   4. Cognition Sleep scientists have documented a strong relationship between sleep and cognition in numerous laboratory studies in rich countries  (Lim and Dinges 2010 ;  Killgore 2010 ). We collected (i) laboratory measures of cognitive function borrowed from cognitive psychology and sleep medicine; and (ii) a measure of attention to incentives at work embedded in the data entry task. Lab Measures of Cognition.  Each afternoon, participants completed the Psychomotor Vigilance Task (PVT), a standard measure of alertness and attention used in sleep medicine ( Basner and Dinges 2011 ). Every other day, they completed cognitive tasks measuring memory (Corsi blocks task) and inhibitory control (Hearts and Flowers task), described in detail in  Dean, Schilbach, and Schofield (2019)  and briefly in  Online Appendix C.5 . All cognitive tasks were incentivized for performance (e.g., speed, accuracy). Attention to Work Incentives.  To test whether sleep affects the ability to attend to important aspects of one\u2019s work environment, for example, the incentives faced, we randomized the visual salience of piece rates across days starting on day 6 of the baseline period. In the salient condition, the current piece rate was highlighted in different colors for each rate and displayed on the screen at all times. We consider this condition the \u201cfull-attention\u201d benchmark, as in  Chetty, Looney, and Kroft (2009) . In the nonsalient condition, noticing and remembering the piece rate was more challenging. A single muted color was used for both piece rates, and (in the second half of the study) the rate was only visible for the first 15 seconds of a half-hour slot, fading out slowly.  Online Appendix Figure\u00a0A.III  provides screenshots of each condition described below. The participant-level attention measure is the difference in average response to piece rate incentives in the full-attention benchmark and in the nonsalient condition. 17   5. Preferences Sleep may affect preferences through its impact on cognition or directly. For instance, sleep has been hypothesized to play a critical role in replenishing self-control ( Vohs and Baumeister, 2016 ) and sleep deprivation has been correlated with cyberloafing at work ( Wagner et\u00a0al. 2012 ) and cheating ( Barnes et\u00a0al. 2011 ). Similarly, sleep could alter the weight placed on sure things versus gambles or on others versus the self ( McKenna et\u00a0al. 2007 ;  Anderson and Dickinson 2010 ;  Holbein, Schafer, and Dickinson 2019 ). To examine such effects we study time preference via financial savings outcomes and choices on a real-effort task, and risk and social preferences via standard experimental economics measures described below. Savings.  We measured savings behavior by providing participants an opportunity to save money in a lockbox at the study office, as in  Schilbach (2019) . At the end of each work day, after receiving their earnings, individuals had the opportunity to deposit or withdraw money. Participants were randomly assigned to receive daily interest rates between 0% and 2% for any money saved in the box. 18  For participants receiving the positive interest rate, at least, the savings account we offered was quite lucrative. The deposits made in this account constitute our main savings outcome. Effort Discounting.  We measured present bias using real-effort choices, following  Augenblick and Rabin (2019) . Participants made decisions about how many pages to type at the end of the day on a particular date under different piece rates. Using choices elicited both in advance and on the day of the work itself, we structurally estimate an individual-level present bias parameter \u03b2 i , once each in the baseline and treatment periods. A complete description of the task is in  Online Appendix C.6.3 . Social and Risk Preferences.  We measured risk and social preferences via standard tasks in the behavioral economics literature. Risk aversion and loss aversion are captured via a multiple price list elicitation similar to those in  Holt and Laury (2002) , and  Charness, Gneezy, and Imas (2013) . Social preferences are measured via dictator, ultimatum, and trust games ( Camerer 2003 ).   6. Realism and External Validity Conducting the study in the context of a month-long data entry job in a controlled environment follows  Kaur, Kremer, and Mullainathan (2015)  and allows for the provision of afternoon naps and precise measurement of the outcomes described above. However, it also comes with some costs. First, the work environment has some unusual and artificial features, such as regular surveys and laboratory measures of cognition and preferences. Second, any labor supply responses we find might be muted in environments where employers more strictly control schedules. In practice, participants tend to spend about 8\u00a0hours at the office each day, with an average arrival time of 10:32 a.m. (std. dev. = 43 minutes) and average departure time of 6:20 p.m. (std. dev. = 57 minutes). This is quite similar to other jobs in our context, with long commutes and unreliable transportation, such that arriving strictly at a given time is difficult.   III.C. Expert Predictions To quantify how our results compare with existing scientific understanding, we conducted surveys of experts in sleep science and economics to elicit their prior beliefs about the treatment effects of the night sleep interventions included in this study ( DellaVigna, Pope, and Vivalt 2019 ). Participation in the survey was solicited via emails to experts in each field. The survey provided information on the design of the study, the magnitude of the increase in night sleep reported in  Section IV.B , and the outcome measures described above. 19  Three versions of the survey were tailored to different respondents: development and labor economists; behavioral economists; and sleep medicine experts. A total of 28 labor and development economists, 19 behavioral economists, and 77 sleep medicine experts responded to the survey. In an effort to keep the survey short, we did not elicit predictions about the effects of the nap treatment. All experts were provided with relevant benchmarks (e.g., the elasticity of effort with respect to the piece rate) and made predictions on labor supply and work output effects. Both types of economists responded with their beliefs about savings. Only behavioral economists were asked to predict changes in present bias, and only sleep experts were asked to predict changes in sustained attention and physical health. The expert predictions are shown in  Figure\u00a0IV  and in  Online Appendix Table\u00a0A.IV  and are discussed when presenting results. Further details are provided in  Online Appendix C.1 .   Figure\u00a0IV Expert Predictions versus RCT Results This figure summarizes the predictions made by experts in economics and sleep science about the expected treatment effect of our (pooled) night sleep interventions. We normalize each prediction, dividing them by the control group\u2019s standard deviation. For comparison, we also present the actual estimated treatment effect. The bars show the interquartile range (25th to 75th percentiles) of the predictions for a given outcome variable. We also show the median prediction (X) and the actual point estimates (diamond) of the treatment effects.  N  refers to the number of responses for each outcome variable. This number varies by outcome because different types of experts (e.g., sleep researchers, behavioral economists) were asked about some different outcomes.  Correct Entries  refers to the number of daily correct characters in the data entry task, that is, a measure of overall output per day.  Labor Supply  refers to the daily number of hours working in the typing task, that is, time at the office excluding voluntary and scheduled pauses.  Physical Health  refers to a variable that averages (normalized) systolic and diastolic blood pressure. We flip the sign of this variable so a positive value means an improvement in health (i.e., a reduction in blood pressure).  Attention  refers to an index pooling inverse response times and minor lapses in the Psychomotor Vigilance Task, a standard lab measure of attention in the sleep literature. Signs are flipped such that higher values refer to increased attention.  Savings  refers to the daily amount deposited minus the amount withdrawn in the savings box during the experiment.  Present Bias  refers to the present-bias parameter \u03b2. Unlike the other variables, the predictions and point estimate refer to the reduction in present bias (increase in \u03b2) rather than a normalized outcome, for ease of interpretation. Figure\u00a0IV Expert Predictions versus RCT Results This figure summarizes the predictions made by experts in economics and sleep science about the expected treatment effect of our (pooled) night sleep interventions. We normalize each prediction, dividing them by the control group\u2019s standard deviation. For comparison, we also present the actual estimated treatment effect. The bars show the interquartile range (25th to 75th percentiles) of the predictions for a given outcome variable. We also show the median prediction (X) and the actual point estimates (diamond) of the treatment effects.  N  refers to the number of responses for each outcome variable. This number varies by outcome because different types of experts (e.g., sleep researchers, behavioral economists) were asked about some different outcomes.  Correct Entries  refers to the number of daily correct characters in the data entry task, that is, a measure of overall output per day.  Labor Supply  refers to the daily number of hours working in the typing task, that is, time at the office excluding voluntary and scheduled pauses.  Physical Health  refers to a variable that averages (normalized) systolic and diastolic blood pressure. We flip the sign of this variable so a positive value means an improvement in health (i.e., a reduction in blood pressure).  Attention  refers to an index pooling inverse response times and minor lapses in the Psychomotor Vigilance Task, a standard lab measure of attention in the sleep literature. Signs are flipped such that higher values refer to increased attention.  Savings  refers to the daily amount deposited minus the amount withdrawn in the savings box during the experiment.  Present Bias  refers to the present-bias parameter \u03b2. Unlike the other variables, the predictions and point estimate refer to the reduction in present bias (increase in \u03b2) rather than a normalized outcome, for ease of interpretation.   III.D. Study Population and Balance Checks We followed two strategies to recruit our study sample. First, recruiters went to low-income neighborhoods in Chennai and spread information about the study, advertising a one-month data entry job. Second, past participants could refer potential new participants to the study. In both cases, recruiters interviewed individuals to determine their eligibility to participate in the study.   1. Eligibility Criteria and Selection Interested individuals participated in a two-stage screening process, involving a brief unpaid survey and a home visit to check whether the individual met the study\u2019s eligibility criteria: (i) being 25 to 55\u00a0years old; (ii) fluency in the local language (Tamil) and the ability to read and write numbers; (iii) having worked fewer than five days per week and earning an average of Rs. 700 ($10) or less per day worked in the previous month; (iv) living in a dwelling able to accommodate the sleep devices used in night sleep treatments and ownership of three or fewer of the sleep devices being offered in the study; (v) the intention to be in Chennai for the following five weeks; and (vi) no children in the household younger than three years. Importantly, this recruitment and screening procedure does not seem to select participants on average levels of sleep quantity and efficiency. In  Table\u00a0I , we find very similar patterns of sleep among individuals in Chennai in the broader sleep survey, as described in  Online Appendix F .   2. Informed Consent All participants went through a detailed informed-consent procedure including information about the work task, other experimental measures and surveys, the actigraphs and the randomized treatments. The specific research hypotheses were not shared with participants to avoid demand effects. Instead, the goal of the research was described as work to understand the \u201cdifficulties underprivileged people in India face, and how these problems affect their lives.\u201d   3. Sample Characteristics Online Appendix Table\u00a0A.I  shows sample characteristics. A typical study participant was about 35\u00a0years old with one or two children and 10\u00a0years of education. Two-thirds of study participants were women. Although only 30% of participants had prior computer experience, participants were eager to learn and improved rapidly in their data entry speed during the baseline period.   4. Balance Checks We test for baseline imbalances in demographics and baseline measures of outcome variables across the experimental conditions in  Online Appendix Tables A.I  and  A.II . Whether we separately consider each treatment cell ( Online Appendix Table\u00a0A.I ) or compare the pooled night sleep treatment groups with the control and the nap and no-nap groups ( Online Appendix Table\u00a0A.II ), the treatment groups were well-balanced across key characteristics. For each treatment arm, a joint  F -test comparing it to the control group indicated no systematic differences on observable characteristics across groups. As is expected given the large number of comparisons, a few statistically significant differences across treatment groups did emerge. Most notably among those, participants in the night sleep treatment groups were about a year younger than those in the control group, and baseline productivity and earnings were about 3%\u20134% lower in the nap group than in the no-nap groups ( Online Appendix Table\u00a0A.II ). We control for age and for the participant-level baseline average of the outcome variables, so these imbalances should not affect our results. 20   IV. Experimental Results IV.A. Empirical Framework Most of our empirical analyses, including all work-related outcomes, estimate treatment effects on outcomes measured at the participant-day level using variants of the following equation:  $$\\begin{equation}\ny_{itd}= \\beta {\\bf T}_{i} + \\gamma _{1}\\bar{y}_{i}^B + \\gamma _{2}{\\bf X}_{it} + \\delta _t + \\lambda _d + \\varepsilon _{itd},\n\\end{equation}$$ (1) where  y itd  is the relevant outcome for participant  i  on their  t th day in the study on calendar date  d .  T i  is a vector of indicator variables capturing the treatment(s) that participant  i  was assigned to. \u03b2 is the vector of coefficients, capturing the effect of each treatment on the outcome of interest. Following  McKenzie (2012) , we control for the average baseline value of the outcome variable  |$\\bar{y}_{i}^B$|  in all specifications, and drop the baseline days from the regression. We also drop days on which participants were absent, since attendance was balanced across groups.  X it  includes controls for participants\u2019 age (quartiles) and gender and, where specified, a dummy variable for whether a given no-nap participant  i  was assigned to work through the nap period or instead to take an enforced break on day  t . This allows us to estimate the effect of naps separately compared with working and taking a break. 21  Finally, we include day-in-study and calendar date fixed effects, captured by \u03b4 t  and \u03bb d , respectively. All standard errors are clustered at the participant level. For some outcomes, such as preferences, we only have one observation in the baseline and one in the treatment period per participant. In those cases, we run participant-level regressions:  $$\\begin{equation}\ny_{i}= \\beta {\\bf T}_{i} + \\gamma _{1}y_{i}^B + \\gamma _{2} {\\bf X}_{i} + \\varepsilon _{i},\n\\end{equation}$$ (2) where again the outcome variable only uses the observations from the treatment period and we control for the baseline observation of the outcome,  |$y_{i}^B$|\u2060 . The vector  X i  includes the same gender and age controls. Because these outcome measurements span multiple days (e.g., present bias) or are on a fixed day-in-study (e.g., the endline survey), this specification does not include day-in-study or calendar-date fixed effects or control for whether no-nap participants worked or took a break.   1. Combining Outcomes into Indices Given the large number of outcomes, we divide them into four major families: work, well-being, cognition, and preferences. We construct a single summary outcome for each family. The work outcomes are naturally summarized by (standardized) earnings in the data entry task, which combines productivity and labor supply into a single quantity. For the other families, we create standardized index variables by residualizing each constituent outcome with respect to day in study and calendar date, standardizing it by the control group\u2019s mean and standard deviation, and then taking a weighted average to form the index. Following  Anderson (2008) , the weights are the inverse of the covariance matrix of the residualized, standardized outcomes. This ensures that outcomes that are highly correlated receive less weight than outcomes that capture independent information. Signs of outcome variables are flipped when necessary, so positive treatment effects imply an improvement in the outcome. 22  We also report treatment effects on an overall index, which combines the four family-level summary outcomes into a single variable. We use the same procedure to create the overall index.   2. Multiple-Hypothesis Testing We report three approaches to dealing with multiple-hypothesis testing issues caused by observing many outcomes. Our simplest approach is to examine a single overall index variable which combines all outcomes, as described above. Our intermediate approach is to consider outcomes at the level of the four families, using one summary variable for each family, while correcting for the existence of multiple families. Finally, at the level of the individual outcomes, we correct for the existence of multiple outcomes within each family. In each case, we report adjusted  p -values that control the family-wise error rate\u2014the probability of at least one false rejection\u2014using a step-down permutation procedure based on  Westfall and Young (1993) . The adjusted  p -values are displayed for the main results in  Table\u00a0IV  and  Online Appendix Tables A.VII, A.VIII , and  A.IX . 23   3. Pre-Analysis Plan This study was preregistered on the AEA Registry and ClinicalTrials.gov, including a pre analysis plan (PAP). We deviate from the PAP in some instances. The main deviations (in our view) are the following. First, we prespecified a regression model that included all interactions of treatments. We soon came to realize we were not well-powered for this analysis and that it would lead to a large number of coefficients and comparisons that would be difficult to present and interpret. We still present the prespecified estimates in  Tables II  and  III  but complement them with a simplified but higher-powered specification that pools the two night sleep treatments and omits the interactions between nap and night sleep treatments in  Table\u00a0IV . Second, we had not fully specified our approach to multiple-hypothesis testing and made some changes after receiving comments and discussing with experts. We added the overall index variable to parsimoniously aggregate all outcomes. We also redefined the four families of outcomes (work, well-being, cognition, and preferences rather than work and decision making) and created a summary variable for each family. Other deviations are detailed in  Online Appendix  Section D.   TABLE II Treatment Effects on Sleep   \n            .\u00a0 Night sleep \n            .\u00a0 Time in bed \n            .\u00a0 Sleep efficiency \n            .\u00a0 Nap sleep \n            .\u00a0 24-Hr sleep \n            .\u00a0 \n            .\u00a0 (1) \n            .\u00a0 (2) \n            .\u00a0 (3) \n            .\u00a0 (4) \n            .\u00a0 (5) \n            .\u00a0 Devices+Encouragement only\u00a0 0.32***\u00a0 0.53***\u00a0 \u22120.54\u00a0 0.00\u00a0 0.33***\u00a0 \u00a0 (0.08)\u00a0 (0.09)\u00a0 (0.66)\u00a0 (0.00)\u00a0 (0.08)\u00a0 Devices+Incentives only\u00a0 0.49***\u00a0 0.84***\u00a0 \u22121.14\u00a0 0.00\u00a0 0.49***\u00a0 \u00a0 (0.08)\u00a0 (0.09)\u00a0 (0.70)\u00a0 (0.00)\u00a0 (0.08)\u00a0 Nap only\u00a0 \u22120.13*\u00a0 \u22120.11\u00a0 \u22120.72\u00a0 0.25***\u00a0 0.09\u00a0 \u00a0 (0.08)\u00a0 (0.09)\u00a0 (0.72)\u00a0 (0.01)\u00a0 (0.08)\u00a0 Devices+Encouragement and nap\u00a0 0.21***\u00a0 0.40***\u00a0 \u22121.06\u00a0 0.24***\u00a0 0.42***\u00a0 \u00a0 (0.07)\u00a0 (0.08)\u00a0 (0.66)\u00a0 (0.01)\u00a0 (0.08)\u00a0 Devices+Incentives and nap\u00a0 0.48***\u00a0 0.58***\u00a0 0.90\u00a0 0.24***\u00a0 0.70***\u00a0 \u00a0 (0.08)\u00a0 (0.09)\u00a0 (0.68)\u00a0 (0.01)\u00a0 (0.08)\u00a0 Control mean\u00a0 5.61\u00a0 8.07\u00a0 69.86\u00a0 0.00\u00a0 5.61\u00a0 Control std. dev.\u00a0 1.20\u00a0 1.37\u00a0 11.28\u00a0 0.00\u00a0 1.20\u00a0 Participant-nights\u00a0 8,454\u00a0 8,454\u00a0 8,454\u00a0 7,191\u00a0 8,035\u00a0 Participants\u00a0 451\u00a0 451\u00a0 451\u00a0 450\u00a0 451\u00a0 \n            .\u00a0 Night sleep \n            .\u00a0 Time in bed \n            .\u00a0 Sleep efficiency \n            .\u00a0 Nap sleep \n            .\u00a0 24-Hr sleep \n            .\u00a0 \n            .\u00a0 (1) \n            .\u00a0 (2) \n            .\u00a0 (3) \n            .\u00a0 (4) \n            .\u00a0 (5) \n            .\u00a0 Devices+Encouragement only\u00a0 0.32***\u00a0 0.53***\u00a0 \u22120.54\u00a0 0.00\u00a0 0.33***\u00a0 \u00a0 (0.08)\u00a0 (0.09)\u00a0 (0.66)\u00a0 (0.00)\u00a0 (0.08)\u00a0 Devices+Incentives only\u00a0 0.49***\u00a0 0.84***\u00a0 \u22121.14\u00a0 0.00\u00a0 0.49***\u00a0 \u00a0 (0.08)\u00a0 (0.09)\u00a0 (0.70)\u00a0 (0.00)\u00a0 (0.08)\u00a0 Nap only\u00a0 \u22120.13*\u00a0 \u22120.11\u00a0 \u22120.72\u00a0 0.25***\u00a0 0.09\u00a0 \u00a0 (0.08)\u00a0 (0.09)\u00a0 (0.72)\u00a0 (0.01)\u00a0 (0.08)\u00a0 Devices+Encouragement and nap\u00a0 0.21***\u00a0 0.40***\u00a0 \u22121.06\u00a0 0.24***\u00a0 0.42***\u00a0 \u00a0 (0.07)\u00a0 (0.08)\u00a0 (0.66)\u00a0 (0.01)\u00a0 (0.08)\u00a0 Devices+Incentives and nap\u00a0 0.48***\u00a0 0.58***\u00a0 0.90\u00a0 0.24***\u00a0 0.70***\u00a0 \u00a0 (0.08)\u00a0 (0.09)\u00a0 (0.68)\u00a0 (0.01)\u00a0 (0.08)\u00a0 Control mean\u00a0 5.61\u00a0 8.07\u00a0 69.86\u00a0 0.00\u00a0 5.61\u00a0 Control std. dev.\u00a0 1.20\u00a0 1.37\u00a0 11.28\u00a0 0.00\u00a0 1.20\u00a0 Participant-nights\u00a0 8,454\u00a0 8,454\u00a0 8,454\u00a0 7,191\u00a0 8,035\u00a0 Participants\u00a0 451\u00a0 451\u00a0 451\u00a0 450\u00a0 451\u00a0 TABLE II Treatment Effects on Sleep   \n            .\u00a0 Night sleep \n            .\u00a0 Time in bed \n            .\u00a0 Sleep efficiency \n            .\u00a0 Nap sleep \n            .\u00a0 24-Hr sleep \n            .\u00a0 \n            .\u00a0 (1) \n            .\u00a0 (2) \n            .\u00a0 (3) \n            .\u00a0 (4) \n            .\u00a0 (5) \n            .\u00a0 Devices+Encouragement only\u00a0 0.32***\u00a0 0.53***\u00a0 \u22120.54\u00a0 0.00\u00a0 0.33***\u00a0 \u00a0 (0.08)\u00a0 (0.09)\u00a0 (0.66)\u00a0 (0.00)\u00a0 (0.08)\u00a0 Devices+Incentives only\u00a0 0.49***\u00a0 0.84***\u00a0 \u22121.14\u00a0 0.00\u00a0 0.49***\u00a0 \u00a0 (0.08)\u00a0 (0.09)\u00a0 (0.70)\u00a0 (0.00)\u00a0 (0.08)\u00a0 Nap only\u00a0 \u22120.13*\u00a0 \u22120.11\u00a0 \u22120.72\u00a0 0.25***\u00a0 0.09\u00a0 \u00a0 (0.08)\u00a0 (0.09)\u00a0 (0.72)\u00a0 (0.01)\u00a0 (0.08)\u00a0 Devices+Encouragement and nap\u00a0 0.21***\u00a0 0.40***\u00a0 \u22121.06\u00a0 0.24***\u00a0 0.42***\u00a0 \u00a0 (0.07)\u00a0 (0.08)\u00a0 (0.66)\u00a0 (0.01)\u00a0 (0.08)\u00a0 Devices+Incentives and nap\u00a0 0.48***\u00a0 0.58***\u00a0 0.90\u00a0 0.24***\u00a0 0.70***\u00a0 \u00a0 (0.08)\u00a0 (0.09)\u00a0 (0.68)\u00a0 (0.01)\u00a0 (0.08)\u00a0 Control mean\u00a0 5.61\u00a0 8.07\u00a0 69.86\u00a0 0.00\u00a0 5.61\u00a0 Control std. dev.\u00a0 1.20\u00a0 1.37\u00a0 11.28\u00a0 0.00\u00a0 1.20\u00a0 Participant-nights\u00a0 8,454\u00a0 8,454\u00a0 8,454\u00a0 7,191\u00a0 8,035\u00a0 Participants\u00a0 451\u00a0 451\u00a0 451\u00a0 450\u00a0 451\u00a0 \n            .\u00a0 Night sleep \n            .\u00a0 Time in bed \n            .\u00a0 Sleep efficiency \n            .\u00a0 Nap sleep \n            .\u00a0 24-Hr sleep \n            .\u00a0 \n            .\u00a0 (1) \n            .\u00a0 (2) \n            .\u00a0 (3) \n            .\u00a0 (4) \n            .\u00a0 (5) \n            .\u00a0 Devices+Encouragement only\u00a0 0.32***\u00a0 0.53***\u00a0 \u22120.54\u00a0 0.00\u00a0 0.33***\u00a0 \u00a0 (0.08)\u00a0 (0.09)\u00a0 (0.66)\u00a0 (0.00)\u00a0 (0.08)\u00a0 Devices+Incentives only\u00a0 0.49***\u00a0 0.84***\u00a0 \u22121.14\u00a0 0.00\u00a0 0.49***\u00a0 \u00a0 (0.08)\u00a0 (0.09)\u00a0 (0.70)\u00a0 (0.00)\u00a0 (0.08)\u00a0 Nap only\u00a0 \u22120.13*\u00a0 \u22120.11\u00a0 \u22120.72\u00a0 0.25***\u00a0 0.09\u00a0 \u00a0 (0.08)\u00a0 (0.09)\u00a0 (0.72)\u00a0 (0.01)\u00a0 (0.08)\u00a0 Devices+Encouragement and nap\u00a0 0.21***\u00a0 0.40***\u00a0 \u22121.06\u00a0 0.24***\u00a0 0.42***\u00a0 \u00a0 (0.07)\u00a0 (0.08)\u00a0 (0.66)\u00a0 (0.01)\u00a0 (0.08)\u00a0 Devices+Incentives and nap\u00a0 0.48***\u00a0 0.58***\u00a0 0.90\u00a0 0.24***\u00a0 0.70***\u00a0 \u00a0 (0.08)\u00a0 (0.09)\u00a0 (0.68)\u00a0 (0.01)\u00a0 (0.08)\u00a0 Control mean\u00a0 5.61\u00a0 8.07\u00a0 69.86\u00a0 0.00\u00a0 5.61\u00a0 Control std. dev.\u00a0 1.20\u00a0 1.37\u00a0 11.28\u00a0 0.00\u00a0 1.20\u00a0 Participant-nights\u00a0 8,454\u00a0 8,454\u00a0 8,454\u00a0 7,191\u00a0 8,035\u00a0 Participants\u00a0 451\u00a0 451\u00a0 451\u00a0 450\u00a0 451\u00a0   TABLE III Fully Disaggregated Treatment Effects of Night Sleep and Nap Treatments   \n            .\u00a0 Overall \n            .\u00a0 Work \n            .\u00a0 Well-being \n            .\u00a0 \n            .\u00a0 Index \n            .\u00a0 Earnings \n            .\u00a0 Productivity \n            .\u00a0 Labor supply \n            .\u00a0 Output \n            .\u00a0 Index \n            .\u00a0 Physical \n            .\u00a0 Psychological \n            .\u00a0 \n            .\u00a0 (1) \n            .\u00a0 (2) \n            .\u00a0 (3) \n            .\u00a0 (4) \n            .\u00a0 (5) \n            .\u00a0 (6) \n            .\u00a0 (7) \n            .\u00a0 (8) \n            .\u00a0 Devices+Encouragement only\u00a0 0.00\u00a0 \u22120.07**\u00a0 \u22120.02\u00a0 \u22120.06*\u00a0 \u22120.07**\u00a0 0.13***\u00a0 0.16**\u00a0 0.02\u00a0 \u00a0 (0.07)\u00a0 (0.03)\u00a0 (0.03)\u00a0 (0.04)\u00a0 (0.03)\u00a0 (0.05)\u00a0 (0.07)\u00a0 (0.09)\u00a0 Devices+Incentives only\u00a0 \u22120.05\u00a0 \u22120.07*\u00a0 \u22120.02\u00a0 \u22120.08**\u00a0 \u22120.08**\u00a0 0.05\u00a0 0.08\u00a0 \u22120.02\u00a0 \u00a0 (0.07)\u00a0 (0.04)\u00a0 (0.03)\u00a0 (0.04)\u00a0 (0.03)\u00a0 (0.05)\u00a0 (0.06)\u00a0 (0.09)\u00a0 Nap only\u00a0 0.11\u00a0 \u22120.07\u00a0 \u22120.01\u00a0 \u22120.07*\u00a0 \u22120.05\u00a0 0.18***\u00a0 0.16**\u00a0 0.19**\u00a0 \u00a0 (0.07)\u00a0 (0.04)\u00a0 (0.03)\u00a0 (0.04)\u00a0 (0.04)\u00a0 (0.06)\u00a0 (0.07)\u00a0 (0.10)\u00a0 Devices+Encouragement and nap\u00a0 0.13*\u00a0 \u22120.07*\u00a0 0.04\u00a0 \u22120.15***\u00a0 \u22120.04\u00a0 0.13***\u00a0 0.11*\u00a0 0.11\u00a0 \u00a0 (0.07)\u00a0 (0.04)\u00a0 (0.03)\u00a0 (0.04)\u00a0 (0.03)\u00a0 (0.05)\u00a0 (0.06)\u00a0 (0.09)\u00a0 Devices+Incentives and nap\u00a0 0.08\u00a0 \u22120.07*\u00a0 0.04\u00a0 \u22120.17***\u00a0 \u22120.07**\u00a0 0.10**\u00a0 0.11*\u00a0 0.07\u00a0 \u00a0 (0.06)\u00a0 (0.04)\u00a0 (0.03)\u00a0 (0.04)\u00a0 (0.03)\u00a0 (0.05)\u00a0 (0.07)\u00a0 (0.09)\u00a0 Participants\u00a0 451\u00a0 451\u00a0 451\u00a0 451\u00a0 451\u00a0 452\u00a0 452\u00a0 452\u00a0 Devices+Encouragement only\u00a0 \u22120.00\u00a0 \u22120.00\u00a0 0.04\u00a0 \u22120.09\u00a0 \u22120.05\u00a0 \u22120.10\u00a0 \u22120.15\u00a0 \u00a0 \u00a0 (0.07)\u00a0 (0.07)\u00a0 (0.17)\u00a0 (0.06)\u00a0 (0.12)\u00a0 (0.09)\u00a0 (0.13)\u00a0 \u00a0 Devices+Incentives only\u00a0 \u22120.03\u00a0 0.04\u00a0 \u22120.12\u00a0 \u22120.04\u00a0 0.11\u00a0 \u22120.16*\u00a0 \u22120.06\u00a0 \u00a0 \u00a0 (0.08)\u00a0 (0.07)\u00a0 (0.19)\u00a0 (0.07)\u00a0 (0.12)\u00a0 (0.08)\u00a0 (0.13)\u00a0 \u00a0 Nap only\u00a0 0.09\u00a0 0.07\u00a0 0.15\u00a0 \u22120.01\u00a0 0.12\u00a0 \u22120.09\u00a0 \u22120.01\u00a0 \u00a0 \u00a0 (0.07)\u00a0 (0.07)\u00a0 (0.17)\u00a0 (0.07)\u00a0 (0.12)\u00a0 (0.09)\u00a0 (0.14)\u00a0 \u00a0 Devices+Encouragement and nap\u00a0 0.05\u00a0 0.13*\u00a0 0.03\u00a0 0.09\u00a0 0.23*\u00a0 0.03\u00a0 0.03\u00a0 \u00a0 \u00a0 (0.08)\u00a0 (0.07)\u00a0 (0.18)\u00a0 (0.07)\u00a0 (0.12)\u00a0 (0.10)\u00a0 (0.13)\u00a0 \u00a0 Devices+Incentives and nap\u00a0 0.14**\u00a0 0.09\u00a0 0.32**\u00a0 0.01\u00a0 0.11\u00a0 \u22120.10\u00a0 0.01\u00a0 \u00a0 \u00a0 (0.07)\u00a0 (0.07)\u00a0 (0.16)\u00a0 (0.07)\u00a0 (0.12)\u00a0 (0.09)\u00a0 (0.14)\u00a0 \u00a0 Participants\u00a0 452\u00a0 452\u00a0 429\u00a0 452\u00a0 452\u00a0 415\u00a0 415\u00a0 \u00a0 \n            .\u00a0 Overall \n            .\u00a0 Work \n            .\u00a0 Well-being \n            .\u00a0 \n            .\u00a0 Index \n            .\u00a0 Earnings \n            .\u00a0 Productivity \n            .\u00a0 Labor supply \n            .\u00a0 Output \n            .\u00a0 Index \n            .\u00a0 Physical \n            .\u00a0 Psychological \n            .\u00a0 \n            .\u00a0 (1) \n            .\u00a0 (2) \n            .\u00a0 (3) \n            .\u00a0 (4) \n            .\u00a0 (5) \n            .\u00a0 (6) \n            .\u00a0 (7) \n            .\u00a0 (8) \n            .\u00a0 Devices+Encouragement only\u00a0 0.00\u00a0 \u22120.07**\u00a0 \u22120.02\u00a0 \u22120.06*\u00a0 \u22120.07**\u00a0 0.13***\u00a0 0.16**\u00a0 0.02\u00a0 \u00a0 (0.07)\u00a0 (0.03)\u00a0 (0.03)\u00a0 (0.04)\u00a0 (0.03)\u00a0 (0.05)\u00a0 (0.07)\u00a0 (0.09)\u00a0 Devices+Incentives only\u00a0 \u22120.05\u00a0 \u22120.07*\u00a0 \u22120.02\u00a0 \u22120.08**\u00a0 \u22120.08**\u00a0 0.05\u00a0 0.08\u00a0 \u22120.02\u00a0 \u00a0 (0.07)\u00a0 (0.04)\u00a0 (0.03)\u00a0 (0.04)\u00a0 (0.03)\u00a0 (0.05)\u00a0 (0.06)\u00a0 (0.09)\u00a0 Nap only\u00a0 0.11\u00a0 \u22120.07\u00a0 \u22120.01\u00a0 \u22120.07*\u00a0 \u22120.05\u00a0 0.18***\u00a0 0.16**\u00a0 0.19**\u00a0 \u00a0 (0.07)\u00a0 (0.04)\u00a0 (0.03)\u00a0 (0.04)\u00a0 (0.04)\u00a0 (0.06)\u00a0 (0.07)\u00a0 (0.10)\u00a0 Devices+Encouragement and nap\u00a0 0.13*\u00a0 \u22120.07*\u00a0 0.04\u00a0 \u22120.15***\u00a0 \u22120.04\u00a0 0.13***\u00a0 0.11*\u00a0 0.11\u00a0 \u00a0 (0.07)\u00a0 (0.04)\u00a0 (0.03)\u00a0 (0.04)\u00a0 (0.03)\u00a0 (0.05)\u00a0 (0.06)\u00a0 (0.09)\u00a0 Devices+Incentives and nap\u00a0 0.08\u00a0 \u22120.07*\u00a0 0.04\u00a0 \u22120.17***\u00a0 \u22120.07**\u00a0 0.10**\u00a0 0.11*\u00a0 0.07\u00a0 \u00a0 (0.06)\u00a0 (0.04)\u00a0 (0.03)\u00a0 (0.04)\u00a0 (0.03)\u00a0 (0.05)\u00a0 (0.07)\u00a0 (0.09)\u00a0 Participants\u00a0 451\u00a0 451\u00a0 451\u00a0 451\u00a0 451\u00a0 452\u00a0 452\u00a0 452\u00a0 Devices+Encouragement only\u00a0 \u22120.00\u00a0 \u22120.00\u00a0 0.04\u00a0 \u22120.09\u00a0 \u22120.05\u00a0 \u22120.10\u00a0 \u22120.15\u00a0 \u00a0 \u00a0 (0.07)\u00a0 (0.07)\u00a0 (0.17)\u00a0 (0.06)\u00a0 (0.12)\u00a0 (0.09)\u00a0 (0.13)\u00a0 \u00a0 Devices+Incentives only\u00a0 \u22120.03\u00a0 0.04\u00a0 \u22120.12\u00a0 \u22120.04\u00a0 0.11\u00a0 \u22120.16*\u00a0 \u22120.06\u00a0 \u00a0 \u00a0 (0.08)\u00a0 (0.07)\u00a0 (0.19)\u00a0 (0.07)\u00a0 (0.12)\u00a0 (0.08)\u00a0 (0.13)\u00a0 \u00a0 Nap only\u00a0 0.09\u00a0 0.07\u00a0 0.15\u00a0 \u22120.01\u00a0 0.12\u00a0 \u22120.09\u00a0 \u22120.01\u00a0 \u00a0 \u00a0 (0.07)\u00a0 (0.07)\u00a0 (0.17)\u00a0 (0.07)\u00a0 (0.12)\u00a0 (0.09)\u00a0 (0.14)\u00a0 \u00a0 Devices+Encouragement and nap\u00a0 0.05\u00a0 0.13*\u00a0 0.03\u00a0 0.09\u00a0 0.23*\u00a0 0.03\u00a0 0.03\u00a0 \u00a0 \u00a0 (0.08)\u00a0 (0.07)\u00a0 (0.18)\u00a0 (0.07)\u00a0 (0.12)\u00a0 (0.10)\u00a0 (0.13)\u00a0 \u00a0 Devices+Incentives and nap\u00a0 0.14**\u00a0 0.09\u00a0 0.32**\u00a0 0.01\u00a0 0.11\u00a0 \u22120.10\u00a0 0.01\u00a0 \u00a0 \u00a0 (0.07)\u00a0 (0.07)\u00a0 (0.16)\u00a0 (0.07)\u00a0 (0.12)\u00a0 (0.09)\u00a0 (0.14)\u00a0 \u00a0 Participants\u00a0 452\u00a0 452\u00a0 429\u00a0 452\u00a0 452\u00a0 415\u00a0 415\u00a0 \u00a0 TABLE III Fully Disaggregated Treatment Effects of Night Sleep and Nap Treatments   \n            .\u00a0 Overall \n            .\u00a0 Work \n            .\u00a0 Well-being \n            .\u00a0 \n            .\u00a0 Index \n            .\u00a0 Earnings \n            .\u00a0 Productivity \n            .\u00a0 Labor supply \n            .\u00a0 Output \n            .\u00a0 Index \n            .\u00a0 Physical \n            .\u00a0 Psychological \n            .\u00a0 \n            .\u00a0 (1) \n            .\u00a0 (2) \n            .\u00a0 (3) \n            .\u00a0 (4) \n            .\u00a0 (5) \n            .\u00a0 (6) \n            .\u00a0 (7) \n            .\u00a0 (8) \n            .\u00a0 Devices+Encouragement only\u00a0 0.00\u00a0 \u22120.07**\u00a0 \u22120.02\u00a0 \u22120.06*\u00a0 \u22120.07**\u00a0 0.13***\u00a0 0.16**\u00a0 0.02\u00a0 \u00a0 (0.07)\u00a0 (0.03)\u00a0 (0.03)\u00a0 (0.04)\u00a0 (0.03)\u00a0 (0.05)\u00a0 (0.07)\u00a0 (0.09)\u00a0 Devices+Incentives only\u00a0 \u22120.05\u00a0 \u22120.07*\u00a0 \u22120.02\u00a0 \u22120.08**\u00a0 \u22120.08**\u00a0 0.05\u00a0 0.08\u00a0 \u22120.02\u00a0 \u00a0 (0.07)\u00a0 (0.04)\u00a0 (0.03)\u00a0 (0.04)\u00a0 (0.03)\u00a0 (0.05)\u00a0 (0.06)\u00a0 (0.09)\u00a0 Nap only\u00a0 0.11\u00a0 \u22120.07\u00a0 \u22120.01\u00a0 \u22120.07*\u00a0 \u22120.05\u00a0 0.18***\u00a0 0.16**\u00a0 0.19**\u00a0 \u00a0 (0.07)\u00a0 (0.04)\u00a0 (0.03)\u00a0 (0.04)\u00a0 (0.04)\u00a0 (0.06)\u00a0 (0.07)\u00a0 (0.10)\u00a0 Devices+Encouragement and nap\u00a0 0.13*\u00a0 \u22120.07*\u00a0 0.04\u00a0 \u22120.15***\u00a0 \u22120.04\u00a0 0.13***\u00a0 0.11*\u00a0 0.11\u00a0 \u00a0 (0.07)\u00a0 (0.04)\u00a0 (0.03)\u00a0 (0.04)\u00a0 (0.03)\u00a0 (0.05)\u00a0 (0.06)\u00a0 (0.09)\u00a0 Devices+Incentives and nap\u00a0 0.08\u00a0 \u22120.07*\u00a0 0.04\u00a0 \u22120.17***\u00a0 \u22120.07**\u00a0 0.10**\u00a0 0.11*\u00a0 0.07\u00a0 \u00a0 (0.06)\u00a0 (0.04)\u00a0 (0.03)\u00a0 (0.04)\u00a0 (0.03)\u00a0 (0.05)\u00a0 (0.07)\u00a0 (0.09)\u00a0 Participants\u00a0 451\u00a0 451\u00a0 451\u00a0 451\u00a0 451\u00a0 452\u00a0 452\u00a0 452\u00a0 Devices+Encouragement only\u00a0 \u22120.00\u00a0 \u22120.00\u00a0 0.04\u00a0 \u22120.09\u00a0 \u22120.05\u00a0 \u22120.10\u00a0 \u22120.15\u00a0 \u00a0 \u00a0 (0.07)\u00a0 (0.07)\u00a0 (0.17)\u00a0 (0.06)\u00a0 (0.12)\u00a0 (0.09)\u00a0 (0.13)\u00a0 \u00a0 Devices+Incentives only\u00a0 \u22120.03\u00a0 0.04\u00a0 \u22120.12\u00a0 \u22120.04\u00a0 0.11\u00a0 \u22120.16*\u00a0 \u22120.06\u00a0 \u00a0 \u00a0 (0.08)\u00a0 (0.07)\u00a0 (0.19)\u00a0 (0.07)\u00a0 (0.12)\u00a0 (0.08)\u00a0 (0.13)\u00a0 \u00a0 Nap only\u00a0 0.09\u00a0 0.07\u00a0 0.15\u00a0 \u22120.01\u00a0 0.12\u00a0 \u22120.09\u00a0 \u22120.01\u00a0 \u00a0 \u00a0 (0.07)\u00a0 (0.07)\u00a0 (0.17)\u00a0 (0.07)\u00a0 (0.12)\u00a0 (0.09)\u00a0 (0.14)\u00a0 \u00a0 Devices+Encouragement and nap\u00a0 0.05\u00a0 0.13*\u00a0 0.03\u00a0 0.09\u00a0 0.23*\u00a0 0.03\u00a0 0.03\u00a0 \u00a0 \u00a0 (0.08)\u00a0 (0.07)\u00a0 (0.18)\u00a0 (0.07)\u00a0 (0.12)\u00a0 (0.10)\u00a0 (0.13)\u00a0 \u00a0 Devices+Incentives and nap\u00a0 0.14**\u00a0 0.09\u00a0 0.32**\u00a0 0.01\u00a0 0.11\u00a0 \u22120.10\u00a0 0.01\u00a0 \u00a0 \u00a0 (0.07)\u00a0 (0.07)\u00a0 (0.16)\u00a0 (0.07)\u00a0 (0.12)\u00a0 (0.09)\u00a0 (0.14)\u00a0 \u00a0 Participants\u00a0 452\u00a0 452\u00a0 429\u00a0 452\u00a0 452\u00a0 415\u00a0 415\u00a0 \u00a0 \n            .\u00a0 Overall \n            .\u00a0 Work \n            .\u00a0 Well-being \n            .\u00a0 \n            .\u00a0 Index \n            .\u00a0 Earnings \n            .\u00a0 Productivity \n            .\u00a0 Labor supply \n            .\u00a0 Output \n            .\u00a0 Index \n            .\u00a0 Physical \n            .\u00a0 Psychological \n            .\u00a0 \n            .\u00a0 (1) \n            .\u00a0 (2) \n            .\u00a0 (3) \n            .\u00a0 (4) \n            .\u00a0 (5) \n            .\u00a0 (6) \n            .\u00a0 (7) \n            .\u00a0 (8) \n            .\u00a0 Devices+Encouragement only\u00a0 0.00\u00a0 \u22120.07**\u00a0 \u22120.02\u00a0 \u22120.06*\u00a0 \u22120.07**\u00a0 0.13***\u00a0 0.16**\u00a0 0.02\u00a0 \u00a0 (0.07)\u00a0 (0.03)\u00a0 (0.03)\u00a0 (0.04)\u00a0 (0.03)\u00a0 (0.05)\u00a0 (0.07)\u00a0 (0.09)\u00a0 Devices+Incentives only\u00a0 \u22120.05\u00a0 \u22120.07*\u00a0 \u22120.02\u00a0 \u22120.08**\u00a0 \u22120.08**\u00a0 0.05\u00a0 0.08\u00a0 \u22120.02\u00a0 \u00a0 (0.07)\u00a0 (0.04)\u00a0 (0.03)\u00a0 (0.04)\u00a0 (0.03)\u00a0 (0.05)\u00a0 (0.06)\u00a0 (0.09)\u00a0 Nap only\u00a0 0.11\u00a0 \u22120.07\u00a0 \u22120.01\u00a0 \u22120.07*\u00a0 \u22120.05\u00a0 0.18***\u00a0 0.16**\u00a0 0.19**\u00a0 \u00a0 (0.07)\u00a0 (0.04)\u00a0 (0.03)\u00a0 (0.04)\u00a0 (0.04)\u00a0 (0.06)\u00a0 (0.07)\u00a0 (0.10)\u00a0 Devices+Encouragement and nap\u00a0 0.13*\u00a0 \u22120.07*\u00a0 0.04\u00a0 \u22120.15***\u00a0 \u22120.04\u00a0 0.13***\u00a0 0.11*\u00a0 0.11\u00a0 \u00a0 (0.07)\u00a0 (0.04)\u00a0 (0.03)\u00a0 (0.04)\u00a0 (0.03)\u00a0 (0.05)\u00a0 (0.06)\u00a0 (0.09)\u00a0 Devices+Incentives and nap\u00a0 0.08\u00a0 \u22120.07*\u00a0 0.04\u00a0 \u22120.17***\u00a0 \u22120.07**\u00a0 0.10**\u00a0 0.11*\u00a0 0.07\u00a0 \u00a0 (0.06)\u00a0 (0.04)\u00a0 (0.03)\u00a0 (0.04)\u00a0 (0.03)\u00a0 (0.05)\u00a0 (0.07)\u00a0 (0.09)\u00a0 Participants\u00a0 451\u00a0 451\u00a0 451\u00a0 451\u00a0 451\u00a0 452\u00a0 452\u00a0 452\u00a0 Devices+Encouragement only\u00a0 \u22120.00\u00a0 \u22120.00\u00a0 0.04\u00a0 \u22120.09\u00a0 \u22120.05\u00a0 \u22120.10\u00a0 \u22120.15\u00a0 \u00a0 \u00a0 (0.07)\u00a0 (0.07)\u00a0 (0.17)\u00a0 (0.06)\u00a0 (0.12)\u00a0 (0.09)\u00a0 (0.13)\u00a0 \u00a0 Devices+Incentives only\u00a0 \u22120.03\u00a0 0.04\u00a0 \u22120.12\u00a0 \u22120.04\u00a0 0.11\u00a0 \u22120.16*\u00a0 \u22120.06\u00a0 \u00a0 \u00a0 (0.08)\u00a0 (0.07)\u00a0 (0.19)\u00a0 (0.07)\u00a0 (0.12)\u00a0 (0.08)\u00a0 (0.13)\u00a0 \u00a0 Nap only\u00a0 0.09\u00a0 0.07\u00a0 0.15\u00a0 \u22120.01\u00a0 0.12\u00a0 \u22120.09\u00a0 \u22120.01\u00a0 \u00a0 \u00a0 (0.07)\u00a0 (0.07)\u00a0 (0.17)\u00a0 (0.07)\u00a0 (0.12)\u00a0 (0.09)\u00a0 (0.14)\u00a0 \u00a0 Devices+Encouragement and nap\u00a0 0.05\u00a0 0.13*\u00a0 0.03\u00a0 0.09\u00a0 0.23*\u00a0 0.03\u00a0 0.03\u00a0 \u00a0 \u00a0 (0.08)\u00a0 (0.07)\u00a0 (0.18)\u00a0 (0.07)\u00a0 (0.12)\u00a0 (0.10)\u00a0 (0.13)\u00a0 \u00a0 Devices+Incentives and nap\u00a0 0.14**\u00a0 0.09\u00a0 0.32**\u00a0 0.01\u00a0 0.11\u00a0 \u22120.10\u00a0 0.01\u00a0 \u00a0 \u00a0 (0.07)\u00a0 (0.07)\u00a0 (0.16)\u00a0 (0.07)\u00a0 (0.12)\u00a0 (0.09)\u00a0 (0.14)\u00a0 \u00a0 Participants\u00a0 452\u00a0 452\u00a0 429\u00a0 452\u00a0 452\u00a0 415\u00a0 415\u00a0 \u00a0   TABLE IV Pooled Treatment Effects of Night Sleep and Nap Treatments   \n            .\u00a0 Overall \n            .\u00a0 Work \n            .\u00a0 Well-being \n            .\u00a0 \n            .\u00a0 Index \n            .\u00a0 Earnings \n            .\u00a0 Productivity \n            .\u00a0 Labor supply \n            .\u00a0 Output \n            .\u00a0 Index \n            .\u00a0 Physical \n            .\u00a0 Psychological \n            .\u00a0 \n            .\u00a0 (1) \n            .\u00a0 (2) \n            .\u00a0 (3) \n            .\u00a0 (4) \n            .\u00a0 (5) \n            .\u00a0 (6) \n            .\u00a0 (7) \n            .\u00a0 (8) \n            .\u00a0 Night sleep treatments\u00a0 \u22120.01 \u00a0 \u22120.04 \u00a0 0.02\u00a0 \u22120.08\u00a0 \u22120.04\u00a0 0.01 \u00a0 0.04\u00a0 \u22120.05\u00a0 \u00a0 (0.04)\u00a0 (0.02)\u00a0 (0.02)\u00a0 (0.02)\u00a0 (0.02)\u00a0 (0.03)\u00a0 (0.04)\u00a0 (0.06)\u00a0 \u00a0 {.79}\u00a0 {.08}\u00a0 {.30}\u00a0 {.00}\u00a0 {.05}\u00a0 {.69}\u00a0 {.35}\u00a0 {.36}\u00a0 \u00a0 \u00a0 [.30]\u00a0 [.32]\u00a0 [.00]\u00a0 [.09]\u00a0 [.97]\u00a0 [.36]\u00a0 [.36]\u00a0 Nap treatment\u00a0 0.12 \u00a0 \u22120.02 \u00a0 0.04\u00a0 \u22120.09\u00a0 \u22120.01\u00a0 0.08 \u00a0 0.05\u00a0 0.12\u00a0 \u00a0 (0.04)\u00a0 (0.02)\u00a0 (0.02)\u00a0 (0.02)\u00a0 (0.02)\u00a0 (0.03)\u00a0 (0.04)\u00a0 (0.05)\u00a0 \u00a0 {.00}\u00a0 {.23}\u00a0 {.03}\u00a0 {.00}\u00a0 {.77}\u00a0 {.01}\u00a0 {.18}\u00a0 {.02}\u00a0 \u00a0 \u00a0 [.25]\u00a0 [.06]\u00a0 [.00]\u00a0 [.77]\u00a0 [.03]\u00a0 [.19]\u00a0 [.04]\u00a0 Participants\u00a0 451\u00a0 451\u00a0 451\u00a0 451\u00a0 451\u00a0 452\u00a0 452\u00a0 452\u00a0 Unadjusted  p -value NS versus nap\u00a0 {.02}\u00a0 {.62}\u00a0 {.49}\u00a0 {.65}\u00a0 {.23}\u00a0 {.16}\u00a0 {.83}\u00a0 {.03}\u00a0 FWER-corrected  p -value NS versus nap\u00a0 \u00a0 [.62]\u00a0 [.66]\u00a0 [.66]\u00a0 [.45]\u00a0 [.41]\u00a0 [.84]\u00a0 [.04]\u00a0 Night sleep treatments\u00a0 \u22120.00 \u00a0 0.03\u00a0 \u22120.01\u00a0 \u22120.00 \u00a0 0.04\u00a0 \u22120.04\u00a0 \u22120.04\u00a0 \u00a0 \u00a0 (0.05)\u00a0 (0.04)\u00a0 (0.11)\u00a0 (0.04)\u00a0 (0.07)\u00a0 (0.06)\u00a0 (0.08)\u00a0 \u00a0 \u00a0 {.93}\u00a0 {.51}\u00a0 {.95}\u00a0 {.95}\u00a0 {.58}\u00a0 {.45}\u00a0 {.65}\u00a0 \u00a0 \u00a0 [.98]\u00a0 [.76]\u00a0 [.95]\u00a0 [.98]\u00a0 [.64]\u00a0 [.64]\u00a0 [.64]\u00a0 \u00a0 Nap treatment\u00a0 0.10 \u00a0 0.08\u00a0 0.20\u00a0 0.07 \u00a0 0.13\u00a0 0.03\u00a0 0.07\u00a0 \u00a0 \u00a0 (0.05)\u00a0 (0.04)\u00a0 (0.11)\u00a0 (0.04)\u00a0 (0.06)\u00a0 (0.05)\u00a0 (0.08)\u00a0 \u00a0 \u00a0 {.03}\u00a0 {.03}\u00a0 {.07}\u00a0 {.05}\u00a0 {.04}\u00a0 {.52}\u00a0 {.33}\u00a0 \u00a0 \u00a0 [.08]\u00a0 [.07]\u00a0 [.07]\u00a0 [.15]\u00a0 [.20]\u00a0 [.51]\u00a0 [.51]\u00a0 \u00a0 Participants\u00a0 452\u00a0 452\u00a0 429\u00a0 452\u00a0 452\u00a0 415\u00a0 415\u00a0 \u00a0 Unadjusted  p -value NS versus nap\u00a0 {.12}\u00a0 {.31}\u00a0 {.21}\u00a0 {.22}\u00a0 {.36}\u00a0 {.30}\u00a0 {.32}\u00a0 \u00a0 FWER-corrected  p -value NS versus nap\u00a0 [.41]\u00a0 [.31]\u00a0 [.31]\u00a0 [.41]\u00a0 [.36]\u00a0 [.36]\u00a0 [.36]\u00a0 \u00a0 \n            .\u00a0 Overall \n            .\u00a0 Work \n            .\u00a0 Well-being \n            .\u00a0 \n            .\u00a0 Index \n            .\u00a0 Earnings \n            .\u00a0 Productivity \n            .\u00a0 Labor supply \n            .\u00a0 Output \n            .\u00a0 Index \n            .\u00a0 Physical \n            .\u00a0 Psychological \n            .\u00a0 \n            .\u00a0 (1) \n            .\u00a0 (2) \n            .\u00a0 (3) \n            .\u00a0 (4) \n            .\u00a0 (5) \n            .\u00a0 (6) \n            .\u00a0 (7) \n            .\u00a0 (8) \n            .\u00a0 Night sleep treatments\u00a0 \u22120.01 \u00a0 \u22120.04 \u00a0 0.02\u00a0 \u22120.08\u00a0 \u22120.04\u00a0 0.01 \u00a0 0.04\u00a0 \u22120.05\u00a0 \u00a0 (0.04)\u00a0 (0.02)\u00a0 (0.02)\u00a0 (0.02)\u00a0 (0.02)\u00a0 (0.03)\u00a0 (0.04)\u00a0 (0.06)\u00a0 \u00a0 {.79}\u00a0 {.08}\u00a0 {.30}\u00a0 {.00}\u00a0 {.05}\u00a0 {.69}\u00a0 {.35}\u00a0 {.36}\u00a0 \u00a0 \u00a0 [.30]\u00a0 [.32]\u00a0 [.00]\u00a0 [.09]\u00a0 [.97]\u00a0 [.36]\u00a0 [.36]\u00a0 Nap treatment\u00a0 0.12 \u00a0 \u22120.02 \u00a0 0.04\u00a0 \u22120.09\u00a0 \u22120.01\u00a0 0.08 \u00a0 0.05\u00a0 0.12\u00a0 \u00a0 (0.04)\u00a0 (0.02)\u00a0 (0.02)\u00a0 (0.02)\u00a0 (0.02)\u00a0 (0.03)\u00a0 (0.04)\u00a0 (0.05)\u00a0 \u00a0 {.00}\u00a0 {.23}\u00a0 {.03}\u00a0 {.00}\u00a0 {.77}\u00a0 {.01}\u00a0 {.18}\u00a0 {.02}\u00a0 \u00a0 \u00a0 [.25]\u00a0 [.06]\u00a0 [.00]\u00a0 [.77]\u00a0 [.03]\u00a0 [.19]\u00a0 [.04]\u00a0 Participants\u00a0 451\u00a0 451\u00a0 451\u00a0 451\u00a0 451\u00a0 452\u00a0 452\u00a0 452\u00a0 Unadjusted  p -value NS versus nap\u00a0 {.02}\u00a0 {.62}\u00a0 {.49}\u00a0 {.65}\u00a0 {.23}\u00a0 {.16}\u00a0 {.83}\u00a0 {.03}\u00a0 FWER-corrected  p -value NS versus nap\u00a0 \u00a0 [.62]\u00a0 [.66]\u00a0 [.66]\u00a0 [.45]\u00a0 [.41]\u00a0 [.84]\u00a0 [.04]\u00a0 Night sleep treatments\u00a0 \u22120.00 \u00a0 0.03\u00a0 \u22120.01\u00a0 \u22120.00 \u00a0 0.04\u00a0 \u22120.04\u00a0 \u22120.04\u00a0 \u00a0 \u00a0 (0.05)\u00a0 (0.04)\u00a0 (0.11)\u00a0 (0.04)\u00a0 (0.07)\u00a0 (0.06)\u00a0 (0.08)\u00a0 \u00a0 \u00a0 {.93}\u00a0 {.51}\u00a0 {.95}\u00a0 {.95}\u00a0 {.58}\u00a0 {.45}\u00a0 {.65}\u00a0 \u00a0 \u00a0 [.98]\u00a0 [.76]\u00a0 [.95]\u00a0 [.98]\u00a0 [.64]\u00a0 [.64]\u00a0 [.64]\u00a0 \u00a0 Nap treatment\u00a0 0.10 \u00a0 0.08\u00a0 0.20\u00a0 0.07 \u00a0 0.13\u00a0 0.03\u00a0 0.07\u00a0 \u00a0 \u00a0 (0.05)\u00a0 (0.04)\u00a0 (0.11)\u00a0 (0.04)\u00a0 (0.06)\u00a0 (0.05)\u00a0 (0.08)\u00a0 \u00a0 \u00a0 {.03}\u00a0 {.03}\u00a0 {.07}\u00a0 {.05}\u00a0 {.04}\u00a0 {.52}\u00a0 {.33}\u00a0 \u00a0 \u00a0 [.08]\u00a0 [.07]\u00a0 [.07]\u00a0 [.15]\u00a0 [.20]\u00a0 [.51]\u00a0 [.51]\u00a0 \u00a0 Participants\u00a0 452\u00a0 452\u00a0 429\u00a0 452\u00a0 452\u00a0 415\u00a0 415\u00a0 \u00a0 Unadjusted  p -value NS versus nap\u00a0 {.12}\u00a0 {.31}\u00a0 {.21}\u00a0 {.22}\u00a0 {.36}\u00a0 {.30}\u00a0 {.32}\u00a0 \u00a0 FWER-corrected  p -value NS versus nap\u00a0 [.41]\u00a0 [.31]\u00a0 [.31]\u00a0 [.41]\u00a0 [.36]\u00a0 [.36]\u00a0 [.36]\u00a0 \u00a0 TABLE IV Pooled Treatment Effects of Night Sleep and Nap Treatments   \n            .\u00a0 Overall \n            .\u00a0 Work \n            .\u00a0 Well-being \n            .\u00a0 \n            .\u00a0 Index \n            .\u00a0 Earnings \n            .\u00a0 Productivity \n            .\u00a0 Labor supply \n            .\u00a0 Output \n            .\u00a0 Index \n            .\u00a0 Physical \n            .\u00a0 Psychological \n            .\u00a0 \n            .\u00a0 (1) \n            .\u00a0 (2) \n            .\u00a0 (3) \n            .\u00a0 (4) \n            .\u00a0 (5) \n            .\u00a0 (6) \n            .\u00a0 (7) \n            .\u00a0 (8) \n            .\u00a0 Night sleep treatments\u00a0 \u22120.01 \u00a0 \u22120.04 \u00a0 0.02\u00a0 \u22120.08\u00a0 \u22120.04\u00a0 0.01 \u00a0 0.04\u00a0 \u22120.05\u00a0 \u00a0 (0.04)\u00a0 (0.02)\u00a0 (0.02)\u00a0 (0.02)\u00a0 (0.02)\u00a0 (0.03)\u00a0 (0.04)\u00a0 (0.06)\u00a0 \u00a0 {.79}\u00a0 {.08}\u00a0 {.30}\u00a0 {.00}\u00a0 {.05}\u00a0 {.69}\u00a0 {.35}\u00a0 {.36}\u00a0 \u00a0 \u00a0 [.30]\u00a0 [.32]\u00a0 [.00]\u00a0 [.09]\u00a0 [.97]\u00a0 [.36]\u00a0 [.36]\u00a0 Nap treatment\u00a0 0.12 \u00a0 \u22120.02 \u00a0 0.04\u00a0 \u22120.09\u00a0 \u22120.01\u00a0 0.08 \u00a0 0.05\u00a0 0.12\u00a0 \u00a0 (0.04)\u00a0 (0.02)\u00a0 (0.02)\u00a0 (0.02)\u00a0 (0.02)\u00a0 (0.03)\u00a0 (0.04)\u00a0 (0.05)\u00a0 \u00a0 {.00}\u00a0 {.23}\u00a0 {.03}\u00a0 {.00}\u00a0 {.77}\u00a0 {.01}\u00a0 {.18}\u00a0 {.02}\u00a0 \u00a0 \u00a0 [.25]\u00a0 [.06]\u00a0 [.00]\u00a0 [.77]\u00a0 [.03]\u00a0 [.19]\u00a0 [.04]\u00a0 Participants\u00a0 451\u00a0 451\u00a0 451\u00a0 451\u00a0 451\u00a0 452\u00a0 452\u00a0 452\u00a0 Unadjusted  p -value NS versus nap\u00a0 {.02}\u00a0 {.62}\u00a0 {.49}\u00a0 {.65}\u00a0 {.23}\u00a0 {.16}\u00a0 {.83}\u00a0 {.03}\u00a0 FWER-corrected  p -value NS versus nap\u00a0 \u00a0 [.62]\u00a0 [.66]\u00a0 [.66]\u00a0 [.45]\u00a0 [.41]\u00a0 [.84]\u00a0 [.04]\u00a0 Night sleep treatments\u00a0 \u22120.00 \u00a0 0.03\u00a0 \u22120.01\u00a0 \u22120.00 \u00a0 0.04\u00a0 \u22120.04\u00a0 \u22120.04\u00a0 \u00a0 \u00a0 (0.05)\u00a0 (0.04)\u00a0 (0.11)\u00a0 (0.04)\u00a0 (0.07)\u00a0 (0.06)\u00a0 (0.08)\u00a0 \u00a0 \u00a0 {.93}\u00a0 {.51}\u00a0 {.95}\u00a0 {.95}\u00a0 {.58}\u00a0 {.45}\u00a0 {.65}\u00a0 \u00a0 \u00a0 [.98]\u00a0 [.76]\u00a0 [.95]\u00a0 [.98]\u00a0 [.64]\u00a0 [.64]\u00a0 [.64]\u00a0 \u00a0 Nap treatment\u00a0 0.10 \u00a0 0.08\u00a0 0.20\u00a0 0.07 \u00a0 0.13\u00a0 0.03\u00a0 0.07\u00a0 \u00a0 \u00a0 (0.05)\u00a0 (0.04)\u00a0 (0.11)\u00a0 (0.04)\u00a0 (0.06)\u00a0 (0.05)\u00a0 (0.08)\u00a0 \u00a0 \u00a0 {.03}\u00a0 {.03}\u00a0 {.07}\u00a0 {.05}\u00a0 {.04}\u00a0 {.52}\u00a0 {.33}\u00a0 \u00a0 \u00a0 [.08]\u00a0 [.07]\u00a0 [.07]\u00a0 [.15]\u00a0 [.20]\u00a0 [.51]\u00a0 [.51]\u00a0 \u00a0 Participants\u00a0 452\u00a0 452\u00a0 429\u00a0 452\u00a0 452\u00a0 415\u00a0 415\u00a0 \u00a0 Unadjusted  p -value NS versus nap\u00a0 {.12}\u00a0 {.31}\u00a0 {.21}\u00a0 {.22}\u00a0 {.36}\u00a0 {.30}\u00a0 {.32}\u00a0 \u00a0 FWER-corrected  p -value NS versus nap\u00a0 [.41]\u00a0 [.31]\u00a0 [.31]\u00a0 [.41]\u00a0 [.36]\u00a0 [.36]\u00a0 [.36]\u00a0 \u00a0 \n            .\u00a0 Overall \n            .\u00a0 Work \n            .\u00a0 Well-being \n            .\u00a0 \n            .\u00a0 Index \n            .\u00a0 Earnings \n            .\u00a0 Productivity \n            .\u00a0 Labor supply \n            .\u00a0 Output \n            .\u00a0 Index \n            .\u00a0 Physical \n            .\u00a0 Psychological \n            .\u00a0 \n            .\u00a0 (1) \n            .\u00a0 (2) \n            .\u00a0 (3) \n            .\u00a0 (4) \n            .\u00a0 (5) \n            .\u00a0 (6) \n            .\u00a0 (7) \n            .\u00a0 (8) \n            .\u00a0 Night sleep treatments\u00a0 \u22120.01 \u00a0 \u22120.04 \u00a0 0.02\u00a0 \u22120.08\u00a0 \u22120.04\u00a0 0.01 \u00a0 0.04\u00a0 \u22120.05\u00a0 \u00a0 (0.04)\u00a0 (0.02)\u00a0 (0.02)\u00a0 (0.02)\u00a0 (0.02)\u00a0 (0.03)\u00a0 (0.04)\u00a0 (0.06)\u00a0 \u00a0 {.79}\u00a0 {.08}\u00a0 {.30}\u00a0 {.00}\u00a0 {.05}\u00a0 {.69}\u00a0 {.35}\u00a0 {.36}\u00a0 \u00a0 \u00a0 [.30]\u00a0 [.32]\u00a0 [.00]\u00a0 [.09]\u00a0 [.97]\u00a0 [.36]\u00a0 [.36]\u00a0 Nap treatment\u00a0 0.12 \u00a0 \u22120.02 \u00a0 0.04\u00a0 \u22120.09\u00a0 \u22120.01\u00a0 0.08 \u00a0 0.05\u00a0 0.12\u00a0 \u00a0 (0.04)\u00a0 (0.02)\u00a0 (0.02)\u00a0 (0.02)\u00a0 (0.02)\u00a0 (0.03)\u00a0 (0.04)\u00a0 (0.05)\u00a0 \u00a0 {.00}\u00a0 {.23}\u00a0 {.03}\u00a0 {.00}\u00a0 {.77}\u00a0 {.01}\u00a0 {.18}\u00a0 {.02}\u00a0 \u00a0 \u00a0 [.25]\u00a0 [.06]\u00a0 [.00]\u00a0 [.77]\u00a0 [.03]\u00a0 [.19]\u00a0 [.04]\u00a0 Participants\u00a0 451\u00a0 451\u00a0 451\u00a0 451\u00a0 451\u00a0 452\u00a0 452\u00a0 452\u00a0 Unadjusted  p -value NS versus nap\u00a0 {.02}\u00a0 {.62}\u00a0 {.49}\u00a0 {.65}\u00a0 {.23}\u00a0 {.16}\u00a0 {.83}\u00a0 {.03}\u00a0 FWER-corrected  p -value NS versus nap\u00a0 \u00a0 [.62]\u00a0 [.66]\u00a0 [.66]\u00a0 [.45]\u00a0 [.41]\u00a0 [.84]\u00a0 [.04]\u00a0 Night sleep treatments\u00a0 \u22120.00 \u00a0 0.03\u00a0 \u22120.01\u00a0 \u22120.00 \u00a0 0.04\u00a0 \u22120.04\u00a0 \u22120.04\u00a0 \u00a0 \u00a0 (0.05)\u00a0 (0.04)\u00a0 (0.11)\u00a0 (0.04)\u00a0 (0.07)\u00a0 (0.06)\u00a0 (0.08)\u00a0 \u00a0 \u00a0 {.93}\u00a0 {.51}\u00a0 {.95}\u00a0 {.95}\u00a0 {.58}\u00a0 {.45}\u00a0 {.65}\u00a0 \u00a0 \u00a0 [.98]\u00a0 [.76]\u00a0 [.95]\u00a0 [.98]\u00a0 [.64]\u00a0 [.64]\u00a0 [.64]\u00a0 \u00a0 Nap treatment\u00a0 0.10 \u00a0 0.08\u00a0 0.20\u00a0 0.07 \u00a0 0.13\u00a0 0.03\u00a0 0.07\u00a0 \u00a0 \u00a0 (0.05)\u00a0 (0.04)\u00a0 (0.11)\u00a0 (0.04)\u00a0 (0.06)\u00a0 (0.05)\u00a0 (0.08)\u00a0 \u00a0 \u00a0 {.03}\u00a0 {.03}\u00a0 {.07}\u00a0 {.05}\u00a0 {.04}\u00a0 {.52}\u00a0 {.33}\u00a0 \u00a0 \u00a0 [.08]\u00a0 [.07]\u00a0 [.07]\u00a0 [.15]\u00a0 [.20]\u00a0 [.51]\u00a0 [.51]\u00a0 \u00a0 Participants\u00a0 452\u00a0 452\u00a0 429\u00a0 452\u00a0 452\u00a0 415\u00a0 415\u00a0 \u00a0 Unadjusted  p -value NS versus nap\u00a0 {.12}\u00a0 {.31}\u00a0 {.21}\u00a0 {.22}\u00a0 {.36}\u00a0 {.30}\u00a0 {.32}\u00a0 \u00a0 FWER-corrected  p -value NS versus nap\u00a0 [.41]\u00a0 [.31]\u00a0 [.31]\u00a0 [.41]\u00a0 [.36]\u00a0 [.36]\u00a0 [.36]\u00a0 \u00a0   IV.B. Effects on Sleep 1. Overview Figure\u00a0V  and  Table\u00a0II  show the impacts of our treatments on the different measures of sleep. We find that the two night sleep treatments quickly and substantially increase night sleep by 27 minutes a night on average. Offering short afternoon naps increases daytime sleep by 14 minutes a day. Thus, it is possible to substantially increase sleep in the sleep-deprived population we study through encouragement, incentives, and nap opportunities.   Figure\u00a0V Impacts on Nighttime and Nap Sleep This figure shows the average of different sleep-related variables for different treatment arms by day in study of the RCT. All outcomes are actigraph measures. We only include workday nights and days in the sample. In Panels A and B, we plot hours of night sleep and hours in bed at night, respectively. Panel C shows sleep efficiency (nighttime sleep/time in bed). In Panel D, we plot the duration of naps at the workplace (in minutes), excluding day 28 because naps were not allowed on that day. Figure\u00a0V Impacts on Nighttime and Nap Sleep This figure shows the average of different sleep-related variables for different treatment arms by day in study of the RCT. All outcomes are actigraph measures. We only include workday nights and days in the sample. In Panels A and B, we plot hours of night sleep and hours in bed at night, respectively. Panel C shows sleep efficiency (nighttime sleep/time in bed). In Panel D, we plot the duration of naps at the workplace (in minutes), excluding day 28 because naps were not allowed on that day.   2. Night Sleep Treatments Only Both night sleep treatments sharply increased sleep, as measured by the actigraphs. On average, individuals who received the Devices + Encouragement and Devices + Incentives treatments only\u2014that is, without also receiving naps\u2014increased their time asleep at night by 19 and 29 minutes a night compared with the control group, respectively ( Figure\u00a0V , Panel A, and  Table\u00a0II , column (1)). Including those who might have additionally received naps, the night sleep treatments increased night sleep by 27 minutes a night on average ( Online Appendix Table\u00a0A.VI ). This is a larger gain than typically achieved by sleeping pills and cognitive behavioral therapy for insomnia ( Riemann and Perlis, 2009 ;  Trauer et\u00a0al. 2015 ). The increase in sleep was driven entirely by additional time in bed rather than improved sleep efficiency. Both night sleep treatment groups increased their time in bed significantly throughout the treatment period\u201432 minutes for the encouragement only group and 50 minutes for the incentives only group ( Figure\u00a0V , Panel B, and  Table\u00a0II , column (2)). 24  We find no significant changes in sleep efficiency compared to the control group ( Figure\u00a0V , Panel C, and  Table\u00a0II , column (3)), even in the middle of the night when all participants are likely to be in bed ( Online Appendix Figure\u00a0A.Ia ). Increasing time spent in bed is a simple and practical way for our study participants to increase their sleep duration. However, improving sleep efficiency appears to be difficult for participants, even with the aid of the devices and tips surrounding sleep hygiene. Participants faced substantial (implicit) incentives for improvement. For example, a participant in the incentives group who improved their sleep efficiency from 70% to 80% would on average earn an additional Rs. 48\u2014about 20% of average typing earnings\u2014in sleep incentives each night (holding fixed eight hours a night in bed). Yet we saw no changes in sleep efficiency. Improving sleep efficiency may require more substantial interventions, including ways to overcome barriers to sleep\u2014such as mosquitoes, crowding, or psychological distress\u2014that remained unaddressed by our treatments ( Online Appendix Figure\u00a0A.IVa ).   3. Nap Treatment Only The nap intervention was effective at increasing participants\u2019 daytime sleep ( Figure\u00a0V , Panel D, and  Table\u00a0II  column (4)). Nearly all participants in the nap treatment (92%) reported falling asleep during their allotted nap time, consistent with actigraph data showing that participants fell asleep in 93% of all nap sessions. The mean actigraph-recorded (unconditional) time asleep during the nap period was 14 minutes, and the median duration was 16 minutes ( Online Appendix Figure\u00a0A.IId ). The \u201cquality\u201d of nap sleep in the office appears to be higher than that of night sleep and naps at home. For instance, sleep efficiency during naps in the office (85%) is higher than efficiency in night sleep (66%) and in naps at home (72%, similar to night sleep), if one excludes in all cases the time taken to first fall asleep. 25  The average number of awakenings per minute of sleep achieved is also lower for the office naps. Better sleep quality during naps in the office\u2014compared to both naps and night sleep at home\u2014is consistent with a more comfortable sleep environment in the office.   4. Interactions, Crowd-Out, and Heterogeneity We find only modest interactions between the sleep treatments in terms of their effects on the various sleep measures. The effect on 24-hour sleep of receiving the encouragement treatment and the nap treatment together is very similar to the sum of the effects of receiving each treatment alone (25 minutes versus 25 minutes,  p  = .98). The same is largely true for providing incentives and naps together (42 minutes versus 35 minutes,  p  = .32). We do find evidence that napping modestly crowds out nighttime sleep: those treated with naps spent seven minutes less in bed at night and slept, on average, eight minutes less per night ( Table\u00a0II , columns (1)\u2013(2)). In contrast, the night sleep treatments do not interfere with naps. Participants randomized to the night sleep treatments did not nap any less when offered a nap ( Table\u00a0II , column (4)). Both treatments increased total time asleep in 24\u00a0hours, although naps alone had a substantially smaller and insignificant effect ( Table\u00a0II , column (5)). 26 Finally, the effect of the night sleep treatments on sleep quantity and efficiency did not differ significantly by baseline sleep patterns, nor by characteristics such as participants\u2019 gender, age, or baseline earnings ( Online Appendix Table\u00a0A.V ). Nor did these factors predict meaningful differences in nap duration for the nap treatment group. The treatments thus seem to have been equally effective at increasing sleep (and leaving efficiency unchanged) for different categories of participants.   IV.C. Overall Effect of Each Treatment on Outcomes Table\u00a0III  presents the treatment effects for the five combinations of night sleep and nap treatments. Given the large number of outcomes and treatments, we focus on the effects on the overall summary index (column (1)) which parsimoniously and efficiently averages our outcomes. 27  Each of the night sleep treatments alone had no effect or a slightly negative (but insignificant) effect on participants: 0.00 std. dev. (std. err. = 0.07) and \u22120.05 std. dev. (std. err. = 0.07), respectively, for the encouragement only and incentives only groups. In contrast, participants in the nap only treatment experienced positive and marginally significant effects of 0.11 std. dev. (std. err. = 0.07,  p  = .11). The effect of naps only differs significantly from that of incentives only ( p  = .02) and suggestively from that of encouragement only ( p  = .11). Those who received a night sleep treatment in addition to naps had very similar overall effects to those who received naps only: 0.13 and 0.08 std. dev. for the encouragement and nap and incentives and nap groups respectively, compared to 0.11 std. dev. for nap only. These results provide evidence that naps have an overall positive effect on outcomes, while increases in night sleep do not. However, since each treatment cell has only around 75 participants, this analysis has limited statistical power. The combination of five treatment cells with numerous outcomes also makes discussion of detailed results unwieldy. We therefore turn to a simplified but higher-powered version of this analysis, which pools the two night sleep treatments\u2014which typically have similar and statistically indistinguishable effects \u2014and does not include a separate indicator for the group that receives both the night sleep and nap treatments. The resulting estimates in  Table\u00a0IV  should be interpreted as weighted averages of treatment effects in the relevant cells. For instance, in this fully pooled specification, the coefficient on the night sleep treatment is the average effect of being assigned to one of the two night sleep treatments (with equal probability), in a population which either receives naps or does not (with equal probability). In  Online Appendix Table\u00a0A.VIII , we instead pool the two night sleep treatments but include a separate indicator for individuals who received a combination of either night sleep treatment along with the nap treatment.   IV.D. Effect of Night Sleep Treatments 1. Overview Experts from sleep science and economics predicted that increased night sleep would result in higher work output and labor supply, improved health and attention, increased financial savings, and reduced present bias ( Figure\u00a0IV ). In contrast to these predictions and an influential literature in sleep science, we find no effect of the pooled night sleep treatments on any of these outcomes. More generally, we find no positive effects of the night sleep treatments on the four family-level summary variables, or on any of the individual outcomes in our pooled specification ( Figure\u00a0VI  and  Table\u00a0IV ). Instead, increases in night sleep come at the cost of significantly reduced labor supply and therefore a marginally significant reduction in work output.   Figure\u00a0VI Summary of Treatment Effects This figure summarizes the treatment effects in our study. We plot the point estimates and 90% confidence intervals for the pooled night sleep interventions in Panel A and the nap intervention in Panel B. All outcomes are standardized, that is, we subtract the mean and divide by the standard deviation of the individuals receiving neither the night sleep nor the nap interventions. The coefficients and confidence intervals are based on the estimates and standard errors in  Table\u00a0IV . The comparison group for the nap treatment is the pooled nap control group, that is, participants not assigned to the nap intervention. The outcome variables, described in more detail in  Section III.B , are as follows. Overall index: aggregates across all the outcomes in the table. Work: (i) earnings; (ii) productivity; (iii) active typing time; and (iv) output from the data entry task. Well-being: (i) \u201cWell-being Index,\u201d a composite index of the physical and mental well-being indices; (ii) \u201cPhysical,\u201d a physical well-being index, a composite of performance in an endline biking task, self-reported illnesses, self-reported pain, self-reported health, and blood pressure; and (iii) \u201cPsychological,\u201d a mental well-being index, a composite of self-reported depression, happiness, life possibility, life satisfaction, and stress. Cognition: (i) \u201cCognition Index,\u201d composite index of a lab-based and a work-based measure of cognitive function; (ii) \u201cLab Tasks,\u201d index of lab measures of attention, memory, and inhibitory control; and (iii) \u201cWork Task,\u201d measure of attention to piece rates in the data entry task. Preferences: (i) \u201cPreferences Index,\u201d composite index of time, social, and risk preference indices; (ii) \u201cTime,\u201d index capturing time preferences, including savings and present bias; (iii) \u201cSocial,\u201d index representing social preferences; and (iv) \u201cRisk,\u201d index representing risk preferences. Figure\u00a0VI Summary of Treatment Effects This figure summarizes the treatment effects in our study. We plot the point estimates and 90% confidence intervals for the pooled night sleep interventions in Panel A and the nap intervention in Panel B. All outcomes are standardized, that is, we subtract the mean and divide by the standard deviation of the individuals receiving neither the night sleep nor the nap interventions. The coefficients and confidence intervals are based on the estimates and standard errors in  Table\u00a0IV . The comparison group for the nap treatment is the pooled nap control group, that is, participants not assigned to the nap intervention. The outcome variables, described in more detail in  Section III.B , are as follows. Overall index: aggregates across all the outcomes in the table. Work: (i) earnings; (ii) productivity; (iii) active typing time; and (iv) output from the data entry task. Well-being: (i) \u201cWell-being Index,\u201d a composite index of the physical and mental well-being indices; (ii) \u201cPhysical,\u201d a physical well-being index, a composite of performance in an endline biking task, self-reported illnesses, self-reported pain, self-reported health, and blood pressure; and (iii) \u201cPsychological,\u201d a mental well-being index, a composite of self-reported depression, happiness, life possibility, life satisfaction, and stress. Cognition: (i) \u201cCognition Index,\u201d composite index of a lab-based and a work-based measure of cognitive function; (ii) \u201cLab Tasks,\u201d index of lab measures of attention, memory, and inhibitory control; and (iii) \u201cWork Task,\u201d measure of attention to piece rates in the data entry task. Preferences: (i) \u201cPreferences Index,\u201d composite index of time, social, and risk preference indices; (ii) \u201cTime,\u201d index capturing time preferences, including savings and present bias; (iii) \u201cSocial,\u201d index representing social preferences; and (iv) \u201cRisk,\u201d index representing risk preferences.   2. Work Outcomes The night sleep treatments did not cause significant improvements in productivity, labor supply, output, or earnings ( Table\u00a0IV , columns (2)\u2013(5)). Although the night sleep treatment groups were 1.3% more productive than the control group (column (3)), this difference of 0.02 std. dev. (std. err. = 0.02) is not statistically significant even without multiple-hypothesis testing corrections. The night sleep treatments reduced labor supply by approximately 10 minutes a day (0.08 std. dev. with std. err. = 0.02,  Table\u00a0IV , column (4)) with no change in days worked ( Online Appendix Figure\u00a0A.VII ). This effect on labor supply remains significant with  p  < .01 when correcting for multiple outcomes in the work family. Given the additional time in bed induced by the night sleep treatments, participants have less time available for work and leisure, which comes at the cost of reduced labor supply. Specifically, participants arrive at work six minutes later in the morning, take two minutes more of breaks at work, and leave for home three minutes earlier on average ( Online Appendix Table\u00a0A.X ). While perhaps obvious ex post, the opportunity costs of increasing sleep are typically neglected in the sleep literature. Indeed, the mean expert prediction was an increase in labor supply of 7%, which is strongly rejected by the data ( p  < .001). The small increase in productivity was not enough to outweigh the reduction in labor supply, leading to a small and marginally significant decrease in earnings and output, respectively (each 0.04 std. dev., std. err. = 0.02). 28  This finding is again in contrast to the mean (median) expert prediction of a 12% (7%) increase in output. The discrepancy can be partly explained by experts overestimating the productivity effects of increased sleep, and partly by their mispredicting that more sleep would increase the time allotted to work. Eighty-three percent of experts made point predictions outside of the 95% confidence interval of our estimate of the effect on output. Our results also contrast with those from natural experiments studying the economic consequences of sleep.  Gibson and Shrader (2018)  exploit variation in sunset times in the United States and estimate that 8.5 minutes of additional sleep per night increases earnings by 1.1% in the short run.  Giuntella and Mazzonna (2019)  use time zone border discontinuities in the United States and find that 19 fewer minutes of sleep are associated with 3% lower earnings. Extrapolating these estimates linearly to our experiment would predict 3.5% and 4.3% increases in earnings, respectively, which we firmly reject ( p  < .01).   3. Well-Being Increased night sleep did not significantly improve physical or psychological well-being ( Table\u00a0IV , columns (6)\u2013(8)). We find no positive effect on the index variable that combines the various measures of psychological well-being, in contrast to a largely observational literature that shows associations between self-reported sleep duration or quality and psychological well-being  (Kahneman and Krueger 2006 ;  Hamilton et\u00a0al. 2007 ;  Zhang et\u00a0al. 2017 ). In fact, the point estimate is negative (\u22120.05 std. dev., std. err. = 0.06,  Table\u00a0IV , column (8)). The individual components of this measure also show no significant improvements ( Online Appendix Table\u00a0A.XI ). Similarly, we find no significant effects of increased night sleep on an index of physical well-being ( Table\u00a0IV , column (7)) that combines objective and self-reported measures of health status. We do find positive (not significant) point estimates for some of the underlying components such as performance in a cycling task and self-reported illness, pain, and daily activity ( Online Appendix Table\u00a0A.XI ). Of course, three weeks is a short time for effects on physical health and behaviors to emerge. It could be that a longer intervention would generate health improvements in line with the observational literature  (e.g., Strine and Chapman 2005 ;  Cappuccio et\u00a0al. 2008 ;  Giuntella and Mazzonna 2019 ). It is also worth noting that the disaggregated analysis paints a slightly different picture when it comes to the effects on physical (but not psychological) well-being.  Online Appendix Table\u00a0A.VIII  reports that the night sleep only treatment has a positive effect on physical well-being (0.12 std. dev.,  p  = .06)\u2014as does the nap only treatment (0.16 std. dev.,  p  = .02)\u2014when one allows the night sleep and nap cell to have a distinct effect. The interpretation is that the night sleep treatments do increase physical well-being in a population that does not receive any naps, but have no average effect on physical well-being in a population where half the people already take naps. Night sleep and naps thus appear to have a negative interaction effect on physical well-being.   4. Cognition We find no effects of increased night sleep on cognition ( Table\u00a0IV , columns (9)\u2013(11)). There is no significant effect on an index of laboratory measures of attention, memory, and inhibitory control ( Table\u00a0IV , column (10)), or on any of the individual outcomes, which closely mimic measures used in laboratory studies ( Online Appendix Table\u00a0A.XII ). A large body of sleep studies shows that inducing sleep deprivation\u2014typically by keeping participants up all night\u2014substantially worsens performance on these tasks  (Lim and Dinges 2010 ;  Killgore 2010 ). The more modest but sustained and policy-relevant increases in sleep in our setting do not have a corresponding positive effect. We also find no evidence of effects on attention measured in a more economic domain: how much people react to salient versus nonsalient incentives ( Table\u00a0IV , column (11)). Consistent with limited attention, participants in the control group reacted 16% less to high incentives when piece rates were nonsalient ( Online Appendix Table\u00a0A.XIII ). Participants in the night sleep treatments behave quite similarly, reacting 15% less to incentives when they were nonsalient. Increased night sleep did thus not close the gap between responses to salient and nonsalient incentives.   5. Preferences Consistent with the lack of positive effects of increased night sleep on the above outcomes, we find no evidence of the night sleep treatments affecting time, risk, or social preferences, or on an index combining these outcomes ( Table\u00a0IV , columns (12)\u2013(15)). We detect no significant effect on an index of two measures of time preferences: savings and present bias ( Table\u00a0IV , column (13)). The night sleep treatments did not meaningfully affect savings behavior, leaving deposits and accumulated interest unaffected ( Online Appendix Table\u00a0A.XIV, Panel A ). Similarly, we find no effects on present bias estimated from effort choices. We do detect present bias in the control group (\u03b2 = 0.92,  Online Appendix Table\u00a0A.XIV, Panel B ). However, increased sleep does not significantly shift this parameter, in contrast to the view that sleep replenishes self-control ( Vohs and Baumeister, 2016 ). We also find no evidence of altered risk aversion, loss aversion, or social preferences in standard experimental tasks ( Table\u00a0IV , column (14)\u2013(15)), in contrast to the findings of  McKenna et\u00a0al. (2007) ,  Dickinson and McElroy (2017) ,  Anderson and Dickinson (2010) , and  Holbein, Schafer, and Dickinson (2019) . Although the results are not precise enough to detect small effects, we can rule out changes greater than 0.16 std. dev. at the 95% level for each of these outcomes ( Online Appendix Table\u00a0A.XV ).   6. Heterogeneity We do not find significant evidence of heterogeneity in the effects of the night sleep treatments.  Online Appendix Table\u00a0A.XVI  considers effects on the overall index variable and tests for heterogeneity by a number of baseline covariates. Baseline night sleep duration, sleep efficiency, and propensity to nap prior to the study do not interact significantly with the night sleep treatments. Nor do demographics such as gender and age. 29   IV.E. Effect of Naps 1. Overview Naps improved outcomes across a range of domains ( Figure\u00a0VI ).  Table\u00a0IV , column (1) reports that naps had an economically meaningful and statistically significant effect on the overall summary index (0.12 std. dev., std. err. = 0.04,  p  < .01). As shown in  Online Appendix Table\u00a0A.IX , the effect is significant whether naps are compared to taking enforced breaks (0.15 std. dev.,  p  < .01) or to working through the afternoon (0.08 std. dev.,  p  = .03). Given the lack of evidence on the impacts of naps on economically meaningful outcomes in real-world settings  (Lovato and Lack 2010 ;  Ficca et\u00a0al. 2010 ), this is an important result in itself. In addition, these results serve as a proof of concept that sleep can significantly alter many of the outcomes we study within a short time frame.   2. Work Outcomes Naps increased productivity. Participants randomized to naps were 2.3% (0.04 std. dev., std. err. = 0.02,  p  = .06) more productive on average over the day ( Table\u00a0IV , column (3)). 30  This effect is sizable, given that productivity is quite inelastic: quadrupling the piece rate increased productivity by only 14%. The productivity effects of naps are similar when compared with the break or the work counterfactuals, suggesting that the effects are due to sleep rather than merely resting ( Online Appendix Table\u00a0A.IX ).  Online Appendix Figure\u00a0A.VIII  shows that the effects are larger in the afternoon (2.7%) but also evident in the morning (1.9%), suggesting either cumulative effects of regular napping or that participants work harder in the morning in anticipation of the nap. 31 By design, nap participants could not work during the 30-minute nap period. They could adjust their labor supply outside of this period, say, by staying at the office longer. We find no evidence of such adjustments ( Table\u00a0IV , column (4)): The nap group spends almost exactly as much time typing as those in the control group on their break days, and 26 minutes (0.20 std. dev.) fewer than the control group on its work days ( Online Appendix Table\u00a0A.IX ). The effects of naps on output and earnings depend on the comparison group ( Online Appendix Table\u00a0A.IX ). Compared to taking a break, naps increased total output by 0.05 std. dev. ( p  = .02). Compared to working, naps instead reduced output by 0.07 std. dev. ( p  < .01). Earnings closely track output: naps increased overall earnings by Rs. 11 a day (0.05 std. dev.,  p  = .05) compared with taking a break, a sizable increase of 4.1%. In contrast, taking time to nap lowered earnings by Rs. 23 (8.3% or 0.10 std. dev.,  p  < .01) compared to simply working through the break. 32   3. Well-Being Naps significantly improved well-being ( Table\u00a0IV , columns (6)\u2013(8)). The overall effect of 0.08 std. dev. ( p  = .03) is driven by a 0.12 std. dev. ( p  = .04) improvement in psychological well-being. The point estimates for all individual components of psychological well-being are positive, with significant effects on happiness, life satisfaction, and sense of life possibility ( Online Appendix Table\u00a0A.XI ). The lack of significant effects on physical well-being is perhaps unsurprising, given the limited effects of naps on total sleep. Moreover, physical health benefits of sleep may require more time to emerge, and some of the outcomes, such as the cycling task, were conducted at the end of the study on a day without naps. 33   4. Cognition Naps boost the cognition index\u2014which combines lab measures of cognition with a measure of attention at work\u2014by 0.10 std. dev. ( p  = .08;  Table\u00a0IV  column (9)). The lab measures of cognition increase by 0.08 std. dev. ( p  = .07; column (10)) on average, driven by an effect on attention ( Online Appendix Table\u00a0A.XII ). We find no significant effects on inhibitory control or memory, in contrast to the sleep literature that tends to find broad effects of sleep on many aspects of cognition  (Killgore 2010 ;  Lim and Dinges 2010 ). We also find that naps increased participants\u2019 attention to work incentives by 0.20 std. dev. ( p  = .07,  Online Appendix Table\u00a0IV, column (11) ). The nap group was nearly fully attentive to nonsalient incentives, reacting to them about as much as they reacted to salient incentives ( Online Appendix Table\u00a0A.XIII ). This result illustrates the improved attentional resources provided by naps in a real-world work environment.   5. Preferences Naps have a positive but not significant effect (0.07 std. dev.,  p  = .15) on the index corresponding to the preferences family ( Table\u00a0IV , column (12)). In this family, we find a positive but insignificant effect of naps on an index of patience (0.13 std. dev.,  p  = .20,  Table\u00a0IV , column (13)). This index combines two real-stakes measures of time preferences: savings at the study office and present bias in an effort-discounting task. Naps caused a 14% increase in deposits and a 13% increase in daily net savings (deposits minus withdrawals,  Online Appendix Table\u00a0A.XIV, Panel A ). These effects are sizable: randomly providing a 1 percentage point higher daily interest rate increased deposits by 31%. The nap group earned 19% more interest over the course of the study, although this effect is imprecisely estimated. 34 Naps also reduce present bias in a real-effort task ( Online Appendix Table\u00a0A.XIV, Panel B ). We estimate an average present bias parameter of \u03b2 = 0.92 in the control group. 35  The nap treatment significantly (unadjusted  p  < .05) reduces present bias to \u03b2 = 0.98, and time preferences in the nap group are statistically indistinguishable from exponential discounting (i.e., \u03b2 = 1). Naps have no significant effects on social or risk preferences ( Table\u00a0IV , columns (14)\u2013(15)). The nap group is not more willing to accept risk or probabilistic losses ( Online Appendix Table\u00a0A.XV ). The same table reports that nap participants send 0.16 std. dev. more (std. err. = 0.10) in dictator games, but not in ultimatum or trust games. Nor do naps affect the choices of receivers in ultimatum or trust games. It is worth noting, however, that the measures of social and risk preferences were elicited in the morning, before daily naps, while time preferences were instead measured in the afternoon ( Online Appendix Table\u00a0A.III ).   6. Heterogeneity As with the night sleep treatments, we do not detect statistically significant heterogeneity in the overall effects of naps by baseline sleep or demographic characteristics ( Online Appendix Table\u00a0A.XVI ). Notably, those who self-reported napping in the week prior to the study did not see higher effects from the nap treatment. This partly alleviates concerns that the treatment effects capture mainly the effect of reducing naps among the control participants rather than increasing naps in the treatment group.   IV.F. Nap versus Night Sleep Treatments Table\u00a0IV  reports  p -values for differences in the effects of the night sleep and nap treatments on each outcome. We find clear evidence that naps and night sleep treatments have significantly different effects on the overall index variable that combines all outcomes ( p  = .02). We are relatively underpowered to instead compare the effects on individual outcomes. The only individual outcome on which naps have a significantly different effect than the night sleep treatments is psychological well-being ( p  = .04). The differences in effects on the indices of well-being, cognition, and preferences are not significant, although the point estimates are larger for naps in each case. For earnings, the point estimates for naps and night sleep treatments are similar and not significantly different. The nap treatment has larger overall effects than the night sleep treatment, despite causing smaller increases in total (24-hour) sleep.  Online Appendix Figure\u00a0A.IX  plots the overall treatment effects for the five disaggregated treatments against the increase in 24-hour sleep generated by the treatment. The figure shows no evidence of a dose response to 24-hour sleep or to night sleep: treatments with larger effects on overall sleep do not have larger effects on the summary of downstream outcomes. In contrast, treatment combinations involving naps have distinctly larger effects than the ones without naps. Naps at the office and nighttime sleep at home are not one-for-one substitutes in our setting, in contrast to some evidence from sleep lab experiments ( Mollicone, Van\u00a0Dongen, and Dinges 2007 ). To formally test for differences in the effects of nighttime sleep and nap sleep per minute of sleep, we turn to an instrumental variable (IV) analysis. Specifically, we estimate an IV regression with duration of nighttime sleep and nap sleep as endogenous variables and the five different combinations of treatments as instruments ( Online Appendix Table\u00a0A.XVII ). 36  As expected given the results above, each additional minute of nap sleep has a substantial effect on the overall index (0.008 std. dev. per minute, or 0.46 std. dev. per hour), significantly different from a minute of nighttime sleep ( p  < .01). However, the per minute benefits of naps and nighttime sleep also differ significantly for the preferences ( p  = .08) and cognition ( p  = .08) indices and suggestively for the well-being index ( p  = .11). 37  The per minute effects are also statistically different\u2014more positive for nap sleep\u2014for psychological well-being ( p  = .05) and the lab measures of cognition ( p  = .08). However, naps also lead to larger reductions in labor supply per minute of sleep than nighttime sleep ( p  = .04). Understandably, a minute of daytime sleep crowds out more work than a minute of nighttime sleep.   V. Discussion and Conclusion Using state-of-the-art objective measurements, we document a novel fact about sleep in India: low-income adults in Chennai are severely sleep-deprived by usual standards. The strikingly low sleep duration and efficiency in our sample could be a widespread but underappreciated feature of the lives of the urban poor in developing areas. Despite the lack of short-run benefits from sleeping more in our setting, severe sleep deprivation may have serious long-run effects on health and well-being and calls for policy makers\u2019 and researchers\u2019 attention. More systematic research on sleep in developing countries is needed to establish the external validity of our findings and study longer-term effects. In our setting, substantial increases in sleep duration were achievable through more time in bed, a change that at least in principle lies within people\u2019s choice sets. Increasing sleep efficiency, however, appears to be more difficult. Providing people with tips regarding good sleep hygiene and devices to make their sleep environment more comfortable did not increase efficiency; nor did incentives to achieve more actual sleep (which substantially reward higher sleep efficiency). As a result, increasing sleep duration entailed significant opportunity costs for our study sample. We find no positive effects of increased night sleep on any of our outcomes, contrasting with predictions made by sleep scientists and economists and evidence from many sleep lab experiments (e.g.,  Van Dongen et\u00a0al. 2003 ;  Lim and Dinges 2010 ;  Killgore 2010 ), and a much smaller body of recent work in economics that uses natural experiments (such as variation in sunset time) to study the effects of sleep  (Gibson and Shrader 2018 ;  Giuntella and Mazzonna 2019 ;  Jagnani 2018 ). It is more consistent with some recent evidence from the field:  Avery, Giuntella, and Jiao (2019)  find only small gains in academic achievement from inducing college students to increase nighttime sleep. What explains this unexpected finding? One plausible explanation is the much lower sleep quality\u2014as proxied by sleep efficiency\u2014in our setting compared with those previously studied in rich countries. The low-quality sleep we discovered in Chennai may not offer the same marginal benefits as the sleep typically available in higher-income settings. A more provocative possibility is that the findings from lab studies may not generalize to the field, even in rich countries. The lab experiments used in sleep science induce severe (often total) sleep deprivation (e.g.,  Lim and Dinges 2010 ) and typically lack steep incentives to perform well on tasks. We instead study the impacts of a more modest and arguably policy-relevant increase in sleep on highly incentivized outcomes. As one of the first studies of the causal effects of sleep in low-income settings, our experiment is not designed to adjudicate these different possible explanations. It does, however, point to the value of an economic perspective on sleep, which considers sleep as a choice variable, and measures both the benefits and the opportunity costs of sleeping more. Our results do not imply that more dramatic changes in sleep environments (e.g., improved housing, noise regulations) or in psychological factors hindering sleep (such as stress) could not have large effects. Improving sleep quality could potentially generate more sleep (due to higher sleep efficiency) and greater benefits from each minute of sleep. Identifying interventions to improve sleep efficiency in contexts like ours, and testing whether increased sleep efficiency unlocks the benefits found in sleep research in rich countries, would also be valuable. It could also be that the benefits of increased night sleep manifest over longer time horizons. Consistent with the hypothesis that increased sleep can have meaningful effects, we find the nap treatment has a significant positive effect on an overall index of outcomes, with positive effects on productivity, well-being, and cognition. The positive effect of naps is an important finding in itself. Naps are a common feature of life around the world, and even more so in tropical settings, where afternoon work may be less productive ( Dinges 1992 ). Yet we know very little about the economic costs and benefits of naps in real-world settings, as concluded by  Ficca et\u00a0al. (2010)  in a recent review. As work in developing countries shifts from self-employment to working in firms with set hours, naps may be crowded out. Our findings suggest that this could be costly, because naps provide workers with benefits that night sleep does not. Large firms may be beginning to recognize this, as evidenced by the adoption of nap policies by employers such as Google and Nike ( Walker 2017 , chapter 15). Given the forgone work time and logistical costs of offering naps, an employer\u2019s decision to provide naps may depend on how much they value their workers\u2019 psychological well-being and attention in the particular work setting. An obvious question raised by our results is why naps were effective when a larger increase in night sleep had no effect. Naps and night sleep are clearly not substitutes in our setting ( Online Appendix Figure\u00a0A.IX  and  Table\u00a0A.XVII ). Naps occurred in a pleasant office environment and may therefore have been higher quality than night sleep. Alternatively, naps may simply enter the production function of our outcomes differently than marginal increases in night sleep due to their timing. Naps were timed to coincide with the circadian dip in the mid-afternoon, when individuals are prone to sleepiness and impaired performance. A short burst of sleep during the circadian dip has been shown to be particularly valuable  (Takahashi 2003 ;  Lovato and Lack 2010 ). Perhaps the broadest question our research raises is to what extent the sizable impacts of sleep in lab studies generalize to field settings. Measuring sleep in the field\u2014made possible by wearables\u2014allows such experiments linking sleep to real-world economic outcomes in a wider range of environments ( Handel and Kolstad 2017 ). We evaluated labor market effects of sleep in the particular context of data entry work. Does sleep matter differently in less cognitively demanding or more physical jobs? In contexts where work is independent or collaborative? For children, as in  Jagnani (2018) ? Are naps in usual environments equally effective? Systematic measurement and tests across a wider variety of environments would facilitate the estimation of the overall impacts of sleep on the economy.   Data Availability Data and code replicating the tables and figures in this article can be found in  Bessone et\u00a0al. (2021)  in the Harvard Dataverse  https://doi.org/10.7910/DVN/GJ9QPC . References Anderson   Clare ,  Dickinson   David L. , \u201c Bargaining and Trust: The Effects of 36-h Total Sleep Deprivation on Socially Interactive Decisions ,\u201d  Journal of Sleep Research ,  19  ( 2010 ),  54 \u2013 63 . Anderson   Michael L. , \u201c Multiple Inference and Gender Differences in the Effects of Early Intervention: A Reevaluation of the Abecedarian, Perry Preschool, and Early Training Projects ,\u201d  Journal of the American Statistical Association ,  103  ( 2008 ),  1481 \u2013 1495 . Augenblick   Ned ,  Rabin   Matthew , \u201c An Experiment on Time Preference and Misprediction in Unpleasant Tasks ,\u201d  Review of Economic Studies ,  86  ( 2019 ),  941 \u2013 975 . Avery   Mallory ,  Giuntella   Osea ,  Jiao   Peiran , \u201c Why Don\u2019t We Sleep Enough? A Field Experiment among College Students ,\u201d ,  2019 . Banks   Siobhan ,  Dinges   David , \u201c Behavioral and Physiological Consequences of Sleep Restriction ,\u201d  Journal of Clinical Sleep Medicine ,  3  ( 2007 ),  519 \u2013 528 . Barnes   Christopher M. ,  Miller   Jared A. ,  Bostock   Sophie , \u201c Helping Employees Sleep Well: Effects of Cognitive Behavioral Therapy for Insomnia on Work Outcomes ,\u201d  Journal of Applied Psychology ,  102  ( 2017 ),  104 . Barnes   Christopher M. ,  Schaubroeck   John ,  Huth   Megan ,  Ghumman   Sonia , \u201c Lack of Sleep and Unethical Conduct ,\u201d  Organizational Behavior and Human Decision Processes ,  115  ( 2011 ),  169 \u2013 180 . Basner   Mathias ,  Dinges   David F. , \u201c Maximizing Sensitivity of the Psychomotor Vigilance Test (PVT) to Sleep Loss ,\u201d  Sleep ,  34  ( 2011 ),  581 \u2013 591 . Bessone   Pedro ,  Rao   Gautam ,  Schilbach   Frank ,  Schofield   Heather ,  Toma   Mattie , \u201c Replication Data for: \u2018The Economic Consequences of Increasing Sleep among the Urban Poor\u2019 ,\u201d ( 2021 ), . Camerer   Colin F. ,  Behavioral Game Theory: Experiments in Strategic Interaction  ( Princeton, NJ :  Russell Sage Foundation/Princeton University Press ,  2003 ). Cappuccio   Francesco P. ,  Taggart   Frances M. ,  Kandala   Ngianga-Bakwin ,  Currie   Andrew ,  Peile   Ed ,  Stranges   Saverio ,  Miller   Michelle A. , \u201c Meta-Analysis of Short Sleep Duration and Obesity in Children and Adults ,\u201d  Sleep ,  31  ( 2008 ),  619 \u2013 626 . Carrier   Julie ,  Land   Stephanie ,  Buysse   Daniel J. ,  Kupfer   David J. ,  Monk   Timothy H. , \u201c The Effects of Age and Gender on Sleep EEG Power Spectral Density in the Middle Years of Life (Ages 20\u201360 Years Old) ,\u201d  Psychophysiology ,  38  ( 2001 ),  232 \u2013 242 . Castro   L. ,  Poyares   D. ,  Leger   D. ,  Bittencourt   L. ,  Tufik   S. , \u201c Objective Prevalence of Insomnia in the Sao Paulo, Brazil Epidemiologic Sleep Study ,\u201d  Annals of Neurology ,  74  ( 2013 ),  537 \u2013 546 . Cespedes   Elizabeth M. ,  Hu   Frank B. ,  Redline   Susan ,  Rosner   Bernard ,  Alcantara   Carmela ,  Cai   Jianwen ,  Hall   Martica H ,  Loredo   Jose S. ,  Mossavar-Rahmani   Yasmin ,  Ramos   Alberto R.  et al.\u00a0 , \u201c Comparison of Self-Reported Sleep Duration with Actigraphy: Results from the Hispanic Community Health Study/Study of Latinos Sue\u00f1o Ancillary Study ,\u201d  American Journal of Epidemiology ,  183  ( 2016 ),  561 \u2013 573 . Charness   Gary ,  Gneezy   Uri ,  Imas   Alex , \u201c Experimental Methods: Eliciting Risk Preferences ,\u201d  Journal of Economic Behavior & Organization ,  87  ( 2013 ),  43 \u2013 51 . Chetty   Raj ,  Looney   Adam ,  Kroft   Kory , \u201c Salience and Taxation: Theory and Evidence ,\u201d  American Economic Review ,  99  ( 2009 ),  1145 \u2013 1177 . Cole   Roger J. ,  Kripke   Daniel F. ,  Gruen   William ,  Mullaney   Daniel J. ,  Christian Gillin   J. , \u201c Automatic Sleep/Wake Identification from Wrist Activity ,\u201d  Sleep ,  15  ( 1992 ),  461 \u2013 469 . Dean   Emma ,  Schilbach   Frank ,  Schofield   Heather , \u201c Poverty and Cognitive Function ,\u201d in  The Economics of Poverty Traps .  Barrett   Christopher B. ,  Carter   Michael R. ,  Chavas   Jean-Paul , eds. ( Chicago: NBER/University of Chicago Press ,  2019 ),  57 \u2013 118 . DellaVigna   Stefano ,  Pope   Devin ,  Vivalt   Eva , \u201c Predict Science to Improve Science ,\u201d  Science ,  366  ( 2019 ),  428 \u2013 429 . Dickinson   David L. ,  McElroy   Todd , \u201c Sleep Restriction and Circadian Effects on Social Decisions ,\u201d  European Economic Review ,  97  ( 2017 ),  57 \u2013 71 . Dinges   David F. , \u201c Adult Napping and Its Effects on Ability to Function ,\u201d  Why We Nap  in  Stampi   Claudio , ed. ( Boston: Birkh\u00e4user ,  1992 ,  118 \u2013 134 . Ficca   Gianluca ,  Axelsson   John ,  Mollicone   Daniel ,  Muto   Vincezo ,  Vitiello   Michael , \u201c Naps, Cognition, and Performance ,\u201d  Sleep Medicine Reviews ,  14  ( 2010 ),  240 \u2013 258 . Gershon   Anda ,  Thompson   Wesley K. ,  Eidelman   Polina ,  McGlinchey   Eleanor L. ,  Kaplan   Katherine A. ,  Harvey   Allison G. , \u201c Restless Pillow, Ruffled Mind: Sleep and Affect Coupling in Interepisode Bipolar Disorder ,\u201d  Journal of Abnormal Psychology ,  121  ( 2012 ),  863 . Gibson   Matthew ,  Shrader   Jeffrey , \u201c Time Use and Labor Productivity: The Returns to Sleep ,\u201d  Review of Economics and Statistics ,  100  ( 2018 ),  783 \u2013 798 . Gildner   Theresa ,  Liebert   Melissa ,  Kowal   Paul ,  Chatterji   Somnath ,  Snodgrass   Josh , \u201c Associations between Sleep Duration, Sleep Quality, and Cognitive Test Performance among Older Adults from Six Middle Income Countries: Results from the Study on Global Ageing and Adult Heath (SAGE) ,\u201d  Sleep ,  10  ( 2014 ),  613 \u2013 621 . Giuntella   Osea ,  Mazzonna   Fabrizio , \u201c Sunset Time and the Economic Effects of Social Jetlag: Evidence from US Time Zone Borders ,\u201d  Journal of Health Economics ,  65  ( 2019 ),  210 \u2013 226 . Hamilton   Nancy A. ,  Nelson   C. A. ,  Stevens   N. ,  Kitzman   Heather , \u201c Sleep and Psychological Well-Being ,\u201d  Social Indicators Research ,  82  ( 2007 ),  147 \u2013 163 . Handel   Benjamin ,  Kolstad   Jonathan , \u201c Wearable Technologies and Health Behaviors: New Data and New Methods to Understand Population Health ,\u201d  American Economic Review ,  107  ( 2017 ),  481 \u2013 485 . Hedner   Jan ,  Pillar   Giora ,  Pittman   Stephen D. ,  Zou   Ding ,  Grote   Ludger ,  White   David P. , \u201c A Novel Adaptive Wrist Actigraphy Algorithm for Sleep-Wake Assessment in Sleep Apnea Patients ,\u201d  Sleep ,  27  ( 2004 ),  1560 \u2013 1566 . Hirshkowitz   Max ,  Whiton   Kaitlyn ,  Albert   Steven M. ,  Alessi   Cathy ,  Bruni   Oliviero ,  DonCarlos   Lydia ,  Hazen   Nancy ,  Herman   John  et al.\u00a0 , \u201c National Sleep Foundation\u2019s Sleep Time Duration Recommendations: Methodology and Results Summary ,\u201d  Sleep Health ,  1  ( 2015 ),  40 \u2013 43 . Holbein   John B. ,  Schafer   Jerome P. ,  Dickinson   David L. , \u201c Insufficient Sleep Reduces Voting and Other Prosocial Behaviours ,\u201d  Nature Human Behaviour ,  3  ( 2019 ),  492 . Holt   Charles A. ,  Laury   Susan K. , \u201c Risk Aversion and Incentive Effects ,\u201d  American Economic Review ,  92  ( 2002 ),  1644 \u2013 1655 . Jackson   Chandra L. ,  Patel   Sanjay R. ,  Braxton Jackson II   W. ,  Lutsey   Pamela L. ,  Redline   Susan , \u201c Agreement between Self-Reported and Objectively Measured Sleep Duration among White, Black, Hispanic, and Chinese Adults in the United States: Multi-Ethnic Study of Atherosclerosis ,\u201d  Sleep ,  41  ( 2018 ),  1 \u2013 12 . Jagnani   Maulik , \u201c Poor Sleep: Sunset Time and Human Capital Production ,\u201d  Mimeo ,  2018 . Jin   Lawrence ,  Ziebarth   Nicolas R. , \u201c Sleep, Health, and Human Capital: Evidence from Daylight Saving Time ,\u201d  Journal of Economic Behavior & Organization ,  170  ( 2020 ),  174 \u2013 192 . Kahneman   Daniel ,  Krueger   Alan , \u201c Development in the Measurement of Subjective Well-Being ,\u201d  Journal of Economic Perspectives ,  20  ( 2006 ),  3 \u2013 24 . Kaur   Supreet ,  Kremer   Michael ,  Mullainathan   Sendhil , \u201c Self-Control at Work ,\u201d  Journal of Political Economy ,  123  ( 2015 ),  1227 \u2013 1277 . Killgore   William , \u201c Effects of Sleep Deprivation on Cognition ,\u201d  Progress in Brain Research ,  185  ( 2010 ),  105 \u2013 129 . Knutson   Kristen L. , \u201c Sleep Duration, Quality, and Timing and Their Associations with Age in A Community without Electricity in Haiti ,\u201d  American Journal of Human Biology ,  26  ( 2014 ),  80 \u2013 86 . Kremer   Michael ,  Rao   Gautam ,  Schilbach   Frank , \u201c Behavioral Development Economics ,\u201d in  Handbook of Behavioral Economics: Foundation and Applications , vol.  2 , , eds. ( Amsterdam: Elsevier ,  2019 ),  345 \u2013 458 . Kurina   Lianne M. ,  Thisted   Ronald A. ,  Chen   Jen-Hao ,  McClintock   Martha K. ,  Waite   Linda J. ,  Lauderdale   Diane S. , \u201c Actigraphic Sleep Characteristics among Older Americans ,\u201d  Sleep Health ,  1  ( 2015 ),  285 \u2013 292 . Kushida   Clete A. ,  Chang   Arthur ,  Gadkary   Chirag ,  Guilleminault   Christian ,  Carrillo   Oscar ,  Dement   William C. , \u201c Comparison of Actigraphic, Polysomnographic, and Subjective Assessment of Sleep Parameters in Sleep-Disordered Patients ,\u201d  Sleep Medicine ,  2  ( 2001 ),  389 \u2013 396 . Lauderdale   Diane S. ,  Knutson   Kristen L. ,  Yan   Lijing L. ,  Liu   Kiang ,  Rathouz   Paul J. , \u201c Sleep Duration: How Well Do Self-Reports Reflect Objective Measures? The CARDIA Sleep Study ,\u201d  Epidemiology ,  19  ( 2008 ),  838 \u2013 845 . Lichstein   K. ,  Stone   K. ,  Donaldson   J. ,  Nau   S. ,  Soeffing   J. ,  Murray   D. ,  Lester   K. ,  Aguillard   N. , \u201c Actigraphy Validation with Insomnia ,\u201d  Sleep ,  2  ( 2006 ),  232 \u2013 239 . Lim   Julian ,  Dinges   David F. , \u201c A Meta-Analysis of the Impact of Short-Term Sleep Deprivation on Cognitive Variables ,\u201d  Psychological Bulletin ,  136  ( 2010 ),  375 \u2013 389 . Lovato   Nicole ,  Lack   Leon , \u201c The Effects of Napping on Cognitive Functioning ,\u201d  Progress in Brain Research ,  185  ( 2010 ),  155 \u2013 166 . Marino   Miguel ,  Li   Yi ,  Rueschman   Michael N. ,  Winkelman   J. W. ,  Ellenbogen   J. M. ,  Solet   J. ,  Dulin   Hilary ,  Berkman   Lisa F. ,  Buxton   Orfeu M. , \u201c Measuring Sleep: Accuracy, Sensitivity, and Specificity of Wrist Actigraphy Compared to Polysomnography ,\u201d  Sleep ,  36  ( 2013 ),  1747 \u2013 1755 . McKenna   Benjamin S. ,  Dickinson   David L. ,  Orff   Henry J. ,  Drummond   Sean P. A. , \u201c The Effects of One Night of Sleep Deprivation on Known-Risk and Ambiguous-Risk Decisions ,\u201d  Journal of Sleep Research ,  16  ( 2007 ),  245 \u2013 252 . McKenzie   David , \u201c Beyond Baseline and Follow-up: The Case for More T in Experiments ,\u201d  Journal of Development Economics ,  99  ( 2012 ),  210 \u2013 221 . Mollicone   D. ,  Van Dongen   H. ,  Dinges   D. , \u201c Optimizing Sleep/Wake Schedules in Space: Sleep during Chronic Nocturnal Sleep Restriction with and without Diurnal Naps ,\u201d  Acta Astronautica ,  60  ( 2007 ),  354 \u2013 361 . Mollicone   D. ,  Van Dongen   H. ,  Rogers   N. ,  Dinges   D. , \u201c Response Surface Mapping of Neurobehavioral Performance: Testing the Feasibility of Split-Sleep Schedules for Space Operations ,\u201d  Acta Astronautica ,  63  ( 2008 ),  833 \u2013 840 . Muralidharan   Karthik ,  Romero   Mauricio ,  W\u00fcthrich   Kaspar , \u201c Factorial Designs, Model Selection, and (Incorrect) Inference in Randomized Experiments ,\u201d ,  2019 . Nicholson   A. ,  Pascoe   P. ,  Roehrs   T. ,  Roth   T. ,  Spencer   M. ,  Stone   M. , \u201c Sustained Performance with Short Evening and Morning Sleeps ,\u201d  Aviation, Space, and Environmental Medicine ,  56  ( 1985 ),  105 \u2013 114 . Ohayon   Maurice ,  Wickwire   Emerson M. ,  Hirshkowitz   Max ,  Albert   Steven M. ,  Avidan   Alon ,  Daly   Frank J. ,  Dauvilliers   Yves ,  Ferri   Raffaele ,  Fung   Constance ,  Gozal   David  et al.\u00a0 , \u201c National Sleep Foundation\u2019s Sleep Quality Recommendations: First Report ,\u201d  Sleep Health ,  3  ( 2017 ),  6 \u2013 19 . Pilcher   J. ,  Michalowski   K. ,  Carrigan   R. , \u201c The Prevalence of Daytime Napping and its Relationship to Nighttime Sleep ,\u201d  Behavioral Medicine ,  27  ( 2001 ),  71 \u2013 76 . Riemann   Dieter ,  Perlis   Michael L. , \u201c The Treatments of Chronic Insomnia: A Review of Benzodiazepine Receptor Agonists and Psychological and Behavioral Therapies ,\u201d  Sleep Medicine Reviews ,  13  ( 2009 ),  205 \u2013 214 . Roure   Nuria ,  Gomez   Silvia ,  Mediano   Olga ,  Duran   Joaquin ,  de la Pe\u00f1a   Monica ,  Capote   Francisco ,  Teran   Joaquin ,  Masa   Juan Fernando ,  Alonso   Maria Luz ,  Corral   Jaime  et al.\u00a0 , \u201c Daytime Sleepiness and Polysomnography in Obstructive Sleep Apnea Patients ,\u201d  Sleep Medicine ,  9  ( 2008 ),  727 \u2013 731 . Sadeh   Avi , \u201c The Role of Actigraphy in Sleep Medicine: An Update ,\u201d  Sleep Medicine Review ,  15  ( 2011 ),  259 \u2013 267 . Sadeh   A. ,  Hauri   P. ,  Kripke   D. ,  Lavie   P. , \u201c The Role of Actigraphy in the Evaluation of Sleep Disorders ,\u201d  Sleep ,  18  ( 1995 ),  288 \u2013 302 . Schilbach   Frank , \u201c Alcohol and Self-Control: A Field Experiment in India ,\u201d  American Economic Review ,  109  ( 2019 ),  1290 \u2013 1322 . Schokman   Aaron ,  Bin   Yu Sun ,  Simonelli   Guido ,  Pye   Jonathon ,  Morris   Richard ,  Sumathipala   Athula ,  Siribaddana   Sisira H. ,  Hotopf   Matthew ,  Rijsdijk   Fruhling ,  Jayaweera   Kaushalya  et al.\u00a0 , \u201c Agreement between Subjective and Objective Measures of Sleep Duration in a Low-Middle Income Country Setting ,\u201d  Sleep Health ,  4  ( 2018 ),  543 \u2013 550 . Schultz   Theodore William ,  Transforming Traditional Agriculture  ,  1964 ). \u00a0 Selvamani   Y. ,  Arokiasamy   Perianayagam ,  Chaudhary   Mamta ,  Himanshu , \u201c Association of Sleep Problems and Sleep Duration with Self-Rated Health and Grip Strength among Older Adults in India and China: Results from the Study on Global Aging and Adult Health (SAGE) ,\u201d  Journal of Public Health ,  26  ( 2018 ),  697 \u2013 707 . Simonelli   Guido ,  Marshall   Nathaniel S. ,  Grillakis   Antigone ,  Miller   Christopher B. ,  Hoyos   Camilla M. ,  Glozier   Nick , \u201c Sleep Health Epidemiology in Low and Middle-Income Countries: A Systematic Review and Meta-Analysis of the Prevalence of Poor Sleep Quality and Sleep Duration ,\u201d  Sleep Health ,  4  ( 2018 ),  239 \u2013 250 . Smith   Austin C. , \u201c Spring Forward at Your Own Risk: Daylight Saving Time and Fatal Vehicle Crashes \u201d,  American Economic Journal: Applied Economics ,  8  ( 2016 ),  65 \u2013 91 . Smith   Michael T. ,  McCrae   Christina S. ,  Cheung   Joseph ,  Martin   Jennifer L. ,  Harrod   Christopher G. ,  Heald   Jonathan L ,  Carden   Kelly A. , \u201c Use of Actigraphy for the Evaluation of Sleep Disorders and Circadian Rhythm Sleep-Wake Disorders: An American Academy of Sleep Medicine Clinical Practice Guideline ,\u201d  Journal of Clinical Sleep Medicine ,  14  ( 2018 ),  1231 \u2013 1237 . Stranges   Saverio ,  Tigbe   William ,  Gomze-Olive   Francesc ,  Thorogood   Margaret ,  Kandala   Ngianga-Bakwain , \u201c Sleep Problems: An Emerging Global Epidemic? Findings from the INDEPTH WHO-SAGE Study among More than 40,000 Older Adults from 8 Countries across Africa and Asia ,\u201d  Sleep ,  35  ( 2012 ),  1173 \u2013 1181 . Strine   T. ,  Chapman   D. , \u201c Associations of Frequent Sleep Insufficiency with Health-Related Quality of Life and Health Behaviors ,\u201d  Sleep Medicine ,  6  ( 2005 ),  23 \u2013 27 . Takahashi   M , \u201c The Role of Prescribed Napping in Sleep Medicine ,\u201d  Sleep Medicine Reviews ,  7  ( 2003 ),  227 \u2013 235 . Trauer   James M. ,  Qian   Mary Y. ,  Doyle   Joseph S. ,  Rajaratnam   Shantha M. W. ,  Cunnington   David , \u201c Cognitive Behavioral Therapy for Chronic Insomnia: A Systematic Review and Meta-Analysis ,\u201d  Annals of Internal Medicine ,  163  ( 2015 ),  191 \u2013 204 . Van Dongen   Hans P. A. ,  Maislin   Greg ,  Mullington   Janet M. ,  Dinges   David F. , \u201c The Cumulative Cost of Additional Wakefulness: Dose-Response Effects on Neurobehavioral Functions and Sleep Physiology from Chronic Sleep Restriction and Total Sleep Deprivation ,\u201d  Sleep ,  26  ( 2003 ),  117 \u2013 126 . Vohs   Kathleen D. ,  Baumeister   Roy F. ,  Handbook of Self-Regulation: Research, Theory, and Applications  ( New York: Guilford Press ,  2016 ). \u00a0 Wagner   David T. ,  Barnes   Christopher M. ,  Lim   Vivien K. G. ,  Lance Ferris   D. , \u201c Lost Sleep and Cyberloafing: Evidence from the Laboratory and a Daylight Saving Time Quasi-Experiment ,\u201d  Journal of Applied Psychology ,  97  ( 2012 ),  1068 . Walker   Matthew P. ,  Why We Sleep: Unlocking the Power of Sleep and Dreams  ( New York :  Scribner ,  2017 ). \u00a0 Watson   Nathaniel F. ,  Badr   Safwan M. ,  Belenky   Gregory ,  Bliwise   Donald L. ,  Buxton   Orfeu M. ,  Buysse   Daniel ,  Dinges   David F. ,  Gangwisch   James  et al.\u00a0 , \u201c Joint Consensus Statement of the American Academy of Sleep Medicine and Sleep Research Society on the Recommended Amount of Sleep for a Healthy Adult: Methodology and Discussion ,\u201d  Sleep ,  38  ( 2015 ),  1161 \u2013 1183 . Westfall   Peter H. ,  Young   S. Stanley ,  Resampling-Based Multiple Testing: Examples and Methods for p-Value Adjustment  ( New York: John Wiley & Sons ,  1993 ). \u00a0 Zhang   Jihui ,  Paksarian   Diana ,  Lamers   Femke ,  Hickie   Ian B. ,  He   Jianping ,  Ries Merikangas   Kathleen , \u201c Sleep Patterns and Mental Health Correlates in US Adolescents ,\u201d  Journal of Pediatrics ,  182  ( 2017 ),  137 \u2013 143 . \u00a9 The Author(s) 2021. Published by Oxford University Press on behalf of the President and Fellows of Harvard College."}, "210705_news_467490.txt": {"page_id": "210705_news_467490.txt", "text": "M ore than a decade has passed without any progress in bringing the global tax system into the modern age. But less than three months after taking office, President Joe Biden has raised hopes of a breakthrough, with  proposals that could kill tax havens dead and force multinationals to pay a fairer share of tax . The change in tone could not be more marked. With last week\u2019s proposal for a global minimum corporate tax rate, Washington has turned away from years of economic orthodoxy that stretched back to the early 1980s and prioritised a neoliberal world vision \u2013 of free-market competition, government indifference and unblinking advocacy of globalisation. Under proposals submitted to tax negotiators from 135 countries at the OECD, the Biden plan would force big companies to pay taxes where their revenues are earned, not where the profits can be shifted to. It would also establish a global minimum tax rate, agreed by the world\u2019s biggest economies. This is a powerful development. For years, big companies have weaved a merry dance through a broken patchwork of international tax rules, advised by an army of lawyers and accountants on where to locate to reduce their bills. The free-market economists favoured by presidents past would have argued for the advantages of globalisation: cheaper products, more choice. But profit-shifting by big companies \u2013 turbocharged in the digital age, with its unparalleled ease of doing business across borders \u2013 has left government coffers increasingly short. According to the Tax Justice Network, the sums lost to exchequers around the world have  risen as high as \u00a3311bn annually . This comes at a time when Covid is driving up national debts to eye-watering levels. Public anger at tax avoidance, and demands on companies to pay a fair share, have also risen since the 2008 financial crisis. US multinationals are serious offenders. The proportion of gross profits they shift into tax havens has soared from 5-10% in the 1990s to between 25% and 30% today. In a race to the bottom designed to attract big companies to competing jurisdictions, the average statutory corporate tax rate across 109 countries assessed by the OECD dropped from 28% at the turn of the millennium to 20.6% in 2020. This was a tactic ostentatiously deployed by George Osborne, who cut the UK rate from 28% a decade ago to the current level of 19% \u2013 with little apparent benefit. Some digital companies pay even lower global effective rates (an average of the tax paid across all jurisdictions a firm operates in). Recent figures show Amazon pays 11.8%, Apple 14.4% and Facebook 12.2%. For tax campaigners, the Biden intervention is a moment of hope. But the fear among progressives is that defeat will be snatched from the jaws of victory. For Biden, ending the race to the bottom would help his administration raise domestic corporate taxes from 21% to 28% without big companies threatening to up sticks and locate profits elsewhere. Much negotiation still remains to be done, not least on the rate at which a global minimum tax would be set. Washington wants 21% but several nations have much lower rates. An agreement among EU nations would not be easy, as rates range from 9% in Hungary and 12.5% in Ireland to 33% in France. Might the agreed rate be so low as to make the initiative meaningless? Might there be too many exemptions?  Possibly, but the hope is that, with the US on board and a warm reception from France and Germany, jurisdictions hosting a large enough slice of global economic activity will sign \u2013 effectively forcing compliance. After decades on the road to nowhere, global tax reform may at last be within reach. Cinemas and streamers fight like Godzilla v Kong Saviours come in all shapes and sizes, and this time the all-conquering heroes have taken the shape of a dinosaur and a giant ape.  Godzilla vs Kong  has  smashed through the pandemic torpor  and paved the way to recovery for what has been stuttering global box office. In its first week of release, the blockbuster pulled in more than \u00a3205m internationally, making it the best-performing debut since the pandemic began by some distance. Faced with finances stretched to breaking point, cinema owners will be breathing a sigh of relief, hoping that this success is proof that moviegoers haven\u2019t been permanently put off the big-screen experience. Hollywood studios have taken advantage of cinema lockdowns to experiment with putting some films straight on to streaming services, but this has not been considered a major success. In the US, the world\u2019s biggest movie market,  Godzilla vs Kong  was made available on WarnerMedia\u2019s HBO Max service at the same time as in theatres, and the big screen held its own. The $48.5m take for the first five days in the US, with only about half of screens open, has been seen as redemption following the flat box-office performance of would-be saviours  Wonder Woman 1984  and  Tenet  earlier in the pandemic. However, Hollywood studios have succeeded in their ambition of shrinking the once-sacrosanct exclusivity period that theatres enjoy from many months to a few weeks. In reality, it has always been the case that most films make the vast majority of ticket sales in a short period after premiere. The pandemic has provided an unprecedented testbed for studios and fuelled an enforced boom in home-based entertainment.Director Adam Wingard promised that his film would be the definitive \u201cdecider\u201d in the battle between Godzilla and King Kong. But cinema owners, streaming services and film studios may just feel that it has turned out to be a draw. Deliveroo flotation should teach Sunak to keep his opinions to himself One week on from  Deliveroo\u2019s car-crash of a flotation , the picture is getting worse. The shares fell by 10% on Friday to 254.5p, taking the tumble from the 390p listing price to slightly more than a third. A fall of that magnitude should dispel any thoughts that the flop can be explained solely as a protest by old-school City fund managers against the supercharged voting rights that Deliveroo founder Will Shu awarded himself. The governance set-up is indeed hated in many quarters, but  online retailer the Hut Group  also has a founder\u2019s controlling share and that IPO flew out of the traps last year. Deliveroo seems a straightforward case of overvaluation at launch. Goldman Sachs and JP Morgan Cazenove, the investment banks running the listing, got their numbers very wrong. Deliveroo still got its \u00a31bn of fresh capital and so has the chance to redeem itself. It has just annoyed the 70,000 of its customers who bought shares. But Rishi Sunak, the chancellor, who foolishly fuelled the Deliveroo hype in the hope of rebranding the London stock market as magnet for technology companies, badly needs to rethink his tactics. The first lesson the chancellor should learn is to keep his name away from individual stocks. Tech investing is a high-risk game. There will be big winners and big losers and, since the chancellor has no special skill in telling one from the other, it is very unwise to endorse specific companies on day one. Success for tech push will be judged by the number of companies coming to market. The second lesson is that the biotech sector, rather than the gig economy, is more likely to be fruitful territory. In the past fortnight,  Oxford Nanopore has chosen to list in London , while  Vaccitech opted for New York . Sunak would do better to spend his time trying to improve that conversion rate. And doing it quietly, behind the scenes."}, "210705_news_467491.txt": {"page_id": "210705_news_467491.txt", "text": "W ho wouldn\u2019t like to be a statue in Boris Johnson\u2019s Britain? Cherished by the powerful and honoured with collective gatherings, as Churchill\u2019s shrine was last week, by supporters of the  London mayoral candidate Laurence Fox . Without themselves needing to organise, these historically neglected members of the inanimate community have within the last few months secured privileges, protections and high-level advocacy that, in addition to their existing plinth status, falls only narrowly short of full suffrage \u2013 and even that cannot confidently be ruled out. Demonstrating that the government does have a heart, albeit one of stone, the culture secretary, Oliver Dowden, accepted that, where sentient arts freelancers could  manage unaided  in lockdown, statues needed help. Not just in their personal struggles against decay and pigeons but against official \u201c bullying \u201d (mercifully for the statues, largely of an ideological as opposed to a violent, Tory, intra-departmental nature). His department identified a \u201cnoisy minority of activists constantly trying to do Britain down\u201d. One respectable statue had been toppled purely because of his slave fortune. Another\u2019s plinth had been scrawled on. A few obscure stone dignitaries had been unceremoniously relocated from their old haunts, for all the world as if they were random old ladies blocking demolition of a  care home . Since these humiliations had hardly been invited by the effigies themselves, everything pointed to institutional statueism. New  laws  would therefore protect statues from ever again being disturbed without government permission. Precedents in historical  iconoclasm  \u2013 Henry VIII\u2019s and Cromwell\u2019s, say, or Johnson\u2019s  devastation of the London skyline  \u2013 were no excuse. Disobedient organisations could lose funding. And what of the statues\u2019 right to be out alone at night, without fear of assault or unwanted touching? In an emotional  article , sounding fully as insulted as any woman being lectured on how to dress by Imran Khan, Johnson lamented the  boarding up  of Churchill\u2019s statue before a Black Lives Matter protest. \u201cIt was outrageous,\u201d he wrote, \u201cthat anyone could even have claimed that the statue needed protection. It was and is miserable to see his statue entombed in its protective sheath.\u201d (Sheath? He\u2019s familiar, then, with the concept.) Yes, the prime minister conceded, such \u201coutrageous\u201d precautions predated BLM. But that, you gathered, occurred in the long period separating Pygmalion\u2019s very successful relationship with a statue from Johnson\u2019s 2019 accession, during which statues were widely considered lifeless, only transiently meaningful, possibly even pathetic relics of antique grandiosity. Maybe they didn\u2019t do Ozymandias at Eton. \u201cWhy attack Churchill?\u201d Johnson demanded. \u201cWhat has it come to when one of this country\u2019s greatest ever leaders \u2013 perhaps our greatest \u2013 has to be shielded from the wrath of the mob?\u201d If, outside  Britain First  and the  Daily Telegraph , many civilians were disappointingly apathetic on this point (only 33% disapproved of the Colston statue\u2019s removal), instructive penalties would re-educate them. The 10-year prison terms introduced in the new crime bill confirm that, in the punitive enforcement of statue veneration, Johnson\u2019s UK now competes with any efficiently run tyranny. \u201cThere has been widespread upset about the damage and desecration of memorials,\u201d the  Home Office lies , by way of explaining why statue damage may now be sentenced more severely than rape. \u201cIt has long been considered,\u201d it adds, just as vaguely, \u201cthat the law is not sufficiently robust in this area.\u201d It has? Can it point us to any manifesto promise, indeed to any serious, even fleeting Tory interest in statuary not of Margaret Thatcher, prior to the Bristol monument being immersed, retrieved and thereafter the sole pretext for  Telegraph  articles about baying/wrathful/imaginary mobs? Maybe any creative impulse is to be welcomed from a government so noted for philistinism Maybe shame about this earlier indifference helps explain the  intensity  of the government\u2019s current passion for monument protection (alas, excluding Stonehenge). Seeking respectable support for this idolatry, Johnson declares common cause, as per, with women. Notwithstanding Maggi Hambling\u2019s recent  catastrophe , there are feminists who think, understandably enough, that the lack of heroic female statuary is worth the effort of rectifying. The government\u2019s first demand, however, has been for a \u201c great big statue \u201d of Captain Tom, \u201cnot just for this generation to remember Captain Tom\u201d, said the vaccines secretary, Nadhim Zahawi, \u201cbut for future generations\u201d. And why not. It might even remind a future Tory government to close its borders in a pandemic. Maybe any creative impulse is to be welcomed from a government so noted for philistinism. But ministerial fetishisation may yet be a mixed blessing for landmark statues, in particular for the Churchill bronze installed in Parliament Square in 1973. Last year, Johnson vowed to resist its relocation with \u201cevery breath in my body\u201d. In practice, the greatest physical risk from this posturing was obviously to the statue. Again, thanks to Churchill\u2019s most embarrassing fan, the war leader is yet more firmly identified as \u2013 perhaps the worst thing for any public statue \u2013 pre-eminently a divisive figure, recognised as flawed by the left and thus ostentatiously venerated by the right. The more Johnson depicts it as worthy of martyrdom (Dowden favours Nelson\u2019s column) the more magnetic the site to smaller-time demagogues. Laurence Fox has duly rhapsodised, plinthside, on the various ways \u2013 from mask defiance to low traffic suspensions \u2013 he aspires, in a kind of reversal of blitz priorities, to shorten Londoners\u2019 lives. Even Churchill\u2019s admirers might welcome, if this is to be his statue\u2019s fate, its overdue  removal  to a place of safety. Long before the Tory project to cultivate public discord out of stone and bronze, the raising of figurative public statues had become a complicated, dated, probably doomed enterprise, a theme underlined by artists contributing to Trafalgar Square\u2019s fourth plinth. Indifference that protected the most dreadful old statues could not be extended to the new. Even if people agreed on a subject \u2013 eg Oscar Wilde \u2013 they were likely to be divided on the execution \u2013 eg Maggi Hambling. In characteristic style, Johnson\u2019s insane statue legislation is premised on an invention, a reverence that never existed."}, "210705_news_467494.txt": {"page_id": "210705_news_467494.txt", "text": "At 5am on a chilly Tuesday morning last month, 1,600 police officers and balaclava-wearing special forces, bristling with arms and battering rams, were ordered into action around the Belgian port city of Antwerp. More than 200 addresses were raided in what was the largest police operation ever conducted in the country and potentially one of the most significant moves yet against the increasingly powerful narco-gangs of western  Europe . There are hopes that Operation Sky will herald the downfall of a generation of local bosses, although the Belgian and Dutch \u201cgodfathers\u201d largely now hide out in Dubai and Turkey, hoping to be out of reach of the authorities. An incredible 27 tonnes of cocaine have been seized on Antwerp\u2019s quays, in container ships and safe houses, with an estimated value of \u20ac1.4bn (\u00a31.2bn), and many arrests have been made. It has been hailed as a mighty blow against what Belgian federal prosecutor Fr\u00e9d\u00e9ric Van Leeuw calls \u201ca world where morality has totally disappeared\u201d, but Operation Sky has also highlighted a chilling development. Europe has eclipsed the US as the Colombian cartels\u2019 favoured market, because of higher prices and much lower risks posed by European governments in terms of interdiction, extradition and seizure of assets. Jeremy McDermott, a former British army officer who is now executive director of the thinktank InSight Crime, said a kilogram of cocaine in the US is worth up to $28,000 wholesale but that rises to $40,000 on average in Europe, and nearly $80,000 in some parts of Europe. \u201cIt is more money for less risk. I see a deliberate decision by some of the top-level Colombian traffickers, based on sources who sat in a series of meetings in 2005-6, where the business decisions were made,\u201d McDermott said. \u201cIt is a business no-brainer. The reason Antwerp and Rotterdam are so attractive is because they are some of the most efficient ports in the world, handling enormous volumes of containers, which allows traffickers to play the numbers game.\u201d The methods of operation being brought to Europe\u2019s major ports, where Dutch and Belgian criminals are sub-contracted by Colombians to move the product onwards to Italian, Albanian, British and Irish organised crime networks, are bringing a level of corruption and violence never before seen in this part of the world. \u201cWe don\u2019t have robberies any more,\u201d said Joris van der Aa, the  Gazet van Antwerpen  newspaper\u2019s respected crime reporter and columnist. \u201cEveryone is working in the drug business.\u201d Among those dragged from their beds on the first Operation Sky raid on 9 March were serving police officers, an employee of the public prosectors office, civil servants, tax officials and hospital administrators suspected of feeding desired information to the gangs, as well as those thought to have been responsible for almost daily incidents of gang-related violence in the city in recent months, ranging from drive-by shootings and grenade attacks on homes to punishment beatings. Nearly 50 arrests were made but many more have followed. Only last Thursday, two prisons and 24 homes were searched in Antwerp, Borgerhout, Borsbeek, Essen, Lokeren, Wilrijk and Wijnegem. A further 11 people were read their rights. Dutch police forcing their way into the building in Wouwse Plantage where a torture chamber constructed in a shipping container was found.  Photograph: Politie Landelijke Eenheid/Reuters Operation Sky had been two years in the making, triggered, in part, by an escalating sense of fear in the city. The mayor of Antwerp,  Bart De Wever , has a 24-hour security brief following threats to his life. But the scale of the savagery that has befallen Antwerp was perhaps most acutely felt last summer with the discovery of a torture chamber in the village of Wouwse Plantage, 30 miles outside the city. A makeshift prison had been found constructed out of seven shipping containers, six of which were used as holding cells. The final one, complete with dentist\u2019s chair with straps on the armrests and footrests, was fitted out for torture. Shears, saws, scalpels, pliers, tape, balaclavas and black cotton bags to be put over the head provided graphic enough evidence of its purpose. But for all the calls made by De Wever for a \u201chigh-pressure hose\u201d to be put on this stain on the city, Operation Sky was, according to Belgian prosecutors, only possible due to the deciphering of what had been described as an uncrackable encrypted messaging service known as SKY ECC, a Canadian communications company suspected by the prosecutors to be a criminal organisation masquerading as a legitimate business catering for privacy-loving people. Prosecutors said smartphones fitted with the SKY ECC app, but with microphone and GPS functions removed, had been distributed throughout the city to those within the network of people suspected to be working for the narco-gangs. Worldwide, there are 171,000 SKY ECC devices registered, mainly in Europe, north America, a number of countries in central and south America \u2013 mainly  Colombia  \u2013 and in the Middle East. But, strikingly, 25% of the active users of these devices are located in Belgium (6,000) and the Netherlands (12,000), and half of those were said to be in use in around Antwerp port. In \u201ccracking\u201d SKY ECC, the Belgian police claimed they had broken into a communications network used and so wholly trusted by the drug traffickers, and those they blackmailed, threatened and bribed, that images of torture and execution orders were freely sent around, along with insider financial and operational information. The torture chamber discovered by Dutch police inside the building in Wouwse Plantage.  Photograph: Politie Landelijke Eenheid/Reuters For a period of three weeks, officers were able to view messages live as they were sent, collecting information, acting only where they feared there was a risk to life, and building an unprecedented picture of the increasingly powerful and vicious criminal networks working out of the ports of Antwerp and Rotterdam. SKY ECC has reportedly denied police had compromised their encrypted messaging platform, saying instead that a malicious phishing app, illegally distributed under the SKY ECC name, was what the police appeared to have hacked.  \u201cThe pictures are worse than what I have seen in some TV series,\u201d Van Leeuw told reporters of the images he had seen on the service. \u201cSettling of scores, contract killings, photos of victims, messages that say if we don\u2019t find a target, we attack the family. It\u2019s absolutely incredible violence.\u201d Belgian prosecutors said investigators intercepted about 1 billion encrypted messages from SKY ECC in total, of which almost half have been decrypted, but which will take months to work through, likely raising many new avenues of inquiry. Jean-Fran\u00e7ois Eap, SKY ECC\u2019s global\u2019s chief executive officer, and Thomas Herdman, a former distributor of Sky Global devices, were charged in the US on 12 March with a conspiracy to violate the federal Racketeer Influenced and Corrupt Organizations Act (Rico). Both have denied wrongdoing. Eap has said \u201cthe unfounded allegations of involvement in criminal activity by me and our company are entirely false\u201d. The prosecutor\u2019s office is realistic about the impact on the criminal underworld of Operation Sky. \u201cWe are aware the criminals will be flexible and will search and find new ways to communicate,\u201d a spokesman said. \u201cWe will have to be alert to follow these new technologies\u201d. But Van der Aa, who has been following the repercussions at street level, believes Operation Sky will be a watershed moment \u2013 whether for good or bad. \u201cIt is a big blow because, in  Belgium  and a great part of the criminal underworld in the Netherlands, they really trusted Sky as a system,\u201d he said. \u201cThey were so full of confidence, and the police now have so much information on how the underworld was structured, bank accounts, all the corrupt contacts are being arrested. It takes years to build these networks. \u201cI think there will be a change of a whole generation of criminals. Twenty seven tonnes of cocaine is a lot to lose. In south America they will be thinking, \u2018Let\u2019s not do business with these Dutch and Belgian guys any more\u2019. \u201cSo, it is just very quiet at the moment. Everyone is waiting for the storm and asking themselves what the police know. Eventually, they will go after the guys in Dubai. \u201cBut the question then is, who will replace them?\u201d"}, "210705_news_467600.txt": {"page_id": "210705_news_467600.txt", "text": "T hings have moved up a notch when it comes to filming dance. New York City Ballet has just announced Sofia Coppola will be  directing  its next dance film, but this latest, When We Fell, already moves the medium into a new sphere. The cinematographer is Ryan Marie Helfant, who usually works with Beyonc\u00e9 and Solange, and here co-directed with choreographer Kyle Abraham. There\u2019s a distinct style and cool distance in Helfant\u2019s aesthetic, filmed in black and white \u2013 in fact it\u2019s more like a soft greyscale \u2013 and there\u2019s something out of time and otherworldly about the result. The first scene is filmed in the vast lobby of the David H. Koch Theater at New York\u2019s Lincoln Center. The architecture dominates the screen, its lines, curves and repetitions, a giant Elie Nadelman sculpture of two women dwarfing the eight dancers\u2019 own figures. This scale changes your perspective on these bodies, sometimes seen in aerial view with a detachment as if observing slow-growing plantlife. Abraham\u2019s steady, unhurried choreography plays out in its own lines, curves and repetitions. He makes dance with compositional nous, with satisfying patterns and subtle motifs. The movement is Merce Cunningham-esque in its careful pace and clear shapes (and also the unitards), but it has ballet\u2019s surrender to beauty, and then unexpected embellishment, like the ripple of a shoulder as a dancer\u2019s arms settle into third position, the same movement a popper might make in hip-hop, transposed into classical dance. The second and third sections are filmed on stage, more conventional but still arresting, not least the morphing and blooming shadows created by the dancers in the final pas de deux. This is dance that quietly draws you in. The mood is contemplative, transporting, the tone set by the lone notes and gentle clashes of Morton Feldman\u2019s Piece for Four Pianos (subsequent sections use more piano music by Jason Moran and Nico Muhly). When We Fell was made during a winter residency in snowy upstate New York, pondering the stillness of the world outside and our year of isolation, and that\u2019s exquisitely rendered here in dance and film."}, "210705_news_467655.txt": {"page_id": "210705_news_467655.txt", "text": "TLS Mastery Beastie Edition Tux Edition Transport Layer Security, or TLS, makes ecommerce and online banking possible. It protects your passwords and your privacy. Let\u2019s Encrypt transformed TLS from an expensive tool to a free one. TLS understanding and debugging is an essential sysadmin skill you must have. TLS Mastery  takes you through: How TLS works\n What TLS provides, and what it doesn\u2019t\n Wrapping unencrypted connections inside TLS\n Assessing TLS configurations\n The Automated Certificate Management Environment (ACME) protocol\n Using Let\u2019s Encrypt to automatically maintain TLS certificates\n Online Certificate Status Protocol\n Certificate Revocation\n CAA, HSTS, and Certificate Transparency\n Why you shouldn\u2019t run your own CA, and how to do it anyway\n and more!\n Stop wandering blindly around TLS. Master the protocol with TLS Mastery! Available in the  Beastie Edition  and the  Tux Edition . The only difference is the cover. Hardcover has both covers. Get the two-cover hardcover at any of the print bookstores below,  or direct from my bookstore . Get the combined editions at: Get the Beastie edition at: Get the Tux edition at: SNMP Mastery SNMP \nSimple Network Management Protocol \nFour lies in one acronym? \nIt\u2019s baroque. It\u2019s arcane. And it\u2019s  everywhere .\n SNMP is one of those system management skills that people acquire by experience, stumbling through one horrid implementation after another and counting their knowledge by their scars. SNMP Mastery is your guide to the secret landscape of one of computing\u2019s most mysterious tools. You will understand: What makes SNMP simple When to use SNMP, and when not to How to use SNMP securely Objects, MIBs, and OIDs to use the net-snmp management toolkit SNMPv3 efficient queries debugging proxies, SMUX, and AgentX View-based Access Control Model (VACM) extending the net-snmp agent logging, traps, and notifications Stop stumbling through the SNMP minefield. Read  SNMP Mastery  today! Scripts from book, private MIB, etc, available here. Get the print or DRM-free ebook at: Paperback or Hardcover  direct from my print bookstore (fulfilled by Aerio) Direct from my e-bookstore  (PDF, epub, mobi) (non-EU only,  thanks to VAT  \u2013 sorry, folks!) Gumroad  (PDF, epub, mobi, for EU folks) Amazon US ,  Amazon AU ,  Amazon UK ,  Amazon CA ,  Amazon DE ,  Amazon FR ,  Amazon IT ,  Amazon ES Kobo Nook Apple The Networknomicon, or SNMP Mastery The Simple Network Management Protocol, SNMP, empowers you to invoke ancient standards from the void. SNMP exposes the secrets of your network and servers, and\u2013if you\u2019re careless\u2013reconfigures them into unspeakable nightmares. It exposes your inadequate brain to the vast alien dimensions underlying modern computing. SNMP is authentic dark magic. Abdul Alhazred\u2019s infamously rumored Networknomicon, or SNMP Mastery, has long been blamed for the Spanish Inquisition, the Second World War, and Cleveland. While nuclear \u201ctesting\u201d was thought to have eradicated all copies of the manuscript, an astute student with a baggy shirt and considerable mob debts recently liberated one tattered survivor from the Miskatonic University Library of Computer Science. Tilted Windmill Press is pleased to present this facsimile edition, freshly translated from Alien Syntax Notation.1 into English by dubiously acclaimed and unquestionably foolhardy author Michael W. Lucas. The publisher is not responsible for extradimensional incursion, overloaded monitoring systems, bouts of madness, saltwater in servers, nonexistent colors, rats in the walls of crumbling ancient data centers, triangles with too many degrees, unparseable MIB files, sojourns out of Time and Space, self-medication, self-trepanation, self-destruction, Lego-induced foot trauma, or any other harm, transformation, or impact of any sort, that might be plausibly or implausibly ascribed to this tome. Get the lavishly illustrated hardcover facsimile from: Direct from the Author\u2019s Print Bookstore (orders handled by Aerio) Amazon US ,  Amazon AU ,  Amazon UK ,  Amazon CA ,  Amazon DE ,  Amazon FR ,  Amazon IT ,  Amazon ES Barnes & Noble (This cosmic horror themed edition of  SNMP Mastery  has a fully illustrated dust jacket, different hardcover laminate art, a variety of warning labels from the Miskatonic Library Department of Computer Science, and as such is only available in hardcover.) Networking for System Administrators Stop waiting for the network team! If basic TCP/IP was hard, network administrators couldn\u2019t do it. Servers give sysadmins a incredible visibility into the network\u2014once they know how to unlock it.  Most sysadmins don\u2019t need to understand window scaling, or the differences between IPv4 and IPv6 echo requests, or other intricacies of the TCP/IP protocols. You need only enough to deploy your own applications and get easy support from the network team. This book teaches you: How modern networks really work\n The essentials of TCP/IP\n The next-generation protocol, IPv6\n The right tools to diagnose network problems, and how to use them\n Troubleshooting everything from the physical wire to DNS\n How to see the traffic you send and receive\n Connectivity testing\n How to communicate with your network team to quickly resolve problems\n A systems administrator doesn\u2019t need to know the innards of TCP/IP, but knowing enough to diagnose your own network issues transforms a good sysadmin into a great one. Fungi are among the most networked creatures in the world. If a mushroom can do it, so can you! Get print or DRM-free ebook at: Paperback or hardcover  at my print bookstore (fulfilled by Aerio) Direct from my bookstore (PDF, epub, mobi) (non-EU only,  thanks to VAT  \u2013 sorry, folks!)\n Gumroad  \u2013 pdf, epub, mobi\n   Nook   Kobo   iBooks Amazon US ,  Amazon AU ,  Amazon UK ,  Amazon CA ,  Amazon DE ,  Amazon FR ,  Amazon IT ,  Amazon ES DNSSEC Mastery DNS is one of the oldest protocols on the Internet, and was designed for a network without hostile users. Anyone who wants to break into a network starts by investigating the target\u2019s Domain Name Service. DNS Security Extensions, or DNSSEC, hardens DNS and brings it into the 21st century. But learning DNSSEC requires wading through years of obsolete tutorials, dead ends, and inscrutable standards. Until now. DNSSEC Mastery will have DNS administrators running DNSSEC with the industry-standard BIND server in hours instead of weeks. You will: Understand what DNSSEC gives you, and what it doesn\u2019t\n Configure your servers to resist attack.\n Verify your environment supports modern DNS\n Debug DNSSEC and the Chain of Trust\n Configure your server to resolve DNSSEC\n Conceal zone data with NSEC3\n Cryptographically sign your zones, and attach them to the Chain of Trust\n Have BIND automatically maintain signatures\n Rollover keys to maintain security\n Implement DNSSEC on private networks\n Use DNSSEC to validate self-signed SSL certificates, ending your dependence on Certificate Authorities\n And more! DNSSEC Mastery transforms DNS from a security risk to a solution. Get print or DRM-free ebook: Tilted Windmill Press , my personal bookstore. (Also available as part of the 3-book bundle at a 10% discount!) (Excludes EU customers  because of VAT -sorry, folks!)\n Gumroad  \u2013 PDF, epub, mobi, DRM-free. Supports EU VAT.\n Nook/Barnes & Noble Kobo Amazon US ,  Amazon AU ,  Amazon UK ,  Amazon CA ,  Amazon DE ,  Amazon FR ,  Amazon IT ,  Amazon ES \n\u201c\u2026the book will take you from no DNSSEC at all to fully implemented in less than 100 pages.\u201d \n\u2014  Justin Sherrill, DragonflyBSD Digest \u201c\u2026a recommended buy.\u201d \n\u2014  Peter Hansteen , author of the Book of PF. Network Flow Analysis Blaming the network is easy. It\u2019s traditional. And it\u2019s (usually) wrong. Servers have log files, performance monitors, and other tools to assess and record system and application behavior. Traditional network devices have one LED on each port, and if you\u2019re lucky the LED blinks when somehing happens. Maybe you have tools to tell you how busy an interface is, but generally a network device is an unreadable black box. Network Flow Analysis opens the black box, demonstrating how to use the freely-available flow-tools software and the capabilities of your existing hardware to assess, analyze, and debug your network. Author Web Site Discount:  Order now!  Use coupon code  ILUVMICHAEL  for a 30% discount! (Books will be shipped directly from No Starch Prss) Or order from Amazon in  print  or for  Kindle . Get a preview!  networkflow_ch4 . A network administrator\u2019s abilities are as good as his awareness of his network. Flow analysis is the industry-standard method of collecting and recording network traffic. Flow analysis lets you see what types of traffic passed between hosts, without having to reproduce the problem. Flow export and analysis transforms network administration from a stomach-churning mix of guesswork, issue reproduction, and head-scratching to a fact-based science that lets you focus on resolving problems at their source. Network administrators: When a problem is yours, own it. When a problem isn\u2019t yours,  prove  it.  Stop asking your users to reproduce problems\n Leverage the capabilities of your existing hardware\n Identify network, server, router, and firewall problems before they become critical\n Find defective and misconfigured software\n Has anyone accessed that old server in the last day, month, or year? Find out.\n Quickly find virus-spewing machines \u2014 even on a global network, even when the culprit is on a different continent\n Is it a network problem, or is it a server problem that looks like a network problem?\n Automatically produce graphs suited for your environment, and provide friendly Web-based interfaces to your customers\n Absence of evidence is not evidence of absence. Get the evidence. Read Network Flow Analysis today.  Get the book today! Get print or DRM-free ebook at: The author\u2019s print bookstore (orders fulfilled by Aerio) Kobo Amazon US ,  Amazon AU ,  Amazon UK ,  Amazon CA ,  Amazon DE ,  Amazon FR ,  Amazon IT ,  Amazon ES Cisco Routers for the Desperate It\u2019s 3AM.  Do you know why your router is on fire? Cisco routers are the industry standard, to which all other routers are compared.  Whether you love them or hate them, you still must live with them.  While they\u2019re generally robust and reliable, when a Cisco router breaks someone needs to fix it.  If you\u2019re in IT, when they say  someone  they mean  you . This book provides a brief introduction to Cisco routers, with a tight focus on what you need to know to get the router up and working in the shortest amount of time.  Reading this book once will give you the knowledge on how your router and internet circuit work.  Leaving it on top of the router for reference during a crisis will give you a step-by-step guide to troubleshooting when things go wrong. Once you understand routers, managing switches isn\u2019t much harder. This 2nd Edition will help you upgrade, manage, and operate Cisco switching hardware, from the tiny 12-port models to the monster backbone devices. Most Cisco books with real technical content are heavy enough to stun a moose with.  This book is small enough to read in a couple of hours and light enough to carry around in your laptop bag. You\u2019ll learn how to upgrade your router, back up your router configuration, implement a private WAN, and more! \u201cThis book isn\u2019t a reference \u2013 it\u2019s a survival guide, a \u2018break glass in case of emergency\u2019 safety harness\u2026 What I found remarkable was how it was obviously written for people like me \u2013 who have little interest in router management but whose jobs depend on the consistent, trusted functioning of such infrastructure.\u201d \n\u2013ASP.NETPRO, on the 1st edition Get it now, at the special Author\u2019s Web Site discount of 30%  using coupon code  ILUVMICHAEL . Not only does this cost you a little bit less than the bookstore, but the author makes a little bit more.\n Kobo iTunes Amazon US ,  Amazon AU ,  Amazon UK ,  Amazon CA ,  Amazon DE ,  Amazon FR ,  Amazon IT ,  Amazon ES \nStalk me on social media\n\n\t"}, "210705_news_467680.txt": {"page_id": "210705_news_467680.txt", "text": "New York, NY (April 06, 2021)\n\t\t\t\t\t A team of Mount Sinai surgeons has performed the world\u2019s first human tracheal transplant\u2014an achievement that has the potential to save the lives of thousands of patients around the world who have tracheal birth defects, untreatable airway diseases, burns, tumors, or severe tracheal damage from intubation, including those who had been hospitalized with COVID-19 and placed on a ventilator. Until now, no long-term treatments existed for these patients with long-segment tracheal damage, and thousands of adults and children died each year as a result. The trachea, also known as the windpipe, is an organ that is essential for speaking, breathing, and normal lung function. The trachea connects the larynx to the lungs and plays a critical role in normal lung function, the immune system, and breathing. Surgeons have been unable to transplant this organ in large part because of the complexity of providing blood flow to the donor trachea, leaving patients with long-segment tracheal disease no option for treatment. Mount Sinai\u2019s historic procedure resulted from 30 years of research, much of it focused on how to revascularize, or provide blood flow to the trachea. The 18-hour procedure took place on Wednesday, January 13, and was led by surgeon-scientist Eric M. Genden, MD, MHCA, FACS, the Isidore Friesner Professor and Chair of Otolaryngology \u2013 Head and Neck Surgery for Mount Sinai Health System and Professor of Neurosurgery, and Immunology, at the Icahn School of Medicine at Mount Sinai. The complex surgery involved a team of more than 50 specialists including surgeons, nurses, anesthesiologists, and residents. \u201cFor the first time, we are able to offer a viable treatment option to patients with life-compromising long-segment tracheal defects, and this development will change the standard of care. It is particularly timely given the growing number of patients with extensive tracheal issues due to COVID-19 intubation. Because of both mechanical ventilation and the nature of the COVID-19-induced airway disease, tracheal airway disease is precipitously increasing, and now we have a treatment. Our trachea transplantation and revascularization protocol is reliable, reproducible, and technically straightforward,\u201d says Dr. Genden. \u201cFor years, the medical and scientific consensus has been that trachea transplantation could not be done because the organ\u2019s complexity made revascularization impossible, and every previous attempt to perform in-human transplantation ended in failure. This surgical achievement is not only the culmination of 30 years of research that began when I was a medical student at Mount Sinai, but was also made possible by the spirit of collaboration that exists at Mount Sinai.\u201d The trachea transplant recipient is a 56-year-old female social worker from New York City. She had severe tracheal damage due to repeated intubation after an asthma attack; several failed surgical attempts to reconstruct her trachea led to even further damage. She breathed through a tracheostomy\u2014a surgically created hole in her neck\u2014and was at high risk of suffocation and death because of the progression of her airway disease and likelihood of her trachea collapsing. Being constantly worried she would go to sleep at night and never wake up was her main motivator to undergo the experimental procedure. During the procedure, the Mount Sinai surgical team removed the trachea and the associated blood vessels from the donor. Then, the surgeons reconstructed the trachea in the recipient from the lungs to the larynx and performed a series of microvascular anastomoses, connecting the small blood vessels that nourish the donor trachea with the recipient\u2019s blood vessels. Surgeons used a portion of the esophagus and thyroid gland to help provide blood supply to the trachea, which led to successful revascularization. Ultimately, this procedure allowed the removal of the recipient\u2019s tracheostomy, giving her an opportunity to breathe through her mouth for the first time in six years. \u201cDespite extensive research on the vascular supply to the organ using human and animal models, there is no real way to fully prepare for conducting a first-ever in-human transplantation such as this,\u201d Dr. Genden said. \u201cFor example, we had no guide for how well the graft would tolerate transplantation, so we worked very quickly. Eighteen hours later, it was clear we had accomplished what many said could not be done. Ultimately, everything went smoothly because we assembled a strong team with extensive surgical expertise in organ transplantation and tracheal reconstruction. Seeing the graft come alive and knowing that the organ was well vascularized was an amazing experience. Knowing that this procedure and 30 years of research will save countless lives was indescribable. It is why we do what we do, to make a difference.\u201d The patient has had no complications or signs of organ rejection and doctors are monitoring her closely to assess her progress and reaction to antirejection therapy. Their observations will inform the development of Mount Sinai\u2019s Tracheal Transplant Program, enabling Dr. Genden to offer this therapeutic approach to patients nationwide and internationally. \u201cMount Sinai has had many firsts and this extraordinary effort was the result of Dr. Genden\u2019s vision, skills, and perseverance\u2014as well as the strength and trust of this incredible patient,\u201d said Sander S. Florman, MD, The Charles Miller, MD Professor of Surgery and Director of the Recanati/Miller Transplantation Institute at Mount Sinai. \u201cThe Transplant Institute is proud to support Dr. Genden\u2019s efforts and to have the experience to help make this a new possibility for these patients.\u201d \u201cFrom the beginning, Mount Sinai has demonstrated excellence and innovation in patient care, and the work of Dr. Genden and the surgeons who made this transplantation both possible and successful builds on that legacy,\u201d says Kenneth L. Davis, MD, President and Chief Executive Officer of Mount Sinai Health System. \u201cThrough ongoing collaboration and research, we will continue to advance our understanding of complex medical challenges and pioneer therapeutic approaches that enable us to deliver on our mission to realize the best possible outcomes for our patients and strengthen the health of our community.\u201d \u201cI applaud the entire Mount Sinai team for pioneering the first successful tracheal transplant. This historic procedure shows that organ and tissue donation is an evolving field that continues to heal and save lives in new and meaningful ways. It is important to remember that this procedure would not have been possible without the selfless organ donor who chose to give the gift of life. If you have not done so, please consider signing up as an organ, eye, and tissue donor. You too can save a life,\u201d explains Helen Irving, President and Chief Executive Officer of LiveOnNY, the organ procurement organization that Mount Sinai worked with to receive the trachea donor. Click on the link below to learn more about Mount Sinai\u2019s Tracheal Transplant Program: www.mountsinai.org/trachealtransplant"}, "210705_news_467690.txt": {"page_id": "210705_news_467690.txt", "text": "Enlarge   /  Most geoengineering plans involve humanity doing what some volcanoes already do: inject particles into the stratosphere that reflect sunlight. Without condoning or condemning the poorly understood tactic, recent reports suggest we should try to understand one proposed strategy to cool the planet: altering the atmosphere to reflect sunlight. Called solar radiation modification (SRM), this strategy is a type of geoengineering that involves scattering particles into the sky to reflect sunlight back out into space so it can't warm the Earth's atmosphere. In theory, SRM could cool off the planet and help limit  global warming  to 1.5\u00baC compared to preindustrial levels. But it's viewed as something of a last-resort tool to tackle climate change. Two new analyses explore what deploying this tactic could mean for the environment and the flora, fauna, and people living in it. In all, the authors of both reports suggest that more work needs to be done to understand SRM. Greater cooperation The idea has gained some traction.  Bill Gates , for example, advocated for it in 2019. The  Geoengineering Model Intercomparison Project (GeoMIP)\u00a0 has released more than 100 studies related to SRM in the decade since the group was formed. But recently, in response to public outcry, the  Swedish Space Corporation  canceled a test flight that would help study SRM. But studies may be what we need most. According to one of the  papers \u00a0published this week, a great many unknowns within SRM still need to be addressed. The document is the outcome of a large group of geoengineering modelers, climate scientists, and ecologists who met digitally over the past two years.  Called the Climate Intervention Biology Working Group , the team was concerned that efforts, like GeoMIP, to understand or model SRM outcomes didn't account for ecology and biodiversity. The paper doesn't come out for or against SRM but rather suggests that these knowledge gaps need filling before the world decides to use the strategy. This research focused on one particular SRM approach, called stratospheric aerosol injection, which involves releasing reflective particles into the stratosphere. The paper lays out an agenda for us to understand what these impacts might be. According to Peter Groffman\u2014professor at the Advanced Science Research Center at City University of New York and one of the paper's authors\u2014it advocates for increased teamwork between climate scientists, geoengineering modelers, and ecologists. In the past, this teamwork was uncommon, he said. \"We went in with the idea that we really needed to bring these groups together to talk about this complex problem,\" he told Ars. Alan Robock is one of the founders of GeoMIP and a co-author on the paper. GeoMIP asks climate scientists around the world to perform standardized tests on how climate would change in certain situations: a continued release of carbon, a reduction of carbon emissions, and under a hypothetical deployment of SRM. Missing ecosystems Currently, GeoMIP's climate models work on parcels of land, which can contain different types of natural vegetation and crops. But considering the importance of accurate modeling in understanding SRM's potential impacts, the paper also suggests expanding GeoMIP to include Earth's many and complex ecosystems. For example, while current modeling can take into account, say, a field of corn, it misses out on the myriad other plants, animals, and insects that live within it. Several of the paper's authors are also ecology scholars who focus on biodiversity. \"They don't have details that ecologists like to look at, at very fine scales,\" Robock told Ars. \"They could do with better simulations, but for that, we would need better data.\" According to Groffman, right now, SRM's impacts on many important ecological functions are unknown. For example, injecting particles into the atmosphere might impact precipitation in unforeseen ways, as both it and temperature are closely linked. Similarly, an SRM strategy might work to cool the planet, but it would still leave carbon lingering in its atmosphere, which can create problems down the line. But it also can change ecosystems in the present. Plants use both sunlight and carbon dioxide\u2014the former decreasing in this hypothetical, and the latter remaining the same. So the tactic might change how they grow in unpredictable ways. This, in turn, could have unforeseen consequences on river flows, groundwater, and the slew of organisms that rely on trees for food and shelter, Groffman said. SRM proposes a kind of indirect pathway toward mitigating the effects of humanity's filling of the atmosphere with carbon without directly impacting the cause. \"You're solving a problem in a different way from how it was created, and that makes huge uncertainties as to how it's going to affect ecosystems,\" he said. Addressing concerns The National Academies of Science, Engineering, and Medicine published  a similar report  a few weeks ago. A large and diverse team\u2014ranging from lawyers to atmospheric chemists\u2014was part of the committee that oversaw its creation. While the ecosystem report focused on aerosol injection, the National Academies looked at two additional methods: marine cloud brightening and cirrus cloud thinning. Cloud brightening involves adding aerosol particles to the lower atmosphere to make clouds more reflective, particularly near coastal regions. The theory behind cloud thinning\u2014which is not technically a proper SRM strategy\u2014is to modify high-altitude clouds to make them thinner, increasing the planet's ability to radiate heat. The 329-page document suggests the creation of a research program in the United States to answer the environmental questions like those raised by the other report, as well as the technical and social questions that come with SRM. We should also investigate its viability, the document notes. It suggests this program be funded at between $100 and $200 million over a five-year period. \"I think the main message from both [reports] is that we would need to understand a lot more about impacts before we did anything,\" said Christopher Field, chair of the committee that drafted the document. Even beyond the ecological and technical questions, the National Academies document dives into how to address public concern\u2014like the public outcry that saw the Swedish test canceled. Some people, for example, don't think a government should be deliberately mucking around with the environment. The document also outlines the importance of addressing these concerns and even proposes ways of addressing them, like a liability system if an SRM strategy goes awry. \u201cI wish... we didn't need to consider this\u201d There are also worries that any emphasis on geoengineering would distract from more pressing issues. \"[Some] people are concerned that even the idea of discussing solar geoengineering might lead to a decreasing emphasis on mitigation,\" Field told Ars. All of the researchers agreed that SRM is not necessarily a solid alternative to reducing greenhouse gases. Field noted that the big hope is that humans decrease their carbon emissions to a sustainable level, but that might not happen in time for the world to reach its climate goals. In the meantime, scientists, policymakers, and environmentalists should have a \"full toolbox\" of strategies to handle global warming, and this means understanding if SRM has a place in it, he said. \"Like everybody, I wish we were in a situation where we didn't need to consider this.\" PNAS, 2021. DOI:  10.1073/pnas.1921854118  ( About DOIs )."}, "210705_news_467704.txt": {"page_id": "210705_news_467704.txt", "text": "1.  Introduction This section is non-normative. WebDriver  defines a protocol for introspection and\nremote control of user agents. This specification extends WebDriver by\nintroducing bidirectional communication. In place of the strict\ncommand/response format of WebDriver, this permits events to stream\nfrom the user agent to the controlling software, better matching the\nevented nature of the browser DOM. 2.  Infrastructure This specification depends on the Infra Standard.  [INFRA] Network protocol messages are defined using CDDL.  [RFC8610] This specification defines a  wait queue  which is a map.  Surely there\u2019s a better mechanism for doing this \"wait for an event\" thing. When an algorithm  algorithm  running  in parallel   awaits  a set of\nevents  events , and  resume id : Pause the execution of  algorithm . Assert:  wait queue  does not contain  resume id . Set  wait queue [ resume id ] to ( events ,  algorithm ). \n     To  resume  given  name ,  id  and  parameters : \n     If  wait queue  does not contain  id , return. Let ( events ,  algorithm ) be  wait queue [ id ] For each  event  in  events : If  event  equals  name : Remove  id  from  wait queue . Resume running the steps in  algorithm  from the\n point at which they were paused, passing  name  and  parameters  as the\n result of the  await .  Should we have something like microtasks to ensure this runs\n before any other tasks on the event loop? 3.  Protocol This section defines the basic concepts of the WebDriver BiDi\nprotocol. These terms are distinct from their representation at the  transport  layer. The protocol is defined using a  CDDL  definition. For the\nconvenience of implementors two seperate CDDL definitions are defined; the  remote end definition  which defines the format of messages produced\non the  local end  and consumed on the  remote end , and the  local end\ndefinition  which defines the format of messages produced on the  remote\nend  and consumed on the  local end 3.1.  Definition  Should this be an appendix? This section gives the initial contents of the  remote end definition  and  local end definition . These are augmented by the definition fragments defined in\nthe remainder of the specification. Remote end definition Command = {\n  id: uint,\n  CommandData,\n  *text => any,\n}\n\nCommandData = (\n  SessionCommand //\n  BrowsingContextCommand\n)\n\nEmptyParams = { *text }\n Local end definition Message = (\n  CommandResponse //\n  ErrorResponse //\n  Event\n)\n\nCommandResponse = {\n  id: uint,\n  result: ResultData,\n  *text => any\n}\n\nErrorResponse = {\n  id: uint / null,\n  error: \"unknown error\" / \"unknown command\" / \"invalid argument\",\n  message: text,\n  ?stacktrace: text,\n  *text => any\n}\n\nResultData = (\n  EmptyResult //\n  SessionResult //\n  BrowsingContextResult //\n  ScriptResult\n)\n\nEmptyResult = {}\n\nEvent = {\n  EventData,\n  *text => any\n}\n\nEventData = (\n  BrowsingContextEvent //\n  ScriptEvent\n)\n 3.2.  Session WebDriver BiDi uses the same  session  concept as WebDriver. 3.3.  Modules The WebDriver BiDi protocol is organized into modules. Each  module  represents a collection of related  commands  and  events  pertaining to a certain aspect of the user\nagent. For example, a module might contain functionality for inspecting and\nmanipulating the DOM, or for script execution. Each module has a  module name  which is a string. The  command name  and  event name  for commands and events defined in the\nmodule start with the  module name  followed by a period \" . \". Modules which contain  commands  define  remote end definition  fragments. These provide choices in the  CommandData  group for the\nmodule\u2019s  commands , and can also define additional definition properties. They\ncan also define  local end definition  fragments that provide additional choices\nin the  ResultData  group for the results of commands in the module. Modules which contain events define  local end definition  fragments that are\nchoices in the  Event  group for the module\u2019s  events . An implementation may define  extension modules . These must have a  module name  that contains a single colon \" : \" character. The\npart before the colon is the prefix; this is typically the same for all\nextension modules specific to a given implementation and should be unique for a\ngiven implementation. Such modules extend the  local end definition  and  remote\nend definition  providing additional groups as choices for the defined  commands  and  events . 3.4.  Commands A  command  is an asynchronous operation, requested by\nthe  local end  and run on the  remote end , resulting in either a\nresult or an error being returned to the  local end . Multiple\ncommands can run at the same time, and commands can potentially be\nlong-running. As a consequence, commands can finish out-of-order. Each  command  is defined by: A  command type  which is defined by a  remote\n end definition  fragment containing a group. Each such group has two fields: method  which is a string literal of the form  [module\nname].[method name] . This is the  command\nname . params  which defines a mapping containing data that to be passed into\nthe command. The populated value of this map is the  command parameters . A  result type , which is defined by a  local\nend definition  fragment. A set of  remote end steps  which define the actions to take for a command\ngiven  command parameters  and return an instance of the command  return\ntype . When commands are send from the  local end  they have a command id. This is an\nidentifier used by the  local end  to identify the response from a particular\ncommand. From the point of view of the  remote end  this identifier is opaque\nand cannot be used internally to identify the command. Note:  This is because the command id is entirely controlled by the  local end  and isn\u2019t necessarily unique over the course of a session. For example a  local\nend  which ignores all responses could use the same command id for each command. The  set of all command names  is a set containing\nall the defined  command names , including any belonging to  extension\nmodules . 3.5.  Events An  event  is a notification, sent by the  remote\nend  to the  local end , signaling that something of interest has\noccurred on the  remote end . An  event type  is defined by a  local\n end definition  fragment containing a group. Each such group has two fields: method  which is a string literal of the form  [module\nname].[event name] . This is the  event\nname . params  which defines a mapping containing event data. The\npopulated value of this map is the  event\nparameters. A  remote end event trigger  which defines when the event is\n triggered and steps to construct the  event type  data. Optionally, a set of  remote end subscribe steps , which define\n steps to take when a local end subscribes to an event. Where defined these\n steps have an associated  subscribe priority  which is an integer\n controlling the order in which the steps are run when multiple events are\n enabled at once, with lower integers indicating steps that run earlier. A  session  has a  global event set  which is a set\ncontaining the event names for events that are enabled for all\nbrowsing contexts. This initially contains the  event name  for events that\nare  in the default event set . A  session  has a  browsing context event map ,\nwhich is a map with  top-level browsing context  keys and values that are a\nset of  event name s for events that are enabled in the given browsing\ncontext. To obtain a list of  event enabled browsing contexts  given  session  and  event name : Let  contexts  be an empty set. For each  context  \u2192  events  of  session \u2019s  browsing context event map : If  events  contains  event name , append  context  to  contexts Return  contexts . To determine if an  event is enabled  given  session ,  event name  and  browsing contexts : Note:   browsing contexts  is a set because a  shared worker  can be associated\n      with multiple contexts. Let  top-level browsing contexts  be an empty set. For each  browsing context  of  browsing contexts , append  browsing\n context \u2019s  top-level browsing context  to  top-level browsing contexts . Let  event map  be the  browsing context event map  for  session . For each  browsing context  of  top-level browsing contexts : If  event map   browsing context , let  browsing context\n events  be  event map [ browsing context ].  Otherwise let  browsing\n context events  be null. If  browsing context events  is not null, and  browsing context events   event name , return true. If the  global event set  for  session   event name  return\n true. Return false. 4.  Transport Message transport is provided using the WebSocket protocol.  [RFC6455] Note:  In the terms of the WebSocket protocol, the  local end  is the\nclient and the  remote end  is the server / remote host. Note:  The encoding of  commands  and  events  as messages is\nsimilar to JSON-RPC, but this specification does not normatively\nreference it.  [JSON-RPC]  The normative requirements on  remote\nends  are instead given as a precise processing model, while no\nnormative requirements are given for  local ends . A  WebSocket listener  is a network endpoint that is able\nto accept incoming  WebSocket  connections. A  WebSocket listener  has a  host , a  port , a  secure flag , and a  list of WebSocket resources . When a  WebSocket listener   listener  is created, a  remote end  must start to listen for WebSocket connections on the host and port\ngiven by  listener \u2019s  host  and  port . If  listener \u2019s  secure flag  is set, then connections\nestablished from  listener  must be TLS encrypted. A  remote end  has a  set  of  WebSocket listeners   active\nlisteners , which is initially empty. A WebDriver  session  has a  WebSocket connection  which is\na network connection that follows the requirements of the  WebSocket protocol . This is initially null. When a client  establishes a WebSocket connection   connection  by\nconnecting to one of the set of  active listeners   listener , the\nimplementation must proceed according to the WebSocket  server-side\nrequirements , with the following steps run when deciding whether to\naccept the incoming connection: Let  resource name  be the resource name from  reading the\n client\u2019s opening handshake . If  resource name  is not in  listener \u2019s  list of WebSocket resources , then stop\n running these steps and act as if the requested service is not\n available. Get a session ID for a WebSocket resource  with  resource name  and let  session id  be that value. If  session id  is null then\n stop running these steps and act as if the requested service is not\n available. If there is a  session  in the list of  active sessions  with  session id  as its  session ID  then let  session  be that\n session. Otherwise stop running these steps and act as if the\n requested service is not available. Run any other implementation-defined steps to decide if the\n connection should be accepted, and if it is not stop running these\n steps and act as if the requested service is not available. Otherwise set  session \u2019s  WebSocket connection  to  connection , and proceed with the WebSocket  server-side\n requirements  when a server chooses to accept an incoming connection.  Do we support > 1 connection for a single session? When  a WebSocket message has been received  for a  WebSocket\nconnection   connection  with type  type  and data  data , a  remote\nend  must  handle an incoming message  given  connection ,  type  and  data . When  the WebSocket closing handshake is started  or when  the\nWebSocket connection is closed  for a  WebSocket connection   connection , a  remote end  must  handle a connection closing  given  connection . Note:  Both conditions are needed because it is possible for a\nWebSocket connection to be closed without a closing handshake. To  construct a WebSocket resource name  given a  session   session : Return the result of concatenating the string \" /session/ \"\n with  session \u2019s  session ID . To  get a session ID for a WebSocket resource  given  resource name : If  resource name  doesn\u2019t begin with the byte string\n \" /session/ \", return null. Let  session id  be the bytes in  resource name  following the\n \" /session/ \" prefix. If  session id  is not the string representation of a  UUID , return null. Return  session id . Note:  An  intermediary node  handling multiple sessions can use one\nor many WebSocket listeners.  WebDriver  defines that\nan  endpoint node  supports at most one session at a time, so it\u2019s\nexpected to only have a single listener. Note:  For an  endpoint node  the  host  in the above steps will\ntypically be \" localhost \". \n     To  handle an incoming message  given a  WebSocket connection   connection , type  type  and data  data : \n     If  type  is not  text ,  respond with an\n  error  given  connection , null, and  invalid argument , and finally\n  return. Assert :  data  is a  scalar value string , because the\n   WebSocket  handling errors in UTF-8-encoded data  would already\n   have  failed the WebSocket\n   connection  otherwise.  Nothing seems to define what  status code  is used for UTF-8 errors. Let  parsed  be the result of  parsing JSON\n into Infra values  given  data . If this throws an exception, then  respond\n with an error  given  connection , null, and  invalid argument , and\n finally return. Match  parsed  against the  remote end definition . If this results in a\n  match: Let  matched  be the map representing the matched data. Assert:  matched   \" id \", \" method \", and\n \" params \". Let  command id  be  matched [\" id \"]. Let  method  be  matched [\" method \"] Run the following steps in parallel: Let  result  be the result of running the  remote end steps  for the\n command with  command name   method  given  command parameters   matched [\" params \"] If  result  is an  error , then  respond with an error  given  connection ,  command id , and  result \u2019s  error code , and finally\n return. Let  value  be  result \u2019s data. Assert:  value  matches the definition for the  result type  corresponding to the command with  command name   method . Let  response  be a new map matching the  CommandResponse  production in the  local end definition  with the  id  field set to  command id  and the  value  field set to  value . Let  serialized  be the result of  serialize an infra value to JSON\n bytes  given  response . Send a WebSocket message  comprised of  serialized  over  connection  and return. Otherwise: Let  command id  be null. If  parsed  is a map and  parsed [\" id \"] exists and is an\n integer greater than or equal to zero, set  command id  to that integer. Let  error code  be  invalid argument . If  parsed  is a map and  parsed [\" method \"] exists and is a\n string, but  parsed [\" method \"] is not in the  set of all\n command names , set  error code  to  unknown command . Respond with an error  given  connection ,  command id , and  error code . \n     To  respond with an error  given a  WebSocket connection   connection ,  command id , and  error code : \n     Let  error data  be a new map matching the  ErrorResponse  production in the  local end definition , with the  id  field\n  set to  command id , the  error  field set to  error code , the  message  field set to an implementation-defined string\n  containing a human-readable definition of the error that occurred and the  stacktrace  field optionally set to an implementation-defined\n  string containing a stack trace report of the active stack frames at the\n  time when the error occurred. Let  response  be the result of  serialize an infra value to JSON bytes  given  error data . Note:   command id  can be null, in which case the  id  field will\n  also be set to null, not omitted from  response . Send a WebSocket message  comprised of  response  over  connection . Note:  This does not end any  session .  Need to hook in to the session ending to allow the UA to close\nthe listener if it wants. 4.1.  Establishing a Connection WebDriver clients opt in to a bidirectional connection by requesting a\ncapability with the name \" webSocketUrl \" and value\ntrue. This specification defines an  additional webdriver capability  with the  capability name  \" webSocketUrl \". 5.  Common Data Types 5.1.  Remote Value Values accessible from the ECMAScript runtime are represented by a mirror\nobject, specified as  RemoteValue . The value\u2019s type is specified in\nthe  type  property. In the case of JSON-representable primitive\nvalues, this contains the value in the  value  property; in the case\nof non-JSON-representable primitives, the  value  property contains a\nstring representation of the value. For non-primitive objects, the  objectId  property contains a string id that provides a unique\nhandle to the object, valid for its lifetime inside the engine. For some\nnon-primitive types, the  value  property contains a representation\nof the data in the ECMAScript object; for container types this can contain\nfurther  RemoteValue  instances. The  value  property can\nbe null if there is a duplicate object i.e. the object has already been\nserialized in the current  RemoteValue , perhaps as part of a\ncycle, or otherwise when the maximum serialization depth is reached. Nodes  are also represented by  RemoteValue  instances. These have\na partial serialization of the node in the value property. Note:  mirror objects do not keep the original object alive in the runtime. If an\nobject is discarded in the runtime subsequent attempts to access it via the\nprotocol will result in an error. A  session  has an  object id map . This is a weak map from objects to\ntheir corresponding id.  Should this be explicitly per realm? remote end definition  and  local end definition RemoteValue = {\n  UndefinedValue //\n  NullValue //\n  StringValue //\n  NumberValue //\n  BooleanValue //\n  BigIntValue //\n  SymbolValue //\n  ArrayValue //\n  ObjectValue //\n  FunctionValue //\n  RegExpValue //\n  DateValue //\n  MapValue //\n  SetValue //\n  WeakMapValue //\n  WeakSetValue //\n  IteratorValue //\n  GeneratorValue //\n  ErrorValue //\n  ProxyValue //\n  PromiseValue //\n  TypedArrayValue //\n  ArrayBufferValue //\n  NodeValue //\n  WindowProxyValue //\n}\n\nObjectId = text;\n\nListValue = [*RemoteValue];\n\nMappingValue = [*[(RemoteValue / text), RemoteValue]];\n\nUndefinedValue = {\n  type: \"undefined\",\n}\n\nNullValue = {\n  type: \"null\",\n}\n\nStringValue = {\n  type: \"string\",\n  value: text,\n}\n\nSpecialNumber = \"NaN\" / \"-0\" / \"+Infinity\" / \"-Infinity\";\n\nNumberValue = {\n  type: \"number\",\n  value: number / SpecialNumber,\n}\n\nBooleanValue = {\n  type: \"boolean\",\n  value: bool,\n}\n\nBigIntValue = {\n  type: \"bigint\",\n  value: text,\n}\n\nSymbolValue = {\n  type: \"symbol\",\n  objectId: ObjectId,\n}\n\nArrayValue = {\n  type: \"array\",\n  objectId: ObjectId,\n  value?: ListValue,\n}\n\nObjectValue = {\n  type: \"object\",\n  objectId: ObjectId,\n  value?: MappingValue,\n}\n\nFunctionValue = {\n  type: \"function\",\n  objectId: ObjectId,\n}\n\nRegExpValue = {\n  type: \"regexp\",\n  objectId: ObjectId,\n  value: text\n}\n\nDateValue = {\n  type: \"date\",\n  objectId: ObjectId,\n  value: text\n}\n\nMapValue = {\n  type: \"map\",\n  objectId: ObjectId,\n  value?: MappingValue,\n}\n\nSetValue = {\n  type: \"set\",\n  objectId: ObjectId,\n  value?: ListValue\n}\n\nWeakMapValue = {\n  type: \"weakmap\",\n  objectId: ObjectId,\n}\n\nWeakSetValue = {\n  type: \"weakset\",\n  objectId: ObjectId,\n}\n\nErrorValue = {\n  type: \"error\",\n  objectId: ObjectId,\n}\n\nPromiseValue = {\n  type: \"promise\",\n  objectId: ObjectId,\n}\n\nTypedArrayValue = {\n  type: \"typedarray\",\n  objectId: ObjectId,\n}\n\nArrayBufferValue = {\n  type: \"arraybuffer\",\n  objectId: ObjectId,\n}\n\nNodeValue = {\n  type: \"node\",\n  objectId: ObjectId,\n  value?: NodeProperties,\n}\n\nNodeProperties = {\n  nodeType: uint,\n  nodeValue: text,\n  localName?: text,\n  namespaceURI?: text,\n  childNodeCount: uint,\n  children?: [*NodeValue],\n  attributes?: {*text => text},\n  shadowRoot?: NodeValue / null,\n}\n\nWindowProxyValue = {\n  type: \"window\",\n  objectId: ObjectId,\n}\n  Add WASM types?  Should WindowProxy get attributes in a similar style to Node?  handle String / Number / etc. wrapper objects specially? To  serialize as a remote value  given an  value , a  max depth ,  node details , and a  set of known objects : In the following list of conditions and associated steps, run the first set\n of steps for which the associated condition is true: Type ( value ) is Undefined \n        Let  remote value  be a map matching the  UndefinedValue  production in the  local end definition . \n        Type ( value ) is Null \n        Let  remote value  be a map matching the  NullValue  production in the  local end definition . \n        Type ( value ) is String \n        \n        Let  remote value  be a map matching the  StringValue  production in the  local end definition , with the  value  property set to  value . \n          This doesn\u2019t handle lone surrogates Type ( value ) is Number \n        Switch on the value of  value : NaN \n            Let  serialized  be  \"NaN\" -0 \n            Let  serialized  be  \"-0\" +Infinity \n            Let  serialized  be  \"+Infinity\" -Infinity \n            Let  serialized  be  \"-Infinity\" Otherwise: \n            Let  serialized  be  value Let  remote value  be a map matching the  NumberValue  production in the  local end definition , with the  value  property set to  serialized . Type ( value ) is Boolean \n        Let  remote value  be a map matching the  BooleanValue  production in the  local end definition , with the  value  property set to  value . \n        Type ( value ) is BigInt \n        Let  remote value  be a map matching the  BigIntValue  production in the  local end definition , with the  value  property set to the result of running the  ToString  operation on  value . \n        Type ( value ) is Symbol \n        Let  remote value  be a map matching the  SymbolValue  production in the  local end definition , with the  objectId  property set to the  object id for an object   value . \n        IsArray ( value ) \n        Let  serialized  be null. If  value  is not in the  set of known objects , and  max depth  is not null and greater than 0, run the following steps: Append  value  to the  set of known objects Let  serialized  be the result of  serialize as a list  given  CreateArrayIterator ( value , value),  max depth ,  node details  and  set of known objects . Let  remote value  be a map matching the  ArrayValue  production\n in the  local end definition , with the  objectId  property set\n to the  object id for an object   value , and the  value  field set to  serialized  if it\u2019s not null, or ommitted otherwise. IsRegExp ( value ) \n        Let  pattern  be  ToString ( Get ( value , \"source\")). Let  flags  be  ToString ( Get ( value , \"flags\")). Let  serialized  be the string-concatenation of \"/\",  pattern , \"/\", and  flags . Let  remote value  be a map matching the  RegExpValue  production in the  local end definition , with the  objectId  property set to the  object id for an object   object  and the value\n set to  serialized value  has a [[DateValue]]  internal slot . \n        Let  serialized  be  ToDateString ( thisTimeValue ( value )). Let  remote value  be a map matching the  DateValue  production in the  local end definition , with the  objectId  property set to the  object id for an object   object  and the value\n set to  serialized . value  has a [[MapData]]  internal slot Let  serialized  be null. If  value  is not in the  set of known objects , and  max depth  is not null and greater than 0, run the following steps: Append  value  to the  set of known objects Let  serialized  be the result of  serialize as a mapping  given  CreateMapIterator ( value , key+value),  max depth ,  node details  and  set of known objects . Let  remote value  be a map matching the  MapValue  production in the  local end definition , with the  objectId  property set to the  object id for an object   value , and the  value  field set to  serialized  if it\u2019s\n not null, or ommitted otherwise. value  has a [[SetData]]  internal slot Let  serialized  be null. If  value  is not in the  set of known objects , and  max depth  is not null and greater than 0, run the following steps: Append  value  to the  set of known objects Let  serialized  be the result of  serialize as a list  given  CreateSetIterator ( value , value),  max depth ,  node details  and  set of known objects . Let  remote value  be a map matching the  SetValue  production in the  local end definition , with the  objectId  property set to the  object id for an object   value , and the  value  field set to  serialized  if it\u2019s\n  not null, or ommitted otherwise. value  has a [[WeakMapData]]  internal slot Let  remote value  be a map matching the  WeakMapValue  production in the  local end definition , with the  objectId  property set to the  object id for an object   value . \n        value  has a [[WeakSetData]]  internal slot Let  remote value  be a map matching the  WeakSetValue  production in the  local end definition , with the  objectId  property set to the  object id for an object   value . \n        value  has an [[ErrorData]]  internal slot Let  remote value  be a map matching the  ErrorValue  production in the  local end definition , with the  objectId  property set to the  object id for an object   value . \n        IsPromise ( value ) \n        Let  remote value  be a map matching the  PromiseValue  production in the  local end definition , with the  objectId  property set to the  object id for an object   value . \n        value  has a [[TypedArrayName]]  internal slot Let  remote value  be a map matching the  TypedArrayValue  production in the  local end definition , with the  objectId  property set to the  object id for an object   value . \n        value  has an [[ArrayBufferData]]  internal slot Let  remote value  be a map matching the  ArrayBufferValue  production in the  local end definition , with the  objectId  property set to the  object id for an object   value . \n        value  is a  platform object  that implements  Node Let  serialized  be null. If  node details  is true, run the following steps: Let  serialized  be a map. \"nodeType\",  Get ( value , \"nodeType\"), false) Set  serialized [\" nodeValue \"] to  Get ( value , \"nodeValue\") If  value  is an  Element  or an  Attribute : Set  serialized [\" localName \" to  Get ( value , \"localName\") Set  serialized [\" namespaceURI \"] to  Get ( value , \"namespaceURI\") Let  child node count  be the  size  of  serialized \u2019s  children . Set  serialized [\" childNodeCount \" to  child node count If  max depth  is equal to 0 let  children  be null.\n Otherwise, let  children  be an empty list and, for each node  child  in the  children  of  value : Let  child depth  be  max depth  - 1 if  max depth  is not null, or null otherwise. Let  serialized  be the result of  serialize as a remote value  with  child ,  child depth ,  node details  and  set of known objects . Append  serialized  to  children . Set  serialized [\" children \"] to  children . If  value  is an  Element : Let  attributes  be a new map. For each  attribute  in  value \u2019s  attribute list : Let  name  be  attribute \u2019s  qualified name Let  value  be  attribute \u2019s  value . Set  attributes [ name ] to  value Set  serialized [\" attributes \"] to  attributes . Let  shadow root  be  value \u2019s  shadow root . If  shadow root  is null, let  serialized shadow  be null.\n  Otherwise run the following substeps: Let  child depth  be  max depth  - 1 if  max depth  is not\n  null, or null otherwise. Let  serialized shadow  be the result of  serialize as a remote value  with  shadow root ,  child depth ,\n  false and  set of known objects . Note:  this means the  objectId  for the shadow root\nwill be serialized irrespective of whether the shadow is open or closed,\nbut no properties of the node will be returned. Set=  serialized [\" shadowRoot \"] to  serialized shadow . Let  remote value  be a map matching the  NodeValue  production in the  local end definition , with the  objectId  property set to the  object id for an object   value , and  value  set to  serialized , if  serialized  is not null. value  is a  platform object  that implements  WindowProxy 1. Let  remote value  be a map matching the  WindowProxyValue  production in the  local end definition , with the  objectId  property set to the  object id for an object   value . \n        value  is a  platform object 1. Let  remote value  be a map matching the  ObjectValue  production in the  local end definition , with the  objectId  property set to the  object id for an object   value . \n        IsCallable ( value ) \n        Let  remote value  be a map matching the  FunctionValue  production in the  local end definition , with the  objectId  property set to the  object id for an object   value . \n        Otherwise: \n        Assert :  type ( value ) is Object let  serialized  be null. If  value  is not in the  set of known objects , and  max depth  is greater than 0, run the following steps: Append  value  to the  set of known objects Let  serialized  be the result of  serialize as a mapping  given  EnumerableOwnPropertyNames ( value , key+value),  max depth ,  node\n  details  and  set of known objects Let  remote value  be a map matching the  ObjectValue  production\n in the  local end definition , with the  objectId  property set\n to the  object id for an object   value , and the  value  field\n set to  serialized . Return  remote value  Does it make sense to use the same depth parameter for nodes and objects\nin general? \n     To  serialize as a list  given  iterable ,  max depth ,  node details  and  set of known objects : \n     Let  serialized  be a new list. For each  child value  in  iterable : Let  child depth  be  max depth  - 1 if  max depth  is not null, or null\n otherwise. Let  serialized child  be the result of  serialize as a remote value  with arguments  child value ,  child depth ,  node details  and  set of\n known objects . Append  serialized child  to  serialized . Return  serialized  this assumes for-in works on iterators \n     To  serialize as a mapping  given  iterable ,  max depth ,  node details  and  set of known objects : \n     Let  serialized  be a new list. For  item  in  iterable : Assert:  IsArray ( item ) Let  property  be  CreateListFromArrayLike ( item ) Assert:  property  is a list of  size  2 Let  key  be  property [0] and let  value  be  property [1] Let  child depth  be  max depth  - 1 if  max depth  is not null, or null\n otherwise. If  Type ( key ) is String, let  serialized key  be  child key ,\n otherwise let  serialized key  be the result of  serialize as a remote\n value  with arguments  child key ,  child depth ,  node details  and  set of known objects . Let  serialized value  be the result of  serialize as a remote value  with arguments  value ,  child depth ,  node details  and  set of known\n objects . Let  serialized child  be (\u00ab serialized key ,  serialized value \u00bb). Append  serialized child  to  serialized . Return  serialized 6.  Modules 6.1.  The session Module The  session  module contains commands and\nevents for monitoring the status of the remote end. 6.1.1.  Definition remote end definition SessionCommand = (SessionStatusCommand //\n                  SessionSubscribeCommand)\n local end definition SessionResult = (StatusResult)\n\n To  update the event map , given  session ,  requested event names ,  browsing contexts , and  enabled : Note:  The return value of this algorithm is a map between event names and\ncontexts. When the events are being enabled globally, the contexts in the return\nvalue are those for which the event was already enabled. When the events are\nenabled for specific contexts, the contexts in the return value are those for\nwhich the event are now enabled but were not previously. When events are\ndisabled, the return value is always empty. Let  global event set  be a  clone  of the  global event set  for  session . Let  event map  be a new map. For each  key  \u2192  value  of the  browsing context event map  for  session : Set  event map [ key ] to a  clone  of  value . Let  event names  be an empty set. For each entry  name  in  requested event names ,\n let  event names  be the union of  event names  and the result of  trying  to  obtain a set of event names  with  name . Let  enabled events  be a new map. If  browsing contexts  is null: If  enabled  is true: For each  event name  of  event names : If  global event set  doesn\u2019t contain  event name : Let  event enabled contexts  be the  event enabled browsing\n  contexts  given  session  and  event name Add  event name  to  global event set . For each  context  of  event enabled contexts , remove  event\n  name  from  event map [ context ]. Set  enabled events [ event name ] to  event enabled contexts . If  enabled  is false: For each  event name  in  event names : If  global event set   event name , remove  event\n name  from  global event set . Otherwise return  error  with  error code   invalid argument . Otherwise, if  browsing contexts  is not null: Let  targets  be an empty map. For each  context id  in  browsing contexts : Let  context  be the result of  trying  to  get a browsing context  with  context id . Let  top-level context  be the  top-level browsing context  for  context . If  event map  does not contain  top-level context , set  event\n map [ top-level context ] to a new set. Set  targets [ top-level context ] to  event map [ top-level context ]. For each  event name  in  event names : If  enabled  is true and  global event set  contains  event name , continue. For each  context  \u2192  target  in  targets : If  enabled  is true and  target  does not contain  event name : Add  event name  to  target . If  enabled events  does not contain  event name , set  enabled\n events [ event name ] to a new set. Append  context  to  enabled events [ event name ]. If  enabled  is false: If  target  contains  event name , remove  event name  from  target . Otherwise return  error  with  error code   invalid\n argument . Set the  global event set  for  session  to  global event set . Set the  browsing context event map  for  session  to  event map . Return  success  with data  enabled events . Note:  Implementations that do additional work when an event is enabled,\ne.g. subscribing to the relevant engine-internal events, will likely perform\nthose additional steps when updating the event map. This specification uses\na model where hooks are always called and then the event map is used to\nfilter only those that ought to be returned to the local end. 6.1.2.  Commands 6.1.2.1.  The session.status Command The  session.status  command returns information about\nwhether a remote end is in a state in which it can create new sessions,\nbut may additionally include arbitrary meta information that is specific\nto the implementation. Command Type\n     SessionStatusCommand = {\n  method: \"session.status\",\n  params: EmptyParams,\n}\n Return Type\n     SessionStatusResult = {\n  ready: bool,\n  message: text,\n}\n The  remote end steps  are: Let  body  be a new  map  with the following properties: \"ready\"\n       The  remote end \u2019s readiness state.\n       \"message\"\n       An implementation-defined string explaining the  remote end \u2019s readiness\n state.\n      Return  success  with data  body 6.1.2.2.  The session.subscribe Command The  session.subscribe  command enables certain events\neither globally or for a set of browsing contexts  This needs to be generalized to work with realms too Command Type\n     SessionSubscribeCommand = {\n  method: \"session.subscribe\",\n  params: SubscribeParameters\n}\n\nSessionSubscribeParameters = {\n  events: [*text],\n  ?contexts: [*BrowsingContext],\n}\n Return Type\n     EmptyResult\n The  remote end steps  with  command parameters  are: Let the  list of event names  be the value of the  events  field of  command parameters Let the  list of contexts  be the value of the  contexts  field of  command parameters  if it is present or null if it isn\u2019t. Let  enabled events  be the result of  trying  to  update the event map  with  current session ,  list of event names  ,  list of contexts  and\n enabled true. Let  subscribe step events  be a new map. For each  event name  \u2192  contexts  in  enabled events : If the  event  with  event name   event name  defines  remote end\n subscribe steps , set  subscribe step events [ event name ] to  contexts . Sort in ascending order   subscribe step events  using the following less\n than algorithm given two entries with keys  event name one  and  event\n name two : Let  event one  be the  event  with name  event name one Let  event two  be the  event  with name  event name two Return true if  event one \u2019s  subscribe priority  is less than  event\n two \u2019s susbscribe priority, or false otherwise. For each  event name  \u2192  contexts  in  subscribe step events : If  list of contexts  is null, let  include contexts  be a list of all  top-level browsing contexts  that are not contained in  contexts , and\n let  include global  be true. Otherwise let  include contexts  be  contexts  and let  include global  be\n false. Run the  remote end subscribe steps  for the  event  with  event name   event name  given  include contexts  and  include global . Return  success  with data null. 6.1.2.3.  The session.unsubscribe Command The  session.unsubscribe  command disables events\neither globally or for a set of browsing contexts  This needs to be generalised to work with realms too Command Type\n     SessionUnsubscribeCommand = {\n  method: \"session.unsubscribe\",\n  params: SubscribeParameters\n}\n Return Type\n     EmptyResult\n The  remote end steps  with  command parameters  are: Let the  list of event names  be the value of the  events  field of  command parameters . Let the  list of contexts  be the value of the  contexts  field of  command parameters  if it is present or null if it isn\u2019t. Try  to  update the event map  with  current session ,  list of event names ,  list of contexts  and enabled false. Return  success  with data null. 6.2.  The browsingContext Module The  browsingContext  module contains commands and\nevents relating to browsing contexts. The progress of navigation is communicated using an immutable  WebDriver\nnavigation status  struct, which has the following items: id The  navigation id  for the navigation, or null when the navigation is\n  canceled before making progress.\n     status A status code that is either\n      \" canceled \",\n      \" pending \", or\n      \" complete \". \n     url The URL which is being loaded in the navigation\n    6.2.1.  Definition remote end definition BrowsingContextCommand = (\n    BrowsingContextGetTreeCommand //\n    BrowsingContextNavigateCommand\n)\n local end definition BrowsingContextResult = (\n    BrowsingContextGetTreeResult //\n    BrowsingContextNavigateResult\n)\n\nBrowsingContextEvent = (\n    BrowsingContextCreatedEvent //\n    BrowsingContextDestroyedEvent //\n    BrowsingContextNavigationStartedEvent //\n    BrowsingContextFragmentNavigatedEvent //\n    BrowsingContextDomContentLoadedEvent //\n    BrowsingContextLoadEvent //\n    BrowsingContextDownloadWillBegin //\n    BrowsingContextNavigationAbortedEvent //\n    BrowsingContextNavigationFailedEvent\n)\n\n 6.2.2.  Types 6.2.2.1.  The browsingContext.BrowsingContext Type remote end definition  and  local end definition BrowsingContext = text;\n Each  browsing context  has an associated  browsing context\nid , which is a string uniquely identifying that browsing context. This is\nimplicitly set when the context is created. For browsing contexts with an\nassociated WebDriver  window handle  the  browsing context id  must be the\nsame as the  window handle . 6.2.2.2.  The browsingContext.BrowsingContextInfo Type local end definition BrowsingContextInfoList = [* BrowsingContextInfo]\n\nBrowsingContextInfo = {\n  context: BrowsingContext,\n  ?parent: BrowsingContext / null,\n  url: text,\n  children: BrowsingContextInfoList / null\n}\n\n The  BrowsingContextInfo  type represents the properties of a\nbrowsing context. \n     To  get the browsing context info  given  context ,  depth  and  max depth : \n     Let  context id  be the  browsing context id  for  context . If  context  has a  parent browsing context  let  parent id  be the  browsing context id  of that parent. Otherwise let  parent id  be null. Let  document  be  context \u2019s  active document . Let  url  be the result of running the  URL serializer , given  document \u2019s  URL . Note:  This includes the fragment component of the URL. Let  child info  be the result of  get the descendent browsing contexts  given  context id ,  depth  + 1, and  max depth . Let  context info  be a  map  matching the  BrowsingContextInfo  production with the  context  field set to  context id , the  parent  field set to  parent id  if  depth  is 0, or unset otherwise, the  url  field set to  url , and the  children  field set to  child info . Return  context info . \n     To  get the descendent browsing contexts  given  parent id ,  depth  and  max depth : \n     If  max depth  is greater than zero, and  depth  is equal to  max depth ,\n return null. Let  parent  be the result of  trying  to  get a browsing context  given  parent id . If  parent  is null, let  child contexts  be a list containing all  top-level\n browsing contexts . Otherwise let  child contexts  be a list containing all  browsing contexts  which are  child browsing contexts  of  parent . Let  contexts info  be a list. For each  context  of  child contexts : Let  info  be the result of  get the browsing context info  given  context ,  depth , and  max depth . Append  info  to  contexts info Return  contexts info 6.2.2.3.  The browsingContext.Navigation Type remote end definition  and  local end definition Navigation = text;\n The  Navigation  type is a unique string identifying an ongoing\nnavigation. TODO: Link to the definition in the HTML spec. 6.2.2.4.  The browsingContext.NavigationInfo Type local end definition : NavigationInfo = {\n  context: BrowsingContext,\n  navigation: Navigation / null,\n  url: text,\n}\n The  NavigationInfo  type provides details of an ongoing navigation. \n     To  get the navigation info , given  context  and  navigation status : \n     Let  context id  be the  browsing context id  for  context . Let  navigation id  be  navigation status \u2019s id. Let  url  be  navigation status \u2019s url. Return a  map  matching the  NavigationInfo  production, with the  context  field set to  context id , the  navigation  field set to  navigation id , and the  url  field set to the\n result of the  URL serializer  given  url . 6.2.3.  Commands 6.2.3.1.  The browsingContext.getTree Command The  browsingContext.getTree  command returns a\ntree of all browsing contexts that are descendents of the given context, or all\ntop-level contexts when no parent is provided. Command Type\n     BrowsingContextGetTreeCommand = {\n  method: \"browsingContext.getTree\",\n  params: BrowsingContextGetTreeParameters\n}\n\nBrowsingContextGetTreeParameters = {\n  ?maxDepth: uint,\n  ?parent: BrowsingContext,\n}\n Return Type\n     BrowsingContextGetTreeResult = {\n  contexts: BrowsingContextInfoList\n}\n \n     The  remote end steps  with  command parameters  are: \n     Let the  parent id  be the value of the  parent  field of  command parameters  if present, or null otherwise. Let  max depth  be the value of the  maxDepth  field of  command\n parameters  if present, or 0 otherwise. Let  depth  be 0. Let  contexts  be the result of  get the descendent browsing contexts ,\n given  parent id ,  depth , and  max depth . Let  body  be a  map  matching the  BrowsingContextGetTreeResult  production, with the  contexts  field set to  contexts . Return  success  with data  body . 6.2.3.2.  The browsingContext.navigate Command The  browsingContext.navigate  command navigates a\nbrowsing context to the given URL. Command Type\n     BrowsingContextNavigateCommand = {\n  method: \"browsingContext.navigate\",\n  params: BrowsingContextNavigateParameters\n}\n\nBrowsingContextNavigateParameters = {\n  context: BrowsingContext,\n  url: text,\n  ?wait: ReadinessState,\n}\n\n ReadinessState = \"none\" / \"interactive\" / \"complete\"\n Return Type\n     BrowsingContextNavigateResult = {\n    navigation: Navigation / null,\n    url: text,\n}\n \n     The  remote end steps  with  command parameters  are: \n     Let  context id  be the value of the  context  field of  command parameters . Let  context  be the result of  trying  to  get a browsing context  with  context id . Assert:  context  is not null. Let  wait condition  be the value of the  wait  field of  command\n parameters  if present, or \" none \" otherwise. Let  url  be the value of the  url  field of  command\n parameters . Let  document  be  context \u2019s  active document . Let  base  be  document \u2019s  base URL . Let  url record  be the result of applying the  URL parser  to  url ,\n with  base URL   base . If  url record  is failure, return  error  with  error code   invalid\n argument . Let  request  be a new  request  whose URL is  url record . Let  navigation id  be the string representation of a  UUID  based on truly random, or pseudo-random numbers. Navigate   context  with resource  request , and using  context  as the  source browsing context , and with navigation id  navigation id . Let ( event received ,  navigate status ) be  await  given\n  \u00ab\" navigation started \", \" navigation failed \",\n  and \" fragment navigated \"\u00bb and  navigation id . Assert:  navigate status \u2019s id is  navigation id . If  navigate status \u2019s status is \" complete \": Let  body  be a  map  matching the  BrowsingContextNavigateResult  production, with the  navigation  field set to  navigation id , and the  url  field set to the result of the  URL serializer  given  navigate status \u2019s url. Return  success  with data  body , and then run the following steps  in\n parallel : Run the  WebDriver-BiDi fragment navigated  steps given  context  and  navigate status Note:  this is the case if the navigation only caused the fragment to\n change. The parallel steps here ensure that we return the command result\n before emitting the event, so the navigation id is known. If  navigate status \u2019s status is \" canceled \" return  error  with  error code   unknown error . TODO: is this the right way to handle errors here? Assert:  navigate status \u2019s status is \" pending \" and  navigation id  is not null. If  wait condition  is \" none \": Let  body  be a  map  matching the  BrowsingContextNavigateResult  production, with the  navigation  field set to  navigation id , and the  url  field set to the result of the  URL serializer  given  navigate status \u2019s url. Return  success  with data  body , and then run the following steps  in\n parallel : Run the  WebDriver-BiDi navigation started  steps given  context  and  navigate status Run the  WebDriver-BiDi navigation started  steps given  context  and  navigate status Note:  this event was previously suppressed to ensure that it would come\n after the command response in the case that  wait condition  is\n \" none \".  Replace this suppression mechanism with an event queue. If  wait condition  is \" interactive \", let  event name  be\n \" domContentLoaded \", otherwise let  event name  be\n \" load \". Let ( event received ,  status ) be  await  given \u00ab event name ,\n  \" download started \", \" navigation aborted \",\n  \" navigation failed \"\u00bb and  navigation id . If  event received  is \" navigation failed \"\n return  error  with  error code   unknown error .  Are we surfacing enough information about what failed and why with\n an error here? What error code do we want? Is there going to be a problem\n where local ends parse the implementation-defined strings to figure out\n what actually went wrong? Let  body  be a  map  matching the  BrowsingContextNavigateResult  production, with the  navigation  field set to  status \u2019s id, and the  url  field set to the result of the  URL serializer  given  status \u2019s url. Return  success  with data  body . 6.2.4.  Events 6.2.4.1.  The browsingContext.contextCreated Event Event Type\n      BrowsingContextCreatedEvent = {\n  method: \"browsingContext.contextCreated\",\n  params: BrowsingContextInfo\n}\n To  Emit a context created event  given  context : Let  related contexts  be a set containing  context . Let  params  be the result of  get the browsing context info  given  context , 0, and 1. Let  body  be a  map  matching the  BrowsingContextCreatedEvent  production, with the  params  field set to  params . Emit an event  with  body  and  related contexts . 6.2.4.2.  The browsingContext.contextDestroyed Event Event Type\n      BrowsingContextDestroyedEvent = {\n  method: \"browsingContext.contextDestroyed\",\n  params: BrowsingContextInfo\n}\n \n    The  remote end event trigger  is: \n    Define the following  browsing context tree discarded  steps: If the  current session  is null, return. Let  context  be the browsing context being discarded. Let  params  be the result of  get the browsing context info , given  context , 0, and 0. Let  body  be a  map  matching the  BrowsingContextDestroyedEvent  production, with the  params  field set to  params . Let  related browsing contexts  be a set containing the  parent browsing\n  context  of  context , if that is not null, or an empty set otherwise. Emit an event  with  body  and  related browsing contexts .  the way this hooks into HTML feels very fragile. See https://github.com/whatwg/html/issues/6194  It\u2019s unclear if we ought to only fire this event for browsing\ncontexts that have active documents; navigation can also cause contexts to\nbecome inaccessible but not yet get discarded because bfcache. 6.2.4.3.  The browsingContext.navigationStarted Event Event Type\n      BrowsingContextNavigationStartedEvent = {\n  method: \"browsingContext.navigationStarted\",\n  params: NavigationInfo\n}\n \n     The  remote end event trigger  is the  WebDriver-BiDi navigation\nstarted  steps given  context  and  navigation status : \n     If the  current session  is null, return. Let  params  be the result of  get the navigation info  given  context  and  navigation status . Let  body  be a  map  matching the  BrowsingContextNavigationStarted  production, with the  params  field set to  params . Let  navigation id  be  navigation status \u2019s id. Let  related browsing contexts  be a set containing  context . Resume  with \" navigation started \",  navigation id , and  navigation status . Emit an event  with  body  and  related browsing contexts . 6.2.4.4.  The browsingContext.fragmentNavigated Event Event Type\n      BrowsingContextFragmentNavigatedEvent = {\n  method: \"browsingContext.fragmentNavigated\",\n  params: NavigationInfo\n}\n \n     The  remote end event trigger  is the  WebDriver-BiDi fragment\nnavigated  steps given  context  and  navigation status : \n     If the  current session  is null, return. Let  params  be the result of  get the navigation info  given  context  and  navigation status . Let  body  be a  map  matching the  BrowsingContextFragmentNavigatedEvent  production, with the  params  field set to  params . Let  navigation id  be  navigation status \u2019s id. Let  related browsing contexts  be a set containing  context . Resume  with \" fragment navigated \",  navigation id , and  navigation status . Emit an event  with  body  and  related browsing contexts . 6.2.4.5.  The browsingContext.domContentLoaded Event Event Type\n      BrowsingContextDomContentLoadedEvent = {\n  method: \"browsingContext.domContentLoaded\",\n  params: NavigationInfo\n}\n \n     The  remote end event trigger  is the  WebDriver-BiDi DOM content\nloaded  steps given  context  and  navigation status : \n     If the  current session  is null, return. Let  params  be the result of  get the navigation info  given  context  and  navigation status . Let  body  be a  map  matching the  BrowsingContextDomContentLoadedEvent  production, with the  params  field set to  params . Let  related browsing contexts  be a set containing  context . Let  navigation id  be  navigation status \u2019s id. Resume  with \" domContentLoaded \",  navigation id , and  navigation status . Emit an event  with  body  and  related browsing contexts . 6.2.4.6.  The browsingContext.load Event Event Type\n      BrowsingContextLoadEvent = {\n  method: \"browsingContext.load\",\n  params: NavigationInfo\n}\n \n     The  remote end event trigger  is the  WebDriver-BiDi load\ncomplete  steps given  context  and  navigation status : \n     If the  current session  is null, return. Let  params  be the result of  get the navigation info  given  context  and  navigation status . Let  body  be a  map  matching the  BrowsingContextLoadEvent  production, with the  params  field set to  params . Let  related browsing contexts  be a set containing  context . Let  navigation id  be  navigation status \u2019s id. Resume  with \" load \",  navigation id  and  navigation status . Emit an event  with  body  and  related browsing contexts . 6.2.4.7.  The browsingContext.downloadWillBegin Event Event Type\n      BrowsingContextDownloadWillBegin = {\n  method: \"browsingContext.downloadWillBegin\",\n  params: NavigationInfo\n}\n \n     The  remote end event trigger  is the  WebDriver-BiDi download\nstarted  steps given  context  and  navigation status : \n     If the  current session  is null, return. Let  params  be the result of  get the navigation info  given  context  and  navigation status . Let  body  be a  map  matching the  BrowsingContextDownloadWillBegin  production, with the  params  field set to  params . Let  navigation id  be  navigation status \u2019s id. Let  related browsing contexts  be a set containing  context . Resume  with \" download started \",  navigation id , and  navigation status . Emit an event  with  body  and  related browsing contexts . 6.2.4.8.  The browsingContext.navigationAborted Event Event Type\n      BrowsingContextNavigationAborted = {\n  method: \"browsingContext.navigationAborted\",\n  params: NavigationInfo\n}\n \n     The  remote end event trigger  is the  WebDriver-BiDi navigation\naborted  steps given  context  and  navigation status : \n     If the  current session  is null, return. Let  params  be the result of  get the navigation info  given  context  and  navigation status . Let  body  be a  map  matching the  BrowsingContextNavigationAborted  production, with the  params  field set to  params . Let  navigation id  be  navigation status \u2019s id. Let  related browsing contexts  be a set containing  context . Resume  with \" navigation aborted \",  navigation id , and  navigation status . Emit an event  with  body  and  related browsing contexts . 6.2.4.9.  The browsingContext.navigationFailed Event Event Type\n      BrowsingContextNavigationFailed = {\n  method: \"browsingContext.navigationFailed\",\n  params: NavigationInfo\n}\n \n     The  remote end event trigger  is the  WebDriver-BiDi navigation\nfailed  steps given  context  and  navigation status : \n     If the  current session  is null, return. Let  params  be the result of  get the navigation info  given  context  and  navigation status . Let  body  be a  map  matching the  BrowsingContextNavigationFailed  production, with the  params  field set to  params . Let  navigation id  be  navigation status \u2019s id. Let  related browsing contexts  be a set containing  context . Resume  with \" navigation failed \",  navigation id , and  navigation status . Emit an event  with  body  and  related browsing contexts . 6.3.  The script Module The  script  module contains commands and events\nrelating to script realms and execution. 6.3.1.  Definition Remote end definition ScriptCommand = (ScriptGetRealmsCommand)\n\n local end definition ScriptResult = (ScriptGetRealmsResult)\n\nScriptEvent = (\n    ScriptRealmCreatedEvent //\n    ScriptRealmDestroyedEvent\n)\n\n 6.3.2.  Types 6.3.2.1.  The script.Realm type Remote end definition  and  local end definition Realm = text;\n Each  realm  has an associated  realm id , which is a string\nuniquely identifying that realm. This is implicitly set when the realm is\ncreated. 6.3.2.2.  The script.RealmInfo type Local end definition RealmInfo = {\n  realm: Realm,\n  type: RealmType,\n  origin: text\n}\n\nRealmType = \"window\" / \"dedicated-worker\" / \"shared-worker\" / \"service-worker\" / \"worker\" / \"paint-worklet\" / \"audio-worklet\" / \"worklet\" / text\n The  RealmInfo  type represents the properties of a realm. \n     To  get the realm info  given  environment settings : \n     Let  realm  be  environment settings \u2019  realm execution context 's Realm component. Let  realm id  be the  realm id  for  realm . Run the steps under the first matching condition: The  global object  specified by  environment settings  is a  Window  object \n        Let  type  be \" window \". The  global object  specified by  environment settings  is a  DedicatedWorkerGlobalScope  object \n        Let  type  be \" dedicated-worker \". The  global object  specified by  environment settings  is a  SharedWorkerGlobalScope  object \n        Let  type  be \" shared-worker \". The  global object  specified by  environment settings  is a  ServiceWorkerGlobalScope  object \n        Let  type  be \" service-worker \". The  global object  specified by  environment settings  is a  WorkerGlobalScope  object \n        Let  type  be \" worker \". The  global object  specified by  environment settings  is a  PaintWorkletGlobalScope  object \n        Let  type  be \" paint-worklet \". The  global object  specified by  environment settings  is a  AudioWorkletGlobalScope  object \n        Let  type  be \" audio-worklet \". The  global object  specified by  environment settings  is a  WorkletGlobalScope  object \n        Let  type  be \" worklet \". Otherwise: \n        Return null. Let  origin  be the  serialization of an origin  given  environment settings \u2019s  origin . Let  realm info  be a map matching the  RealmInfo  production,\n  with the  realm  field set to  realm id , the  type  field set to  type  and the  origin  field set to  origin . Return  realm info  We currently don\u2019t provide information about realms of unknown\n       types. That might be a problem for e.g. extension-related realms. Note:  Future variations of this specification will retain the invariant that\n         the last component of the type name after splitting on \" - \"\n         will always be \" worker \" for globals implementing  WorkerGlobalScope , and \" worklet \" for globals\n         implementing  WorkletGlobalScope . 6.3.3.  Commands 6.3.3.1.  The script.getRealms Command The  script.getRealms  command returns a list of\nall realms, optionally filtered to  realms  of a specific type, or to the\nrealm associated with the  document  currently loaded in a specified  browsing context . Command Type\n     ScriptGetRealmsCommand = {\n  method: \"script.getRealms\",\n  params: GetRealmsParameters\n}\n\nGetRealmsParameters = {\n  ?context: BrowsingContext,\n  ?type: RealmType,\n}\n Return Type\n     RealmInfoList = [* RealmInfo]\n\nScriptGetRealmsResult = {\n  realms: RealmInfoList\n}\n \n     The  remote end steps  with  command parameters  are: \n     Let  environment settings  be a list of all the  environment settings objects  that have their  execution ready flag  set. If  command parameters  contains  context : Let  context  be the result of  trying  to  get a browsing context  with  command parameters [\" context \"]. Let  document  be  context \u2019s  active document . Let  context environment settings  be a list. For each  settings  of  environment settings : If any of the following conditions hold: Append  settings  to  context environment settings . Set  environment settings  to  context environment settings . Let  realms  be a list. For each  settings  of  environment settings : Let  realm info  be the result of  get the realm info  given  settings If  command parameters  contains  type  and  realm\n info [\" type \"] is not equal to  command\n parameters [\" type \"] then  continue . If  realm info  is not null, append  realm info  to  realms . Let  body  be a map matching the  GetRealmsResult  production,\n with the  realms  field set to  realms . Return  success  with data  body .  Extend this to also allow realm parents e.g. for nested workers? Or get all ancestor workers.  We might want to have a more sophisticated filter system than just a\n       literal match. 6.3.4.  Events 6.3.4.1.  The script.realmCreated Event Event Type\n      ScriptRealmCreatedEvent = {\n  method: \"script.realmCreated\",\n  params: RealmInfo\n}\n \n    The  remote end event trigger  is: \n   \n   \n    6.3.4.2.  The script.realmDestroyed Event Event Type\n     RealmDestroyedParameters = {\n  realm: Realm\n}\n\nScriptRealmDestroyedEvent = {\n  method: \"script.realmDestoyed\",\n  params: RealmDestroyedParameters\n}\n \n    The  remote end event trigger  is: \n    \n     Define the following  unloading document cleanup steps  with  document : \n     Let  related browsing contexts  be an empty set. Append  document \u2019s  browsing context  to  related browsing\n  contexts . For each  worklet global scope  in  document \u2019s  worklet global scopes : Let  realm  be  worklet global scope \u2019s  relevant Realm . Let  realm id  be the  realm id  for  realm . Let  params  be a map mathcing the  RealmDestroyedParameters  production, with the  realm  field set of  realm id . Let  body  be a map matching the  RealmDestroyedEvent  production, with the  params  field set to  params . Emit an event  with  body  and  related browsing contexts . Let  environment settings  be the  environment settings object  whose  responsible document  is  document . Let  realm  be  environment settings \u2019  realm execution context 's Realm component. Let  realm id  be the  realm id  for  realm . Let  params  be a map mathcing the  RealmDestroyedParameters  production, with the  realm  field set to  realm id . Let  body  be a map matching the  RealmDestroyedEvent  production, with the  params  field set to  params . Emit an event  with  body  and  related browsing contexts . Whenever a  worker event loop   event loop  is destroyed, either because the\nworker comes to the end of its lifecycle, or prematurely via the  terminate a\nworker  algorithm: Let  environment settings  be the  environment settings object  for which  event loop  is the  responsible event loop . Let  related browsing contexts  be the result of  get related browsing\n  contexts  given  environment settings . Let  realm  be  environment settings \u2019s  environment settings object\u2019s Realm . Let  realm id  be the  realm id  for  realm . Let  params  be a map mathcing the  RealmDestroyedParameters  production, with the  realm  field set of  realm id . Let  body  be a map matching the  RealmDestroyedEvent  production, with the  params  field set to  params . 6.4.  Log The  log  module contains functionality and events\nrelated to logging. A  session  has a  log event buffer  which is a  map  from  browsing context id  to a list of log events for that context that have not\nbeen emitted. User agents may impose a maximum size on this buffer, subject to\nthe condition that if events A and B happen in the same context with A occuring\nbefore B, and both are added to the buffer, the entry for B must not be removed\nbefore the entry for A. To  buffer a log event  given  contexts  and  event : Let  buffer  be the  current session 's  log event buffer . Let  context ids  be a new list. For each  context  of  contexts : Append the  browsing context id  for  context  to  context ids . For each  context id  in  context ids : Let  other contexts  be an empty list For each  other id  in  context ids : If  other id  is not equal to  context id , append  other id  to  other\n  contexts . If  buffer  does not contain  context id , let  buffer [ context id ] be a\n new list. Append ( event ,  other contexts ) to  buffer [ context id ]. Note:  we store the other contexts here so that each event is only emitted\nonce. In practice this is only relevant for workers that can be associated with\nmultiple browsing contexts.  Do we want to key this on browsing context or top-level browsing context?\nThe difference is in what happens if an event occurs in a frame and that frame\nis then navigated before the local end subscribes to log events for the top\nlevel context. 6.4.1.  Definition remote end definition LogEvent = (\n  LogEntryAddedEvent\n)\n 6.4.2.  Types 6.4.2.1.  log.LogEntry LogLevel = \"debug\" / \"info\" / \"warning\" / \"error\"\n\nLogEntry = (\n  GenericLogEntry //\n  ConsoleLogEntry //\n  JavascriptLogEntry\n)\n\nBaseLogEntry = {\n  level: LogLevel,\n  text: text / null,\n  timestamp: int,\n  ?stackTrace: [*StackFrame],\n}\n\nGenericLogEntry = {\n  BaseLogEntry,\n  type: text,\n}\n\nConsoleLogEntry = {\n  BaseLogEntry,\n  type: \"console\",\n  method: text,\n  realm: Realm,\n  args: [*RemoteValue],\n}\n\nJavascriptLogEntry = {\n  BaseLogEntry,\n  type: \"javascript\",\n}\n\n Each log event is represented by a  LogEntry  object. This has a  type  property which represents the type of log entry added, a  level  property representing severity, a  text  property\nwith the log message string itself, and a  timestamp  property\ncorresponding to the time the log entry was generated. Specific variants of the  LogEntry  are used to represent logs from different sources, and\nprovide additional fields specific to the entry type. 6.4.2.2.  log.StackFrame StackFrame = {\n  url: text,\n  functionName: text,\n  lineNumber: int,\n  columnNumber: int,\n}\n\n A frame in a stacktrace is represented by a  StackFrame  object. This\nhas a  url  property, which represents the URL of the script, a  functionName  property which represents the name of the executing\nfunction, and  lineNumber  and  columnNumber  properties,\nwhich represent the line and column number of the executed code. The  current stack trace  is a representation of the stack of the  running execution context . The details of this are unspecified, and so the\nbehaviour here is implementation defined, but the general process is as follows: Let  stack trace  be a new list. For each stack frame  frame  in the stack of the running execution context,\n  starting from the most recently executed frame, run the following steps: Let  url  be the result of running the  URL serializer , given\n the  URL  of  frame \u2019s associated script resource. Let  functionName  be the name of  frame \u2019s associated function. Let  lineNumber  and  columnNumber  be the one-based line and zero-based\n column numbers, respectively, of the location in  frame \u2019s associated\n script resource corresponding to  frame . Let  frame info  be a new map matching the  StackFrame  production, with the  url  field set to  url , the  functionName  field set to  functionName , the  lineNumber  field set to  lineNumber  and the  columnNumber  field set to  columnNumber . Append  frame info  to  stack trace . Return  stack trace 6.4.3.  Events 6.4.3.1.  entryAdded Event Type\n      LogEntryAddedEvent = {\n  method: \"log.entryAdded\",\n  params: LogEntry,\n}\n The  remote end event trigger  is: Define the following  console steps  with  method ,  args , and  options : If  method  is \" error \" or \" assert \", let  level  be\n \" error \". If  method  is \" debug \" or\n \" trace \" let  level  be \" debug \". If  method  is\n \" warn \" or  warning , let  level  be\n \" warning \". Otherwise let  level  be \" info \". Let  timestamp  be a  time value  representing the current date and time in UTC. Let  text  be an empty string. If  Type (| args [0]) is String, and  args [0] contains a  formatting\n specifier , let  formatted args  be  Formatter ( args ). Otherwise let  formatted args  be  args .  This is underdefined in the console spec, so it\u2019s unclar if we can get\n interoperable behaviour here. For each  arg  in  formatted args : If  arg  is not the first entry in  args , append a U+0020 SPACE to  text . If  arg  is a  primitive value , append  ToString ( arg ) to  text . Otherwise append an implementation-defined string to  text . Let  serialized args  be a new list. For each  arg  of  args , append the result of  serialize as a remote\n value  given  arg , null, true, and an empty  set  to  serialized args . Let  realm  be the  realm id  of the  current Realm Record . Let  stack  be the  current stack trace . Let  entry  be a map matching the  ConsoleLogEntry  production,\n with the the  level  field set to  level , the  text  field set to  text , the  timestamp  field set to  timestamp , the  stackTrace  field set to  stack  if  stack  is not null, or\n omitted otherwise, the  method  field set to  method , the  realm  field set to  realm  and the  args  field set to  serialized\n args . Let  body  be a map matching the  LogEntryAddedEvent  production, with\n the  params  field set to  entry . Let  settings  be the  current settings object Let  related browsing contexts  be the result of  get related browsing\n contexts  given  settings . Let  emitted  be the result of  emit an event  with  body  and  related\n browsing contexts . If  emitted  is false, append ( related browsing contexts ,  body ) to the  current session 's  log event buffer . Define the following  error reporting steps  with arguments  script ,  line number ,  column number ,  message  and  handled : If  handled  is true return. Let  settings  be  script \u2019s  settings object . Let  stack  be the  current stack trace  for the exception. Let  entry  be a map matching the  JavascriptLogEntry  production,\n with  level  set to \" error \",  text  set to  message , and the  timestamp  field set to  timestamp . Let  related browsing contexts  be the result of  get related browsing\n contexts  given  settings . Let  emitted  be the result of  emit an event  with  body  and  related\n browsing contexts . If  emitted  is false,  buffer a log event  given  related browsing contexts  and  body .  Lots more things require logging. CDP has LogEntryAdded types xml,\njavascript, network, storage, appcache, rendering, security, deprecation,\nworker, violation, intervention, recommendation, other. These are in addition to\nthe js exception and console API types that are represented by different methods.  Allow implementation-defined log types 7.  Patches to Other Specifications This specification requires some changes to external specifications to provide the necessary\nintegration points. It is assumed that these patches will be committed to the other specifications\nas part of the standards process. 7.1.  HTML The  a browsing context is discarded  algorithm is modified to read as follows: \n     To discard a browsing context  browsingContext , run these steps: \n     If this is not a recursive invocation of this algorithm, call any  browsing context\n  tree discarded  steps defined in other applicable specifications with  browsingContext . Discard all  Document  objects for all the entries in  browsingContext \u2019s  session\n  history . If  browsingContext  is a  top-level browsing context , then  remove a browsing context   browsingContext .  The actual patch might be better to split the algorithm into an outer algorithm that is\ncalled by external callers and an inner algorithm that\u2019s used for recursive calls. That\u2019s quite hard\nto express as a patch to the specification since it requires changing multiple parts. The  report an error  algorithm is modified with an additional step at the\nend: Call any  error reporting steps  defined in external specifications\n   with  script ,  line ,  col ,  message , and true if the error is  handled , or false otherwise. 7.2.  Console Other specifications can define  console steps . When any method of the  console  interface is called, with method name  method  and argument  args : If that method does not call the  Printer  operation, call any  console\n steps  defined in external specification with arguments  method ,  args  and, undefined. Otherwise, at the point when the  Printer  operation is called with\n arguments  name ,  printerArgs  and  options  (which is undefined if the\n argument is not provided), call any  console steps  defined in\n external specification with arguments  name ,  printerArgs , and  options ."}, "210705_news_467723.txt": {"page_id": "210705_news_467723.txt", "text": "W\u00e4hrend das 5G-Netz in  Deutschland  noch ausgebaut wird, l\u00e4uft die Forschung am Nachfolger 6G l\u00e4ngst. Die Bundesregierung will das Mobilfunknetz der n\u00e4chsten Generation mit mehreren hundert Millionen Euro f\u00f6rdern, sagte Bundesforschungsministerin  Anja Karliczek  ( CDU ) in einem Interview mit dem  Handelsblatt . \u00bb6G wird die mobile Datentechnologie der Zukunft sein und unsere Kommunikation im n\u00e4chsten Jahrzehnt revolutionieren\u00ab, sagte Karliczek. \u00bbWir m\u00fcssen jetzt schon an das \u00dcbermorgen denken und neue Schl\u00fcsseltechnologien und Standards in den Kommunikationstechnologien von Beginn an mitgestalten.\u00ab Demnach soll das 6G-Netz bis 2025 mit rund 700 Millionen Euro gef\u00f6rdert werden. Ab 2030 soll es dann das 5G-Netz abl\u00f6sen. Mit 6G w\u00fcrden Daten der Zeitung zufolge mehr als 100 Mal schneller \u00fcbertragen als mit 5G \u2013 und das \"mit gro\u00dfen Vorteilen f\u00fcr die mobile Kommunikation jedes einzelnen Menschen, aber auch f\u00fcr unsere Industrie und Landwirtschaft\", sagte Karliczek. Das er\u00f6ffne neue M\u00f6glichkeiten der Zusammenarbeit \u00fcber Entfernung, nicht nur im B\u00fcroalltag, sondern auch in der Produktion. In der Medizin sei eine Behandlung aus der Ferne dann auch viel besser m\u00f6glich. Die hohen Investitionen seien n\u00f6tig, um langfristig die technologische Souver\u00e4nit\u00e4t Deutschlands und Europas zu st\u00e4rken, hie\u00df es weiter. Auch die EU hatte im Januar unter dem Titel \"Hexa-X\" eine gro\u00dfe 6G-Initiative gestartet und stellt daf\u00fcr 900 Millionen Euro zur Verf\u00fcgung."}, "210705_news_467790.txt": {"page_id": "210705_news_467790.txt", "text": "Joe Biden\u2019s secretary of state, Antony Blinken, said on Sunday the US is concerned about China\u2019s  aggressive actions against Taiwan  and warned it would be a \u201cserious mistake\u201d for anyone to try to change the status quo in the western Pacific by force. \u201cWhat we\u2019ve seen, and what is of real concern to us, is increasingly aggressive actions by the government in Beijing directed at  Taiwan , raising tensions in the Straits,\u201d Blinken told NBC\u2019s Meet the Press. Blinken also said China\u2019s failure to provide access to global health experts made the Covid-19 pandemic worse than it had to be, and it was important to \u201cget to the bottom\u201d of the origin of the novel coronavirus. Tensions between Washington and Beijing are high. On Thursday, China blamed the US for tensions after an American warship sailed close to Taiwan. On Friday the White House said it was keeping a close watch on increased Chinese military activities in the Taiwan Strait, and called Beijing\u2019s actions potentially destabilizing. The US has a longstanding commitment to ensure that Taiwan has the ability to defend itself and to sustain peace and security in the western Pacific, Blinken said. Asked if the US would respond militarily to a Chinese action in Taiwan, Blinken declined to comment on a hypothetical. \u201cAll I can tell you is we have a serious commitment to Taiwan being able to defend itself,\u201d he said. \u201cWe have a serious commitment to peace and security in the western Pacific. We stand behind those commitments. And in that context, it would be a serious mistake for anyone to try to change that status quo by force.\u201c Taiwan has complained of repeated missions by China\u2019s air force near the island, which China claims. On Friday, the state department issued new guidelines that will enable US officials to meet more freely with officials from Taiwan, a move that deepens relations with Taipei amid stepped-up Chinese military activity. A state department spokesman, Ned Price, said the new guidelines followed a congressionally mandated review and would \u201cprovide clarity \u2026 on effective implementation of our \u2018one China\u2019 policy\u201d \u2013 a reference to the  longstanding US policy  under which Washington officially recognizes Beijing rather than Taipei. Blinken\u2019s sharp words about Covid-19, meanwhile, underscored criticism from other members of the  Biden administration  over Beijing\u2019s lack of transparency in the crucial early days of the pandemic. China did not give access to international experts or share information in real time to provide true transparency, Blinken told NBC. As a result, the virus \u201cgot out of hand faster and with, I think, much more egregious results than it might otherwise\u201d, Blinken said. The World Health Organization director-general, Tedros Adhanom Ghebreyesus, said on 30 March data was withheld from WHO investigators who traveled to China to research the origins of the pandemic. A WHO report, written jointly with Chinese scientists, said the virus had probably been transmitted from bats to humans through another animal, and that a lab leak was \u201cextremely unlikely\u201d as a cause. Tedros said the issue required further investigation. The events highlight why there needs to be a stronger global health security system to ensure this doesn\u2019t happen again, Blinken said. Reforms must include a commitment to transparency, information sharing and access for experts \u201cand China has to play a part in that\u201d. Blinken said it was important to reach a more conclusive accounting of how the pandemic began. \u201cWe need to do that precisely so we fully understand what happened, in order to have the best shot possible preventing it from happening again,\u201d he said. \u201cThat\u2019s why we need to get to the bottom of this.\u201d When the WHO report was issued in March, the US, the European Union and other western powers called for China to give \u201cfull access\u201d to independent experts to all data about the original outbreak in late 2019."}, "210705_news_467793.txt": {"page_id": "210705_news_467793.txt", "text": "Describes a subway concept called \"Planetran\" comprising electromagnetically supported and propelled cars traveling in underground evacuated tubes, able to cross the United States in one hour. It is designed to interface with local transit systems, and the tunnel complex also contains utility transmission and auxiliary freight-carrying systems. Tunnels represent a major problem area and most of the cost. They will be placed several hundred feet underground in solid rock formations. It will require advanced tunnel-boring machines, such as hypersonic projectile spallation, laser beam devices, and the \"Subterrene\" heated tungsten probe that melts through igneous rocks. Planetran is rated as a system high in conservation of energy. For every car being accelerated, there is one decelerating in an adjoining tube. The decelerating cars return energy to the system. The tubes have a reduced atmosphere, making drag losses much smaller than for aircraft. Coast-to-coast energy costs are expected to be less than $1.00 per passenger. This report is part of the RAND Corporation Paper series. The paper was a product of the RAND Corporation from 1948 to 2003 that captured speeches, memorials, and derivative research, usually prepared on authors' own time and meant to be the scholarly or scientific contribution of individual authors to their professional fields. Papers were less formal than reports and did not require rigorous peer review. Permission is given to duplicate this electronic document for personal use only, as long as it is unaltered and complete. Copies may not be duplicated for commercial purposes. Unauthorized posting of RAND PDFs to a non-RAND Web site is prohibited. RAND PDFs are protected under copyright law. For information on reprint and linking permissions, please visit the  RAND Permissions  page. The RAND Corporation is a nonprofit institution that helps improve policy and decisionmaking through research and analysis. RAND's publications do not necessarily reflect the opinions of its research clients and sponsors."}, "210705_news_467863.txt": {"page_id": "210705_news_467863.txt", "text": "A new study uses a brain-computer interface (BCI) to observe the neural activity in monkeys during the process of learning. Why it matters:  The internal state of the brain is often a mystery \u2014 including to ourselves \u2014 but new neural interfaces are making it easier for scientists to observe the mind in action. How it works:  In a  paper published  recently in  Nature Neuroscience , a large team of researchers hooked up a group of monkeys to BCIs while the study subjects were trained to play a basic computer game. Details:  The researchers found that neural fluctuations took place when the monkeys were surprised by something happening during the game. Those monkeys that showed greater engagement performed better during following rounds of the test, indicating that arousal and engagement \u2014 internal states that can be difficult to track without a BCI \u2014 can affect the act of learning something new. What they're saying:  \"Our understanding of what happens in the brain as one learns is super limited right now,\" says Byron Yu, a professor at Carnegie Mellon University and a co-author of the paper. But BCI \"gives us an amazing window into how this happens.\" Context:  BCI technology has emerged as a major area of scientific research and increasingly consumer technology as the interfaces have slowly improved. On Friday, Elon Musk's BCI company Neuralink  released a video  of a monkey with chips embedded on each side of its brain as it played a basic video game using only its thoughts. Yes, but:  BCI technology would have to progress significantly \u2014 and the applications would need to go beyond video games \u2014 before many people would be willing to have an interface  drilled into their skull . Go deeper:   Elon Musk's Neuralink wants to read your brain"}, "210705_news_467867.txt": {"page_id": "210705_news_467867.txt", "text": "Hello, and welcome back to Beyond NERVA! Really quickly, I apologize that I haven\u2019t published more recently. Between moving to a different state, job hunting, and the challenges we\u2019re all facing with the current medical situation worldwide, this post is coming out later than I was hoping. I have been continuing to work in the background, but as you\u2019ll see, this engine isn\u2019t one that\u2019s easy to take in discrete chunks! Today, we jump into one of the most famous designs of advanced nuclear thermal rocket: the \u201cnuclear lightbulb,\u201d more properly known as the closed cycle gas core nuclear thermal rocket. This will be a multi-part post on not only the basics of the design, but a history of the way the design has changed over time, as well as examining both the tests that were completed as well as the tests that were proposed to move this design forward. Cutaway of simplified LRC Closed Cycle Gas Core NTR, image credit Winchell Chung of Atomic Rockets One of the challenges that we saw on the liquid core NTR was that the fission products could be released into the environment. This isn\u2019t really a problem from the pollution side for a space nuclear reactor (we\u2019ll look at the extreme version of this in a couple months with the open cycle gas core), but as a general rule it is advantageous to avoid it most of the time to keep the exhaust mass low (why we use hydrogen in the first place). In ideal circumstances, and with a high enough thrust-to-weight ratio, eliminating this release could even enable an NTR to be used in surface launches. That\u2019s the potential of the reactor type we\u2019re going to be discussing today, and in the next few posts. Due to the complexities of this reactor design, and how interconnected all the systems are, there may be an additional pause in publication after this post. I\u2019ve been working on the details of this system for over a month and a half now, and am  almost  done covering the basics of the fuel itself\u2026 so if there\u2019s a bit of delay, please be understanding! The closed cycle gas core uses uranium hexafluoride (UF6) as fuel, which is contained within a fused silica \u201cbulb\u201d to form the fuel element \u2013 hence the popular name \u201cnuclear lightbulb\u201d. Several of these are distributed through the reactor\u2019s active zone, with liquid hydrogen coolant flowing through the silica bulb, and then the now-gaseous hydrogen passing around the bulbs and out the nozzle of the reactor. This is the most conservative of the gas core designs, and only a modest step above the vapor core designs we examined last time, but still offers significantly higher temperatures, and potentially higher thrust-to-weight ratios, than the VCNTR. A combined research effort by NASA\u2019s Lewis (now Glenn) Research Center and United Aircraft Corporation in the 1960s and 70s made significant progress in the design of these reactors, but sadly with the demise of the AEC and NASA efforts in nuclear thermal propulsion, the project languished on the shelves of astronuclear research for decades. While it has seen a resurgence of interest in the last few decades in popular media, most designs for spacecraft that use the lightbulb reactor reference the efforts from the 60s and 70s in their reactor designs- despite this being, in many ways, one of the most easily tested advanced NTR designs available. Today\u2019s blog post focuses on the general shape of the reactor: its basic geometry, a brief examination of its analysis and testing, and the possible uses of the reactor. The next post will cover the analytical studies of the reactor in more detail, including the limits of what this reactor could provide, and what the tradeoffs in the design would require to make a practical NTR, as well as the practicalities of the fuel element design itself. Finally, in the third we\u2019ll look at the testing that was done, could have been done with in-core fission powered testing, the lessons learned from this testing, and maybe even some possibilities for modern improvements to this well-known, classic design. With that, let\u2019s take a look at this reactor\u2019s basic shape, how it works, and what the advantages of and problems with the basic idea are. Nuclear Lightbulb: Nuclear Powered Children\u2019s Toy (ish) Easy Bake Oven, image Wikimedia For those of us of a certain age, there was a toy that was quite popular: the Easy-Bake Oven. This was a very simple toy: an oven designed for children with minimal adult supervision to be able to cook a variety of real baked goods, often with premixed dry mixes or simple recipes. Rather than having a more normal resistive heating element as you find in a normal oven, though, a special light bulb was mounted in the oven, and the waste heat from the bulb would heat the oven enough to cook the food. Closed cycle gas core bulb, image DOE colorized by Winchell Chung The closed cycle gas core NTR takes this idea, and ramps it up to the edges of what materials limits allow. Rather than a tungsten wire, the heat in the bulb is generated by a critical mass of uranium hexafluoride, a gas at room temperature that\u2019s used in, among other things, fissile fuel enrichment for reactors and other applications. This is contained in a fused silica bulb made up of dozens of very thin tubes \u2013 not much different in material, but very different in design, compared to the Easy-Bake Oven \u2013 which contains the fissile fuel, and prevents the fission products from escaping. The fuel turns from gas to plasma, and forms a vortex in the center of the fuel element. Axial cross-section of the fuel/buffer/wall region of the lightbulb, Rodgers 1972 To further protect the bulb from direct contact with the uranium and free fluorine, a gaseous barrier of noble gas (either argon or neon) is injected between the fuel and the wall of the bulb itself. Because of the extreme temperatures, the majority of the electromagnetic radiation coming off the fuel isn\u2019t in the form of infrared (heat), but rather as ultraviolet radiation, which the silica is transparent to, minimizing the amount of energy that\u2019s deposited into the bulb itself. In order to further protect the silica bulb, microparticles of the same silica are added to the neon flow to absorb some of the radiation the bulb isn\u2019t transparent to, in order to remove that part of the radiation before it hits the bulb. This neon passes around the walls of the chamber, creating a vortex in the uranium which further constrains it, and then passes out of one or both ends of the bulb. It then goes through a purification and cooling process using a cryogenic hydrogen heat exchanger and gas centrifuge, before being reused. Now, of course there is still an intense amount of energy generated in the fuel which will be deposited in the silica, and will attempt to melt the bulb almost instantly, so the bulb must be cooled regeneratively. This is done by liquid hydrogen, which is also mostly transparent to the majority of the radiation coming off the fuel plasma, minimizing the amount of energy the coolant absorbs from anything but the silica of the bulb itself. Finally, the now-gaseous hydrogen from both the neon and bulb cooling processes, mixed with any hydrogen needed to cool the pressure vessel, reflectors of the reactor, and other components, is mixed with microparticles of tungsten to increase the amount of UV radiation emitted by the fuel. This then passes around the bulbs in the reactor, getting heated to their final temperature, before exiting the nozzle of the NTR. Overall configuration, Rodgers 1972 The most commonly examined version of the lightbulb uses a total of seven bulbs, with those bulbs being made up of a spiral of hydrogen coolant channels in fused silica. This was pioneered by NASA\u2019s Lewis Research Center (LRC), and studied by United Aircraft Corp of Mass (UA). These studies were carried out between 1963 and 1972, with a very small number of follow-up studies at UA completing by 1980. This design was a 4600 MWt reactor fueled by 233U, an isp of 1870 seconds, and a thrust-to-weight ratio of 1.3. A smaller version of this system, using a single bulb rather than seven, was proposed by the same team for probe missions and the like, but unfortunately the only papers are behind paywalls. During the re-examination of nuclear thermal technology in the early 1990s by NASA and the DOE, the design was re-examined briefly to assess the advantages that the design could offer, but no advances in the design were made at the time. Since then, while interest in this concept has grown, new studies have not been done, and the design remains dormant despite the extensive amount of study which has been carried out. What\u2019s Been Done Before: Previous Studies on the Lightbulb Bussard 1958 The first version of the closed cycle gas core proposed by Robert Bussard in 1946. This design looked remarkably like an internal combustion firing chamber, with the UF6 gas being mechanically compressed into a critical density with a piston. Coolant would be run across the outside of the fuel element and then exit the reactor through a nozzle. While this design hasn\u2019t been explored in any depth that I\u2019ve been able to determine, a new version using pressure waves rather than mechanical pistons to compress gas into a critical mass has been explored in recent years (we\u2019ll cover that in the open cycle gas core posts). Starting in 1963, United Aircraft (UA, a subsidiary of United Technologies) worked with NASA\u2019s Lewis Research Center (LRC) and Los Alamos Scientific Laboratory (LASL) on both the open and closed cycle gas core concepts, but the difficulties of containing the fuel in the open cycle concept caused the company to focus exclusively on the closed cycle concepts. Interestingly, according to Tom Latham of UA (who worked on the program), the design was limited in both mass and volume by the then-current volume of the proposed Space Shuttle cargo bay. Another limitation of the original concept was that no external radiators could be used for thermal management, due to the increased mass of the closed radiator system and its associated hardware. System flow diagram, Rodgers 1972 The design that evolved was quite detailed, and also quite efficient in many ways. However, the sheer number of interdependent subsystems makes is fairly heavy, limiting its potential usefulness and increasing its complexity. In order to get there, a large number of studies were done on a number of different subsystems and physical behaviors, and due to the extreme nature of the system design itself many experimental apparatus had to be not only built, but redesigned multiple times to get the results needed to design this reactor. We\u2019ll look at the testing history more in depth in a future blog post, but it\u2019s worth looking at the types of tests that were conducted to get an idea of just how far along this design was: RF Heating Test Apparatus, Roman 1969 Both direct current and radio frequency testing of simulated fuel plasmas were conducted, starting with the RF (induction heating) testing at the UA facility in East Hartford, CT. These studies typically used tungsten in place of uranium (a common practice, even still used today) since it\u2019s both massive and also has somewhat similar physical properties to uranium. At the time, argon was considered for the buffer gas rather than neon, this change in composition will be something we\u2019ll look at later in the detailed testing post. Induction heating works by using a vibrating magnetic field to heat materials that will flip their molecular direction or vibrate, generating heat. It is a good option for nuclear testing since it is able to more evenly heat the simulated fuel, and can achieve high temperatures \u2013 it\u2019s still used for nuclear fuel element testing not only in the Compact Fuel Element Environment Test (CFEET) test stand, which I\u2019ve covered here  http://beyondnerva.com/nuclear-test-stands-and-equipment/non-nuclear-thermal-testing/cfeet-compact-fuel-element-environmental-test/  , but also in the Nuclear Thermal Rocket Environmental Effects Simulator, which I covered here:  http://beyondnerva.com/nuclear-test-stands-and-equipment/non-nuclear-thermal-testing/ntrees/  . One of the challenges of this sort of heating, though, is the induction coil, the device that creates the heating in the material. In early testing they managed to melt the copper coil they were using due to resistive heating (the same method used to make heat in a space heater or oven), and constructing a higher-powered apparatus wasn\u2019t possible for the team. This led to direct current heating testing to achieve higher temperatures, which uses an electrical arc through the tungsten plasma. This isn\u2019t as good at simulating the way that heat is distributed in the plasma body, but could achieve higher temperatures. This was important for testing the stability of the vortex generated by not only the internal heating of the fuel, but also the interactions between the fuel and the neon containment system. Spectral flux from the edge of the fuel body, Rodgers 1972 (will be covered more in depth in another post) Another concern was determining what frequencies of radiation silicon, aluminum and neon were transparent to. By varying the temperature of the fissioning fuel mass, the frequency of radiation could, to a certain degree, be tuned to a frequency that maximized how much energy would pass through both the noble gas (then argon) and the bulb structure itself. Again, at the time (and to a certain extent later), the bulb configuration was slightly different: a layer of aluminum was added to the inner surface of the bulb to reflect more thermal radiation back into the fissioning fuel in order to increase heating, and therefore increase the temperature of the fuel. We\u2019ll look at how this design option changed over time in future posts. More studies and tests were done looking at the effects of neutron and gamma radiation on reactor materials. These are significant challenges in any reactor, but the materials being used in the lightbulb reactor are unusual, even by the standards of astronuclear engineering, so detailed studies of the effects of these radiation types were needed to ensure that the reactor would be able to operate throughout its required lifetime. Fused silica test article, Vogt 1970 Perhaps one of the biggest concerns was verifying that the bulb itself would maintain both its integrity and its functionality throughout the life of the reactor. Silica is a material that is  highly  unusual in a nuclear reactor, and the fact that it needed to remain not only transparent but able to contain both a noble gas seeded with silica particles  and  hydrogen while remaining transparent to a useful range of radiation while being bombarded with neutrons (which would change the crystalline structure) and gamma rays (which would change the energy states of the individual nuclei to varying degrees) was a major focus of the program. On top of that, the walls of the individual tubes that made up the bulbs needed to be incredibly thin, and the shape of each of the individual tubes was quite unusual, so there were significant experimental manufacturing considerations to deal with. Neutron, gamma and beta (high energy electron) radiation could all have their effect on the bulb itself during the course of the reactor\u2019s lifetime, and these effects needed to be understood and accounted for. While these tests were mostly successful, with some interesting materials properties of silica discovered along the way, when Dr. Latham discussed this project 20 years later, one of the things he mentioned was that modern materials science could possibly offer better alternatives to the silica tubing \u2013 a concept that we will touch on again in a future post. Another challenge of the design was that it required seeding two different materials into two different gasses: the neon/argon had to be seeded with silica in order to protect the bulb, and the hydrogen propellant needed to be seeded with tungsten to make it absorb the radiation passing through the bulb as efficiently as possible while minimizing the increase in the mass of the propellant. While the hydrogen seeding process was being studied for other reactor designs \u2013 we saw this in the radiator liquid fueled NTR, and will see it again in the future in open cycle gas core and some solid core designs we haven\u2019t covered yet \u2013 the silica seeding was a new challenge, especially because the material being seeded and the material the seeded gas would travel through was the same as the material that was seeded into the gas. Image DOE via Chris Casilli on Twitter Finally, there\u2019s the challenge of nuclear testing. Los Alamos Scientific Laboratory conducted some tests that were fission-powered, which proved the concept in theory, but these were low powered bench-top tests (which we\u2019ll cover in depth in the future). To  really  test the design, it would be ideal to do a hot-fire test of an NTR. Fortunately, at the time the Nuclear Furnace test-bed was being completed (more on NERVA hot fire testing here:  http://beyondnerva.com/2018/06/18/ntr-hot-fire-testing-part-i-rover-and-nerva-testing/  and the exhaust scrubbers for the Nuclear furnace here:  http://beyondnerva.com/nuclear-test-stands-and-equipment/nuclear-furnace-exhaust-scrubbers/  ). This meant that it was possible to use this versatile test-bed to test a single, sub-scale lightbulb in a controlled, well-understood system. While this test was never actually conducted, much of the preparatory design work for the test was completed, another thing we\u2019ll cover in a future post. A Promising, Developed, Unrealized Option The closed cycle gas core nuclear thermal rocket is one of the most perrenially fascinating concepts in astronuclear history. Not only does it offer an option for a high-temperature nuclear reactor which is able to avoid many of the challenges of solid fuel, but it offers better fission product containment than any other design besides the vapor core NTR. It is also one of the most complex systems that has ever been proposed, with two different types of closed cycle gas systems involving heat exchangers and separation systems supporting seven different fuel chambers, a host of novel materials in unique environments, the need to tune both the temperature and emissivity of a complex fuel form to ensure the reactor\u2019s components won\u2019t melt down, and the constant concerns of mass and complexity hanging over the heads of the designers. Most of these challenges were addressed in the 1960s and 1970s, with most of the still-unanswered questions needing testing that simply wasn\u2019t possible at the time of the project\u2019s cancellation due to shifting priorities in the space program. Modern materials science may offer better solutions to those that were available at the time as well, both in the testing and operation of this reactor. Sadly, updating this design has not happened, but the original design remains one of the most iconic designs in astronuclear engineering. In the next two posts, we\u2019ll look at the testing done for the reactor in detail, followed by a detailed look at the reactor itself. Make sure to keep an eye out for them! If you would like to support my work, consider becoming a Patreon supporter at  https://www.patreon.com/beyondnerva  . Not only do you get early access to blog posts, but I post extra blogs, images from the 3d models I\u2019m working on of both spacecraft and reactors, and more! Every bit helps. You can also follow me on Twitter ( https://twitter.com/BeyondNerva ) for more content and conversation! References McLafferty, G.H. \u201cInvestigation of Gaseous Nuclear Rocket Technology \u2013 Summary Technical Report\u201d 1969  https://ntrs.nasa.gov/archive/nasa/casi.ntrs.nasa.gov/19700008165.pdf Rodgers, R.J. and Latham, T.S. \u201cAnalytical Design and Performance Studies of the Nuclear Light Bulb Engine\u201d 1972  https://ntrs.nasa.gov/archive/nasa/casi.ntrs.nasa.gov/19730003969.pdf Latham, T.S. \u201cNuclear Light Bulb,\u201d 1992  https://ntrs.nasa.gov/archive/nasa/casi.ntrs.nasa.gov/19920001892.pdf Related  "}, "210705_news_467902.txt": {"page_id": "210705_news_467902.txt", "text": "Abstract This article uses the history of an unrealized technology to rethink conventional accounts of American spaceflight that cast the space cabin as the ultimate expression of human\u2019s capacity to technologically master their environments. Drawing on archival and published sources, I detail the history of the bioregenerative life-support system, a system in which simple organisms\u2014most commonly algae\u2014would inhabit the spacecraft and, through a series of interspecies symbioses, maintain cabin conditions and sustain astronaut life. By homing in on the maintenance practices of the system and taking seriously the kinds of interspecies possibilities they would have engendered, this account does the work of recovering how the history of American spaceflight as we know it today was not at all inevitable, and in fact it could well have been a thoroughly multispecies affair. At the same time, by offering an exaggerated example of the ways astronauts during space travel were (and are) in reality wholly reliant upon a host of technical systems for survival, the bioregenerative system points to the ways that this history not only  could  have been otherwise but  was  otherwise: the human in outer space is always already a problem of safely delivering a threatened body through an altogether inimical environment and back again. The maintenance practices of spacecraft life-support systems, real or imagined, thus afford occasion to recover a new layer of historical relations that, in turn, provide a frame through which to resignify the meaning of the space cabin in the history of American spaceflight: from an emblem of technoscientific supremacy to a place of interdependency.   Perfect smoothness is only possible in idealizations, while the rough and the real converge. \u2014Peter Sloterdijk,  Globes The American astronaut in the 1960s was the stuff of national-racial-gender fictions. \u201cClean cut,\u201d \u201call-American,\u201d \u201cfamily man\u201d\u2014the astronaut was a paragon of virtue, the perennial darling of Norman Rockwell paintings and  Life  magazine covers. Culled from the \u201chypermasculine\u201d world of military test-pilot culture, the first cohorts of American astronauts\u2014assembled by the National Aeronautics and Space Administration (NASA) in 1959\u2014reprised the pioneering semiotics of cowboys on the Western frontier. 1  These were distinctly American heroes who would gallantly surmount the frontier of the unknown in the name of securing America\u2019s rightful place in the heavens and on Earth as an international, interplanetary superpower. At the same time, these men in many ways came to figure as beings that  exceeded  their humanness. Indeed, as Vivian Sobchack points out, astronauts often appeared \u201cremarkably asexual,\u201d with most made out to seem as \u201clibidinally interesting as a Ken doll.\u201d 2  As generic archetypes of heroism, astronauts were made to \u201creject their biology and sexuality\u2014push it from their minds and bodies to concentrate on the technology required to penetrate and impregnate not a woman but the universe.\u201d 3 As a more-than-human being able to survive in environments that were by definition hostile to his life, the astronaut effortlessly defied the corporeal ties that bound him to Earth and his species. Indispensable to this preternatural mythology was the space cabin, the vessel that transported astronauts through these forbidding environments and back again. Indeed, in forming a self-sustaining world in the face of a lethal extra-spacecraft environment, the space cabin was propelled by its own mythos of sorts: the ark, the biblical structure, sealed and closed to the outside diluvial world, that maintained planetary life when no other place could. In the self-harboring environment of the ark, it was humans who controlled the composition of their lifeworld; the only nature that could survive was that which was invited into the safety of the floating refuge. In this the ark signaled a fundamental break between nature and humans. As Peter Sloterdijk puts it: \u201c[In the ark,] it is not nature which makes provision for humans in all things; rather humans are condemned to care for themselves. .\u00a0.\u00a0.  In the floating house, nature no longer harbors humans\u2014not even seemingly .\u201d 4 Ascending from this iconography of valiant frontiersmen and self-sufficient closed worlds, the space cabin became the paradigmatic exemplar of an unyielding faith in the capacity of humans to technologically master their environments. Now that humans had successfully defied the strictures of planetary life by techno-scientifically engineering ways to live beyond its bounds, the possibilities for subjecting the natural world to total human control seemed boundless. It is a confidence that has proven enduring\u2014traces of its influence can be located in ideas spanning Spaceship Earth, the late 1960s model of technocratic governance advanced by Buckminster Fuller, to proposed geoengineering fixes to climate change today. 5  This article, though, tells a different story, one that considers the space cabin not as the ultimate expression of humans\u2019 supremacy over their environment but instead as a space that foregrounds their vital reliance upon it. My focus here is on the life-support system, the technological system that furnished human space cabin passengers with all their vital needs: breathable oxygen, nutrient stores, and waste removal. I zero in on the history of one type of technology in particular, the  bioregenerative  life-support system. This system proposed a spaceship cabin design in which simple organisms\u2014most commonly algae\u2014would inhabit the spacecraft and, through a series of interspecies symbioses, maintain cabin conditions and sustain astronaut life. These botanical life forms would recycle human waste products into nutrients, photosynthesize toxic gases into vital ones, and hydrolyze urine and sweat into potable water\u2014the stuff of high modernist, space-punk alchemical dreams. 6 Though it was researched extensively by NASA, the bioregenerative life-support system was ultimately never put into practice (for reasons explained below). 7  But its history affords occasion to think more critically and carefully about the cultural iconography of the space cabin and the astronauts it contained. I am here interested in the kinds of historical possibilities that come into focus when we train attention on, as Maria Puig de la Bellacasa puts it, the \u201csignificant practices and experiences made invisible or marginalized by dominant, \u2018successful,\u2019 forms of technoscientific mobilization\u201d\u2014where here, the practices in question remain invisible by dint of their unrealization. 8  By homing in on the maintenance practices of the system and taking seriously the kinds of interspecies possibilities they would have engendered, this account does the work of recovering how, as Hustak and Myers (following Star) put it, the story \u201ccould have been otherwise\u201d: the history of American spaceflight as we know it today was not at all inevitable, and in fact it could well have been a thoroughly multispecies affair. 9  At the same time, by exaggerating the ways that astronauts were (and are) in fact wholly dependent on a host of life-support systems for survival, the bioregenerative system draws attention to the ways that this history not only  could  have been otherwise but in fact  was  otherwise: the human in outer space is always already a problem of safely delivering a threatened body through an altogether inimical environment and back again. The techniques for maintaining life at its sheer limits of existence, whether real or imagined, thus afford occasion to resituate the historical place of the human in dominant accounts of spaceflight by foregrounding what was previously only acknowledged in passing (if at all) in the triumphalist narratives so integral to ideas like Spaceship Earth: an enfeebled human, utterly dependent on the various sociotechnical life-support systems of the cabin for survival. 10  The space cabin, once an emblem of technoscientific supremacy, is here resignified as a place of  interdependency . In what follows, I plumb the history of American spacecraft engineering and design for instances of interspecies encounters even as they were imagined to transpire in the face of the most unyielding of environments. My story unfolds in two acts. I move first to examine single-organism bioregenerative systems (those that used algae), attending in particular to the ways that, in the closed world of the space cabin, the demands of physiology\u2014of both the human and botanical kinds\u2014assumed the mantle of world-making practices. I then turn to consider the story of the  multi- species system, focusing here on the latent doctrine of \u201cecological faith\u201d that underpinned its design. 11  To be sure, my intent here is not to deracinate the history of American spaceflight from its imperial and military genealogies. Indeed, as detailed below, this is a story propelled by the logic of weapons systems, the largesse of the US military, and the violence of atomic warfare\u2014icons all of the long, bellicose American twentieth century and the tentacular reaches of technoscientific potency. But traversing this topology with an eye to the microtechnical, incongruities emerge. Drilling down to the level of maintenance foregrounds the fact that these practices  mattered \u2014they were literally a matter of life and death for the astronauts who would rehearse them. Ultimately, redistributing the narrative weight to give voice to the otherwise-muted registers of this history provides a means of denaturalizing the standard account by pointing to concrete moments when the meaning of spaceflight could have been otherwise. Seen through this lens, human frailness and fragility emerge as central elements of the otherwise-hubristic dream of space exploration and colonization.   Algae in the Atmosphere From the vantage of the history of plants, the American postwar era could aptly be described as the \u201calgae epoch.\u201d 12  Long the object of study among botanists and biologists striving to unlock its enigmatic mechanisms of photosynthesis, algae circa mid-century gave rise to a host of technoscientific imaginings. Over the course of these photosynthesis researches, scientists ascertained that algae were relatively simple plants to grow and remarkably potent sources of protein. In the context of rising neo-Malthusian alarmism about food shortages in the face of global population boom, these findings assumed a broader salience: algae could be mobilized as a low-cost source of food for humans all over the globe. Seeking to harness this magic bullet potential, in the 1950s multiple major research organizations across the United States\u2014including the Carnegie Institution, the Rockefeller Foundation, the National Institutes of Health, and the Atomic Energy Commission\u2014invested significant research funds toward the development of large-scale algae cultivation technologies. 13 By quenching famines and providing a cheap source of nutrients, algae promised to improve the human condition on Earth. At the same time, as one of the most efficient photosynthesizers on the planet, the plant seized the imaginations of those who aimed to secure  new  geographies of human existence: by supplying breathable oxygen and decomposing human waste, algae might impel sojourns to otherwise-unwelcoming places like the deepest trenches of the ocean or the outer bounds of the atmosphere. Thus, alongside these mid-century, large-scale harvesting initiatives, the US military awarded countless grants to biologists, botanists, nutritionists, and sanitation engineers to develop techniques for culturing algae in closed environments\u2014the navy, for use in deep-sea submarines; the air force, for long-duration, high-altitude flights. Algae would effect a new kind of controlled environment, one in which the hyperartificial space of the closed world was infused with a blue-green botanical tint. It would furnish a method of deterrestrializing the human soldier in the service of territorializing\u2014and weaponizing\u2014the extreme limits of the planet. When NASA was established in October 1958, these algae research initiatives migrated to its aegis (although the air force never wholly abandoned its efforts). NASA\u2019s first two flight programs, Mercury (1958\u201363) and Gemini (1961\u201366), sent astronauts into orbit for short flight durations\u2014between fifteen minutes and three weeks. These programs relied on \u201cphysicochemical\u201d systems to sustain astronaut life: water, oxygen, and food were carried onboard to last the entirety of the mission. In the future, though, NASA\u2019s ambitions included flights of longer durations and to more distant places\u2014most notably with a series of manned Mars landings\u2014when, because of weight concerns, it would no longer be feasible to bring full stores of supplies along. It was here that the bioregenerative system presented as an especially auspicious design alternative. 14 These military and NASA research grants yielded several promising algae-based life-support instruments. Successful models included the \u201ccontinuous culture chamber\u201d from Jack Myers, a botanist at the University of Texas, Austin; the Recyclostat from Robert Krauss of the University of Maryland; and the Microterella, from sanitation engineers William J. Oswald and Clarence G. Golueke of the University of California, Berkeley. The devices all comprised the same components\u2014an algae sample, an aqueous growing solution, and a light source to simulate solar energy\u2014but targeted different life-support system functions. Using a dual bacteria-algae system, the Microterella would break down solid human waste and, as a byproduct, dispense drinkable water. 15  This system, its designers believed, would be relatively straightforward and \u201chighly dependable.\u201d 16  Designed to facilitate gas exchange between algae and astronaut, the continuous culture chamber and the Recyclostat were quite elaborate instruments. Algae\u2019s capacity to synthesize oxygen declined significantly as it grew in density: a denser culture inhibited the algae cells at lower parts of the growth chamber from being exposed to the light source, and metabolism rates in general declined significantly as the plants matured. To maximize efficiency, these devices sought to maintain algae \u201calways at one point on its growth curve\u201d\u2014a bloom state when it was most pure and most productive. 17  Achieving this meant that the culture solution would require near-constant dilution with fresh growth medium and regular culling to remove mature algae cells and prevent the proliferation of mutant cells that could contribute to a more variable growth rate. The device also needed to maintain the algae in a state of continuous circulation within the growth tank to afford each cell equal exposure time to the light source. When it came to sustaining the lives of other creatures, these models bore encouraging results. In 1958, Jack Myers\u2019s continuous culture device kept two mice in a sealed glass container alive for over a month and, after several tweaks to their systems, others encountered similar results. 18  Scaling up to the human environment, though, meant contending with the devices\u2019 fundamental design flaw and control-obsessed NASA\u2019s greatest fear: instability. With so many interlocking components necessary to keep the algae in its most productive, pure state, the threat of system malfunction or failure loomed large. 19  Even a cursory list of daily tasks that would demand an astronaut\u2019s attention here is telling. The astronaut\u2019s maintenance responsibilities would include: regulating growth chamber temperature, culling algae overgrowth, diluting the culture medium, analyzing pH levels and nutrient content of algae samples, repairing leaks and monitoring for accumulation of toxic gases, removing algae foam buildup, changing light bulbs to maintain consistency in light exposure, sterilizing the various cogs and stoppers of the system, and feeding the algae with vital nutrients not supplied by human waste products. That the system evinced watchfulness was due not to its own fragility but rather to that of the humans who would depend on it: algae proved unwieldy in its most productive bloom state but, if allowed to effloresce unencumbered, would readily progress to a more mature\u2014but less predictable and more variable\u2014state. The astronaut\u2019s long-term survival, then, was tethered to the purity of his life-support system, a purity that, in turn, hinged on the astronaut\u2019s sustained attention to both the hardware and wetware of the system. As one commentator quipped, the astronaut\u2019s continued existence would rest on his \u201chorticultural efficiency\u201d\u2014his \u201cskilled attention\u201d and \u201cgreen thumb.\u201d 20 It\u2019s worth pausing here to consider the system\u2019s parallels\u2014discursive and material\u2014with another system that counted the human among its instrumentalized constituent parts: the weapons system. Under the catchall phrase, \u201cthe human factor,\u201d military researchers had long fretted that the might and strength of military weapons might well prove boundless if not for the element of the human. Particularly in the context of high-altitude and high-speed flight, a soldier\u2019s biological and intellectual fallibilities\u2014his capacity for motion sickness, hypoxia, frostbite, disorientation from vertigo, and his inability to rapidly coordinate various time-sensitive elements, just to name a few\u2014became components to be accounted for in system designs. 21  Researchers were acutely aware of the overlaps between the fields of military aerospace and extra-atmospheric biomedical research; indeed, as Maura Mackowski details, nearly the entirety of NASA\u2019s knowledge about living bodies in space stemmed from air force experiments and technoscientific innovations. 22  Spaceflight engineers were thus well-primed to think of the astronaut in terms of frailty\u2014a unit of uncertainty inscribed in an otherwise-seamless, integrated environmental system. 23  As NASA administrator Joseph Saunders put it succinctly: \u201cMan is the weakest link in the weapons system. I hope this will not be true for the biologic organism in the bioregenerative [life-support] system.\u201d 24 Yet this command-and-control military discourse of  weakest links  and  horticultural efficiency , forceful as it may be, points to the essential reality of the bioregenerative system: vulnerable bodies (in this case: both human and non) in the extreme environment of outer space. This was a system forged by organisms who readily \u201cinvolved\u201d themselves in each other\u2019s lives to the extent that the existence of one was intimately linked to the existence of another. 25  Astronauts would share their most intimate bodily productions\u2014urine, excrement, sweat, carbon dioxide\u2014in exchange for vitality. To be sure, this intimacy was not of a purely harmless ilk\u2014ultimately, these transactions were extractive in nature. Yet there was reciprocity in this extraction, a manifestation of what Isabelle Stengers calls the \u201creciprocal capture\u201d of symbiosis: the sustained mutual interest in the other\u2019s well-being, if only as a way to sustain the self. 26  Considered through the lens of self-interested attachments and extractive intimacies, the space cabin\u2019s patina as a place of pure control begins to fade. Instead, we find a space cabin configured as a  space of encounter , in which the comminglings of human biology and algae physiology emerge as place-making practices: techniques of daily intimacy that, as Lisa Messeri puts it, \u201cscale down the cosmos to the level of human experience.\u201d 27  And in the context of the closed environment of the space cabin, these place-making practices scale back up to the level of  world -making practices, as conditions that structure the everyday unfoldings of life in outer space. 28   NASA Gets Ecological All the while, administrative clamor for a bioregenerative system was escalating. By the early 1960s, the United States by all accounts was abysmally lagging behind the Soviet Union\u2019s space-related achievements. With President Kennedy\u2019s 1961 vow to land an American astronaut on the moon by the end of the decade, NASA amplified its operations across nearly every aspect of space travel\u2013related research. In the bioregenerative systems context, NASA continued to award research contracts to botanists and biologists but also endeavored to expand these efforts by bringing a new scientific discipline into the research fold: ecology. American ecology at the time was in the midst of something of a renaissance. United under the guise of \u201csystems ecology,\u201d the field was actively distancing itself from the more descriptive work of previous generations and was instead focused on reframing itself as a science with explanatory power. 29  This new ecology, anchored in the study of  eco systems, was part of a broader trend in the postwar social, natural, and life sciences in which cybernetics, systems, signals, and feedback loops trafficked as the foundational conceptual vocabulary. 30  Accordingly, ecologists moved to conceive the planet less in terms of species diversity or biogeography (categories that emphasized difference) and instead in terms of the universal system dynamics that underpinned even the most seemingly disparate terrestrial environments\u2014the dynamics that, they believed, maintained ecosystems in a steady-state equilibrium. From these efforts emerged a distinct understanding of the ontology of the natural world, as one defined by the interplay between complexity and stability: the more complex a system\u2014the more mechanisms it developed to maintain itself in equilibrium\u2014the less susceptible it was to perturbations and the more readily it could recalibrate back to its stable state. At the forefront of this turn to ecological systems were brothers Howard and Eugene Odum. Their avowed commitment to the explanatory power of the relationship between stability and complexity is difficult to overstate. 31  In the 1950s, at the behest of the US Atomic Energy Commission (AEC), the brothers conducted a survey of a coral reef in Enewetok Atoll, one of several sites in the South Pacific where the AEC was executing its ongoing nuclear test program. 32  Considered against the backdrop of the crippling devastation and destruction generated by the atomic bombs detonated at Hiroshima and Nagasaki, the Odums\u2019 findings were striking: in the wake of a multimegaton thermonuclear explosion, the reef had largely recovered, appearing within a year about the same as it had before the blast. For the Odums, this phenomenon of self-restoration was evidence of the profound invincibility of the reef\u2019s steady-state stability; over the course of millions of years, they concluded, the region had clearly established a series of \u201cself-regulating interactions\u201d that yielded armaments with which to return to its pre-exposure state. The coral reef at Enewetok constituted an unparalleled case of the \u201csurvival of the stable.\u201d 33 It was this link between stability and complexity that laid the foundation for both Odums\u2019 bioregenerative life-support system designs. It also informed their trenchant critique of the algae approach. That the algae system, structured around a single organism, was unstable came as no revelation to them. \u201cFrom what we know about our own biosphere life system,\u201d Eugene Odum asserted, \u201cit is evident that no one species is ever \u2018reliable\u2019 (i.e., stable) unless it is functioning as part of an ecosystem containing other symbiotic organisms\u201d\u2014the system, in other words, was inherently not diverse enough to establish the mechanisms necessary for maintaining stability. 34  Instead, the Odums proposed the  multi- species life-support system, a system in which a variety of organisms, all known to coexist in the same ecosystem in the natural world, would be reassembled and reorganized into the spaceship environment to form what they argued would be an \u201cintegrated, self-maintaining system.\u201d 35  These organisms would be selected not for any particular species-specific traits, but instead for their functional role in ecosystems\u2014their role as metabolizers of waste, producers of energy, consumers of detritus, etc. The key to designing these systems, then, lay in identifying the \u201cminimum number of functional components and the minimum diversity .\u00a0.\u00a0. needed for stability in an ecosystem.\u201d 36  The Odums\u2019 method would be one of trial and error: \u201cIntroduce men into a closed system of 2-1/2 acres per man along with multiple compatible species .\u00a0.\u00a0. continue the seeding pressure until a self-stabilized, fully competitive system begins to be sustained.\u201d Eventually, they continued, \u201cbiogeochemical circuits would become organized by selection, and the constant pressure of maintaining a man present would provide a return circuit for him as part of a new climax ecosystem for space.\u201d 37 Though it was entirely artificial, the multi-species life-support system was informed by the same engineering principles that formed the chassis of the ecosystems of the planet\u2014the \u201ccomplexes for survival and maintenance.\u201d 38  Moreover, by mimicking environments known to have existed in the natural world for millions of years in steady-state equilibrium, the multi-species system would have the added benefit of demanding little by way of inputs or maintenance on the part of the astronaut; the stability of this equilibrium, when translated into the space cabin setting, would intrinsically confer reliability. So assured were the Odums in the soundness of a design plan anchored in the stability-complexity entanglement that they eschewed the need to understand the component parts of the system in exact, numericized terms. \u201cIn an ecosystem .\u00a0.\u00a0. of which man is only one,\u201d Howard Odum (writing with his postdoc Robert Beyers) argued, \u201cthis role of one species does not need to be so precise.\u201d 39  In fact, in many ways, quantification would be impossible: no one element could truly be \u201ctested\u201d since, as they put it, \u201cthe \u2018success\u2019 of an organism\u201d depended as much on the size and diversity of its biological environment as on its \u201cinternal physiology.\u201d 40  The multi-species system spurned total scientific knowledge in favor of an acceptance of the inevitable  limits  of human epistemology. Instead, the Odums\u2019 was a design cavalierly entrenched in a doctrine of belief, of faith and trust\u2014in the soundness of the ecological theories upon which it was founded and the capacity of humans to harness said theories for the closed world of space. A generous reading would note that, in its steadfast commitment to producing an environment that could maintain itself in the face of perturbations, the multi-species system was at least in theory conceptually aligned with NASA\u2019s central engineering tenet: control theory. 41  Control theory is a field of engineering concerned with the automation of nonlinear systems. At the basic level, it deals with the dynamics between system inputs and outputs as mediated through a sensor monitoring system: the sensor controls the system inputs to make its  actual  outputs as identical as possible to the  desired  outputs. Nearly every aspect of American spaceflight engineering drew in some way on control theory. 42  The shuttle launch system, for example, involved a complex of monitor systems that automatically adjusted vehicle pitch and yaw to maintain it at predetermined positions and velocities during liftoff, thereby greatly reducing the need for human piloting during flight. 43  In this schema, the multi-species system could be thought of as control theory taken to the logical extreme: a system so naturally attuned to automation that the human factor would be completely eliminated. The engineering telos of perfect smoothness in systems operations was here subverted to be a telos of ecosystems\u2014nature, that most perfect of systems engineers. NASA administrators, though, were less than enchanted. After denying Howard Odum\u2019s initial grant proposal, they awarded his brother Eugene a meager one in 1964 but declined to renew it after its three-year period concluded. Perhaps understandably, in NASA\u2019s view the proposals had displaced the astronaut and his well-being to such an extent that the system became one in which it was \u201cimmaterial as to which species gains ascendency\u201d; the Odums\u2019 system of complexity was interpreted as one concerned only with the maintaining of \u201csome life existing in some balance.\u201d 44  \u201cSpaceflight,\u201d as one administrator decried, \u201cis not interested in the ecosystem.\u201d 45 For his part, Howard Odum was incensed by what he saw as the nearsightedness of NASA. More than a decade later, he was still complaining about their obstinacy:  Despite our efforts for several years .\u00a0.\u00a0. we were unable to convince the NASA decision process that a complex ecosystem was the stable way to supply gaseous support for humans in space. NASA was receiving advice from physiologists whose whole training was pure culture and reductionist. They regarded multiple species self-organization as poor science because it involved  processes and elements not to be understood completely . 46 Processes and elements not to be understood completely . The logic of the multi-species life-support system was the paragon of the Odums\u2019 technocratic dream of ecologically informed spaceflight: harness the most natural of phenomena\u2014the steady-state equilibria that endowed planetary ecosystems with stability\u2014to engender the most artificial of environments. It was, at its heart, a reproach\u2014simultaneously humble and hubristic\u2014to the high modernist engineering fantasy of space travel: why presume the capacity to disarticulate the interstitial workings of ecosystems, themselves the product of millions of years of ever more divergences and nonlinear unfoldings? For the Odums, the planet\u2019s masterful skill in self-correcting back to a state of equilibrium in the face of disturbances\u2014even those incurred at the scale of an atomic bomb\u2014was evidence enough to trust that, when marshalled for the built environment, nature\u2019s foundational ecological processes would continue to endure. The idea of a verdant wilderness spaceship was certainly a fanciful one. It was also one that, from the historian\u2019s perspective, elucidates a certain paradox implanted in the logic of the algae system. The multi-species system was organized around what we might call a virtue of \u201cnaturalness,\u201d a conviction in the wisdom of nature\u2019s evolution-derived designs. This naturalness would yield a life-support system that neared total automation and, by extension, significantly reduce demand for the astronaut\u2019s maintenance labor. Put differently, in the multi-species system, astronauts and their nonhuman confreres would need only to passively cohabitate in order to survive. By contrast, the algae system was designed as a system of purity\u2014\u201cpure culture and reductionist,\u201d as Howard Odum so derided it. A pure culture, unencumbered by mutant cells, irregular growth cycles, or the presence of other microorganisms, would deliver the dual effects of maximizing system efficiency and reducing its potential mechanical malfunctions to a small range of unpredictable behaviors on the part of the algae. At the same time, this inexorable insistence on purity was what made the system an unstable one: the permanency of algae\u2019s youthful bloom state hinged on its continuous upkeep. More than mere cohabitation, this aspiration for control, for maintaining the algae in its pure but unstable state, would precipitate  in habitation. 47  In other words, these interspecies symbioses and networks of intimacy were  structured into the very hardware of the system . I think of the bioregenerative system, then, as a profoundly human and humanizing system, foregrounding\u2014by design\u2014the mucky substrates and gelatinous productions that are inextricable from what it means to be alive, and what it means to be human. Part and parcel of the interspecies world-making practices of the space cabin, the system also effected a new ontology of the human in space: \u201cthe astronaut,\u201d broken down and reinscribed within the life-support ecosystem, would emerge as a new  form of life , a way of being in the world in which a sense of one\u2019s self would be anchored in relationality, in which the act of inhabiting would demand a radical dependency. 48  \u201cLife,\u201d inextricable from its milieu as Canguilhem teaches us, here assumes a new valence of meaning, becoming a phenomenon distributed, spatialized, and schematized across the space cabin environment. 49   Conclusion In the end, neither life-support system was ever incorporated into NASA space cabin designs. The multi-species system was quickly deemed too unworkable and never progressed beyond the Odums\u2019 experimentation phase, and confidence in the viability of the algae-based system quietly waned as concerns about the system\u2019s overall reliability continued to mount; by the mid-1970s, NASA\u2019s support for algae research had winnowed to just a few lingering contracts that dealt with basic questions of photosynthesis. 50 Recently, historians have detailed the ways that the American space program\u2019s ideological goal of territorial conquest shaped the conditions of possibility for particular forms of knowledge across a wide range of disciplines, compellingly showing, for example, how seemingly benign fields like architecture and design came to be enlisted as technoscientific handmaidens to a nationalist project of colonizing outer space. 51  My critique here has been through an obverse but complementary approach. By pointing to the ways the story could have otherwise unfolded, the machinic and biological relations attendant to spaceflight\u2014whether real or imagined\u2014yield a disjunctive meeting point between the quotidian practices of daily cabin life and the historical processes of erasure deployed to enact particular techno-culturo-imperial fantasies. They afford occasion to decouple the space cabin\u2019s discursive signification of human supremacy and yoke it instead to associations of fragility. 52  They also refract back to assume potency in the Earthly realm, reaffirming the irreducible truth that, as Natasha Myers puts it, \u201cwe are of the plants\u201d: humans\u2019 sheer existence, terrestrial or beyond, rests on the continued provision of vital chemicals copiously, but exclusively, generated by the photosynthetic powers of botanical creatures. 53  Underpinning the rhetorical bravado of the preternatural cowboy-astronaut are a host of interspecies and sociotechnical maintenance practices that sustain a biologically feeble human in the extreme, and extremely threatening, environment of outer space. Indeed, as Peter Sloterdijk reminds us, \u201cperfect smoothness\u201d is only possible in idealization, while in reality, \u201cthe rough and the real converge\u201d: considered through its practices, the history of spaceflight becomes one riddled with precarity, instability, and fallibility. 54  In the spaceship cabin, interdependence is, literally, the way of life.   Acknowledgments I am grateful to Sophia Roosth, Devin Kennedy, Marc Aidinoff, Matt Hersch, and the anonymous reviewers for their close and careful readings of earlier versions of this article. I also thank Thom van Dooren and Jamie Lorimer for shepherding this article into print. Research for this article was supported by a University of Florida Smathers Libraries Travel to Collections grant. 1. Hersch, \u201cReturn of the Lost Spaceman.\u201d See also Cunningham,  All-American Boys ; McCurdy,  Space and the American Imagination ; Neufeld,  Spacefarers ; Hersch,  Inventing the American Astronaut . 2. Sobchack, \u201cVirginity of Astronauts,\u201d 107. 3. Ibid., 108. See also Casper and Moore, \u201cInscribing Bodies, Inscribing the Future.\u201d Sexuality was not the only excretion-related bit of human biology from which these men were representationally untethered. Indeed, as Munns and Nickelsen note, the daily management of human waste products rarely\u2014if ever\u2014figured even in astronauts\u2019 own popular accounts of their journeys. See Munns and Nickelsen, \u201cAlgatron versus the Fecal Bag.\u201d 4. Sloterdijk,  Globes , 240; emphasis added. See also H\u00f6hler, \u201cEnvironment as a Life Support System.\u201d 5. See, e.g., Fuller,  Operating Manual for Spaceship Earth ; Boulding, \u201cEconomics of the Coming Spaceship Earth\u201d; Ward,  Spaceship Earth . For more, see Anker,  From Bauhaus to Ecohouse ; Anker, \u201cBuckminster Fuller as Captain of Spaceship Earth\u201d; H\u00f6hler,  Spaceship Earth in the Environmental Age, 1960\u20131990 . 6. By high modernist, I mean the unflagging belief that took hold in the twentieth century in the authority of science and engineering as a means of effecting a rational social order. See Scott,  Seeing like a State . 7. As will be explained in more detail, NASA\u2019s early flight programs sustained astronaut life through wholly artificial means: oxygen for breathing was carried on board in cryogenic tanks, carbon dioxide was removed from the air using a chemical purification system, and urine and waste were either stored or periodically vented off the spacecraft. 8. Puig de la Bellacasa, \u201cMaking Time for Soil,\u201d 692. In this, the article is informed by the recent scholarly turn to excavating historically undervalued forms of labor in the history of technology through the alternating analytic lenses of \u201cmaintenance\u201d and \u201ccare.\u201d See Russell and Vinsel, \u201cHail the Maintainers\u201d; Multispecies Editing Collective, \u201cTroubling Species\u201d; Puig de la Bellacasa,  Matters of Care ; Puig de la Bellacasa, \u201cMatters of Care in Technoscience\u201d; Puig de la Bellacasa, \u201cNothing Comes without Its World\u201d; Martin, Myers, and Viseu, \u201cPolitics of Care in Technoscience\u201d; Viseu, Myers, Martin, and Suchman, \u201cPolitics of Care in Technoscience\u201d; Murphy, \u201cUnsettling Care\u201d; Denis and Pontille, \u201cMaterial Ordering and the Care of Things\u201d; van Dooren, \u201cCare.\u201d 9. Star, \u201cPower, Technology, and the Phenomenology of Conventions,\u201d as quoted by Hustak and Myers, \u201cInvolutionary Momentum.\u201d See also Masco, \u201cEnd of Ends.\u201d 10. As Roger Launius points out, outer space\u2019s status as an extreme environment and the historical implications of this status have largely gone unexamined by environmental historians. See Launius, \u201cWriting the History of Space\u2019s Extreme Environment.\u201d Though see Harrison,  Spacefaring ; Mindell,  Digital Apollo ; and Mackowski,  Testing the Limits . At the same time, the  extreme  has recently emerged as a useful analytic in anthropology for thinking through the multiple valences of the relationship between \u201climits\u201d and \u201chorizons\u201d that it brings into focus. See, e.g., Valentine, Olson, and Battaglia, \u201cExtreme\u201d; Valentine, Olson, and Battaglia, \u201cEncountering the Future\u201d; Battaglia, Valentine, and Olson, \u201cRelational Space\u201d; Olson,  American Extreme ; Battaglia, \u201cArresting Hospitality\u201d; Valentine, \u201cAtmosphere\u201d; Helmreich,  Alien Ocean ; Helmreich, \u201cExtraterrestrial Relativism\u201d; Messeri,  Placing Outer Space ; and Olson, \u201cEcobiopolitics of Space Biomedicine.\u201d See also Roosth, \u201cLife, Not Itself.\u201d 11. I use \u201cmulti-species\u201d when discussing the historical object of analysis, and \u201cmultispecies\u201d (no hyphen) to refer to the field of multispecies studies. See Kirksey and Helmreich, \u201cEmergence of Multispecies Ethnography\u201d; van Dooren et al., \u201cMultispecies Studies.\u201d 12. \u201cAlgae\u201d refers to many different species. I here use it most commonly to refer to the  Chlorella  strain. For the history of photosynthesis research, see Nickelsen,  Explaining Photosynthesis . 13. See Burlew,  Algal Culture . For more on the global politics of nutrition in postwar international relations, see Cullather, \u201cForeign Policy of the Calorie.\u201d For more on the history of algae and agriculture research in the context of neo-Malthusian fears, see Belasco, \u201cAlgae Burgers for a Hungry World?\u201d; Belasco,  Meals to Come ; Cullather, \u201cStretching the Surface of the Earth\u201d; Rao, \u201cImagined Reality\u201d; Schlosser, \u201cMalthus at Mid-Century.\u201d 14. See Kammermeyer,  Atmosphere in Space Cabins and Closed Environments ; Mattson, \u201cKeeping Astronauts Alive\u201d; Konecci, \u201cSpace Ecological Systems\u201d; Bongers, \u201cSustaining Life in Space.\u201d For more on the dream of a flight to Mars, see Lambright,  Why Mars ; Conway,  Exploration and Engineering . 15. Oswald and Golueke, \u201cMan in Space.\u201d For more on Oswald and Golueke, see Munns and Nickelsen, \u201cAlgatron versus the Fecal Bag.\u201d 17. Myers and Clark, \u201cCulture Conditions and the Development of the Photosynthetic Mechanism.\u201d 18. Myers, \u201cUse of Photosynthesis in a Closed Ecological System\u201d; Bowman and Thomae, \u201cAlgae Life Support System\u201d; Krauss, \u201cPhysiology and Biochemistry of Algae\u201d; Bowman and Thomae, \u201cLong-Term Nontoxic Support of Animal Life with Algae.\u201d 19. Myers and Clark, \u201cCulture Conditions and the Development of the Photosynthetic Mechanism.\u201d 20. Ward et al., \u201cUse of Algae and Other Plants in the Development of Life Support Systems.\u201d 21. See Mindell,  Between Human and Machine ; Galison, \u201cOntology of the Enemy.\u201d 22. Mackowski,  Testing the Limits . See also Mindell,  Digital Apollo ; Pitts,  Human Factor . 23. See also Olson, \u201cEcobiopolitics of Space Biomedicine.\u201d 24. Saunders, introduction. 25. Hustak and Myers, \u201cInvolutionary Momentum.\u201d 26. Stengers,  Cosmopolitics I , 35\u201336. 27. Messeri,  Placing Outer Space , 2. 28. Ibid.; Tsing,  Mushroom at the End of the World . See also Valerie Olson\u2019s work on the space system as a \u201ctechnology of reality.\u201d Olson,  American Extreme . 29. See, e.g., Odum, \u201cNew Ecology.\u201d For more on the history of ecology in the United States, see Hagen,  Entangled Bank ; Worster,  Nature\u2019s Economy ; Bocking,  Ecologists and Environmental Politics ; Bocking,  Nature\u2019s Experts ; Golley,  History of the Ecosystem Concept in Ecology ; Kingsland,  Evolution of American Ecology . 30. See Hayles,  How We Became Posthuman ; Kay,  Who Wrote the Book of Life? ; Keller,  Refiguring Life ; Hayles,  Chaos Bound ; Lilienfeld,  Rise of Systems Theory ; Pickering,  Cybernetic Brain . 31. Both Howard and Eugene Odum are seminal figures in the history of American ecology and are considered to have pioneered the discipline as a science of systems. For more on the Odums, see Golley,  History of the Ecosystem Concept in Ecology ; Hagen,  Entangled Bank ; Taylor, \u201cTechnocratic Optimism, H. T. Odum, and the Partial Transformation of Ecological Metaphor after World War II.\u201d It\u2019s worth noting that the meaning of terms like  stability  and  complexity  were far from universally agreed-upon terms. In addition, as the Odums\u2019 critics were quick to point out, this emphasis on the stability/complexity entanglement constituted a deeply teleological idea of the natural world. 32. As many scholars have detailed, the environmental sciences in the United States have long been entwined with the history of the nuclear program. See, e.g., Creager,  Life Atomic ; Kwa, \u201cRadiation Ecology, Systems Ecology, and the Management of the Environment\u201d; Bruno, \u201cThe Bequest of the Nuclear Battlefield\u201d; Rothschild, \u201cEnvironmental Awareness in the Atomic Age\u201d; Bocking,  Ecologists and Environmental Politics ; Masco, \u201cBad Weather.\u201d For more on the epistemological implications of the Odums\u2019 military-sponsored ecological research in the South Pacific, see DeLoughrey, \u201cMyth of Isolates.\u201d 33. Odum and Odum, \u201cTrophic Structure and Productivity,\u201d 318. 34. Odum and Beyers, \u201cProposal to the NASA Biospace Program.\u201d 35. Cooke, Beyers, and Odum, \u201cCase for the Multispecies Ecological System,\u201d 129. 36. Odum and Beyers, \u201cProposal to the NASA Biospace Program.\u201d 37. Odum, \u201cLimits of Remote Ecosystems Containing Man.\u201d 39. Odum and Beyers, \u201cProposal to Environmental Biology Section.\u201d 40. Odum and Beyers, \u201cProposal to the NASA Biospace Program.\u201d 41. It was also not without precedent. Although it was constructed along the opposite design approach\u2014modify the body of the astronaut to make him more equipped for space, rather than modify his environment to make it more hospitable to sustaining human life\u2014the notion of the cyborg, as it was originally proposed in the early 1960s, was formulated around the same principle of technoscientifically engineering biological processes of steady-state stability to adapt the organism for life in space. See Clynes and Kline, \u201cCyborgs and Space.\u201d See also Gray,  Cyborg Handbook . 42. For more, see Keller, \u201cOrganisms, Machines, and Thunderstorms.\u201d 43. Indeed, as David Mindell points out, astronauts ultimately did very little \u201cflying\u201d in the piloting sense, generally only manning the vehicle during landings (giving credence to the infamous quip put to print by Tom Wolfe that astronauts were in fact nothing more than \u201cSpam in a can\u201d). See Mindell,  Digital Apollo ; Wolfe,  Right Stuff . 44. NASA and American Institute of Biological Sciences,  Bioregenerative Systems , 137. 46. Odum, \u201cModel of a Closed Terrarium with People\u201d; emphasis added. 47. Here I am indebted to Lisa Messeri\u2019s work of theorizing \u201cthe inhabitable\u201d as both a place-making practice and as a kind of potentiality, a way of imagining the conditions necessary for an exoplanet to be deemed inhabitable. See Messeri,  Placing Outer Space . I build on her work by thinking through what it might mean to render the cabin \u201cinhabitable\u201d in the multispecies studies sense, which construes \u201cinhabiting\u201d not as the passive coexistence between species, but the active and coparticipatory making of worlds. See, e.g., Haraway,  Companion Species Manifesto ; Haraway,  When Species Meet ; Haraway,  Staying with the Trouble . 48. Here, I follow Stefan Helmreich\u2019s move to consider how scientific knowledge about life forms (in his case, pelagic microbes) stages new forms of life in broader social and cultural contexts. See Helmreich,  Alien Ocean . 49. Canguilhem, \u201cLiving and Its Milieu.\u201d See also Helmreich,  Alien Ocean ; Olson, \u201cEcobiopolitics of Space Biomedicine.\u201d 50. As of this writing, NASA continues to research potential uses of plants in space, but these experiments are concerned more immediately with cultivation for agricultural purposes. For more, see Gitelson and Lisovsky,  Man-Made Closed Ecological Systems . The Soviet Union, by contrast, came much closer to actual implementation of a bioregenerative system, conducting multiple long-term experiments involving humans and algae in closed systems throughout the Cold War. The most extraordinary of these was Bios-3, a sealed underground facility constructed in Siberia in 1972 that harbored three crew members for one year using plants as the only source of oxygen. See Salisbury, Gitelson, and Lisovsky, \u201cBios-3.\u201d 51. See, e.g., Scott, \u201cEarthlike\u201d; H\u00f6hler,  Spaceship Earth in the Environmental Age, 1960\u20131990 ; Anker, \u201cEcological Colonization of Space.\u201d See also Scott,  Outlaw Territories ; Easterling,  Extrastatecraft . 52. Indeed, even in the context of space travel today, sterile as it may be, scholars have uncovered encounters of contingency and sociality between cohabitating botanical and human life forms in the space cabin. See, e.g., Battaglia, \u201cDiary of a Space Zucchini\u201d; Ford, \u201cCultivating an Outer Space Ecology\u201d; Oman-Reagan, \u201cSocial Life of Plants, in Space.\u201d 53. Myers, \u201cPhotosynthesis.\u201d 54. Sloterdijk,  Globes , 770.   References Anker, Peder . \u201c Buckminster Fuller as Captain of Spaceship Earth .\u201d  Minerva   45 , no.  4  ( 2007 ):  417 \u2013 34 . Anker, Peder . \u201c The Ecological Colonization of Space .\u201d  Environmental History   10 , no.  2  ( 2005 ):  239 \u2013 68 .  . Anker, Peder .  From Bauhaus to Ecohouse: A History of Ecological Design .  Baton Rouge :  LSU Press ,  2010 . Battaglia, Debbora . \u201c Arresting Hospitality: The Case of the \u2018Handshake in Space.\u2019 \u201d  Journal of the Royal Anthropological Institute   18  ( 2012 ):  S76 \u2013 S89 . Battaglia, Debbora ,  Valentine, David , and  Olson, Valerie . \u201c Relational Space: An Earthly Installation .\u201d  Cultural Anthropology   30 , no.  2  ( 2015 ):  245 \u2013 56 .  . Belasco, Warren . \u201c Algae Burgers for a Hungry World? The Rise and Fall of Chlorella Cuisine .\u201d  Technology and Culture   38 , no.  3  ( 1997 ):  608 \u2013 34 .  . Belasco, Warren .  Meals to Come: A History of the Future of Food .  Berkeley :  University of California Press ,  2006 . Bocking, Stephen .  Ecologists and Environmental Politics: A History of Contemporary Ecology .  New Haven, CT :  Yale University Press ,  1997 . Bocking, Stephen .  Nature\u2019s Experts: Science, Politics, and the Environment .  New Brunswick, NJ :  Rutgers University Press ,  2004 . Bongers, Leonard . \u201c Sustaining Life in Space: A New Approach .\u201d  Aerospace Medicine   35  ( 1964 ):  139 \u2013 44 . Boulding, Kenneth E.  \u201c The Economics of the Coming Spaceship Earth .\u201d In  Environmental Quality in a Growing Economy; Essays from the Sixth RFF Forum , edited by  Jarrett, Henry ,  3 \u2013 15 .  Baltimore :  Johns Hopkins University Press ,  1966 . Bowman, Russel O. , and  Thomae, Fred W.  \u201c An Algae Life Support System .\u201d  Aerospace Engineering   19  ( 1960 ):  26 \u2013 30 . Bowman, Russel O. , and  Thomae, Fred W.  \u201c Long-Term Nontoxic Support of Animal Life with Algae .\u201d  Science   134 , no.  3471  ( 1961 ):  55 \u2013 56 . Bruno, Laura A.  \u201c The Bequest of the Nuclear Battlefield: Science, Nature, and the Atom during the First Decade of the Cold War .\u201d  Historical Studies in the Physical and Biological Sciences   33 , no.  2  ( 2003 ):  237 \u2013 60 .  . Burlew, John Swaim , ed.  Algal Culture: From Laboratory to Pilot Plant .  Washington, DC :  Carnegie Institution ,  1953 . Canguilhem, Georges . \u201c The Living and Its Milieu .\u201d Translated by  Savage, John .  Grey Room   3  ( 2001 ):  7 \u2013 31 .  . Casper, Monica J. , and  Moore, Lisa Jean . \u201c Inscribing Bodies, Inscribing the Future: Gender, Sex, and Reproduction in Outer Space .\u201d  Sociological Perspectives   38 , no.  2  ( 1995 ):  311 \u2013 33 .  . Clynes, Manfred E. , and  Kline, Nathan S.  \u201c Cyborgs and Space .\u201d  Astronautics  ( 1960 ):  29 \u2013 33 . Conway, Erik M.   Exploration and Engineering: The Jet Propulsion Laboratory and the Quest for Mars .  Baltimore :  Johns Hopkins University Press ,  2015 . Cooke, G. Dennis ,  Beyers, Robert , and  Odum, Eugene . \u201c The Case for the Multispecies Ecological System, with Special Reference to Succession and Stability .\u201d In NASA and American Institute of Biological Sciences,  Bioregenerative Systems ,  129 \u2013 39 . Creager, Angela N. H.   Life Atomic: A History of Radioisotopes in Science and Medicine .  Chicago :  University of Chicago Press ,  2013 . Cullather, Nick . \u201c The Foreign Policy of the Calorie .\u201d  American Historical Review   112 , no.  2  ( 2007 ):  337 \u2013 64 . Cullather, Nick . \u201c \u2018Stretching the Surface of the Earth\u2019: The Foundations, Neo-Malthusianism, and the Modernising Agenda .\u201d  Global Society   28 , no.  1  ( 2014 ):  104 \u2013 12 .  . Cunningham, Walter .  The All-American Boys .  New York :  Macmillan ,  1977 . DeLoughrey, Elizabeth M.  \u201c The Myth of Isolates: Ecosystem Ecologies in the Nuclear Pacific .\u201d  Cultural Geographies   20 , no.  2  ( 2013 ):  167 \u2013 84 .  . Denis, J\u00e9r\u00f4me , and  Pontille, David . \u201c Material Ordering and the Care of Things .\u201d  Science, Technology, and Human Values   40 , no.  3  ( 2014 ):  338 \u2013 67 .  . Easterling, Keller .  Extrastatecraft: The Power of Infrastructure Space .  Brooklyn, NY :  Verso ,  2014 . Eugene Odum Papers\u2014Institute of Ecology . UGA 97\u2013045.  Hargrett Rare Book and Manuscript Library, University of Georgia . Fuller, R. Buckminster .  Operating Manual for Spaceship Earth .  Carbondale :  Southern Illinois University Press ,  1969 . Galison, Peter . \u201c The Ontology of the Enemy: Norbert Wiener and the Cybernetic Vision .\u201d  Critical Inquiry   21 , no.  1  ( 1994 ):  228 \u2013 66 . Gitelson, J. I. , and  Lisovsky, G. M.   Man-Made Closed Ecological Systems .  London :  Taylor and Francis ,  2003 . Golley, Frank B.   A History of the Ecosystem Concept in Ecology: More than the Sum of the Parts .  New Haven, CT :  Yale University Press ,  1993 . Gray, Chris Hables , ed.  The Cyborg Handbook .  New York :  Routledge ,  1995 . Hagen, Joel Bartholemew .  An Entangled Bank: The Origins of Ecosystem Ecology .  New Brunswick, NJ :  Rutgers University Press ,  1992 . Haraway, Donna Jeanne .  The Companion Species Manifesto: Dogs, People, and Significant Otherness .  Chicago :  Prickly Paradigm ,  2003 . Haraway, Donna Jeanne .  Staying with the Trouble: Making Kin in the Chthulucene .  Durham, NC :  Duke University Press ,  2016 . Haraway, Donna Jeanne .  When Species Meet .  Minneapolis :  University of Minnesota Press ,  2008 . Harrison, Albert A.   Spacefaring: The Human Dimension .  Berkeley :  University of California Press ,  2001 . Hayles, N. Katherine .  Chaos Bound: Orderly Disorder in Contemporary Literature and Science .  Ithaca, NY :  Cornell University Press ,  1990 . Hayles, N. Katherine .  How We Became Posthuman: Virtual Bodies in Cybernetics, Literature, and Informatics .  Chicago :  University of Chicago Press ,  1999 . Helmreich, Stefan .  Alien Ocean: Anthropological Voyages in Microbial Seas .  Berkeley :  University of California Press ,  2009 . Helmreich, Stefan . \u201c Extraterrestrial Relativism .\u201d  Anthropological Quarterly   85 , no.  4  ( 2012 ):  1125 \u2013 39 .  . Hersch, Matthew H.   Inventing the American Astronaut .  New York :  Palgrave Macmillan ,  2012 . Hersch, Matthew H.  \u201c Return of the Lost Spaceman: America\u2019s Astronauts in Popular Culture, 1959\u20132006 .\u201d  Journal of Popular Culture   44 , no.  1  ( 2011 ):  73 \u2013 92 .  . H\u00f6hler, Sabine . \u201c The Environment as a Life Support System: The Case of Biosphere 2 .\u201d  History and Technology   26 , no.  1  ( 2010 ):  39 \u2013 58 .  . H\u00f6hler, Sabine .  Spaceship Earth in the Environmental Age, 1960\u20131990 .  London :  Pickering and Chatto ,  2015 . Howard T. Odum Papers . George A. Smathers Libraries\u2014Special and Area Studies Collection,  University of Florida . Hustak, Carla , and  Myers, Natasha . \u201c Involutionary Momentum: Affective Ecologies and the Sciences of Plant/Insect Encounters .\u201d  Differences   23 , no.  3  ( 2012 ):  74 \u2013 118 .  . Kammermeyer, Karl , ed.  Atmosphere in Space Cabins and Closed Environments .  New York :  Appleton, Century, Crofts ,  1966 . Kay, Lily E.   Who Wrote the Book of Life? A History of the Genetic Code .  Stanford, CA :  Stanford University Press ,  2000 . Keller, Evelyn Fox . \u201c Organisms, Machines, and Thunderstorms: A History of Self-Organization, Part Two. Complexity, Emergence, and Stable Attractors .\u201d  Historical Studies in the Natural Sciences   39 , no.  1  ( 2009 ):  1 \u2013 31 .  . Keller, Evelyn Fox .  Refiguring Life: Metaphors of Twentieth-Century Biology .  New York :  Columbia University Press ,  1995 . Kingsland, Sharon E.   The Evolution of American Ecology, 1890\u20132000 .  Baltimore :  Johns Hopkins University Press ,  2005 . Kirksey, S. Eben , and  Helmreich, Stefan . \u201c The Emergence of Multispecies Ethnography .\u201d  Cultural Anthropology   25 , no.  4  ( 2010 ):  545 \u2013 576 .  . Konecci, Eugene . \u201c Space Ecological Systems .\u201d In  Bioastronautics , edited by  Schaefer, Karl E. ,  274 \u2013 304 .  New York :  Macmillan ,  1964 . Krauss, Robert W.  \u201c The Physiology and Biochemistry of Algae, with Special Reference to Continuous-Culture Techniques for Chlorella .\u201d In NASA and American Institute of Biological Sciences,  Bioregenerative Systems ,  97 \u2013 110 . Kwa, Chunglin . \u201c Radiation Ecology, Systems Ecology, and the Management of the Environment .\u201d In  Science and Nature: Essays in the History of the Environmental Sciences , edited by  Shortland, Michael ,  213 \u2013 49 .  Chalfont St. Giles :  British Society for the History of Science ,  1993 . Lambright, W. Henry .  Why Mars: NASA and the Politics of Space Exploration .  Baltimore :  Johns Hopkins University Press ,  2014 . Launius, Roger D.  \u201c Writing the History of Space\u2019s Extreme Environment .\u201d  Environmental History   15 , no.  3 :  526 \u2013 32 .  . Lilienfeld, Robert .  The Rise of Systems Theory: An Ideological Analysis .  New York :  Wiley ,  1978 . Mackowski, Maura Phillips .  Testing the Limits: Aviation Medicine and the Origins of Manned Space Flight .  College Station :  Texas A&M University Press ,  2006 . Martin, Aryn ,  Myers, Natasha , and  Viseu, Ana . \u201c The Politics of Care in Technoscience .\u201d  Social Studies of Science   45 , no.  5  ( 2015 ):  625 \u2013 41 .  . Masco, Joseph . \u201c Bad Weather: On Planetary Crisis .\u201d  Social Studies of Science   40 , no.  1  ( 2010 ):  7 \u2013 40 .  . Masco, Joseph . \u201c The End of Ends .\u201d  Anthropological Quarterly   85 , no.  4  ( 2012 ):  1107 \u2013 24 .  . Mattson, Howard . \u201c Keeping Astronauts Alive .\u201d  International Science and Technology   54  ( 1966 ):  28 \u2013 37 . McCurdy, Howard E.   Space and the American Imagination .  Washington, DC :  Smithsonian Institution Press ,  1997 . Messeri, Lisa .  Placing Outer Space: An Earthly Ethnography of Other Worlds .  Durham, NC :  Duke University Press ,  2016 . Mindell, David A.   Between Human and Machine: Feedback, Control, and Computing before Cybernetics .  Baltimore :  Johns Hopkins University Press ,  2002 . Mindell, David A.   Digital Apollo: Human and Machine in Spaceflight .  Cambridge, MA :  MIT Press ,  2008 . Multispecies Editing Collective . \u201c Troubling Species: Care and Belonging in a Relational World .\u201d  RCC Perspectives: Transformations in Environment and Society , no.  1  ( 2017 ). Munns, David P. D. , and  Nickelsen, K\u00e4rin . \u201c The Algatron versus the Fecal Bag: Reconsidering the Space Program from the Ground Up ,\u201d unpublished manuscript, n.d. Murphy, Michelle . \u201c Unsettling Care: Troubling Transnational Itineraries of Care in Feminist Health Practices .\u201d  Social Studies of Science   45 , no.  5  ( 2015 ):  717 \u2013 37 .  . Myers, Jack . \u201c The Use of Photosynthesis in a Closed Ecological System .\u201d In  Physics and Medicine of the Atmosphere and Space , edited by  Benson, Otis  and  Strughold, Hubertus ,  388 \u2013 96 .  New York :  Wiley ,  1960 . Myers, Jack , and  Clark, L. B.  \u201c Culture Conditions and the Development of the Photosynthetic Mechanism: An Apparatus for the Continuous Culture of Chlorella .\u201d  Journal of General Physiology   28 , no.  2  ( 1944 ):  103 \u2013 12 . NASA and American Institute of Biological Sciences .  Bioregenerative Systems: A Conference Held in Washington, DC, November 15\u201316, 1966, Sponsored by National Aeronautics and Space Administration and the American Institute of Biological Sciences .  Washington, DC :  Scientific and Technical Information Division, Office of Technology Utilization, National Aeronautics and Space Administration ,  1968 . Neufeld, Michael J.   Spacefarers: Images of Astronauts and Cosmonauts in the Heroic Era of Spaceflight .  Washington, DC :  Smithsonian Institution Scholarly Press ,  2013 . Nickelsen, K\u00e4rin .  Explaining Photosynthesis: Models of Biochemical Mechanisms, 1840\u20131960 .  Dordrecht :  Springer ,  2015 . Odum, Eugene P.  \u201c The New Ecology .\u201d  BioScience   14 , no.  7  ( 1964 ):  14 \u2013 16 .  . Odum, Eugene P. , and  Beyers, Robert . \u201c Proposal to Environmental Biology Section, Biospace Program .\u201d N.d. Box 44, \u201cOdum-Beyers Grant\u201d Folder, Hargrett Rare Book and Manuscript Library,  University of Georgia ,  Athens . Odum, Eugene P. , and  Beyers, Robert . \u201c Proposal to the NASA Biospace Program .\u201d  1964 . Box 44, \u201cOdum-Beyers Grant\u201d Folder, Hargrett Rare Book and Manuscript Library,  University of Georgia ,  Athens . Odum, Howard T.  \u201c Limits of Remote Ecosystems Containing Man .\u201d  American Biology Teacher   25 , no.  6  ( 1963 ):  429 \u2013 43 .  . Odum, Howard T.  \u201c Model of a Closed Terrarium with People .\u201d N.d. (circa  1980 ). Box 40, \u201cModels of a Closed Terrarium with Humans\u201d Folder, Howard T. Odum Papers, Special and Area Studies Collections, George A. Smathers Libraries,  University of Florida ,  Gainesville, Florida . Odum, Howard T. , and  Odum, Eugene P.  \u201c Trophic Structure and Productivity of a Windward Coral Reef Community on Eniwetok Atoll .\u201d  Ecological Monographs   25 , no.  3  ( 1955 ):  291 \u2013 320 .  . Olson, Valerie A.   American Extreme .  Minneapolis :  University of Minnesota Press , forthcoming. Olson, Valerie A.  \u201c The Ecobiopolitics of Space Biomedicine .\u201d  Medical Anthropology   29 , no.  2  ( 2010 ):  170 \u2013 93 .  . Oman-Reagan, Michael P.  \u201c The Social Life of Plants, in Space .\u201d  Astrosociological Insights   4 , no.  2  ( 2015 ):  4 \u2013 8 . Oswald, W. , and  Golueke, C.  \u201c Man in Space\u2014He Takes along His Wastes Problem .\u201d  Wastes Engineering   32  ( 1961 ):  456 \u2013 59 . Pickering, Andrew .  The Cybernetic Brain: Sketches of Another Future .  Chicago :  University of Chicago Press ,  2010 . Pitts, John A. S.   The Human Factor: Biomedicine in the Manned Space Program to   1980 .  Washington, DC :  Scientific and Technical Information Branch, National Aeronautics and Space Administration ,  1985 . Puig de la Bellacasa, Maria . \u201c Making Time for Soil: Technoscientific Futurity and the Pace of Care .\u201d  Social Studies of Science   45 , no.  5  ( 2015 ):  691 \u2013 716 .  . Puig de la Bellacasa, Maria .  Matters of Care: Speculative Ethics in More Than Human Worlds .  Minneapolis :  University of Minnesota Press ,  2017 . Puig de la Bellacasa, Maria . \u201c Matters of Care in Technoscience: Assembling Neglected Things .\u201d  Social Studies of Science   41 , no.  1  ( 2011 ):  85 \u2013 106 . Puig de la Bellacasa, Maria . \u201c \u2018Nothing Comes without Its World\u2019: Thinking with Care .\u201d  Sociological Review   60 , no.  2  ( 2012 ):  197 \u2013 216 .  . Rao, Mohan . \u201c An Imagined Reality: Malthusianism, Neo-Malthusianism, and Population Myth .\u201d  Economic and Political Weekly   29 , no.  5  ( 1994 ):  PE40 \u2013 PE52 . Roosth, Sophia . \u201c Life, Not Itself: Inanimacy and the Limits of Biology .\u201d  Grey Room  no.  57  ( 2014 ):  56 \u2013 81 . Rothschild, Rachel . \u201c Environmental Awareness in the Atomic Age. Radioecologists and Nuclear Technology .\u201d  Historical Studies in the Natural Sciences   43 , no.  4  ( 2013 ):  492 \u2013 530 .  . Salisbury, Frank B. ,  Gitelson, Josef I. , and  Lisovsky, Genry M.  \u201c Bios-3: Siberian Experiments in Bioregenerative Life Support .\u201d  BioScience   47 , no.  9  ( 1997 ):  575 \u2013 85 . Saunders, Joseph . \u201c Introduction .\u201d In NASA and American Institute of Biological Sciences,  Bioregenerative Systems ,  vii \u2013 viii . Schlosser, Kolson . \u201c Malthus at Mid-Century: Neo-Malthusianism as Bio-Political Governance in the Post-WWII United States .\u201d  Cultural Geographies   16 , no.  4  ( 2009 ):  465 \u2013 84 .  . Scott, Felicity D.  \u201c Earthlike .\u201d  Grey Room   65  ( 2016 ):  6 \u2013 35 .  . Scott, Felicity D.   Outlaw Territories: Environments of Insecurity/Architectures of Counterinsurgency .  New York :  Zone Books ,  2016 . Scott, James C.   Seeing like a State: How Certain Schemes to Improve the Human Condition Have Failed .  New Haven, CT :  Yale University Press ,  1998 . Sloterdijk, Peter .  Globes: Spheres Volume II: Macrospherology . Translated by  Hoban, Wieland .  South Pasadena, CA :  Semiotext(e) ,  2014 . Sobchack, Vivian . \u201c The Virginity of Astronauts: Sex and the Science Fiction Film .\u201d In  Alien Zone: Cultural Theory and Contemporary Science Fiction Cinema , edited by  Kuhn, Annette ,  103 \u2013 15 .  London :  Verso ,  1990 . Star, Susan Leigh . \u201c Power, Technology, and the Phenomenology of Conventions: On Being Allergic to Onions .\u201d  Sociological Review   38 , no.  S1  ( 1990 ):  26 \u2013 56 .  . Stengers, Isabelle .  Cosmopolitics I . Translated by  Bononno, Robert .  Minneapolis :  University of Minnesota Press ,  2010 . Taylor, Peter . \u201c Technocratic Optimism, H. T. Odum, and the Partial Transformation of Ecological Metaphor after World War II .\u201d  Journal of the History of Biology   21 , no.  2  ( 1988 ):  213 \u2013 44 .  . Tsing, Anna Lowenhaupt .  The Mushroom at the End of the World: On the Possibility of Life in Capitalist Ruins .  Princeton, NJ :  Princeton University Press ,  2015 . Valentine, David . \u201c Atmosphere: Context, Detachment, and the View from above Earth .\u201d  American Ethnologist   43 , no.  3  ( 2016 ):  511 \u2013 24 .  . Valentine, David ,  Olson, Valerie A. , and  Battaglia, Debbora . \u201c Encountering the Future: Anthropology and Outer Space .\u201d  Anthropology News   50 , no.  9  ( 2009 ):  11 \u2013 15 .  . Valentine, David ,  Olson, Valerie A. , and  Battaglia, Debbora . \u201c Extreme: Limits and Horizons in the Once and Future Cosmos .\u201d  Anthropological Quarterly   85 , no.  4  ( 2012 ):  1007 \u2013 26 .  . van Dooren, Thom . \u201c Care .\u201d  Environmental Humanities   5 , no.  1  ( 2014 ):  291 \u2013 94 .  . van Dooren, Thom ,  M\u00fcnster, Ursula ,  Kirksey, Eben ,  Chrelew, Matthew , and  Tsing, Anna , eds. \u201c Multispecies Studies .\u201d  Environmental Humanities   8 , no.  1  ( 2016 ). Ana, Viseu ,  Myers, Natasha ,  Martin, Aryn , and  Suchman, Lucy , eds. \u201c The Politics of Care in Technoscience .\u201d  Social Studies of Science   45 , no.  5 . Ward, Barbara .  Spaceship Earth .  New York :  Columbia University Press ,  1966 . Ward, C. H. ,  Wilks, S. S. ,  Craft, H. L. , and  Wilks, C. H.  \u201c Use of Algae and Other Plants in the Development of Life Support Systems .\u201d  American Biology Teacher   25 , no.  7  ( 1963 ):  512 \u2013 21 .  . Wolfe, Tom .  The Right Stuff .  New York :  Farrar, Straus, and Giroux ,  1979 . Worster, Donald .  Nature\u2019s Economy: A History of Ecological Ideas .  1977 .  Cambridge :  Cambridge University Press ,  1994 . \u00a9 2017 Leah V. Aronowsky 2017 This is an open access article distributed under the terms of a Creative Commons license (CC BY-NC-ND 3.0)."}, "210705_news_467922.txt": {"page_id": "210705_news_467922.txt", "text": "Welcome to my first blog post as National Data Guardian for Health and Social Care. I am incredibly excited to be taking up this role. During my tenure, I intend to do my utmost to listen to the public and to advise and guide those making important decisions about data use, so that information is used for public benefit in line with people\u2019s expectations. Prior to my appointment, the Health and Social Care Committee asked me to share with them my background, motivations and priorities for this role. I submitted a statement, which is available to  read in full online . I am honoured to be following in the footsteps of the late Dame Fiona Caldicott, who did so much during her time as England\u2019s first National Data Guardian. She served as a fierce champion for patients, carers, and social care service users in matters related to their data. We have much to thank her for, and her impact on health and social care information governance leaves a legacy we all continue to benefit from. I take seriously the role that the National Data Guardian should play in supporting and influencing the safe, confidential and effective use of citizens\u2019 health and care data. In doing so, I\u2019m keen to support data use to both improve the treatment and care of individuals now, and to advance innovation in illness prevention, treatment, and service planning for shared public benefit in future. We need only look to the last year we\u2019ve all lived through to see evidence on a global scale of how essential the effective use of data has been to the world\u2019s response to the pandemic. Data has provided invaluable insights into questions such as where and how the disease is spreading, who is most vulnerable, how we can best protect ourselves and our loved ones, and what treatments are and are not effective. Using these insights to inform critical decision making has undoubtedly saved lives. Whilst supporting such innovative data use, I will remain mindful of my responsibility as National Data Guardian to act as an independent \u2018critical friend\u2019 to health and care organisations, ensuring that I always call to attention the inherent risks of data sharing and use, including unintended consequences and \u2018mission creep\u2019, alongside its benefits. Public trust in institutions can be fragile for many reasons and is easily lost, and the effective use of accurate, complete data ultimately rests on the health of the two-way relationship between the individuals whom the data is about, and those collecting and using it. I\u2019ll seek to provide advice that maintains a balance between the extremes of individual clinicians and organisations being either too paralysed by risk aversion to use data, or acting as if positive thinking and the integrity of well-meaning individuals alone are enough to make systems worthy of public trust. My motivation to be National Data Guardian is both professional and personal. As a clinician, I\u2019ve seen first-hand the growing benefits that data-driven technology is delivering: for example, through individuals having access to digital tools and resources to strengthen their self-management, clinicians having access to AI tools for clinician decision support, and teams seeing real-time aggregated data on incidents to improve the safety of their service. I\u2019m also aware of the need to address more fundamental issues, such as the integrity of our digital infrastructure, system governance and clinician confidence to support timely information sharing between (and sometimes within) organisations. My personal family experience has also brought home the importance of getting these basics right. I\u2019m currently very aware of the frustration, anxiety and sometimes distress that can ensue when critical healthcare information isn\u2019t readily available to the clinicians who need it, when they need it. In my view, \u2018cutting-edge\u2019 digital innovation and the arguably less glamorous basics go hand in hand (indeed the success of the former relies on the latter); it\u2019s vital therefore that continuous attention is paid to the balance of investment between each. As the pandemic abates, and as a society we explore what should (and what perhaps can\u2019t) return to \u2018how things used to be\u2019, it is essential that decision makers hear the public\u2019s views about how data and data-driven technology should be used in future. We must understand if attitudes have shifted, and what people expect and want, as we emerge from this (as with any) crisis. Under Dame Fiona, the Office of the National Data Guardian co-commissioned a series of citizens\u2019 juries with the University of Manchester and NHSX to explore these questions; they started last month, and I am looking forward to participating as an observer. I\u2019ve no doubt that listening to a diverse range of citizens\u2019 views on this topic will provide valuable insights to inform discussions about matters such as the use of health and care data for COVID certification \u2013 discussions to which I intend to contribute. Dame Fiona worked with an unwavering commitment for many years to safeguard public trust and advocate for the responsible, transparent use of health and care data. I intend to honour her legacy by building upon the enduring foundations she laid. We share a belief that there should be \u2018no surprises\u2019 for the public when it comes to their data: a belief that will guide me as I build upon the long-standing  NDG priorities . Because without trust, and the dialogue and transparency that builds trust, we will not earn the public\u2019s support for initiatives that seek to improve health and care using data. I am therefore starting my tenure as I mean to go on, with an active listening exercise. Over the coming months I am greatly looking forward to meeting many of the groups and organisations for whom data matters: to listen, pose questions, and reach a rich understanding of the data landscape, and how organisations within it can build and maintain trust now, from a diverse range of perspectives. Alongside my NDG duties, I will continue to work as a psychiatrist within adult mental health care. Maintaining a clinical position is important to me, as from previous experience, I\u2019ve learnt that (for me at least), the longer I spend away from the frontline, the easier systemic problems seem to fix (\u2018If people would just\u2026\u2019). So I intend to stay actively rooted in the complex clinical reality of providing and receiving healthcare and support to the population I will also serve in my national role. As I listen, act and reflect over my first year as National Data Guardian, I intend to share my thoughts and findings in a series of blogs. I\u2019m sure there will be a lot to talk about in 2021, and I look forward to having those conversations."}, "210705_news_467923.txt": {"page_id": "210705_news_467923.txt", "text": "Seer is looking for passionate people to join our growing team as we scale from dozens of well known customers like LVMH, Saks Fifth Avenue to thousands. We\u2019re a small well-funded startup backed by YC, Foundation Capital, Global Founders Capital, and SoftBank based in Palo Alto, CA. Come help us build a new way for retailers to connect and sell to their customers! Role: As an early member of our engineering team, you'll get to drive key product initiatives on the frontend, make foundational architectural and design decisions, and play a pivotal role in making Seer a category-defining product. Responsibilities: Implement the features and user interfaces Architect efficient and reusable front-end systems that drive complex web applications Collaborate with Designers, Product Managers, and Software Engineers to deliver compelling user-facing products The Ideal Candidate: A strong team player, with a do whatever it takes attitude Detail-oriented and can keep up with timelines Able to balance speed and quality when building Requirements: 3+ years professional experience Strong Javascript skills Positive and solution-oriented mindset Communicate well with engineers and non-engineers alike Skills:  JavaScript, Vue, HTML, CSS Bonus Points: \nYou have experience in writing UI Automation tests with Selenium, Cypress or Puppeteer..\nPast experience working in startups About Seer Seer is a software platform that helps retail sales people create personalized lookbooks that can be sent via sms, email. Our 2 year goal is to become an indispensable communication and content creation tool for retail salespeople. Our 5 year goal is to become the #1 sales tool in the world helping individuals start businesses (like shopify did for e-commerce) and redefine the role of retail sales associates. In March 2021, we raised $2.3M in seed funding. Looking for an engineering director who has experience leading and scaling a team. If you want to learn more about Seer, check out our website, our post on product hunt and this article on Vogue Business: https://www.voguebusiness.com/technology/startup-spotlight-seer-the-luxury-clienteling-software https://www.helloseer.com/ https://www.producthunt.com/posts/seer-4"}, "210705_news_467965.txt": {"page_id": "210705_news_467965.txt", "text": "It\u2019s rare that you see a game that gives top billing in its marketing to the quality of its procedurally generated levels. Normally PCG is sprinkled in a game to add a bit of variety, or to make up for the lack of actual level design. But, for 2017\u2019s  Unexplored , the rest of the game is there to justify the stellar levels. Unexplored presents itself as a fairly standard roguelite \u2013 enter a randomly generated dungeon, descend 20 levels and retrive the amulet of Yendor. The gameplay features a realtime combat based around timing and aiming your swings, but otherwise plays things by the book.  But it doesn\u2019t take long realize why they much such a big deal out of the procedural generation. Unexplored level design takes more after 2D Zelda games than it does Rogue. Instead of just wandering at random, you quickly find that the path forward is blocked, forcing you to solve puzzles, find items and keys, defeat enemies to continue. There\u2019s a huge variety of structure, all randomly generated, but nearly every level is a tightly packed, interesting space.  Full video here So far I\u2019ve discussed some of the key concepts for this sort of level generation:  Lock and Key Dungeons  and  Graph Rewriting . Then I described  the  tools  Dormans used to design everything. But I was a bit brief on how simple find-replace rules can build such complex dungeons. So this article we\u2019ll dive into those rules, building on that previous knowledge. The details and images you\u2019ll see come more or less straight out of Ludoscope. I can\u2019t talk about everything. There are two main generators \u2013 one for the dungeon as a whole, and one for floorplans of each level. Each one has many modules and about 5000 individual find-replace rules. The Ludoscope tooling allowed Dormans to iterate very rapidly! That does mean a lot of the \u201csecret\u201d for the varied dungeons is simply a ton of different manually coded cases.  The top level modules for the floorplan generator. Each has dozens of rules and instructions! I\u2019m going to focus on the floorplan generator, and abridge things somewhat, but it\u2019s still a long article. If you just want the highlights, I\u2019d read the  general structure section , then  Cyclic Dungeon Generation , and skip the rest. General structure of a floorplan By the time we start generating floor plan, the dungeon generator has already created 20 level requests, and marked each one with some specifics it needs, like exits/entrances, items and bosses. It also decides some high level details of the level, such as the name. The floorplan generator then takes each one of those level requests, and generates all the rooms, additional encounters and enemies, and then designs a tile-based map to hold everything.  There\u2019s around 50 PhantomGrammar modules in in the generator, but it\u2019s easier just to look at the main steps: Designing the overall layout First, a square grid of empty cells is constructed. Then a start, end, and large rough circle are drawn on the grid. One of the major cycle types is chosen, and the circle converted to use it. Minor cycles add complications to the main one Add some dead ends Shuffle some of the nodes Resolve specifics that have been left general so far Choose puzzles, locks, doors, enemies Decide which nodes are enclosed rooms, caves, width of corridors Randomly add treasures Convert from grid of graph nodes to tile map Double the grid resolution, fill in corridor tiles between nodes. Increase the grid resolution 5x Draw rooms and corridors Draw special rooms and set pieces Randomize terrain with some noise Pick position of items/enemies within rooms Smooth off sharp edges, randomly draw vegetation Here\u2019s a timelapse for a particular level. Initial 5\u00d75 Grid Generate the main cycle Use the cycle (in this case, to make a central lake) Add minor cycles Add more abstract details Resolve abstractions to specific items Convert to coarse grid Convert to fine grid Cell noise to get patterns of terrain Apply terrain to map Add items and objects Beauty pass adding smoothing and vegetation Designing the Overall Layout Each level is designed on a 5\u00d75 (or similar) grid of graph nodes. Because almost all subsequent operations are done via graph replacement which has no notion of shape, location or rotation, this grid ensures that the ensuing graph still follows a 2 dimensional plan. The grid nodes are never deleted or moved, just annotated, so when we come to turn the graph into a tilemap later, it\u2019s an easy operation. One of the first things the generator does is draw a start, end, and a big roughly circular loop stretching between them. We\u2019ll get to \u201cwhy a loop\u201d shortly, but first I\u2019ll show how graph replacement can be used to draw shapes, as an illustration of how one codes in a graph replacement system. Drawing a Circle PhantomGrammar replacement rules are very flexible, but one limitation is that each rule always matches a fixed number of nodes. Therefore, anything of variable size, or high complexity, needs to be broken down into a series of smaller rules that can operate on different parts of the overall graph, often setting intermediate values to be \u201cfixed\u201d by later rules.  Initial setup Pick a random starting location Draw a short random walk Randomly add more empty nodes Trace clockwise from the first cycle node around the existing empty nodes Randomly add the goal, and cycleEntrance and cycleTarget Even this diagram is somewhat abridged \u2013 there\u2019s 40 different rules involved in this process, mostly needed accounting for variations. Similar, simpler, patterns occur all over the code. Whenever something of variable size is needed, there\u2019s a set of rules to set the intial condition, more rules that \u201cgrow\u201d the pattern repeatedly, and a final cleanup step. Cyclic Dungeon Generation The drawn circle goes on to become the backbone of the level structure.  Dormans calls this a \u201c cyclic dungeon generator\u201d , and it\u2019s a feature that gives the levels a meaningful arc of progress and pacing. The idea is simple. The generator draws a large circular loop, with a entrance and goal node attached. The entrance and exit divide the loop into two indpendent arcs, both leading between the entrace and the goal. The game then picks from a number of predefined  major cycle types  which each specify how to use those two arcs. The cycle type defines the narrative ebb and flow of the level. The simplest cycle type simply uses the two arcs as two alternative routes the goal, each with a different set of obstacles. More sophsticated cycles can make use of the arcs in a wide variety of ways. Many games have pre-authored high level structure, but this specific approach has a lot to recommend: Drawing random circles is easy. Because the structures have two parallel arcs to work with (rather than a more common tree structure), there\u2019s a lot more possible interesting ways to arrange them. And it\u2019s easy to arrange for a  one way door  to cut down on player backtracking, a common trick used in hand authored dungeons. The cycle types are extremely general. They  only  control the main flow of the dungeon, and specify very little about specific forks, layout or details, which are all filled in later. Many of the cycle types contain special rules to make them more generic, such as randomly picking what sort of obstacles are found on an arc, or randomly locating where keys appear. ThreeLeverHub is a cycle type that locks the goal (g) behind three keys randomly placed in the level, and assigns a guardian enemy to watch the door. Unexplored has 24 different cycle types, several of which are shown in the diagram above. Most of the cycles involve placing keys, doors and one way \u201cvalves\u201d to force player progress to follow a known plot. But not all have a strict path. Hubs, for example, have the entire loop easy to navigate, but lock the actual exit behind some sort of challenge. Hubs are used for levels with multiple exits. Another oddball cycle type is the lake \u201ccycle\u201d, which treats the loop as the border of a large central lake, and has a more free roaming feel than many other designs. Embelishments After the major cycle has been generated, extra nodes are added to complicate the dungeon futher. Minor cycles, are short detours from the main cycle that can be added, often including more keys and obstacles. There\u2019s also rules for making cycles longer, or adding dead ends. Combinations of these rules are run until the level has grown to the desired size. Resolution of Abstraction Once the overall layout has been decided, we actually need to populate the dungeon with specific enemies, puzzles, rewards and so on. But for a generator with this level of complexity, it won\u2019t do to simply pick things at random. Doing so would make it very difficult to tune, and impossible to get a cohesive feel to levels.  Instead, the generator starts off with very abstract terms, and progressively refines things until everything has been fully decided. For example, rooms start off just as a specific \u201cpath\u201d node, which is handled by the major cycle. Then later, we\u2019ll decide what sort of node we have (cave / tunnel / room, etc). A step even later than that categorizes rooms into specific types (library / forge/ prison etc) and even later, appropriate items and decorations are chosen to fit those rooms.  Unexplored uses two main techniques.  Biomes  are used to encourage consistent choices between otherwise independent systems, and  non-terminal symbols  to indicate a placeholder for that needs resolution later. Biomes One of the first things chosen about a level is associating it with one or more themes. Themes are broad concepts like \u201cfire\u201d, \u201cwood\u201d, \u201ccaves\u201d and more specific items like \u201callrooms\u201d, \u201cwaterfalls\u201d. The themes themselves don\u2019t directly affect the level at all, but many resolution steps will reference the level theme to conditionally enable/disable content. For example, the fire theme can cause lava to generate, fight fire based enemies, ban water feature from the map and cause fire themed items to appear more frequently. The minor themes often enable some specific feature, so you\u2019ll suddenly find a level full of one way paths, or teleports, and so on. Themes are one of many similar annotations (collectively, what I am calling \u201cbiomes\u201d) that are set early on to influence later choices. Some others include: Enemy pools are set per level Roles are set on sub-areas of the dungeon by the major cycle to indicate what sort of obstacle you are likely to encounter Terrains types are used to pick a consistent set of tiles for styling the level. Mystery types are used in one of the expansions to tie together a thread of clues into a coherent story. Node types determine the set of rules that are used for shaping each part of dungeon Room types determine a set of rules to run local to that room (usually to add decorations) I\u2019ve never encountered the \u201cmechspiders\u201d theme as it is rarely set, but I won\u2019t forget it when I do! Non-terminal symbols A non-terminal symbol  is an graph node that has a replacment rule associated it. They are literally a stand-in for something that will be decided more concretely later.  For example, early stages of the generator use a node type called \u201cObstacle\u201d. It\u2019s not a specific obstacle, it can be anything that impedes the player, such as an enemy, puzzle or trap. Only late in the generator do we decide what specific one it is, usually with reference to the theme and role. That means all the intermediate parts of the generator can have patterns that match any obstacle. Obstacles are sometimes tagged with specific data such as a difficulty level, so they aren\u2019t picked completely at random. Another important non-terminal pair is a Lock and Key. As discussed in  lock and key dungeons , these aren\u2019t literally collectable keys and locked doors, it can stand for  anything  where the player must first locate they key before being able to traverse the lock, be it a key item, switch, or pieces of knowledge. The same lock/key structure is used for both hard locks where the player must find the key, and soft locks, where the key isn\u2019t strictly necessary, it just helps. A rule for resolving a lock/key. In this case the key is a potion of poison immunity, and the lock is an encounter with 1-2 giant poisonous rats.  Like Obstacles, Locks/Keys have many rules in the middle parts of the generator that deal with them before they\u2019ve been resolved, so those rules work regardless of what sort of key it is. Keys have a special edge pointing to their corresponding Lock, so even as the two nodes are shuffled and moved around the graph, they can always be kept consistent.  Many of the non-terminals stores similar relationships, such as hints to what they hint about, enemies to patrol areas. These extra relationships allow the nodes to be manipulated while ensuring the dungeon is still completable, and makes sense. In fact, the relationships are part of the level output. This information is not only fantastic for debugging, but the game uses it as a sort of safety check. At any time, you can \u201cPray For Help\u201d, and the game can determine what\u2019s stopping you making further progress, and fix things. In-game debug view of a given level. From Graph to Grid After the resolution phase, we have a a graph of nodes, each heavily annotated with the specifics of how it should appear, which it should contain, etc. But we still have no actual map.  Basic Tiles The basic 5\u00d75 grid that all the previous generation used is expanded by a factor of two to make space for corridor pieces between each node, which are marked as either barriers or doors. Then the grid is expanded by a factor of 5 to give the actual grid of the map. Each 5\u00d75 block then has special rules applied to it to give it a specific shape. Doors are shrunk to a single tile, and rooms are grown into a larger rectangle with a boundary wall. Most other areas, such as barriers, caves,  tunnels etc, have small cellular automata to give them a rough shape.  There are some more elaborate patterns baked in, such as narrow bridges, or tauntingly out of reach rewards. Convert to fine grid Rooms, Items and Set Pieces Though much of the game is spent in open caves, rooms are given special attention. Each room is tagged with a specific type according to what is in it, such as a library, forge, store room and so on. Each room type comes with a special set of rules about how to generate its interior. This customises the cosmetic appearance of the room, what items appear in it, and set pieces.  Set pieces are specific small features that are placed with a pattern matching process. For example, libraries have multiple bookshelf set pieces. Each bookshelf looks for an appropriate place to be drawn \u2013 it needs to be placed against a wall, and not cover up a door or other important feature. Some of the set pieces have quite complicated rules. E.g. Decorative columns need to find an appropriate empty corner to start in, then have other patters for increasing the length of the colonnade across the width of a room. Items are similarly placed with rules. Many items can go on any empty space just sitting on the floor, but chests have several patterns to generate nice alcoves. Terrain Terrain gives some cosmetic variation to the natural areas of levels, such as fields, forests and so on.  Terrain defines a simple 2 tone pattern by randomly assigning a value to each cell, then applying some smoothing. PhantomGrammar has  specific operations  for dealing with cellular automata like this. The two tones become terrain types A and terrain B. Then, inside terrainB, 4 seed points are picked and then two more terrain types (C and D) are made by growing outward from those seeds. The border of the level is forced to terrain type A, and extra B cells are drawn to cut off C and D regions.  The four terrain types in green, with \u201cD\u201d marking locked off cells that cannot have terrain. The 4 terrain types are then superimposed on a level. The exact use of the terrains varies by levels. Usually, they each become a different tile type, but some special levels have further processing to cluster vegetation into specific patterns. Finishing Up Now that the majority of the level is locked in, there is little left to do. Items are placed in appropriate places, and some vegetation laid down. There\u2019s numberous fixup and small cosmetic tweaks that are done at this stage, then all that remains is to pass the tile map plus annotations to the game engine, which can construct a level at runtime from  The final level Conclusion I\u2019ve done my best to give some details on the generation. Unexplored is one of the most complicated systems I\u2019ve seen, but I suspect thanks to the system of graph rewriting, and the Ludoscope tool, it became feasible to be designed by a single developer. That\u2019s truly a powerful system worth more attention. But I also see many trends in common with other games I\u2019ve looked at. I\u2019ve spoken many times of the power of generating something abstract first and filling in the details second. This pattern is repeated over and over again in Unexplored, in little and large. Though it might be better to describe it here as three phases \u2013 Unexplored has a sort of middle phase where the abstract dungeon is warped, manipulated and made more complex before any resolution starts. Another take away is that Unexplored feels pretty varied simply because of the amount of rules authored. While there\u2019s no pre-authored levels, there\u2019s all sorts of story vignettes, puzzles, hints and adventures which have been explicitly designed. I\u2019m sure the tooling made this sort of thing easy to add, but it still represents a lot of design work that procedural generation does not shortcut. I\u2019d say the strength of Unexplored\u2019s dungeons is in their coherency and structure, packed into a compact space. As Joris himself has  observed , \u201c it is so much more interesting to generate small levels than it is to generate large ones \u201c. I will probably be visiting Unexplored and Joris Dorman\u2019s work in future, as his body of work both in academia and games is huge and has many interesting ideas in it. Analysing Unexplored has already turned  into   four   separate   articles , and it\u2019s still more compressed than I would have liked. I\u2019m particularly looking forward to  Unexplored 2: The Wayfarer\u2019s Legacy  which is built using similar tools, but is  even more ambitious  in scope. The graphics are vastly improved and and looks like  it\u2019ll feature NPC AI  using the same graph system. Dormans\u2019 work is slowly making the field of procedural generation a little less\u2026 unexplored."}, "210705_news_467977.txt": {"page_id": "210705_news_467977.txt", "text": "Die Bundesregierung will das Mobilfunknetz der n\u00e4chsten Generation mit mehreren hundert Millionen Euro f\u00f6rdern. \"6G wird die mobile Datentechnologie der Zukunft sein und unsere Kommunikation im n\u00e4chsten Jahrzehnt revolutionieren\", sagte Forschungsministerin Anja Karliczek (CDU) dem  Handelsblatt . \"Wir m\u00fcssen jetzt schon an das \u00dcbermorgen denken und neue Schl\u00fcsseltechnologien und Standards in den Kommunikationstechnologien von Beginn an mitgestalten.\" Demnach soll das  6G-Netz bis 2025 mit rund 700 Millionen Euro gef\u00f6rdert  werden. Ab 2030 soll es dann das  5G-Netz  abl\u00f6sen. Mit 6G w\u00fcrden Daten laut der Zeitung mehr als 100 Mal schneller \u00fcbertragen als mit 5G - \"mit gro\u00dfen Vorteilen f\u00fcr die mobile Kommunikation jedes einzelnen Menschen, aber auch f\u00fcr unsere Industrie und Landwirtschaft\", sagte Karliczek. 6G und XR f\u00fcr Homeoffice und Produktion Das er\u00f6ffne neue M\u00f6glichkeiten der Zusammenarbeit \u00fcber Entfernung, nicht nur im B\u00fcroalltag, sondern auch in der Produktion. In der Medizin sei eine Behandlung aus der Ferne dann auch viel besser m\u00f6glich. Die Ministerin nennt auch \"Extended Reality\" (XR) als Beispiel. Mit 6G und XR sollen Personen in 3D und hoher Aufl\u00f6sung in Echtzeit auf mobile Endger\u00e4te oder in R\u00e4ume gestreamt und projiziert werden k\u00f6nnen. Die hohen Investitionen seien n\u00f6tig, um langfristig die technologische Souver\u00e4nit\u00e4t Deutschlands und Europas zu st\u00e4rken, hie\u00df es weiter. Auch die EU hatte k\u00fcrzlich mit \"Hexa-X\" eine gro\u00dfe 6G-Initiative gestartet und stellt daf\u00fcr 900 Millionen Euro zur Verf\u00fcgung. Karliczek und die EU wollen damit die Abh\u00e4ngigkeit von externen Netzwerkausr\u00fcstern reduzieren und so Debatten wie aktuell um Huawei vermeiden. Nokia leitet europ\u00e4isches Projekt Hexa-X Der finnische Telekommunikationskonzern Nokia hat das  europ\u00e4ische Entwicklungsprojekt Hexa-X als 6G-Flaggschiffinitiative  zusammen mit 21 anderen Unternehmen bereits Ende 2020 angek\u00fcndigt. Die technische Leitung von Hexa-X \u00fcbernimmt der schwedische Netzausr\u00fcster Ericsson, zudem sind die Mobilfunkbetreiber Telef\u00f3nica und Orange, der franz\u00f6sische IT-Dienstleister Atos, Siemens sowie Intel dabei. Das Konsortium soll etwa zweieinhalb Jahre lang die Grundpfeiler f\u00fcr 6G entwickeln. (mit Material der dpa) / ( fds )"}, "210705_news_467982.txt": {"page_id": "210705_news_467982.txt", "text": "\nA Machine Learning Framework for Julia\n   \u00a0 \u00a0 \u00a0 \u00a0\n Star MLJ (Machine Learning in Julia) is a toolbox written in Julia providing a common interface and meta-algorithms for selecting, tuning, evaluating, composing and comparing  over 150 machine learning models  written in Julia and other languages. In particular MLJ wraps a large number of  scikit-learn  models. MLJ is released under the MIT licensed and sponsored by the  Alan Turing Institute . For more elementary introductions to MLJ usage see  Basic introductions  below. The first code snippet below creates a new Julia environment  MLJ_tour  and installs just those packages needed for the tour. See  Installation  for more on creating a Julia environment for use with MLJ. Julia installation instructions are  here . using Pkg\nPkg.activate(\"MLJ_tour\", shared=true)\nPkg.add(\"MLJ\")\nPkg.add(\"MLJIteration\")\nPkg.add(\"EvoTrees\") In MLJ a  model  is just a container for hyper-parameters, and that's all. Here we will apply several kinds of model composition before binding the resulting \"meta-model\" to data in a  machine  for evaluation using cross-validation. Loading and instantiating a gradient tree-boosting model: using MLJ\nBooster = @load EvoTreeRegressor # loads code defining a model type\nbooster = Booster(max_depth=2)   # specify hyper-parameter at construction\nbooster.nrounds=50               # or mutate post facto This model is an example of an iterative model. As is stands, the number of iterations  nrounds  is fixed. Let's create a new model that automatically learns the number of iterations, using the  NumberSinceBest(3)  criterion, as applied to an out-of-sample  l1  loss: using MLJIteration\niterated_booster = IteratedModel(model=booster,\n                                 resampling=Holdout(fraction_train=0.8),\n                                 controls=[Step(2), NumberSinceBest(3), NumberLimit(300)],\n                                 measure=l1,\n                                 retrain=true) Combining the model with categorical feature encoding: pipe = @pipeline ContinuousEncoder iterated_booster First, we define a hyper-parameter range for optimization of a (nested) hyper-parameter: max_depth_range = range(pipe,\n                        :(deterministic_iterated_model.model.max_depth),\n                        lower = 1,\n                        upper = 10) Now we can wrap the pipeline model in an optimization strategy to make it \"self-tuning\": self_tuning_pipe = TunedModel(model=pipe,\n                              tuning=RandomSearch(),\n                              ranges = max_depth_range,\n                              resampling=CV(nfolds=3, rng=456),\n                              measure=l1,\n                              acceleration=CPUThreads(),\n                              n=50) Loading a selection of features and labels from the Ames House Price dataset: X, y = @load_reduced_ames; Binding the \"self-tuning\" pipeline model to data in a  machine  (which will additionally store  learned  parameters): mach = machine(self_tuning_pipe, X, y) Evaluating the \"self-tuning\" pipeline model's performance using 5-fold cross-validation (implies multiple layers of nested resampling): julia> evaluate!(mach,\n                 measures=[l1, l2],\n                 resampling=CV(nfolds=5, rng=123),\n                 acceleration=CPUThreads(),\n                 verbosity=2)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 _.measure          \u2502 _.measurement \u2502 _.per_fold                                    \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 LPLoss{Int64} @410 \u2502 16900.0       \u2502 [17000.0, 16200.0, 16200.0, 16400.0, 18600.0] \u2502\n\u2502 LPLoss{Int64} @632 \u2502 6.57e8        \u2502 [6.38e8, 6.19e8, 5.92e8, 5.67e8, 8.7e8]       \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n_.per_observation = [[[20300.0, 21800.0, ..., 7910.0], [4300.0, 31900.0, ..., 12600.0], [22000.0, 91600.0, ..., 35500.0], [2980.0, 35700.0, ..., 6240.0], [9140.0, 30000.0, ..., 3050.0]], [[4.13e8, 4.74e8, ..., 6.26e7], [1.85e7, 1.02e9, ..., 1.59e8], [4.83e8, 8.38e9, ..., 1.26e9], [8.86e6, 1.28e9, ..., 3.89e7], [8.35e7, 9.01e8, ..., 9.31e6]]]\n_.fitted_params_per_fold = [ \u2026 ]\n_.report_per_fold = [ \u2026 ] Try out MLJ yourself in the following batteries-included Binder  notebook . No installation required. Offer a consistent way to use, compose and tune machine learning models in Julia, Promote the improvement of the Julia ML/Stats ecosystem by making it easier to use models from a wide range of packages, Unlock performance gains by exploiting Julia's support for parallelism, automatic differentiation, GPU, optimization etc. Data agnostic, train models on any data supported by the  Tables.jl  interface, Extensive, state-of-the art, support for model composition ( pipelines  and  learning networks ) (see more  below ), Convenient syntax to tune and evaluate (composite) models. Consistent interface to handle probabilistic predictions. Extensible  tuning interface , to support growing number of optimization strategies, and designed to play well with model composition. The generic model composition API's provided by other toolboxes we have surveyed share one or more of the following shortcomings, which do not exist in MLJ: Composite models do not inherit all the behavior of ordinary models. Composition is limited to linear (non-branching) pipelines. Supervised components in a linear pipeline can only occur at the end of the pipeline. Only static (unlearned) target transformations/inverse transformations are supported. Hyper-parameters in homogeneous model ensembles cannot be coupled. Model stacking, with out-of-sample predictions for base learners, cannot be implemented (using the generic API alone). Hyper-parameters and/or learned parameters of component models are not easily inspected or manipulated (by tuning algorithms, for example) Composite models cannot implement multiple operations, for example, both a  predict  and  transform  method (as in clustering models) or both a  transform  and  inverse_transform  method. Some of these features are demonstrated in  this notebook For more information see the  MLJ design paper  or our detailed  paper  on the composition interface. Users are encouraged to provide feedback on their experience using MLJ and to report issues. For a query to have maximum exposure to maintainers and users, start a discussion thread at  Julia Discourse Machine Learning  and tag your issue \"mlj\". Queries can also be posted as  issues , or on the  #mlj  slack workspace in the Julia Slack channel. Bugs, suggestions, and feature requests can be posted  here . See also,  Known Issues Initially it is recommended that MLJ and associated packages be installed in a new  environment  to avoid package conflicts. You can do this with julia> using Pkg; Pkg.activate(\"my_MLJ_env\", shared=true) Installing MLJ is also done with the package manager: julia> Pkg.add(\"MLJ\") Optional:  To test your installation, run julia> Pkg.test(\"MLJ\") It is important to note that MLJ is essentially a big wrapper providing unified access to  model providing packages . For this reason, one generally needs to add further packages to your environment to make model-specific code available. This happens automatically when you use MLJ's interactive load command  @iload , as in julia> Tree = @iload DecisionTreeClassifier # load type\njulia> tree = Tree() # instance where you will also be asked to choose a providing package, for more than one provide a  DecisionTreeClassifier  model. For more on identifying the name of an applicable model, see  Model Search .  For non-interactive loading of code (e.g., from a module or function) see  Loading Model Code . It is recommended that you start with models from more mature packages such as DecisionTree.jl, ScikitLearn.jl or XGBoost.jl. MLJ is supported by a number of satellite packages (MLJTuning, MLJModelInterface, etc) which the general user is  not  required to install directly. Developers can learn more about these  here . See also the alternative instalation instructions for  Customizing Behavior . If you have experience in programming in another language but are new to Julia, then we highly recommend Aaron Christinson's tutorial  Dispatching Design Patterns  which is nicely compressed in his  half-hour video presentation . However, one doesn't need to be able to program in Julia to start using MLJ. The present document, although littered with examples, is primarily intended as a complete reference. Resources for learning MLJ are: To get direct help from maintainers and other users, see  Getting help and reporting problems . Users are also welcome to join the  #mlj  Julia slack channel to ask questions and make suggestions. An overview of MLJ design: @article{Blaom2020,\n  doi = {10.21105/joss.02704},\n  url = {https://doi.org/10.21105/joss.02704},\n  year = {2020},\n  publisher = {The Open Journal},\n  volume = {5},\n  number = {55},\n  pages = {2704},\n  author = {Anthony D. Blaom and Franz Kiraly and Thibaut Lienart and Yiannis Simillides and Diego Arenas and Sebastian J. Vollmer},\n  title = {{MLJ}: A Julia package for composable machine learning},\n  journal = {Journal of Open Source Software}\n} An in-depth view of MLJ's model composition design: @misc{blaom2020flexible,\n      title={Flexible model composition in machine learning and its implementation in {MLJ}}, \n      author={Anthony D. Blaom and Sebastian J. Vollmer},\n      year={2020},\n      eprint={2012.15505},\n      archivePrefix={arXiv},\n      primaryClass={cs.LG}\n}"}, "210705_news_467997.txt": {"page_id": "210705_news_467997.txt", "text": "When adults claim to have suddenly recalled painful events from their childhood, are those memories likely to be accurate? This question is the basis of the \u201cmemory wars\u201d that have roiled psychology for decades. And the validity of buried trauma turns up as a point of contention in court cases and in television and movie story lines. Warnings about the reliability of a forgotten traumatic event that is later recalled\u2014known formally as a delayed memory\u2014have been endorsed by leading mental health organizations such as the American Psychiatric Association (APA). \u00a0 The skepticism is based on a body of research showing that memory is unreliable and that simple manipulations in the lab can make people believe they had an experience that never happened. Some prominent cases of recovered memory of child abuse have turned out to be false, elicited by overzealous therapists. But psychotherapists who specialize in treating adult survivors of childhood trauma argue that laboratory experiments do not rule out the possibility that some delayed memories recalled by adults are factual. Trauma therapists assert that abuse experienced early in life can overwhelm the central nervous system, causing children to split off a painful memory from conscious awareness. They maintain that this psychological defense mechanism\u2014known as dissociative amnesia\u2014turns up routinely in the patients they encounter. Tensions between the two positions have often been framed as a debate between hard-core scientists on the false-memory side and therapists in clinical practice in the delayed-memory camp. But clinicians who also do research have been publishing peer-reviewed studies of dissociative amnesia in leading journals for decades. A study published\u00a0in February in the  American Journal of Psychiatry ,  the flagship journal of the APA, highlights the  considerable scientific evidence that bolsters the arguments of trauma therapists . The new paper uses magnetic resonance imaging (MRI) to study\u00a0amnesia, along with various other dissociative experiences that are often said to occur in the wake of severe child abuse, such as feelings of unreality and depersonalization. In an editorial published in the same issue of the journal, Vinod Menon, a professor of psychiatry and behavioral sciences at the Stanford University School of Medicine, praised the researchers for \u201c[uncovering]  a potential brain circuit mechanism underlying individual differences in dissociative symptoms  in adults with early-life trauma and PTSD [post-traumatic stress disorder].\u201d Milissa Kaufman is senior author of the new MRI study and head of the dissociative disorders and trauma research program at McLean Hospital, a teaching hospital affiliated with Harvard Medical School. She notes that, as with earlier MRI studies of trauma survivors, this one shows that there is a neurological basis for dissociative symptoms such as\u00a0amnesia. \u201cWe think that these brain studies can help reduce the stigma associated with our work,\u201d Kaufman says. \u201cLike many therapists who treat adult survivors of severe child abuse, I have seen some patients who recover memories of abuse.\u201d Since 1980, dissociative amnesia has been listed as a common symptom of PSTD in every edition of the  Diagnostic and Statistical Manual of Mental Disorders  ( DSM )\u2014psychiatry\u2019s diagnostic bible. The condition has been backed up not just by psychiatric case studies but by dozens of studies involving victims of child abuse, natural disaster, torture, rape, kidnapping, wartime violence and other trauma. For example, two decades ago psychiatrist James Chu, then director of the trauma and dissociative disorders program at McLean Hospital, published a study involving dozens of women receiving in-patient treatment who had experienced childhood abuse.  A majority of the women reported previously having partial or complete amnesia of these events , which they typically remembered not in a therapy session but while at home alone or with family or friends. In many instances, Chu wrote, these women \u201cwere able to find strong corroboration of their recovered memories.\u201d False-memory proponents have warned that the use of leading questions by investigators might seed an untrue recollection.   As psychiatrist Michael I. Goode wrote of Chu\u2019s study in a letter to the editor, \u201cParticipants were asked \u2018if there was a period during which they \u201cdid not remember that this [traumatic] experience happened.\u201d\u2019  With this question alone, the actuality of the traumatic experience was inherently validated  by the investigators.\u201d MRI studies conducted over the past two decades have found that PTSD patients with dissociative amnesia exhibit reduced activity in the amygdala\u2014a brain region that controls the processing of emotion\u2014and increased activity in the prefrontal cortex, which controls planning, focus and other executive functioning skills.   In contrast, PTSD patients who report no lapse in their memories of trauma exhibit increased activity in the amygdala and reduced activity in the prefrontal cortex. \u201cThe reason for these differences in neuronal circuitry is that PTSD patients with dissociative symptoms such as amnesia and depersonalization\u2014a group comprising somewhere between 15 and 30 percent of all PTSD patients\u2014shut down emotionally in response to trauma,\u201d says Ruth Lanius, a professor of psychiatry and director of the PTSD research unit at the University of Western Ontario, who has conducted  several of these MRI studies . Children may try to detach from abuse to avoid intolerable emotional pain, which can result in forgetting an experience for many years, she maintains. \u201cDissociation involves a psychological escape when a physical escape is not possible,\u201d Lanius adds. False-memory researchers remain skeptical of the brain-imaging studies. Henry Otgaar, a professor of legal psychology at Maastricht University in the Netherlands, who has co-authored more than 100 academic publications on false-memory research and who often serves as an expert witness for defendants in abuse cases,   maintains that  intact autobiographical memories are rarely\u2014if ever\u2014repressed . \u201cThese brain studies provide biological evidence just for the  claims  of patients who report memory loss due to dissociation,\u201d he says. \u201cThere are many alternative explanations for these correlations\u2014say, retrograde amnesia, in which the forgetting is due to a brain injury.\u201d In an effort to provide a firmer grounding for their arguments, Kaufman and her McLean colleagues used artificial intelligence to develop a model of the connections between diverse brain networks that could account for dissociative symptoms. They fed the computer MRI data on 65 women with histories of childhood abuse who had been diagnosed with PTSD, along with their scores on a commonly used inventory of dissociative symptoms. \u201cThe computer did the rest,\u201d Kaufman says. Her key finding is that severe dissociative symptoms likely involve the connections between two specific brain networks that are active at the same time: the so-called default mode network\u2014which kicks in when the mind is at rest and involves remembering the past and envisioning the future\u2014and the frontoparietal control network\u2014which is involved in problem-solving. The McLean study is not the first attempt to apply machine learning to dissociative symptoms. In a paper published in the September 2019 issue of the  British Journal of Psychiatry ,  researchers showed how MRI scans of the brain structures of 75 women\u201432 with dissociative identity disorder, for which dissociative amnesia is a key symptom, and 43 matched controls\u2014 could discriminate between people with or without the disorder nearly 75 percent of the time . Kaufman says additional research needs to be carried out before clinicians can begin using brain connectivity as a diagnostic tool to assess the severity of dissociative symptoms in their patients. \u201cThis study is just a first step on the pathway to precision medicine in our field,\u201d she says. Richard Friedman, a professor of clinical psychiatry at Weill Cornell Medical College, considers the goal of the McLean researchers laudable. But he notes that the road ahead remains challenging and warns that the history of psychology is filled with \u201cobjective assessments\u201d for a particular diagnosis or state of mind that never lived up to their hype. Friedman cites the case of lie-detector tests, in which false positives and false negatives abound.   While a brain-based test that could diagnose dissociative symptoms is not likely anytime soon, research on neurobiological explanations show the controversy over forgetting and remembering traumatic memories is far from settled."}, "210705_news_468034.txt": {"page_id": "210705_news_468034.txt", "text": "W\u00e4hrend auf immer mehr Mobiltelefonen gef\u00e4hrliche SMS zur Sendungsverfolgung eingehen, werden Kooperationen mit der KI-Macht China mittlerweile ebenfalls als riskant eingestuft. China spielt \u2013 im wahrsten Sinne des Wortes \u2013 auch eine gro\u00dfe Rolle im kommenden Strategiespiel \"Age of Empires 4\", ist aber nur ein Teil vieler Neuerungen. Ebenfalls als Bedrohung angesehen werden die Pl\u00e4ne zur Rentenpolitik der EU, sodass sich Widerstand regt. Unterschiedlicher Meinung sind auch die Fantasy-Fans \u00fcber die wiederentdeckte russische Fernseh-Verfilmung von Tolkiens \"Herr der Ringe\" von 1991, die bereits innerhalb weniger Tage zum Internet-Hit geworden ist. Das Wichtigste vom Tage in K\u00fcrze. Seit Tagen erhalten Nutzer von Smartphones und anderen Handys verst\u00e4rkt  Kurznachrichten, die zum Klicken eines Links auffordern . Dahinter verbirgt sich in den meisten F\u00e4llen das Android-Schadprogramm FluBot. Das  Bundesamt f\u00fcr Sicherheit in der Informationstechnik warnt eindringlich vor dieser \"Smishing\"-Welle  (SMS-Phishing), \u00fcber die per gef\u00e4lschten Mitteilungen Zugangsdaten ergaunert werden. Gewarnt wird mittlerweile auch vor  Forschungspartnerschaften zu K\u00fcnstlicher Intelligenz (KI) mit chinesischen Partnern . Bis vor Kurzem haben viele deutsche Unternehmen noch die Zusammenarbeit mit chinesischen Firmen in diesem Bereich massiv vorangetrieben, aber der Hype und die Euphorie rund um das \"Wirtschaftswunderland\" sind gr\u00f6\u00dftenteils verflogen.  Kooperationen mit der KI-Macht China werden zum hei\u00dfen Eisen . Bild  1  von  25  (Bild: Microsoft) KI spielt auch in Strategiespielen eine gro\u00dfe Rolle, aber in \" Age of Empires 4 \" hantiert die chinesische Fraktion noch mit Schwarzpulver und Raketenartillerie. Die Entwickler von \"Age of Empires 4\" haben l\u00e4ngere Gameplay-Segmente gezeigt und Hintergrundinformationen zu ihrem kommenden Strategiespiel verraten. Zu sehen sind vier der acht spielbaren Fraktionen: China, Delhi, England und die Mongolen. \"AoE 4\" gilt als  spiritueller Nachfolger von \"Age of Empires 2\" mit neuen Twists . Freude am Spiel hatten offensichtlich auch die Produzenten und Darsteller der  russischen Fernseh-Verfilmung von Tolkiens \"Herr der Ringe\"  aus dem Jahre 1991. Die verloren geglaubte Verfilmung des Bestsellers wurde im M\u00e4rz wieder gefunden und digitalisiert. Bei YouTube hat allein der erste von zwei Teilen des Films bislang bereits mehr als 1,8 Millionen Aufrufe. Fantasy-Fans sind allerdings gespalten angesichts dieses  30 Jahre alten Fernsehfilms . Gespalten sind auch die Meinungen zu den  Rentenpl\u00e4nen der Europ\u00e4ischen Union . Auf der Agenda der EU steht die Verl\u00e4ngerung der Lebensarbeitszeit \u00fcber 70 Jahre hinaus und die forcierte Privatisierung der Altersversorgung. Der Widerstand formiert sich bereits gegen die  Drohung der EU-Kommission, sich um die Renten zu k\u00fcmmern . Auch noch wichtig: Die  digitale Hannover Messe beginnt  heute und l\u00e4uft rein virtuell bis 16.4. Sie gilt als Probelauf f\u00fcr ein Hybridmodell aus klassischer Pr\u00e4senzmesse und virtuellen Veranstaltungen. Rein digital findet auch die  Nvidia Graphics Technology Conference  (GTC) 2021 statt und sie beginnt am heutigen Sp\u00e4tnachmittag mit einer Ansprache von Nvidia-Chef Jensen Huang aus seiner K\u00fcche. Vodafone schaltet heute als erster Mobilfunk-Provider ein eigenst\u00e4ndiges 5G-Kernnetz live, das v\u00f6llig unabh\u00e4ngig von der Vorg\u00e4nger-Technologie LTE funkt. US-Pr\u00e4sident Joe Bidens nationaler Sicherheitsberater Jake Sullivan und Handelsministerin Gina Raimondo besprechen sich heute online mit Chefs f\u00fchrender Unternehmen zum globalen Chipmangel beziehungsweise der Widerstandsf\u00e4higkeit der Lieferketten f\u00fcr Halbleiter-Produkte. An dem Gespr\u00e4che werden dem Wei\u00dfen Haus zufolge unter anderem Vertreter folgender Unternehmen teilnehmen: Google, Dell, Ford, AT&T, HP, Intel, Micron, TSMC und Samsung. ( fds )"}, "210705_news_468035.txt": {"page_id": "210705_news_468035.txt", "text": "Die Liste der skurrilen Projekte aus der Makerszene rund um den Raspi ist lang: Witzig ist zum Beispiel eine Ausgabe der  Lobbyistenaktivit\u00e4t in den USA \u00fcber ein analoges Zeigerinstrument . J\u00fcngst geriet ein Projekt in die Schlagzeilen, das einen Raspi  per Laser M\u00fccken eliminieren  lie\u00df. Ad-hoc starten Dass ein Raspi f\u00fcr ITler, Netzwerkbetreuer und PC-Enthusiasten attraktiv ist, liegt an folgenden Tatsachen: Er kostet wenig und reicht leistungsm\u00e4\u00dfig an einen PC von vor einigen Jahren heran. Je nach Version verbr\u00e4t er trotzdem nur wenig Energie. Obendrein hat er viele Schnittstellen, um ihn mit anderen Ger\u00e4ten zu verbinden, zum Beispiel Temperatursensoren. Er eignet sich etwa als g\u00fcnstige Messsonde, die sich problemlos und sicher mit einem bestehenden Netzwerk verkn\u00fcpfen l\u00e4sst. Hinzu kommt ein stabiles Linux-Betriebssystem, das eine F\u00fclle von Standards unterst\u00fctzt. Software sprudelt aus einem schier unersch\u00f6pflichen Vorrat. Die Dokumentation profitiert von flei\u00dfigen Communitys. So gen\u00fcgt f\u00fcr die Inbetriebnahme g\u00e4ngiger Projekte f\u00fcr den Raspi eine SD-Speicherkarte mit der Lite Ausgabe von Raspberry Pi OS. Typischerweise \u00fcbernehmen einfache Shell-Skripte die Regie beim Einrichten der oft anspruchsvollen Software-Stacks der Raspi-Projekte. Der Nutznie\u00dfer ist der ITler, der binnen Minuten ein n\u00fctzliches Ger\u00e4t in der Hand h\u00e4lt, zum Beispiel mit Pi-hole oder AdGuard. Sie filtern  Werbung und Malware  nicht nur f\u00fcr einen einzelnen PC, sondern auch f\u00fcr kleine Netze. Im Windschatten anderer erfolgreicher Projekte wie zum Beispiel Nextcloud wachsen  Ableger heran, die ihre Macher auf den Raspi-Betrieb anpassen . Das erleichtert nicht nur die Inbetriebnahme, sondern auch den dauerhaften Einsatz. Obendrein bringen solche Projekte oft Extras mit, die man sonst von Hand in eine selbst gepflegte Installation einbauen m\u00fcsste. Meist sind die Projekte leicht im Internet zu finden: H\u00e4ngen Sie einfach \"pi\" vor oder hinter einschl\u00e4gige Namen, wie bei Nextcloudpi oder  PiVPN . Eine weitere Quelle f\u00fcr n\u00fctzliche Fertigsoftware f\u00fcr den Raspi erschlie\u00dft sich der Einplatinencomputer \u00fcber Container. Wenn erst mal Docker eingerichtet ist, steht die gesamte Docker-Welt offen: Zum Beispiel l\u00e4sst sich der  Raspi dann als Passwort-Server  einsetzen. Auch die E-Mail-Stacks  Mailu  und  mailcow  sind Beispiele f\u00fcr solche Docker-Umgebungen, die Raspis im Handumdrehen in einen vollwertigen E-Mail-Server verwandeln. Ausreizen F\u00fcr passionierte ITler liegt der Reiz des Raspi jenseits fertiger Projekte: Mit einer Minimalinstallation plus VPN-Client und SSH-Server schaffen sie sich vor\u00fcbergehend  ein Sprungbrett in fremde Netze . Sie werfen den Raspi dort ab. Der VPN-Client verbindet sich automatisch mit einer Gegenstelle unter Ihrer Kontrolle. Per SSH springen Sie dann ins fremde Netz. Bei SSH f\u00fchren mehrere Wege zum Ziel: Es kann einzelne TCP-Ports durch einen Tunnel weiterleiten. SSH kann aber als SOCKS-Proxy auch das fremde Netz zum Gateway f\u00fcr den lokalen Browser oder andere Dienste machen. Das ist sehr n\u00fctzlich, wenn man dort Ger\u00e4te warten soll und deren Webinterface nicht ins Internet \u00f6ffnen m\u00f6chte. Solcherlei Treiben setzt freilich das Wissen und Einverst\u00e4ndnis des Betreibers des fremden Netzes voraus. Ein Raspi eignet sich als kleiner Server, wie  openmediavault als Projekt f\u00fcr den NAS-Selbstbau  zeigt. Der Kleincomputer kann aber auch Protokollbr\u00fccken f\u00fcr Dateidienste auf Basis von SMB, also Windows-Freigaben bilden. So kann er veraltete, nur SMB1-taugliche Ger\u00e4te gefahrlos an ein l\u00e4ngst auf SMB2 umgestelltes Netz ankoppeln: Der Raspi spricht dabei in einem privaten Netz mit dem gef\u00e4hrdeten SMB1-Altger\u00e4t. Er reicht dessen Dateien schlie\u00dflich per SMB2 oder 3 ins externe Netz weiter. Ins private Netz gerichtet, arbeitet er als Server, ins externe Netz als Client. Was Sie dazu tun m\u00fcssen: Samba auf dem Raspi einrichten. Es \u00fcber die Optionen  interfaces  und  bind interfaces only  im Abschnitt  [global]  der Konfigurationsdatei smb.conf nur auf dem Interface im privaten Netz lauschen lassen. Idealerweise schieben Sie diesen \"Fake Server\" dem unsicheren Altger\u00e4t unter dem Namen unter, unter dem es seine Dateien bisher abgelegt hat. Dann brauchen Sie nur die Netzwerkkonfiguration des Altger\u00e4tes anzufassen. Im externen Netz mountet dann der Raspi das Zielverzeichnis per SMB. Generell gl\u00e4nzt der Raspi im Netz: Mit WLAN- und Ethernet-Schnittstelle ausger\u00fcstet kann er als Router zwischen Funk- und Drahtnetz vermitteln. Oder Sie lassen ihn als WLAN-Access-Point arbeiten, der eine Br\u00fccke zwischen den beiden Medien spannt. Seine St\u00e4rke dabei liegt nicht beim Durchsatz, sondern in den Filterf\u00e4higkeiten: Per Firewall l\u00e4sst sich fein dosieren, was durchkommt. Ebenso fein l\u00e4sst sich auch beobachten, was Ger\u00e4te untereinander austauschen oder was ins Internet abflie\u00dft. Richtig interessant wird es, wenn ein Projekt die Hardwaref\u00e4higkeiten des Raspi ausreizt.  PiKVM  bewerkstelligt das: Es spannt das Video-Capture-Modul f\u00fcr den Raspi ein, um einen Raspberry Pi 4 in einen IP-KVM-Switch zu verwandeln. Sie brauchen dann nur noch das Modul sowie den USB-Port mit einem beliebigen PC und dem Netzwerk zu verbinden. Schon k\u00f6nnen Sie den PC aus der Ferne bedienen, als s\u00e4\u00dfen Sie davor. Cool, oder? In  c\u2019t 9/2021  zeigen wir, wie sich der Raspberry Pi etwa als Router f\u00fcrs VPN in Unternehmen und zu Hause n\u00fctzlich machen kann. Wir haben Komfort, Hygiene und Sicherheit beim Bezahlen per Smartphone und Smartwatch untersucht, B\u00fcrom\u00e4use getestet und einen Farmbot f\u00fcr den heimischen Garten gebaut. Dies und noch viel mehr lesen Sie in Ausgabe 9/2021, die ab dem 9. April im  Heise-Shop  und am gut sortierten Zeitschriftenkiosk erh\u00e4ltlich ist. ( ps )"}, "210705_news_468042.txt": {"page_id": "210705_news_468042.txt", "text": "China\u2019s top disease control official has admitted that the efficacy of the country\u2019s domestically produced vaccines is low as it emerged the authorities are considering mixing them to try to offer greater protection against coronavirus. The rare admission of weakness on the part of Beijing\u2019s pandemic approach came from the director of the China Center for Disease Control and Prevention, Gao Fu, who said Chinese vaccines \u201cdon\u2019t have very high protection rates\u201d. \u201cIt\u2019s now under formal consideration whether we should use different vaccines from different technical lines for the immunisation process,\u201d he said at a conference on Saturday in the south-western city of Chengdu. The efficacy of a coronavirus vaccine from Sinovac, a Chinese developer, at preventing symptomatic infections has been found to be as low as 50.4% by researchers in  Brazil . By comparison, the vaccine made by Pfizer has been found to be have an efficacy rate of 97%. On Sunday Gao appeared to walk back his comments, according to Chinese state tabloid, the Global Times. Gao told the outlet that international interpretation of his comments as an admission had been a \u201cmisunderstanding\u201d. \u201cThe protection rates of all vaccines in the world are sometimes high, and sometimes low. How to improve their efficacy is a question that needs to be considered by scientists around the world,\u201d Gao said. But the earlier statement by Gao is likely to cause concern in the growing number of other countries who are relying on Chinese vaccines. Beijing has distributed hundreds of millions of doses in other countries while also trying to promote doubt about the efficacy of western vaccines. Turkey and Singapore are among countries that have placed orders while a rollout of Sinovac\u2019s vaccine in Indonesia in January involved 3m doses and was aimed at medical workers and public servants. It subsequently approved Sinovac for use in elderly people. It emerged last month that a number of people in the United Arab Emirates (UAE) were being invited to get a third shot of the Sinopharm coronavirus vaccine after antibody tests indicated they did not have a sufficient immune response following two doses of the Chinese-made vaccine. Beijing has yet to approve any foreign vaccines for use in China, where the coronavirus emerged in late 2019. China has administered about 161m doses since vaccinations began last year \u2013 most people will require two shots \u2013 and aims to fully inoculate 40% of its 1.4 billion population by June. Last month,  authorities in Beijing resumed  issuing visa processing for foreigners from dozens of countries, but only if they have been inoculated against Covid-19 with a Chinese-made vaccine. The move raised questions about the motivations behind the demand, given China\u2019s vaccines are not approved in many of the countries to which it has opened travel and that it will not accept foreign vaccines made elsewhere, including those approved by the World  Health  Organization. By the middle of last month, China had approved five vaccines for either general or emergency use, including three that are also being distributed to other countries either through trade or aid. The push to supply internationally has been labelled a  \u201cvaccine diplomacy\u201d  campaign to boost  China\u2019s place as a global health contributor , with take-up mostly in Asia, Africa and South America."}, "210705_news_468066.txt": {"page_id": "210705_news_468066.txt", "text": "The London Stock Exchange registered its best start to the year for company floats in more than a decade, despite a  limp debut for Deliveroo . High-profile IPO listings from the takeaway food firm, consumer reviews site Trustpilot and a host of smaller firms on the Alternative Investment Market (Aim) made the start of 2021 a boom quarter, according to the latest snapshot from EY.  The financial services firm said 12  IPOs  raised \u00a35.2bn on the main market, while eight garnered \u00a3441m on the smaller Aim, in the most lucrative first quarter since 2007. Despite Deliveroo\u2019s poor start to trading \u2013 its shares closed at 254p on Friday after listing at 390p \u2013 London remains the place in Europe for private companies to go public, EY claimed. Scott McCubbin, an EY partner, said: \u201cThe UK has had the strongest opening quarter for IPOs for 14 years, with the markets successfully weathering the effects of Brexit and bouncing back from the stall in activity caused by the onset of the pandemic a year ago. With an effective vaccine rollout under way, momentum and confidence in the UK IPO market should continue to build, but future growth may vary depending on the sector.\u201d The performance during the first three months of 2021 is in stark contrast to the same period last year, when there were just three IPOs on the main market and two on Aim, raising a combined total of \u00a3615m. Despite the pandemic, a host of big-name firms are planning to go public in London later this year. Cybersecurity company Darktrace could list on the London exchange with a valuation in the region of \u00a33.8bn. The EG Group, which owns petrol stations and fast food outlets, is also on investors\u2019 watchlist with a listing that could value it at \u00a310bn. The Scottish beer firm Brewdog is another contender, having long flirted with a London flotation. Rishi Sunak recently  loosened City listing rules  to attract more tech floats in a move that helped grease the wheels of Deliveroo\u2019s choice of London for its IPO. But McCubbin warned that the UK would struggle to rival markets across the Atlantic unless the chancellor went further. \u201cSuch a positive performance in the first quarter shows confidence in the strong fundamentals of the UK IPO market. While some believe there is a risk of compromising on current strengths if the UK seeks to adapt to bolster its tech status, the UK would likely have to make some significant changes if it were to rival the US in this area,\u201d he said."}, "210705_news_468071.txt": {"page_id": "210705_news_468071.txt", "text": "12.58am  BST 00:58 This blog is now closed. For up to date coverage of the coronavirus pandemic, head to the link below: 11.19pm  BST 23:19 Melissa Davey Since rare but severe clotting was seen in some people following vaccination with AstraZeneca\u2019s Covid-19 vaccine, researchers worldwide have been grappling to understand why the clotting syndrome, known as \u201cthrombosis with thrombocytopenia\u201d (clotting with a low platelet count), occurs. Melissa Davey brings you up to speed here: 10.57pm  BST 22:57 Pub-goers in  London  have likened the first night of outdoor hospitality to \u201cVE Day\u201d as lockdown restrictions eased on Monday, but added that there is \u201cvery little\u201d social distancing. Attila Kulcsar, a media communications manager, said the crowds on Monday felt \u201clike a return to the \u2018real\u2019 Soho of the 1990s\u201d. \u201cTonight\u2019s atmosphere beats the whole of last summer... it really is like how I imagine VE Day,\u201d the 54-year-old told PA. \u201cIt\u2019s almost like watching Hogarth paintings come to life in 2021. There is a wonderfully raucous hysteria everywhere. It\u2019s very celebratory.\u201d However, Kulcsar said there was \u201cvery little social distancing\u201d adding that there is a feeling that restrictions have ended.  \u201cThere are lots of police around Soho as well as Covid Marshalls, but there is no word from them about social distancing... if anything they seem to be joining in the festivities,\u201d he said.    Police officers walk among the crowds as people drink in the street in the Soho area of London, on April 12, 2021 as coronavirus restrictions are eased across the country in step two of the government\u2019s roadmap out of England\u2019s third national lockdown. Photograph: Tolga Akmen/AFP/Getty Images \n  Updated\n             at 10.59pm BST 10.32pm  BST 22:32 All over-50s in  England  have been offered a coronavirus vaccine a few days before the mid-April deadline set by the government \u2013 meaning the second phase of the rollout to younger cohorts can now begin. Despite fears of a supply slowdown and possible knock in confidence after a change in advice on who could get the Oxford/AstraZeneca jab,  Boris Johnson  hailed the passing of \u201canother hugely significant milestone\u201d. With more than 32 million people now having had a first dose and 7.6 million of those having received their second, the prime minister said \u201cmany thousands of lives\u201d had been saved. 10.23pm  BST 22:23 People enjoy drinks outdoors in  London  and  Manchester  on the first night since England\u2019s coronavirus restrictions were eased:     Customers enjoy drinks at tables outside the pubs and bars in the Soho area of London, on April 12, 2021 as coronavirus restrictions are eased across the country in step two of the government\u2019s roadmap out of England\u2019s third national lockdown. Photograph: Tolga Akmen/AFP/Getty Images \n     Customers enjoy drinks at tables outside the bars in the Northern Quarter of central Manchester, on April 12, 2021 as coronavirus restrictions are eased across the country in step two of the government\u2019s roadmap out of England\u2019s third national lockdown. Photograph: Oli Scarff/AFP/Getty Images \n  10.07pm  BST 22:07 Three Covid-19 patients died on Monday in an intensive care unit in  Bucharest  after the oxygen supply system failed, the emergency situations department (DSU) said in a statement reported by AFP. \u201cVentilators stopped working because the oxygen level rose above the limit\u201d, the department said in a statement, adding that an investigation had been launched into the incident at the Victor Babes hospital in the  Romanian  capital. Five other patients were transferred to other hospitals in the capital. The three patients who died were elderly and went into cardiac arrest, a doctor told AFP. \u201cIt seems to have been an accident, but an investigation already started. All those guilty must be held responsible,\u201d prime minister Florin Citu said in a press statement. 9.36pm  BST 21:36 Ireland limits AstraZeneca vaccine to over-60s Ireland  will only administer the AstraZeneca coronavirus vaccine to people over 60, the country\u2019s chief medical officer said Monday, after it was linked to rare blood clotting cases, according to AFP. The move echoes action taken by several other European countries, while the UK has limited the jab\u2019s use to over-30s. The European Medicines Agency puts the risk of developing a rare blot clot after receiving the jab at around one in 100,000.  Ireland\u2019s National Immunisation Advisory Committee (NIAC) said the AstraZeneca product \u201cis not recommended for those aged under 60 years\u201d. The updated recommendations would be incorporated into the vaccination programme and vaccine re-allocated, chief medical officer Ronan Glynn said. Ireland has so far given almost 750,000 of its five million people an initial vaccine dose, according to the latest available statistics.  9.19pm  BST 21:19 Almost 200 Dutch tourists have traded lockdown in the  Netherlands  for eight days of voluntary confinement at a Greek beach resort as part of a test to see if safe holidays can be arranged during the Covid pandemic, Reuters reports: Updated\n             at 9.21pm BST 8.34pm  BST 20:34 Surge testing deployed in south London after South African variant found Authorities are rolling out surge testing in parts of  south London  after cases of the South African Covid-19 variant were detected. Forty-four confirmed cases and 30 probable cases have been found in the boroughs of  Wandsworth  and  Lambeth . All identified cases are isolating or have completed their isolation, and their contacts have been traced and asked to isolate. Dr Susan Hopkins, chief medical adviser for NHS Test and Trace, said the cluster of cases is \u201csignificant\u201d. \u201cIt\u2019s really important people in the local area play their part in stopping any further spread within the local community,\u201d she said. \u201cPCR testing is now available for all and I would strongly encourage everyone, whether they live, work or travel through the boroughs, to get tested even if they don\u2019t have any symptoms of coronavirus.\u201d Updated\n             at 8.35pm BST 8.27pm  BST 20:27 The  Canadian  province of  Ontario  is shutting schools and moving to remote learning due to rising infection levels, premier Doug Ford said on Monday. He did not say when pupils would return to in-person teaching, Reuters reports. Canada\u2019s most populous province imposed restrictions on non-essential retail and issued a four-week stay-at-home order last week. The hospitality industry was also shut for both indoor and outdoor dining. The measures were brought in as occupancy levels in intensive care units shot up and hospitals began cancelling elective surgeries for the second time since the pandemic began. 8.11pm  BST 20:11 Turkey \u2019s coronavirus taskforce will recommend a tougher set of restrictions as the country battles its third wave, health minister Fahrettin Koca said on Monday in comments reported by Reuters. Koca also told the press briefing that the country was expecting further vaccine deliveries from China\u2019s Sinovac Biotech and Pfizer and BioNTech in the coming months. Turkey\u2019s daily coronavirus infection numbers have soared above 50,000. A senior government official told the news agency that president Tayyip Erdo\u011fan is likely to order a tightening of restrictions this week ahead of the tourism season. 7.47pm  BST 19:47 A scientist has described \u201ca joyous day\u201d as people in  England  enjoyed a relaxation of restrictions, but warned that people should remain cautious. Lawrence Young, a virologist and professor of molecular oncology at Warwick Medical School, stressed that the virus and its variants is \u201cstill out there\u201d. \u201cIt\u2019s a joyous day after all that we have sacrificed over the last few months,\u201d he said, adding that the fall in infections and hospitalisations was due to both the success of the vaccine rollout and lockdown.  Young said: \u201cSo while taking the opportunity to enjoy shopping and outside hospitality, we must remain cautious - the virus is still out there and very infectious.  \u201cWe can\u2019t ignore what\u2019s going on in the rest of the world - every other day new variants are being reported and infection is rife.\u201d 6.41pm  BST 18:41 United Nations secretary-general Antonio Guterres has urged governments to consider imposing a \u201csolidarity or wealth tax\u201d on the rich who made money during the coronavirus pandemic in a bid to cut extreme inequality, Reuters reports. \u201cWe must make sure funds go where they are needed most. Latest reports indicate that there has been a $5 trillion surge in the wealth of the world\u2019s richest in the past year,\u201d Guterres told a U.N. meeting on financing for development. \u201cI urge governments to consider a solidarity or wealth tax on those who have profited during the pandemic, to reduce extreme inequalities,\u201d he said.  Guterres also repeated his call for Covid-19 vaccines to be made available to all countries and appealed for more money to fully fund the COVAX vaccine sharing facility. 6.24pm  BST 18:24 Greece  will get a first tranche of the single-dose Johnson & Johnson vaccine on Wednesday and will start inoculating people next week, a senior health ministry official said, Reuters reports. The Greek statement came as the U.S. company began delivering its vaccine to countries across the EU, a few days later than initially planned due to production issues. \u201cWe will receive the first 33,600 doses of the single-dose vaccine by Johnson & Johnson on Wednesday, April 14,\u201d the country\u2019s secretary general in charge of vaccinations, Marios Themistocleous, told a weekly briefing. Greece has inoculated more than 2 million of its 11 million population with at least one shot of the Pfizer, AstraZeneca or Moderna vaccines. Along with new deliveries from those companies, Greece has said it expects 1.3 million doses of the Johnson & Johnson vaccine by June. 6.05pm  BST 18:05 The head of  Italy\u2019s  southern Campania region said he would ignore a government order to give the elderly vaccination priority and would instead focus on workers in order to safeguard the local economy. The central government has often clashed with the powerful presidents of Italy\u2019s 20 regions over Covid-19 policy, with a blurred division of power hampering efforts to speed up vaccine rollout and protect the most vulnerable, Reuters reports. Italy has administered a first shot to 66% of those aged over 80, in line with the European Union average, but only 17% of those between 70 and 79, lagging its main partners, and has been reporting around 400 deaths a day in recent weeks. Vincenzo De Luca, the president of Campania around Naples, said: \u201cOnce the over-80s and the vulnerable are done we do not intend to proceed by age groups,\u201d he told reporters, saying that the instructions would kill off the economy. \u201cRigour is one thing, stupidity is another,\u201d De Luca added."}, "210705_news_468072.txt": {"page_id": "210705_news_468072.txt", "text": "A weather-based surveillance system that could offer advanced warning of outbreaks of meningitis is being piloted across sub-Saharan  Africa  in a bid to save lives, researchers have revealed. According to the Meningitis Research Foundation, meningitis affects  about 5 million people around the world  each year, one in 10 of whom die, while two in 10 are left with lasting impacts, such as brain damage. One area that has had major outbreaks of bacterial meningitis \u2013 a  contagious and particularly serious form of the condition  \u2013 is the so-called \u201c meningitis belt \u201d that cuts across a host of countries in sub-Saharan Africa, with outbreaks most common between November and June. That, experts say, is in part because hot and dusty conditions raise the risk of bacterial meningitis: among various mechanisms,  previous studies  have suggested dust can irritate the lining of the nose and throat, making it easier for microbes to get into the bloodstream and cause infection. And the situation could become worse: charities have  previously warned  that the climate crisis could lead to a rise in meningitis cases. Now researchers have announced a pilot is under way that could offer advanced warning of when and where meningitis outbreaks or even epidemics may occur by harnessing state-of-the-art weather forecasts to predict where conditions that fuel meningitis cases are likely to develop. \u201cPreviously it was only really possible to use the current weather conditions to see what was going on, and use that to project how outbreaks would come. Now we are able to use forecasts for up to two weeks ahead, which obviously gives much more warning for deployment of resources,\u201d said Prof Doug Parker of the University of Leeds, the lead scientist at the  African SWIFT  collaboration, one of the bodies involved in the pilot project. While experts say the project cannot prevent infections, it could save lives by ensuring the limited resources of the countries are in place \u2013 for example around testing, diagnosis and treatment. \u201cBy making sure the appropriate medical services are on offer to those most likely to be affected by a meningitis outbreak, the number of fatalities and the severity of symptoms is being reduced,\u201d said Dr Joshua Talib, research associate at the UK Centre for Ecology and Hydrology, who has been involved in the work. The two-year project, which is being run across 26 countries in the \u201cmeningitis belt\u201d, began in late 2019 with the development of new ways of using the latest weather forecasts. These are now being pilotedfor their effectiveness and usefulness when it comes to predicting and managing meningitis cases. \u201cWe are getting positive feedback from the medics,\u201d said Parker, adding that detailed evaluation of the impact of the project is under way. But there is more to do. \u201cDue to the resolution of sub-seasonal forecast data, this prediction can only be provided on a county [or] country-level,\u201d said Talib. \u201cScientists are working hard to improve the predictability and resolution of sub-seasonal forecasts.\u201d The pilot, which is backed by the UK government\u2019s Global Challenges Research Fund, is a joint effort by the National Centre for Atmospheric Science, African SWIFT and the African Centre of Meteorological Applications for Development. However, Parker said the future of the project was under threat as a result of recent cuts to the Global Challenges Research Fund. Linda Glennie, research director at the charity  Meningitis  Research Foundation, which is not involved with the project but funded early research into the use of weather data to predict outbreaks, welcomed the pilot. \u201cSub-Saharan Africa has the highest burden of meningitis in the world. For at least a century, meningitis epidemics have swept across the region during the dry season. Despite the success of immunisation, even in the last decade, hundreds of thousands of people living in the region have faced the devastating consequences of the disease,\u201d she said. \u201cAlthough epidemics occur frequently, they are difficult to predict. Any tools which can forecast where meningitis is likely to strike will help to ensure vaccinations and antibiotics are readily available to save lives\u201d."}, "210705_news_468081.txt": {"page_id": "210705_news_468081.txt", "text": "Kryptografie richtig einzusetzen ist kein Hexenwerk; aber etwas mehr als Copy&Paste braucht es schon. Der Thementag Kryptografie der renommierten heise devSec vermittelt praxisrelevantes Wissen \u00fcber Funktion und Einsatzm\u00f6glichkeiten und zeigt gleichzeitig typische Stolperfallen \u2013 und das alles zugeschnitten auf die konkreten Bed\u00fcrfnisse von Devs: Der Spagat zwischen Krypto und Benutzbarkeit: Warum es nicht nur sicher sein muss \"Devs are users, too\": So setzt man Krypto richtig ein Authentifizierung: Stand der Technik und Ausblick Transportsicherheit mit TLS 1.3 in der Praxis Von der Wishlist zur realen App Angriffe gegen moderne Kryptografie \u2013 ein \u00dcberblick Ein Panel zum Thema \"Ist Datenschutz ein Hindernis?\" am Beispiel Corona-Warn-Apps rundet das Programm ab. Die Teilnahme kostet 299 Euro plus Mehrwertsteuer. Am 10. Juni folgt der ganzt\u00e4gige Workshop \"Authentifizierung einfach und sicher gemacht mit Keycloak\". Die n\u00e4chsten Thementage widmen sich dann DevSecOps (29. Juni) und Web-Application-Security (1. Juli) und k\u00f6nnen auch im Bundle gebucht werden. Weitere Informationen und die M\u00f6glichkeit sich anzumelden, finden Sie auf: ( ju )"}, "210705_news_468082.txt": {"page_id": "210705_news_468082.txt", "text": "Es ist ein halbes Jahrhundert her, dass diese ironisch gemeinte Werbetafel in den Siebzigerjahren, als der Flugzeughersteller Boeing von einer schweren Krise heimgesucht wurde, Reisende auf ihrem Weg zum Flughafen Sea-Tac begr\u00fc\u00dfte: \"W\u00fcrde die letzte Person, die Seattle verl\u00e4sst, bitte das Licht ausmachen?\" Aber Seattle ging schlie\u00dflich nicht den Weg des Niedergangs wie etwa Detroit. Noch vor Ende des Jahrzehnts benannten zwei S\u00f6hne der Stadt, Bill Gates und Paul Allen, ihre Softwarefirma von Micro-Soft in Microsoft um, zogen von New Mexico zur\u00fcck in ihre Heimat und lie\u00dfen sich in einem Vorort auf der anderen Seite des Lake Washington nieder. Wie w\u00e4re es der Stadt wohl ergangen, wenn Gates und Allen stattdessen beschlossen h\u00e4tten, Microsoft in Albuquerque aufzubauen? Wir werden es nie erfahren. Aber Seattles Aufschwung war mehr vom Gl\u00fcck abh\u00e4ngig, als die Menschen normalerweise zugeben wollen. Zufall spielt eine Rolle Wir denken uns gerne Gr\u00fcnde aus, um zu erkl\u00e4ren, warum bedeutende Ver\u00e4nderungen geschehen oder wie es zu enormen Verschiebungen kommt: Wir h\u00f6ren gro\u00dfartige Behauptungen \u00fcber innovative Kultur oder geografische Vorteile. Die Realit\u00e4t ist jedoch, dass der Zufall eine gro\u00dfe Rolle bei der Umgestaltung der wirtschaftlichen Geschicke der Region Seattle gespielt hat \u2013 und auch in anderen Regionen in der Vergangenheit gespielt hat und immer noch spielt. Die Geschichte von Orten wird ebenso sehr von zuf\u00e4lligen pers\u00f6nlichen Entscheidungen, z. B. wie erw\u00e4hnt \u00fcber den Wohnort, oder von \"schwarzen Schw\u00e4nen\" wie dem Finanzcrash 2008 bestimmt wie vom Schicksal. Und obwohl dies vielleicht weniger befriedigende M\u00f6glichkeiten bietet, die Zukunft vorherzusagen \u2013 sie ist sicherlich mehr ein Flickenteppich von Gr\u00fcnden, als professionelle Futuristen glauben machen wollen \u2013 sind sie nicht nur f\u00fcr Seattle, sondern auch f\u00fcr das Silicon Valley zutreffend, die zentrale Region der Innovation in den Vereinigten Staaten. Es gab schon immer eine Menge Debatten dar\u00fcber, was die Einzigartigkeit des Silicon Valley ausmacht \u2013 das zuf\u00e4lligerweise von dem Technologiejournalisten Don Hoefler 1971 so benannt wurde, im selben Jahr, als das \"Turn off the lights\"-Plakat in Seattle erschien. Was auch immer die Gr\u00fcnde daf\u00fcr sind, dass das Valley seither das weltweit dominierende Zentrum f\u00fcr technologische Innovationen geblieben ist, seine Wurzeln liegen eindeutig in einer Reihe von gl\u00fccklichen Zuf\u00e4llen. Zun\u00e4chst beschloss William Shockley, die Bell Labs zu verlassen und seine neue Halbleiterfirma in Palo Alto zu gr\u00fcnden, weil er in der N\u00e4he seiner alternden Mutter leben wollte. Dann, ein paar Jahre sp\u00e4ter, f\u00fchrte eine Kartellrechtsklage des Justizministeriums gegen AT&T zu einer erzwungenen kostenlosen Lizenzierung der Technologie f\u00fcr integrierte Schaltkreise des Unternehmens. Dies l\u00f6ste die Explosion der Transistortechnik und der Computer aus \u2013 eine Welle von Disruptionen, die bis heute weiterbesteht. Weniger innovativ als das Image Doch trotz des fast schon religi\u00f6sen Glaubens an den eigenen Ruf der Innovation hat das Valley nur relativ wenige gro\u00dfe, dramatische Konzepte hervorgebracht, die ganz neue Lebens- und Arbeitsweisen generiert haben, wie Doug Engelbarts Hypertext und Maus, Alan Kays Dynabook (ein Vorl\u00e4ufer des Tablets) oder Marc Weisers \"Ubiquitous Computing\". Stattdessen hat sich das Silicon Valley auf die Produktentwicklung konzentriert und ist in etwas anderem sehr geschickt geworden: im Aufsp\u00fcren profitabler neuer Ideen. \"Wann immer es eine spannende neue Idee gibt, wimmelt es im Valley pl\u00f6tzlich von ihr\", sagte mir Jensen Huang, der Chef des Chipherstellers Nvidia. \"Die Leute m\u00fcssen auf eine gute Idee warten \u2013 und gute Ideen gibt es nicht jeden Tag.\" Dieser Fokus wurde durch die St\u00e4rke der Venture-Capital-Branche im Valley und ihre Effizienz bei der Finanzierung neuer Startups noch verst\u00e4rkt. Im Jahr 2019 \u00fcbertraf die Bay Area mit mehr als 50 Milliarden Dollar an Risikokapitalfinanzierungen bei weitem die Summe in jeder anderen Region der Vereinigten Staaten. All das liegt einem Wandel zugrunde, der dazu gef\u00fchrt hat, dass sich die Region weg von der Fertigung hin zu Hardware-Engineering und Software-Design bewegt hat. (Nvidia selbst wurde gegr\u00fcndet, um Grafikprozessoren f\u00fcr Videospiele zu entwickeln, und wandte sich dann sp\u00e4ter in Richtung von Machine-Learning-Anwendungen.) Aber gute Ideen sind nicht nur selten \u2013 sie sind auch notorisch schwer vorherzusagen. Das Web, die Suchmaschinen und das maschinelle Lernen haben die Gurus im Silicon Valley allesamt letztlich \u00fcberrascht. Das lag zu einem gro\u00dfen Teil daran, dass jahrzehntelang die rasant steigende Leistung und die sinkenden Kosten von Computern neue, unerwartete Dinge m\u00f6glich machten. Mit jeder neuen Generation von Silizium kamen Innovationen wie am Schn\u00fcrchen: Desktop-PCs, Laptops, digitales Audio und Video, Smartphones und das Internet der Dinge. Welche \u00dcberraschungen? \u00dcberraschungen sind jetzt, da das Mooresche Gesetz, der wichtigste Glaubensartikel des Valley, seit 2013 ins Stottern geraten ist, vielleicht nicht mehr so h\u00e4ufig anzutreffen. In der Tat ist es zumindest in einer wichtigen Hinsicht zum Stillstand gekommen. Die Kosten pro Transistor \u2013 die einst mit der gleichen exponentiellen Rate fielen, mit der die Transistordichte zunahm \u2013 haben sich seit mehr als drei Generationen der Chipherstellung nicht mehr ver\u00e4ndert. \"Wir hatten im Grunde einen Freifahrtschein\", sagte mir Carver Mead, der Physiker, der eigentlich den Begriff \"Moore's Law\" gepr\u00e4gt hat, vor einigen Jahren. \"Es ist wirklich verr\u00fcckt, aber das hat sich ausgezahlt.\" Nun aber ist die freie Fahrt vorbei. Bedeutende technologische Fortschritte werden nur noch als Reaktion auf den menschlichen Einfallsreichtum kommen. Und das bedeutet, dass es f\u00fcr das Silicon Valley an der Zeit ist, bescheidener zu werden. Die sogenannte Serendipity, den gl\u00fccklichen Zufall, sollte man besonders im Hinterkopf behalten, wenn hochkar\u00e4tige Unternehmen aus dem Valley entfleuchen. Erst im vergangenen Dezember k\u00fcndigten Hewlett Packard Enterprise und Oracle an, ihren Hauptsitz nach Texas zu verlegen, und Tesla deutete an, dass es diesem Beispiel folgen k\u00f6nnte. Diese Schritte haben eine neue Runde des H\u00e4ndereibens und der Spekulationen dar\u00fcber ausgel\u00f6st, ob das Valley sein \"Mojo\" verloren hat. Aber die Frage wird eben nicht zum ersten Mal gestellt. In der Vergangenheit gab es Zeiten, in denen der Fortschritt ins Stocken zu geraten schien, nur um dann mit einem Durchbruch, der v\u00f6llig aus dem Nichts zu kommen schien, zur\u00fcckzuschlagen. Mobile Hardware: Europe first Bis 2006 hatte man das Gef\u00fchl, dass die Innovation im Valley abebbt und die Fortschritte bei mobiler Hardware zuerst in Europa bei Unternehmen wie Nokia und Psion stattfinden. Doch im darauffolgenden Jahr stellte Steve Jobs das iPhone vor, das die beiden gr\u00f6\u00dften Misserfolge aus diesem Bereich, Apples PDA Newton und der Communicator von General Magic, letztlich neu interpretierte. Das Valley wurde fast \u00fcber Nacht wieder zur weltweit dominierenden Region f\u00fcr Innovationen in der IT. Nordkalifornien war schon seit dem Goldrausch eine Boom-and-Bust-Wirtschaft. Als Teenager, der in Palo Alto aufwuchs, h\u00f6rte ich von Massenentlassungen im NASA-Forschungslabor Ames und bei der Lockheed Missiles and Space Company, die dazu f\u00fchrten, dass Wellen von Ingenieuren die Stadt verlie\u00dfen. Daran wurde ich nach dem Dot-Com-Zusammenbruch erinnert, als ich einen Startup-Veteranen auf einer Konferenz sah und feststellte, dass ich ihn schon seit einigen Jahren nicht mehr gesehen hatte. \"Wo bist du gewesen?\" fragte ich. Er habe den Staat verlassen, um bei seiner Familie zu leben, aber die Dinge h\u00e4tten sich gebessert und jetzt sei er zur\u00fcck, antwortete er. Das bedeutet nicht, dass das \u00dcberleben des Valley eine Selbstverst\u00e4ndlichkeit ist. Heute gibt es trotz anhaltend starker Investitionen und Risikokapital neue Gr\u00fcnde f\u00fcr Unsicherheit, abgesehen vom abgew\u00fcrgten Halbleiterzyklus. Einer hat mit der F\u00e4higkeit zu tun, Talente heranzuholen. Das Silicon Valley verdankt seine Existenz in vielerlei Hinsicht der Mystik, die in den 1970er Jahren entstand und eine magnetische Kraft erzeugte, die kontinuierlich die besten und kl\u00fcgsten K\u00f6pfe aus der ganzen Welt angezogen hat. In der Tat k\u00f6nnte dies ein Schl\u00fcssel zum Verst\u00e4ndnis dessen sein, was die Region von anderen Innovationszentren unterscheidet. Als zehn Megabyte eine gro\u00dfe Sache waren Ich stolperte zum ersten Mal dar\u00fcber, als ich Mitte der 1980er Jahre als technischer Redakteur beim Byte-Magazin arbeitete. Ein lokaler Hardware-Designer nahm mich zu einer indischen B\u00e4ckerei in Sunnyvale mit, die voller Frauen in Saris und deren Ehem\u00e4nnern war, die als Ingenieure besch\u00e4ftigt waren. Sie waren ins Valley gekommen, als wichtige Knowledge Worker f\u00fcr die schnell wachsende Festplattenindustrie. (Zehn Megabyte Festplattenspeicher waren eine gro\u00dfe Sache!) Auch Europ\u00e4er, Asiaten und Lateinamerikaner kamen und brachten intellektuelle Kraft und Unternehmergeist mit. Innerhalb eines Jahrzehnts war es m\u00f6glich, im Valley von Viertel zu Viertel zu fahren und in jedem eine andere Sprache auf den Ladenschildern und Werbetafeln zu sehen. Nun aber sind in den USA m\u00e4chtige einwanderungsfeindliche Kr\u00e4fte am Werk und es ist durchaus m\u00f6glich \u2013 selbst unter der Biden-Administration \u2013, dass neue Barrieren f\u00fcr ausl\u00e4ndische Fachkr\u00e4fte und Unternehmer eine der wichtigsten Zutaten f\u00fcr den Erfolg des Valleys zerst\u00f6ren. Ein weiterer Grund f\u00fcr die Unsicherheit ist, dass der n\u00e4chste gro\u00dfe Technologiewandel noch nicht da ist, geschweige denn klar. Als sich das Tempo des Moore'schen Gesetzes im letzten Jahrzehnt verlangsamte, vollzog das Valley einen \u00dcbergang der beiden j\u00fcngsten Innovationsgenerationen \u2013 von Social-Media-Plattformen zu Software und Dienstleistungen, die auf maschinellem Lernen basieren. Das Risikokapital schwenkte um und die Finanzierung f\u00fcr soziale Medien, die 2012 ihren H\u00f6hepunkt erreicht hatte, fiel bis 2016 fast auf Null, da sich die Investoren in Startups mit maschinellem Lernen st\u00fcrzten. Es gibt heute jedoch kaum einen Konsens dar\u00fcber, was das \"n\u00e4chste gro\u00dfe Ding\" sein k\u00f6nnte \u2013 oder wann es kommt. Die Futuristen verweisen auf Augmented Reality \u2013 einige Optimisten glauben, dass damit die gesamte asiatische Flachbildschirmindustrie in Gefahr ist, wenn wir alle AR-Brillen tragen \u2013 als wahrscheinlichen Kandidaten f\u00fcr die Plattform, die den n\u00e4chsten Investitionszyklus ausl\u00f6sen wird. Aber das kann noch einige Jahre dauern. Software und Biologie zusammen? Oder vielleicht verschmelzen Software und Biologie endlich miteinander: Die synthetische Biologie hat durch den Erfolg der j\u00fcngsten mRNA-Coronaviren-Impfstoffe einen deutlichen Schub erhalten. Oder vielleicht wird Quantencomputing zur kommerziellen Realit\u00e4t und senkt die Kosten f\u00fcr Rechenzentren wie die von Google drastisch. Oder \u00fcberlegen Sie, was es bedeuten w\u00fcrde, wenn sich ein Apple-Auto als ebenso erfolgreich erweisen w\u00fcrde wie das iPhone. (Aber darauf w\u00fcrde ich mich nicht verlassen.) Es ist jedoch genauso wahrscheinlich, dass es eine lange Durststrecke geben wird und das Valley sich in einer \u00e4hnlichen Situation wiederfindet wie Seattle, als es sich zu sehr auf Boeing verlie\u00df. Noch besorgniserregender ist, dass sich China als jener erbitterte Konkurrent erweisen k\u00f6nnte, den das Silicon Valley einst in Japan bef\u00fcrchtete. Es ist durchaus m\u00f6glich, dass die wirkliche Bedrohung f\u00fcr die n\u00e4chste gro\u00dfe Technologieplattform zuerst aus Shanghai oder Shenzhen oder Peking kommen wird. Wer einmal den Stadtteil Zhongguancun in der chinesischen Hauptstadt besucht hat, kommt nicht umhin, die \u00c4hnlichkeit zum Valley in der Konzentration von Talent und Kapital zu erkennen. Trotzdem scheint es unklug, gegen den gl\u00fccklichen Zufall und damit auch gegen das Silicon Valley zu wetten. Vorhersagen \u00fcber seinen bevorstehenden Untergang waren regelm\u00e4\u00dfig und kurzsichtig. Ich habe diese Lektion pers\u00f6nlich gelernt, nachdem ich an dem Buch \"The High Cost of High Tech\" (\"Die hohen Kosten der Hochtechnologie\") aus dem Jahr 1985 mitgeschrieben habe, in dem ich argumentierte, dass die Umwelt- und Arbeitskosten des Wachstums die Expansion des Silicon Valley bald begrenzen w\u00fcrden. Mein Co-Autor war damals Lenny Siegel, der sp\u00e4ter B\u00fcrgermeister von Mountain View wurde, der Stadt, in der Google heute seinen Hauptsitz hat. Ups. ( bsc )"}, "210705_news_468101.txt": {"page_id": "210705_news_468101.txt", "text": "The health of hundreds of thousands of meat plant workers in  Brazil  is at risk from an industry-backed plan to reduce breaks given to employees, say workers\u2019 rights groups in the country. In the midst of a pandemic that has claimed the lives of more than 350,000 Brazilians, President Jair Bolsonaro\u2019s government, parliament and the meat industry have been pushing for a move to review the laws and regulations protecting workers at slaughter plants. New rules under discussion would limit the regular breaks given to workers enduring cold temperatures, which labour specialists say helps reduce the potential for injury. \u201cIt is inconceivable that during the worst health crisis in history, when slaughterhouse workers were qualified as essential and continued to work normally to guarantee the supply of food to society, they should have any rights related to health and safety at work removed,\u201d said Lincoln Cordeiro, who works for the Labour Prosecution Service, a federal agency in Brazil, independent of the government. The 20-minute breaks every hour and 40 minutes allow for \u201cthermal recovery\u201d from the cold temperatures. The proposed changes would mean these breaks would only be granted to employees subject to temperatures below 4C or moving loads between places with a temperature difference of 10C. This would cover about 5% of meat plant workers, Cordeiro says. \u201cThere are studies showing that continuous work in a cold environment deteriorates muscles and neural functioning,\u201d Cordeiro said. \u201cExposure to cold air also causes inflammatory changes and worsening of the respiratory system.\u201d The review of labour rules at meat plants takes place at a time when questions are being raised in Europe about the sustainability of Brazilian meat exports, worth a record $17bn in 2020. Under Bolsonaro,  illegal deforestation  at sensitive biomes has rocketed. A vast expanse of Amazon rainforest seven times larger than Greater London was deforested between August 2019 and July last year. \u201cThe corporate sector has put pressure on the government, arguing that the current rules are causing damages,\u201d said C\u00e9lio Elias, leader of the Democratic Federation of Food Industry Workers in Santa Catarina, a major poultry-producing state. \u201c[But] if workers\u2019 protection is undermined, we\u2019ll see a large number of people injured and mutilated. We have no doubt about that.\u201d Processing workers at the Minerva SA meat plant in Barretos, Brazil. Government and industry have been pushing for a reduction in workers\u2019 protections.  Photograph: Bloomberg/Getty Images There were nearly 23,000 accidents at meat slaughter plants in Brazil in 2019, according to the  Statistical Yearbook of Accidents at Work , an average of 62 a day. As well as the proposed changes to the country\u2019s labour code, Bolsonaro\u2019s government has announced plans to review federal rules (known as NR36) covering minimum distance between workers and the use of adequate furniture to avoid accidents, as well as break times. The Brazilian Association of Animal Protein (ABPA), which represents the country\u2019s poultry and pork industries, said the proposed changes to Brazil\u2019s labour code would \u201cbring them in line with international rules\u201d and give workers a more flexible model of breaks which doesn\u2019t oblige them to leave the premises. Both the ABPA and the labour secretariat of the Ministry of Economy said changes to NR36 were designed to \u201cto simplify, harmonise and reduce bureaucracy\u201d. The ABPA added in a statement that the revision of NR36 was necessary \u201cdue to advances in production technologies\u201d and that \u201cthe work is essentially guided by constant improvement of health and safety conditions for all workers\u201d. The Brazilian Association of Meat Exporting Industries (Abiec), which includes bovine slaughterhouses, did not comment on the proposed changes to the labour code or NR36.  Carlos Juliano Barros is an investigative reporter for Rep\u00f3rter Brasil Sign up for the  Animals farmed monthly update  to get a roundup of the best farming and food stories across the world and keep up with our investigations. You can send us your stories and thoughts at  animalsfarmed@theguardian.com"}, "210705_news_468110.txt": {"page_id": "210705_news_468110.txt", "text": "I wrote a program to clean up scans of handwritten notes while simultaneously reducing file size. Example input and output: Left:  input scan @ 300 DPI, 7.2MB PNG / 790KB JPG.  Right: \noutput @ same resolution, 121KB PNG. Disclaimer:  the process described here is more or less what the\n Office Lens  app does already, and there\u2019s probably any number of\nother tools that do similar things. I\u2019m not claiming to have come up\nwith a radical new invention \u2013 just my own implementation of a useful\ntool. If you\u2019re in a hurry, just check out the  github  repo, or jump down\nto the  results  section, where you can play with\ninteractive 3D diagrams of color clusters. Motivation Some of my classes don\u2019t have an assigned textbook. For these, I like\nto appoint weekly \u201cstudent scribes\u201d to share their lecture notes with\nthe rest of the class, so that there\u2019s some kind written resource for\nstudents to double-check their understanding of the material. The\nnotes get posted to a course website as PDFs. At school we have a \u201csmart\u201d copier capable of scanning to PDF, but the\ndocuments it produces are\u2026 less than attractive.  Here\u2019s some example\noutput from a handwritten homework page: Seemingly at random, the copier chooses whether to  binarize  each\nmark (like the  x \u2019s), or turn them into abysmally blocky JPGs (like\nthe square root symbols). Needless to say, we can do better. Overview We start out with a scan of a lovely page of student notes like this one: The original PNG image scanned at 300 DPI is about 7.2MB; the same\nimage converted to a JPG at quality level 85 is about 790KB.  Since\nPDFs of scans are typically just a  container format  around PNG\nor JPG, we certainly don\u2019t expect to  reduce  the required storage\nsize when converting to PDF. 800KB per page is pretty hefty \u2013 for the\nsake of loading times, I\u2019d love to see things closer to\n100KB/page. Although this student is a very neat note-taker, the scan shown above\nlooks a bit messy (through no fault of her own). There\u2019s lots of\nbleed-through from the opposite side of the page, which is both\ndistracting for the viewer and hard for a JPG or PNG encoder to\ncompress, compared to a constant-color background. This is what the output of my  noteshrink.py  program looks like: It\u2019s a comparatively tiny PNG file, weighing in at just 121KB. My\nfavorite part? Not only did the image get  smaller , it also got\n clearer ! Process and color image fundamentals Here are the steps required to produce the compact, clean image above: Identify the background color of the original scanned image. Isolate the foreground by thresholding on difference from background color. Convert to an indexed color PNG by choosing a small number of\n\u201crepresentative colors\u201d from the foreground. Before we delve into each one of these steps, it might be useful to\nrecap  how  color images are stored digitally. Because humans have\nthree different types of color-sensitive cells in their eyes, we can\nreconstruct any color by combining various intensities of red, green,\nand blue light.  The resulting system equates colors with 3D\npoints in the  RGB colorspace , illustrated here: Although a true vector space would allow an infinite number of\ncontinuously varying pixel intensities, we need to discretize colors\nin order to store them digitally \u2013 typically assigning 8 bits each to\nthe red, green, and blue channels.  Nevertheless, considering colors\nin an image analogously to points in a continuous 3D space provides\npowerful tools for analysis, as we shall see when we step through the\nprocess outlined above. Identifying the background color Since the majority of the page is free from ink or lines, we might\nexpect the paper color to be the one that appears most frequently in\nthe scanned image \u2013 and if the scanner always represented every bit\nof unmarked white paper as the same RGB triplet, we would have no\nproblems picking it out. Regrettably, this is not the case; random\nvariations in color appear due to dust specks and smudges on the\nglass, color variations of the page itself, sensor noise, etc.  So in\nreality, the \u201cpage color\u201d can spread across thousands of distinct\nRGB values. The original scanned image is 2,081 x 2,531, with a total area of\n5,267,011 pixels. Although we  could  consider each individual pixel,\nit\u2019s much faster to work on a representative sample of the input\nimage. The  noteshrink.py  program samples 5% of the input image by\ndefault (more than sufficient for scans at 300 DPI), but for now,\nlet\u2019s look at an even smaller subset of 10,000 pixels chosen at random\nfrom the original scan: Although it bears scant resemblance to the actual scanned page \u2013\nthere\u2019s no text to be found \u2013 the distribution of colors in the two\nimages is pretty much identical. Both are mostly grayish-white, with a\nhandful of red, blue, and dark gray pixels. Here are the same 10,000\npixels, sorted by brightness (e.g. the sum of their R, G, and B\nintensities): Viewed from afar, the bottom 80-90% of the image all seems to be the\nsame color; however, closer inspection reveals quite a bit of variation.\nIn fact, the most frequent color in the image above, with RGB value\n(240, 240, 242), accounts for just 226 of the 10,000 samples \u2013 less\nthan 3% of the total number of pixels. Because the  mode  here accounts for such a small percentage of the\nsample, we should question how reliably it describes the distribution\nof colors in the image. We\u2019ll have a better chance of identifying a\nprevalent page color if we first reduce the  bit depth  of the image\nbefore finding the mode. Here\u2019s what things look like when we move\nfrom 8 bits per channel to 4 by zeroing out the four\n least significant bits : Now the most frequently occurring color has RGB value (224, 224, 224),\nand accounts for 3,623 (36%) of the sampled pixels. Essentially, by\nreducing the bit depth, we are grouping similar pixels into larger\n\u201cbins\u201d, which makes it easier to find a strong peak in the data. There\u2019s a tradeoff here between reliability and precision: small bins\nenable finer distinctions of color, but bigger bins are much more\nrobust. In the end, I went with 6 bits per channel to identify the\nbackground color, which seemed like a good sweet spot between the two\nextremes. Isolating the foreground Once we have identified the background color, we can  threshold  the\nimage according to how similar each pixel in the image is to it.  One\nnatural way to calculate the similarity of two colors is to compute\nthe  Euclidean distance  of their coordinates in RGB space; however,\nthis simple method fails to properly segment the colors shown below: Here\u2019s a table specifying the colors and their Euclidean distances from the background color: Color Where found R G B Dist. from BG white background 238 238 242 \u2014 gray bleed-through from back 160 168 166 129.4 black ink on front of page 71 73 71 290.4 red ink on front of page 219 83 86 220.7 pink vertical line at left margin 243 179 182 84.3 As you can see, the dark gray bleed-through that we would like to\nclassify as background is actually  further  away from the white page\ncolor than the pink line color which we hope to classify as\nforeground. Any threshold on Euclidean distance that marks pink as\nforeground would necessarily also have to include the bleed-through. We can get around this issue by moving from RGB space to\n Hue-Saturation-Value  (HSV) space, which deforms the RGB cube\ninto the cylindrical shape illustrated in this cutaway view: The HSV cylinder features a rainbow of colors distributed circularly\nabout its outside top edge;  hue  refers to the angle along this\ncircle. The central axis of the cylinder ranges from black at the\nbottom to white at the top, with gray shades in between \u2013 this entire\naxis has zero  saturation , or intensity of color, and the vivid hues\non the outside circumference all have a saturation of 1.0. Finally,\n value  refers to the overall brightness of the color, ranging from\nblack at the bottom to bright shades at the top. So now let\u2019s reconsider our colors above, this time in terms of value\nand saturation: Color Value Saturation Value diff. from BG Sat. diff from BG white 0.949 0.017 \u2014 \u2014 gray 0.659 0.048 0.290 0.031 black 0.286 0.027 0.663 0.011 red 0.859 0.621 0.090 0.604 pink 0.953 0.263 0.004 0.247 As you might expect, white, black, and gray vary significantly in\nvalue, but share similarly low saturation levels \u2013 well below \neither red or pink. With the additional information provided by HSV,\nwe can successfully mark a pixel as belonging to the foreground if\neither one of these criteria holds: the value differs by more than 0.3 from the background color,  or the saturation differs by more than 0.2 from the background color The former criterion pulls in the black pen marks, whereas the latter\npulls in the red ink as well as the pink line. Both criteria\nsuccessfully exclude the gray bleed-through from the foreground.\nDifferent images may require different saturation/value thresholds;\nsee the  results  section for details. Choosing a set of representative colors Once we isolate the foreground, we are left with a new set of colors\ncorresponding to the marks on the page. Let\u2019s visualize the set \u2013 but\nthis time, instead of considering colors as a collection of pixels, we\nwill consider them as 3D points in the RGB colorspace. The resulting\nscatterplot ends up looking quite \u201cclumpy\u201d, with several bands of\nrelated colors: Our goal now is to convert the original 24 bit-per-pixel image into an\n indexed color  image by choosing a small number (8, in this example)\nof colors to represent the whole image. This has two effects: first,\nit reduces the file size because specifying a color now requires only\n3 bits (since \\(8 = 2^3\\)). Furthermore, it makes the resulting image\nmore visually cohesive because similarly colored ink marks are likely\nto be assigned the same color in the final output image. To accomplish this goal we will use a data-driven method that\nexploits the \u201cclumpy\u201d nature of the diagram above. Choosing colors\nthat correspond to the centers of clusters will\nlead to a set of colors that accurately represents the underlying\ndata.  In technical terms, we\u2019ll be solving a  color quantization \nproblem (which is itself just a special case of\n vector quantization ), through the use of  cluster analysis . The particular methodological tool for the job that I picked is\n k -means clustering . Its overall goal is to find a set of\nmeans or centers which minimizes the average distance from each point\nto the nearest center. Here\u2019s what you get when you use it to pick out\nseven different clusters on the dataset above: In this diagram, the points with black outlines represent foreground\ncolor samples, and the colored lines connect them to their closest\ncenter in the RGB colorspace. When the image is converted to indexed\ncolor, each foreground sample will get replaced with the color of the\nclosest center.  Finally, the circular outlines indicate the distance\nfrom each center its furthest associated sample. Whistles and bells Aside from being able to set the value and saturation thresholds, the\n noteshrink.py  program has several other notable features. By\ndefault, it increases the vividness and contrast of the final palette\nby rescaling the minimum and maximum intensity values to 0 and 255,\nrespectively. Without this adjustment, the 8-color palette for the\nscan above would look like this: The adjusted palette is more vibrant: There is also an option to force the background color to white after\nisolating the foreground colors.  To further reduce the PNG image\nsizes after conversion to indexed color,  noteshrink.py  can\nautomatically run  PNG optimization  tools such as  optipng ,\n pngcrush , or  pngquant . The program\u2019s final output combines several output images together\ninto PDFs like  this one  using ImageMagick\u2019s  convert  tool.  As a\nfurther bonus,  noteshrink.py  automatically sorts input filenames\nnumerically (as opposed to alphabetically, as the shell  globbing \noperator does).  This is helpful when your dumb scanning program \nproduces output filenames like  scan 9.png  and  scan 10.png , and you\ndon\u2019t want their order to be swapped in the PDF. Results Here are some more examples of the program output. The first one\n( PDF ) looks great with the default\nthreshold settings: Here is the visualization of the color clusters: The next one ( PDF ) required lowering\nthe saturation threshold to 0.045 because the blue-gray lines are so\ndrab: Color clusters: Finally, an example scanned in from engineer\u2019s graph paper\n( PDF ). For this one, I\nset the value threshold to 0.05 because the contrast between the\nbackground and the lines was so low: Color clusters: All together, the four PDFs take up about 788KB, averaging about about\n 130KB per page of output. Conclusions and future work I\u2019m glad I was able to produce a practical tool that I can use to\nprepare scribe note PDFs for my course websites. Beyond that, I really\nenjoyed preparing this writeup, especially because it prodded me to\ntry to improve on the essentially 2D visualizations displayed on the\nWikipedia  color quantization  page, and also to finally learn\n three.js  (very fun, would use again). If I ever revisit this project, I\u2019d like to play around with\nalternative quantization schemes. One that occurred to me this week\nwas to use  spectral clustering  on the  nearest neighbor graph  of \na set of color samples \u2013 I thought this was an exciting new idea when I\ndreamed it up, but it turns out there\u2019s a  2012 paper  that proposes this\nexact approach. Oh well. You could also try using  expectation maximization  to form a\n Gaussian mixture model  describing the color distribution \u2013 not sure\nif that\u2019s been done much in the past. Other fun ideas include trying\nout a \u201cperceptually uniform\u201d colorspace like  L*a*b*  to cluster in, and\nalso to attempt to automatically determine the\n \u201cbest\u201d number of clusters  for a given image. On the other hand, I\u2019ve got a backlog of blog entries to shove out the\ndoor, so I\u2019m going to put a pin in this project for now, and invite you to go\ncheckout the  noteshrink.py   github  repository. Until next time! Comments"}, "210705_news_468122.txt": {"page_id": "210705_news_468122.txt", "text": "Zwei Satelliten aus den rasant wachsenden Netzwerken von SpaceX und OneWeb sind vor wenigen Tagen einer gef\u00e4hrlichen Ann\u00e4herung entgangen. Das berichtet das US-Magazin The Verge unter Berufung auf die US Space Force und OneWeb, von SpaceX habe es keine \u00c4u\u00dferung gegeben. Die beiden Raumfahrtunternehmen bauen derzeit Netzwerke f\u00fcr Satelliteninternet auf und starten daf\u00fcr gro\u00dfe Zahlen von Satelliten, die unter anderem die Kollisionsgefahr im Erdorbit erh\u00f6hen. W\u00e4hrend die Raumfahrtabteilung der US-Streitkr\u00e4fte den Umgang mit dem Vorfall nun lobt, weist The Verge darauf hin, dass SpaceX das System f\u00fcr eine automatische Kollisionsvermeidung in dem betroffenen Starlink-Satelliten aus unbekannten Gr\u00fcnden deaktiviert und OneWeb habe handeln lassen. Kontaktaufnahme per E-Mail Die ersten Ann\u00e4herungsalarme hatte OneWeb demnach am 30. M\u00e4rz erhalten, kurz nachdem die  j\u00fcngsten 36 Satelliten von Russland aus gestartet worden waren . Weil OneWebs Satelliten in einem h\u00f6heren Orbit stationiert sind, m\u00fcssen sie auf ihrem Weg an ihre Positionen durch das schnell wachsende Netz von Starlink-Satelliten.  Die Space Force  habe nun gewarnt, dass zwei Satelliten der beiden Unternehmen einander dabei auf bis zu 60 Meter nahe kommen k\u00f6nnten, bei einer Kollisionswahrscheinlichkeit von \u00fcber einem Prozent. Bei OneWeb habe man dann eine E-Mail an SpaceX geschickt und schlie\u00dflich h\u00e4tten sich beide Unternehmen koordiniert. Dem Bericht zufolge  hat SpaceX dann ein System deaktiviert, das eigentlich daf\u00fcr sorgen soll, dass die eigenen Starlink-Satelliten anderen Ger\u00e4ten auf Kollisionskurs automatisch ausweichen. So habe OneWeb Ma\u00dfnahmen ergreifen k\u00f6nnen, damit der eigene Satellit ausweicht. Warum genau SpaceX das getan hat, sei aber unklar. Von OneWeb gab es demnach auch Kritik an dem System, denn es reiche ja nicht, wenn solch ein System f\u00fcr andere nicht vorhersehbare Entscheidungen treffe. Victoria Samson vom Think Tank Secure World Foundation wird sogar noch deutlicher und fragt, welchen Sinn ein solches System \u00fcberhaupt habe, wenn man es vor einer m\u00f6glichen Kollision abschalten m\u00fcsse. W\u00e4hrend Satellitenbetreiber immer wieder Ausweichman\u00f6ver einleiten m\u00fcssen, damit ihre Ger\u00e4tschaften nicht  mit ausrangierten Satelliten oder Weltraumschrott  kollidieren, sind Ann\u00e4herungsalarme zwischen zwei aktiven Satelliten selten. Die Sorge davor w\u00e4chst aber angesichts der Pl\u00e4ne f\u00fcr gigantische Netzwerke wie Starlink von SpaceX. Weltweit gibt es keine Vorgaben daf\u00fcr, wie Betreiber damit umgehen sollen. Bei einer Kollision w\u00fcrde jedoch auch jede Menge neuer Weltraumschrott entstehen, der das Problem nur vergr\u00f6\u00dfert. Vor anderthalb Jahren war SpaceX schon einmal an einer nicht ganz einwandfreien Ann\u00e4herung beteiligt,  damals hatte es von der ESA Kritik  am Vorgehen des Raumfahrtunternehmens von Elon Musk gegeben. Das legt inzwischen zwar die Positionen der Starlink-Satelliten offen, nicht aber Einzelheiten zur KI f\u00fcr Kollisionsvermeidung. In dem Artikel ist eine interaktive Grafik eingebunden, die \u00fcber den Berliner Dienstleister Datawrapper erstellt und ausgeliefert wird. Zum Datenschutz bei Datawrapper siehe deren  Datenschutzerkl\u00e4rung . Pers\u00f6nliche oder personenbeziehbare Daten von Lesern der interaktiven Charts werden nicht gesammelt. ( mho )"}, "210705_news_468128.txt": {"page_id": "210705_news_468128.txt", "text": "W e may all, under lockdown, have fallen prey to feelings of powerlessness. In reaction, Georgie Thomas and Cassie Symes, aka the duo Thick\u2019n\u2019Fast, have pivoted to the other extreme, imagining themselves \u201cworld kings\u201d, in the infant Boris Johnson\u2019s formulation \u2013 or world queens, given that Symes and Thomas consider global leadership through a female lens. In this comic play, streaming nightly from Applecart Arts, the pair are gifted sudden jurisdiction over the world\u2019s affairs. So what do they \u2013 what would any of us \u2013 do with it?  It\u2019s a catchy premise \u2013 but General Secretary doesn\u2019t get the most out of it. There are diverting moments, as the duo send up their own naivety \u2013 and their vulnerability to the seductions of power. They multi-role capably as newsreaders, a tech supremo, and two mouthy YouTube influencers. The montage sequence that finds our fish-out-of-water heroines mugging up on geopolitics is nicely done, as they fend off incoming calls from Michelle Obama and introduce a \u201creciprocal orgasm tax\u201d to penalise bad sex.  But finally, this would-be satire has little to say about our failing systems of governance or why even the best of political intentions seem to founder. General Secretary too often comes across like a first draft. From the outset, when the UN\u2019s decision to offer them power goes wholly unexplained, the show feels remote-from-the-knuckle, its plotting slack and cartoonish even when the performances threaten something more nuanced. The show\u2019s ear for media- and politics-speak is not acute enough, and the comic potential of the clash (more conspicuous on screen than it might be onstage) between low production values and world-straddling ambitions is not exploited.  The satire detector flashes when Symes and Thomas blame Silicon Valley for the rookie rulers\u2019 descent into tyranny. But that idea is left very vague, as world peace is secured by an undistinguished comedy song that concludes the show. At a time when opportunities for live performers are few and far between, you can forgive Symes and Thomas dreaming for themselves the biggest opportunity imaginable. But it\u2019s not really seized in this fitfully likable but lightweight political fantasy. "}, "210705_news_468136.txt": {"page_id": "210705_news_468136.txt", "text": "This post is a basic introduction to running HTTPS servers and clients in Go\nusing TLS. It assumes some familiarity with public-key crypto. Feel free to\ncheck out my earlier posts about  RSA  and the\n Diffie-Hellman Key Exchange ; TLS uses\nthe elliptic-curve version of Diffie-Hellman. I won't be covering how the\nprotocol itself works in detail here, but if you're interested I recommend to\nread up on the subject. All the code for this post is available  in this repository . A (very) brief intro to TLS TLS (Transport Layer Security) is a protocol designed to enable client-server\ncommunication over the Internet in a way that prevents eavesdropping, tampering\nand message forgery. It's described in  RFC 8446 . TLS relies on state-of-the art cryptography; this is also why it's recommended\nusing the newest version of TLS available, which is 1.3 (as of early 2021).\nRevisions of the TLS protocol clean up potentially unsafe corner cases, remove\nweak encryption algorithms and generally try to make the protocol more secure. When a client connects to a server with plain HTTP, it starts sending plaintext\ndata wrapped in TCP packets right after completing the standard TCP handshake\n(SYN -> SYN-ACK -> ACK). Using TLS, the situation is somewhat more complicated\n: \nTLS diagram After completing the TCP handshake, the server and client perform a TLS\nhandshake to agree on a shared secret that's unique only to them (and to this\nspecific session). This shared secret is then used to securely encrypt all\ndata being exchanged between them. While there's a lot going on here, it's\nsomething the TLS layer implements for us. We just have to set up the TLS server\n(or client) properly; the actual diff between an HTTP and an HTTPS server in Go\nis minimal. TLS certificates Before we jump to the code showing how to set up an HTTPS server in Go using\nTLS, let's talk about  certificates . In the diagram above, you'll notice that\nthe server sends a certificate to the client as part of its very first\n ServerHello  message. Formally these are known as X.509 certificates,\ndescribed by  RFC 5280 . Public key cryptography plays a major part in TLS. A certificate is a standard\nway to wrap the server's public key, along with its identity and a signature by\na trusted authority (typically a  Certificate Authority ). Suppose you want to\ntalk to  https://bigbank.com ; how do you know it's really Big Bank there asking\nfor your password? What if someone is sitting on your cable connection,\nintercepting all traffic and pretending to be Big Bank (classical MITM -\nman-in-the-middle attack). The certificate process is designed to prevent this scenario. When your client's\nunderlying TLS implementation accesses  https://bigbank.com , it expects Big\nBank's certificate with a public key, signed by a trusted certificate authority\n(CA). Certificate signatures can form a tree (bank's key signed by A, which is\nsigned by B, which is signed by C, etc.), but at the end of the chain it must\nhave some certificate authority trusted by your client. Modern browsers have a\nlist of pre-trusted CAs (along with their own certificates) built-in. Since your\ncable snooper cannot forge a trusted certificate's signature, they cannot\nimpersonate Big Bank. Generating self-signed certificates in Go For local testing, it's often very useful to be able to work with self-signed\ncertificates. A self-signed certificate is a certificate for some entity E with\na public key P, but the key is signed not by a known certificate authority, but\nrather by P itself. While self-signed certificates have some additional\nlegitimate uses, we'll focus on their use for testing here. Go's standard library has excellent support for everything related to crypto,\nTLS and certificates. Let's see how to generate a self-signed certificate in Go! privateKey ,   err   :=   ecdsa . GenerateKey ( elliptic . P256 (),   rand . Reader ) if   err   !=   nil   { \n   log . Fatalf ( \"Failed to generate private key: %v\" ,   err ) } This code uses the  crypto/ecdsa ,  crypto/elliptic  and  crypto/rand \npackages to generate a new key pair , using the P-256 elliptic curve,\nwhich is one of the allowed curves in TLS 1.3. Next, we'll create a  certificate template : serialNumberLimit   :=   new ( big . Int ). Lsh ( big . NewInt ( 1 ),   128 ) serialNumber ,   err   :=   rand . Int ( rand . Reader ,   serialNumberLimit ) if   err   !=   nil   { \n   log . Fatalf ( \"Failed to generate serial number: %v\" ,   err ) } \n\n template   :=   x509 . Certificate { \n   SerialNumber :   serialNumber , \n   Subject :   pkix . Name { \n     Organization :   [] string { \"My Corp\" }, \n   }, \n   DNSNames :    [] string { \"localhost\" }, \n   NotBefore :   time . Now (), \n   NotAfter :    time . Now (). Add ( 3   *   time . Hour ), \n\n   KeyUsage :                x509 . KeyUsageDigitalSignature , \n   ExtKeyUsage :             [] x509 . ExtKeyUsage { x509 . ExtKeyUsageServerAuth }, \n   BasicConstraintsValid :   true , } Each certificate needs a unique serial number; typically, certificate\nauthorities will have these stored in some database but for our local needs a\nrandom 128-bit number will do. This is what the first few lines of the snippet\nare doing. Next comes the  x509.Certificate  template. For more information on what the\nfields mean, see the  crypto/x509 package docs , as well as  RFC 5280 . We'll just note that the certificate\nis valid for 3 hours, and is only valid for the  localhost  domain. Next: derBytes ,   err   :=   x509 . CreateCertificate ( rand . Reader ,   & template ,   & template ,   & privateKey . PublicKey ,   privateKey ) if   err   !=   nil   { \n   log . Fatalf ( \"Failed to create certificate: %v\" ,   err ) } The certificate is created from the template, and is signed with the private key\nwe've generated earlier. Note that  &template  is passed in both for the\n template  and  parent  parameters of  CreateCertificate . The latter is\nwhat makes this certificate  self-signed . This is it, we have the private key for our server and its certificate (which\ncontains the public key, among other information). All that's left now is to\nserialize them into files. First, the certificate: pemCert   :=   pem . EncodeToMemory ( & pem . Block { Type :   \"CERTIFICATE\" ,   Bytes :   derBytes }) if   pemCert   ==   nil   { \n   log . Fatal ( \"Failed to encode certificate to PEM\" ) } if   err   :=   os . WriteFile ( \"cert.pem\" ,   pemCert ,   0644 );   err   !=   nil   { \n   log . Fatal ( err ) } log . Print ( \"wrote cert.pem\\n\" ) And then, the private key: privBytes ,   err   :=   x509 . MarshalPKCS8PrivateKey ( privateKey ) if   err   !=   nil   { \n   log . Fatalf ( \"Unable to marshal private key: %v\" ,   err ) } pemKey   :=   pem . EncodeToMemory ( & pem . Block { Type :   \"PRIVATE KEY\" ,   Bytes :   privBytes }) if   pemKey   ==   nil   { \n   log . Fatal ( \"Failed to encode key to PEM\" ) } if   err   :=   os . WriteFile ( \"key.pem\" ,   pemKey ,   0600 );   err   !=   nil   { \n   log . Fatal ( err ) } log . Print ( \"wrote key.pem\\n\" ) We serialize the certificate and the key into PEM files, which looks like this\n(for the certificate): ----- BEGIN   CERTIFICATE ----- MIIBbjCCARSgAwIBAgIRALBCBgLhD1I / 4 S0fRZv6yfcwCgYIKoZIzj0EAwIwEjEQ MA4GA1UEChMHTXkgQ29ycDAeFw0yMTAzMjcxNDI1NDlaFw0yMTAzMjcxNzI1NDla MBIxEDAOBgNVBAoTB015IENvcnAwWTATBgcqhkjOPQIBBggqhkjOPQMBBwNCAASf wNSifB2LWDeb6xUAWbwnBQ2raSQTqqpaR1C1eEiy6cgqUiiOlr4jUDDiFCly + AS9 pNNe8o63 / Gab / 98 dwFNQo0swSTAOBgNVHQ8BAf8EBAMCB4AwEwYDVR0lBAwwCgYI KwYBBQUHAwEwDAYDVR0TAQH / BAIwADAUBgNVHREEDTALgglsb2NhbGhvc3QwCgYI KoZIzj0EAwIDSAAwRQIgYlJYGIwSvA + AmsHe8P34B5 + hlfWEK4 + kBmydJ65XJZMC IQCzg5aihUXh7Rm0L1K3JrG7eRuTuFSkHoAhzk4cy6FqfQ == ----- END   CERTIFICATE ----- If you've ever set up SSH keys, the format should look familiar. We can use\nthe  openssl  command-line tool to show its contents: $ openssl x509 -in cert.pem -text\nCertificate:\n    Data:\n        Version: 3 (0x2)\n        Serial Number:\n            b0:42:06:02:e1:0f:52:3f:e1:2d:1f:45:9b:fa:c9:f7\n        Signature Algorithm: ecdsa-with-SHA256\n        Issuer: O = My Corp\n        Validity\n            Not Before: Mar 27 14:25:49 2021 GMT\n            Not After : Mar 27 17:25:49 2021 GMT\n        Subject: O = My Corp\n        Subject Public Key Info:\n            Public Key Algorithm: id-ecPublicKey\n                Public-Key: (256 bit)\n                pub:\n                    04:9f:c0:d4:a2:7c:1d:8b:58:37:9b:eb:15:00:59:\n                    bc:27:05:0d:ab:69:24:13:aa:aa:5a:47:50:b5:78:\n                    48:b2:e9:c8:2a:52:28:8e:96:be:23:50:30:e2:14:\n                    29:72:f8:04:bd:a4:d3:5e:f2:8e:b7:fc:66:9b:ff:\n                    df:1d:c0:53:50\n                ASN1 OID: prime256v1\n                NIST CURVE: P-256\n        X509v3 extensions:\n            X509v3 Key Usage: critical\n                Digital Signature\n            X509v3 Extended Key Usage:\n                TLS Web Server Authentication\n            X509v3 Basic Constraints: critical\n                CA:FALSE\n            X509v3 Subject Alternative Name:\n                DNS:localhost\n    Signature Algorithm: ecdsa-with-SHA256\n         30:45:02:20:62:52:58:18:8c:12:bc:0f:80:9a:c1:de:f0:fd:\n         f8:07:9f:a1:95:f5:84:2b:8f:a4:06:6c:9d:27:ae:57:25:93:\n         02:21:00:b3:83:96:a2:85:45:e1:ed:19:b4:2f:52:b7:26:b1:\n         bb:79:1b:93:b8:54:a4:1e:80:21:ce:4e:1c:cb:a1:6a:7d\n HTTPS server in Go Now that we have the certificate and private key in hand, we are ready to run an\nHTTPS server! Once again, the standard library makes it very easy, though it's\nimportant to mention that security is a very tricky issue. Before exposing your\nserver to the public Internet, consider consulting with a security engineer\nabout best practices and what configuration options to be aware of . Here's a basic HTTPS server in Go: func   main ()   { \n   addr   :=   flag . String ( \"addr\" ,   \":4000\" ,   \"HTTPS network address\" ) \n   certFile   :=   flag . String ( \"certfile\" ,   \"cert.pem\" ,   \"certificate PEM file\" ) \n   keyFile   :=   flag . String ( \"keyfile\" ,   \"key.pem\" ,   \"key PEM file\" ) \n   flag . Parse () \n\n   mux   :=   http . NewServeMux () \n   mux . HandleFunc ( \"/\" ,   func ( w   http . ResponseWriter ,   req   * http . Request )   { \n     if   req . URL . Path   !=   \"/\"   { \n       http . NotFound ( w ,   req ) \n       return \n     } \n     fmt . Fprintf ( w ,   \"Proudly served with Go and HTTPS!\" ) \n   }) \n\n   srv   :=   & http . Server { \n     Addr :      * addr , \n     Handler :   mux , \n     TLSConfig :   & tls . Config { \n       MinVersion :                 tls . VersionTLS13 , \n       PreferServerCipherSuites :   true , \n     }, \n   } \n\n   log . Printf ( \"Starting server on %s\" ,   * addr ) \n   err   :=   srv . ListenAndServeTLS ( * certFile ,   * keyFile ) \n   log . Fatal ( err ) } It serves a single handler on the root path. The interesting part is the\nTLS configuration, as well as the  ListenAndServeTLS  call, which takes the\npaths to a certificate file and a private key file (in PEM format, just as\nwe generated them earlier). The TLS configuration has many possible fields;\nhere, I opted for a relatively strict protocol of forcing TLS 1.3 at minimum.\nTLS 1.3 comes with strong security out of the box, so this is a good option if\nyou can ensure all your clients understand this version (and in 2021, they\nshould!) The diff from a plain HTTP server is fewer than 10 lines of code! The bulk of\nthe server's code (handlers for specific routes) is completely oblivious to the\nunderlying protocol and won't change. With this server running locally (and serving on port 4000 by default), Chrome\nwill initially balk when accessing it: That's because a web browser will not, by default, accept a self-signed\ncertificate. As mentioned above, browsers come with a hard-coded list of CAs\nthey trust, and our self-signed certificate is obviously not one of them. We\ncan still proceed to the server by clicking Advanced and then allowing Chrome\nto go on, accepting the risk explicitly. It will then show us the website,\nalbeit grudgingly (with a red \"Not secure\" sign in the address bar). If we try to  curl  to the server, we'll also get an error : $ curl -Lv  https://localhost:4000\n\n*   Trying 127.0.0.1:4000...\n* TCP_NODELAY set\n* Connected to localhost (127.0.0.1) port 4000 (#0)\n* ALPN, offering h2\n* ALPN, offering http/1.1\n* successfully set certificate verify locations:\n*   CAfile: /etc/ssl/certs/ca-certificates.crt\n  CApath: /etc/ssl/certs\n* TLSv1.3 (OUT), TLS handshake, Client hello (1):\n* TLSv1.3 (IN), TLS handshake, Server hello (2):\n* TLSv1.3 (IN), TLS handshake, Encrypted Extensions (8):\n* TLSv1.3 (IN), TLS handshake, Certificate (11):\n* TLSv1.3 (OUT), TLS alert, unknown CA (560):\n* SSL certificate problem: unable to get local issuer certificate\n* Closing connection 0\ncurl: (60) SSL certificate problem: unable to get local issuer certificate\nMore details here: https://curl.haxx.se/docs/sslcerts.html\n\ncurl failed to verify the legitimacy of the server and therefore could not\nestablish a secure connection to it. To learn more about this situation and\nhow to fix it, please visit the web page mentioned above.\n By reading the docs, we can find that  curl  can be made to trust our server\nby providing it with the server's certificate into the  --cacert  flag. If\nwe try that: $ curl -Lv --cacert <path/to/cert.pem>  https://localhost:4000\n\n*   Trying 127.0.0.1:4000...\n* TCP_NODELAY set\n* Connected to localhost (127.0.0.1) port 4000 (#0)\n* ALPN, offering h2\n* ALPN, offering http/1.1\n* successfully set certificate verify locations:\n*   CAfile: /home/eliben/eli/private-code-for-blog/2021/tls/cert.pem\n  CApath: /etc/ssl/certs\n* TLSv1.3 (OUT), TLS handshake, Client hello (1):\n* TLSv1.3 (IN), TLS handshake, Server hello (2):\n* TLSv1.3 (IN), TLS handshake, Encrypted Extensions (8):\n* TLSv1.3 (IN), TLS handshake, Certificate (11):\n* TLSv1.3 (IN), TLS handshake, CERT verify (15):\n* TLSv1.3 (IN), TLS handshake, Finished (20):\n* TLSv1.3 (OUT), TLS change cipher, Change cipher spec (1):\n* TLSv1.3 (OUT), TLS handshake, Finished (20):\n* SSL connection using TLSv1.3 / TLS_AES_128_GCM_SHA256\n* ALPN, server accepted to use h2\n* Server certificate:\n*  subject: O=My Corp\n*  start date: Mar 29 13:30:25 2021 GMT\n*  expire date: Mar 29 16:30:25 2021 GMT\n*  subjectAltName: host \"localhost\" matched cert's \"localhost\"\n*  issuer: O=My Corp\n*  SSL certificate verify ok.\n* Using HTTP2, server supports multi-use\n* Connection state changed (HTTP/2 confirmed)\n* Copying HTTP/2 data in stream buffer to connection buffer after upgrade: len=0\n* Using Stream ID: 1 (easy handle 0x557103006e10)\n> GET / HTTP/2\n> Host: localhost:4000\n> user-agent: curl/7.68.0\n> accept: */*\n>\n* TLSv1.3 (IN), TLS handshake, Newsession Ticket (4):\n* Connection state changed (MAX_CONCURRENT_STREAMS == 250)!\n< HTTP/2 200\n< content-type: text/plain; charset=utf-8\n< content-length: 33\n< date: Mon, 29 Mar 2021 13:31:34 GMT\n<\n* Connection #0 to host localhost left intact\nProudly served with Go and HTTPS!\n Success! We can also talk to our server using a custom HTTPS client written in Go. Here's\nthe code: func   main ()   { \n   addr   :=   flag . String ( \"addr\" ,   \"localhost:4000\" ,   \"HTTPS server address\" ) \n   certFile   :=   flag . String ( \"certfile\" ,   \"cert.pem\" ,   \"trusted CA certificate\" ) \n   flag . Parse () \n\n   cert ,   err   :=   os . ReadFile ( * certFile ) \n   if   err   !=   nil   { \n     log . Fatal ( err ) \n   } \n   certPool   :=   x509 . NewCertPool () \n   if   ok   :=   certPool . AppendCertsFromPEM ( cert );   ! ok   { \n     log . Fatalf ( \"unable to parse cert from %s\" ,   * certFile ) \n   } \n\n   client   :=   & http . Client { \n     Transport :   & http . Transport { \n       TLSClientConfig :   & tls . Config { \n         RootCAs :   certPool , \n       }, \n     }, \n   } \n\n   r ,   err   :=   client . Get ( \"https://\"   +   * addr ) \n   if   err   !=   nil   { \n     log . Fatal ( err ) \n   } \n   defer   r . Body . Close () \n\n   html ,   err   :=   io . ReadAll ( r . Body ) \n   if   err   !=   nil   { \n     log . Fatal ( err ) \n   } \n   fmt . Printf ( \"%v\\n\" ,   r . Status ) \n   fmt . Printf ( string ( html )) } The only part making this different from a standard HTTP client is the TLS\nsetup. The important bit is setting up the  RootCAs  field of the\n tls.Config  struct. This is telling Go which certificates the client can\ntrust. Other options for generating certificates You may not know that Go comes with a tool to generate self-signed\nTLS certificates, right in the standard installation. If you have Go installed\nat  /usr/local/go , you can run this tool with: $ go run /usr/local/go/src/crypto/tls/generate_cert.go -help\n In general, it accomplishes the same goal as the first code snippet in this\npost, but whereas my snippet makes some opinionated decisions about what to\ngenerate,  generate_cert  is configurable with flags and supports several\ndifferent options. As we've seen, while self-signed certificates can work for testing, they're not\nideal for all scenarios. For example, it's difficult to make browsers trust\nthem, and even then the user experience doesn't fully replicate the \"real world\"\none. Another option for generating local certificates for testing is the\n mkcert tool . It creates a local\ncertificate authority (CA), and adds it to your system's trusted list of CAs.\nIt then generates certificates signed by this authority for you, so as far as\nthe browser is concerned, they're fully trusted. If we run our simple HTTPS server with a certificate/key generated by\n mkcert , Chrome will happily access it without warnings; we can also see\nthe details in the Security tab of developer tools: curl  will also successfully contact the server without requiring a\n cacert  flag, because it checks the system's trusted CAs already. If you're looking for  real  certificates, Let's Encrypt is of course a natural\noption, using the  certbot  client or something\nsimilar. In Go, libraries like  certmagic  can automate the interaction with\nLet's Encrypt for servers. Client authentication (mTLS) So far the examples we've seen have the server providing its (CA-signed)\ncertificate to the client to prove that the server is legitimately who it claims\nto be (e.g. your bank's website, before you agree to provide your password). This idea is easy to extend to  mutual authentication , where the client also\nhas a signed certificate to prove its identity. In the world of TLS, this is\ncalled  mTLS  (for  mutual  TLS), and could be useful in many settings where\ninternal services have to communicate with each other securely. Public-key\ncrypto is generally considered more secure than passwords. Here's a simple HTTPS server with client authentication. The lines that changed\nfrom the earlier HTTPS server are highlighted: func   main ()   { \n   addr   :=   flag . String ( \"addr\" ,   \":4000\" ,   \"HTTPS network address\" ) \n   certFile   :=   flag . String ( \"certfile\" ,   \"cert.pem\" ,   \"certificate PEM file\" ) \n   keyFile   :=   flag . String ( \"keyfile\" ,   \"key.pem\" ,   \"key PEM file\" )    clientCertFile   :=   flag . String ( \"clientcert\" ,   \"clientcert.pem\" ,   \"certificate PEM for client authentication\" )    flag . Parse () \n\n   mux   :=   http . NewServeMux () \n   mux . HandleFunc ( \"/\" ,   func ( w   http . ResponseWriter ,   req   * http . Request )   { \n     if   req . URL . Path   !=   \"/\"   { \n       http . NotFound ( w ,   req ) \n       return \n     } \n     fmt . Fprintf ( w ,   \"Proudly served with Go and HTTPS!\" ) \n   }) \n\n    // Trusted client certificate.    clientCert ,   err   :=   os . ReadFile ( * clientCertFile )    if   err   !=   nil   {      log . Fatal ( err )    }    clientCertPool   :=   x509 . NewCertPool ()    clientCertPool . AppendCertsFromPEM ( clientCert ) \n   srv   :=   & http . Server { \n     Addr :      * addr , \n     Handler :   mux , \n     TLSConfig :   & tls . Config { \n       MinVersion :                 tls . VersionTLS13 , \n       PreferServerCipherSuites :   true ,        ClientCAs :                  clientCertPool ,        ClientAuth :                 tls . RequireAndVerifyClientCert ,      }, \n   } \n\n   log . Printf ( \"Starting server on %s\" ,   * addr ) \n   err   =   srv . ListenAndServeTLS ( * certFile ,   * keyFile ) \n   log . Fatal ( err ) } The changes are pretty much what you'd expect; in addition to setting up its\nown certificate, key and TLS config, the server loads a client certificate and\nsets  TLSConfig  to trust it. Naturally, this could also be the certificate\nof a locally trusted CA which signs client certificates. And this is an HTTPS client which authenticates itself when connecting to a\nserver; again, the lines that changed from the earlier (non-mTLS) client are\nhighlighted: func   main ()   { \n   addr   :=   flag . String ( \"addr\" ,   \"localhost:4000\" ,   \"HTTPS server address\" ) \n   certFile   :=   flag . String ( \"certfile\" ,   \"cert.pem\" ,   \"trusted CA certificate\" )    clientCertFile   :=   flag . String ( \"clientcert\" ,   \"clientcert.pem\" ,   \"certificate PEM for client\" )    clientKeyFile   :=   flag . String ( \"clientkey\" ,   \"clientkey.pem\" ,   \"key PEM for client\" )    flag . Parse () \n\n    // Load our client certificate and key.    clientCert ,   err   :=   tls . LoadX509KeyPair ( * clientCertFile ,   * clientKeyFile )    if   err   !=   nil   {      log . Fatal ( err )    } \n   // Trusted server certificate. \n   cert ,   err   :=   os . ReadFile ( * certFile ) \n   if   err   !=   nil   { \n     log . Fatal ( err ) \n   } \n   certPool   :=   x509 . NewCertPool () \n   if   ok   :=   certPool . AppendCertsFromPEM ( cert );   ! ok   { \n     log . Fatalf ( \"unable to parse cert from %s\" ,   * certFile ) \n   } \n\n   client   :=   & http . Client { \n     Transport :   & http . Transport { \n       TLSClientConfig :   & tls . Config { \n         RootCAs :        certPool ,          Certificates :   [] tls . Certificate { clientCert },        }, \n     }, \n   } \n\n   r ,   err   :=   client . Get ( \"https://\"   +   * addr ) \n   if   err   !=   nil   { \n     log . Fatal ( err ) \n   } \n   defer   r . Body . Close () \n\n   html ,   err   :=   io . ReadAll ( r . Body ) \n   if   err   !=   nil   { \n     log . Fatal ( err ) \n   } \n   fmt . Printf ( \"%v\\n\" ,   r . Status ) \n   fmt . Printf ( string ( html )) } Before we try it, we'll need to change our certificate generating script to\ngenerate certificates suitable for clients as well. The change is in this line: ExtKeyUsage :             [] x509 . ExtKeyUsage { x509 . ExtKeyUsageServerAuth }, Which changes to: ExtKeyUsage :             [] x509 . ExtKeyUsage { x509 . ExtKeyUsageServerAuth ,   x509 . ExtKeyUsageClientAuth }, Now let's do a trial run. Start by generating separate certificates/keys for\nclient and server: # client cert \n\n$ go run tls-self-signed-cert.go\n 2021 /04/03  05 :51:25 wrote cert.pem\n 2021 /04/03  05 :51:25 wrote key.pem\n$ mv cert.pem clientcert.pem\n$ mv key.pem clientkey.pem\n\n # server cert \n\n$ go run tls-self-signed-cert.go\n 2021 /04/03  05 :51:42 wrote cert.pem\n 2021 /04/03  05 :51:42 wrote key.pem\n Running the mTLS server, it should pick up the right files based on flag\ndefaults: $ go run https-server-mtls.go\n2021/04/03 05:54:51 Starting server on :4000\n In a separate window, if we run the older (non-mTLS) client, we get an error: $ go run https-client.go\n2021/04/03 05:55:24 Get \"https://localhost:4000\": remote error: tls: bad certificate\nexit status 1\n And the server log will show that \"client didn't provide a certificate\".\nHowever, if we run the new mTLS client, it works: $ go run https-client-mtls.go\n200 OK\nProudly served with Go and HTTPS!\n While this demonstrates the mechanics of running mTLS servers and clients, in\nreality there'd be a lot more to do, especially to manage certificates,\ncertificate renewal and revocation, and trusted CAs. This is called Public Key\nInfrastructure (PKI), and it's a large topic outside the scope of this modest\npost."}, "210705_news_468167.txt": {"page_id": "210705_news_468167.txt", "text": "Boris Johnson must urgently take control of the UK\u2019s presidency of vital UN climate talks, amid a shower of green policy setbacks and growing concern over the lack of a coherent all-government climate strategy, senior international figures have said. The  Cop26 climate summit  is viewed as one of the last chances to put the world on track to meet the goals of the  2015 Paris climate agreement , of holding global heating well below 2C, and preferably no more than 1.5C, above pre-industrial levels. There are just over six months left before the crunch talks are scheduled to begin in Glasgow in November. Major figures in the global climate talks, including veteran diplomats, scientists and respected campaigners, have expressed concern to the Guardian that while the UK is making progress on agreement at Cop26, a series of missteps by the government is in danger of undermining the UK\u2019s leadership and the success of the talks. Christiana Figueres, the former  UN climate chief who led the 2015 Paris climate agreement , warned: \u201cThere have been recent decisions in the UK that are not aligning with the ambition of the net zero target. It is worrisome. There are raised eyebrows among world leaders watching the UK.\u201d Mary Robinson, chair of the Elders group of independent global leaders, and a former UN climate envoy, said poor countries were questioning the UK\u2019s actions, particularly in  cutting overseas aid . \u201cPeople are shocked [by the aid cut],\u201d she said. \u201cThe poorest countries are the moral authority at the Cop, they drive the urgency, they drive the credibility. You need them fully behind the UK presidency to get the good ambition needed.\u201d Emmanuel Gu\u00e9rin, an executive director at the European Climate Foundation, who was one of the top French officials at the Paris talks, added: \u201cThis is not putting the UK in an easy situation. It is very, very suboptimal. There is a lack of consistency between the UK\u2019s domestic announcements and its international objective of success at the Cop.\u201d Britain is widely acknowledged to have been a leader among rich nations on climate in the past two decades. It had the world\u2019s first  Climate Change Act , was one of the first large countries to  set a target of reaching net zero emissions by 2050 , and had success in drastically cutting emissions \u2013 all major reasons why the choice of the UK to host Cop26 was  widely applauded . Johnson has continued this push, with promises to  \u201cbuild back greener\u201d  from the Covid-19 pandemic, to  stop funding fossil fuels overseas , and a bold pledge to  cut UK emissions in the next decade by 68%  compared with 1990 levels, going further than any other major economy. However, the prime minister\u2019s green rhetoric has been accompanied by a series of actions that have left many observers aghast, and that appear contrary to leading an international push to net zero. These include: Jennifer Morgan, the chief of Greenpeace International, said the measures showed the government had little regard for how the UK\u2019s actions would be seen at a crucial point in climate negotiations. \u201cThese decisions are going in the wrong direction, and it is disturbing,\u201d she said. \u201cThey are not prioritising the climate \u2013 domestically or internationally. Most developing countries are now very nervous. The clock is ticking. The prime minister needs to make it clear this is the top priority. If not, then the Cop will be a failure.\u201d Jeffrey Sachs, professor at Columbia University and a leading expert on sustainable development, stressed that the UK\u2019s domestic actions this year had a global impact. He said: \u201cThe world is looking to the UK this year for leadership at Cop26. Everything that they do in the right direction helps Cop26; everything that they do in the wrong direction hurts Cop26.\u201d There is still time for the UK to remedy the impression of a disorganised and inconsistent approach, many believe. Rachel Kyte, formerly a top World Bank official at the Paris climate talks who is now the dean of the Fletcher School at Tufts University in Massachusetts, said: \u201cWhat the UK is doing is like dad dancing \u2013 it is not that they\u2019re evil, just that they are very uncoordinated. They have not yet perfected a whole government approach to getting to net zero.\u201d That has meant departments acting alone, without regard to the overarching needs of the Cop26 presidency, and has been apparent in decisions such as the communities department giving the green light to the Cumbrian coalmine, and the Treasury taking an axe to overseas aid, regardless of the consequences for the climate. Other governments have shown much greater coordination is possible. The US under President Biden has put climate at the heart of policy. \u201cEvery cabinet member in the US is on message with climate, it\u2019s a whole government approach,\u201d said Kyte. \u201cWhat the UK has done has been distracting, it is not helpful.\u201d Some of the rows seem to have taken ministers by surprise. After the proposal for a new coalmine in Cumbria was  given the green light last November , the communities secretary, Robert Jenrick,  decided against calling in the decision  for review, meaning it could go ahead. But in February, James Hansen, the former Nasa scientist dubbed the \u201cgodfather of climate change\u201d, made a key intervention with a  starkly worded letter to Johnson  that said the decision showed \u201ccontemptuous disregard for the future of young people\u201d. Eventually, in mid-March, Jenrick was forced to  concede a public inquiry , in an effort to quash the row. Sir David King, the former UK chief scientist, said the long-running controversy showed a lack of coherent thinking from the top of government. \u201cI have been in government, I know that you can get direction from the prime minister,\u201d he said. \u201cThings have been happening through ministers that have not fully focused on the climate issue, because climate may not be embedded yet in every ministry. Saying we have an excuse to do coal does not wash. We need to show leadership if we are expecting other countries to follow our example.\u201d Most concerning to observers is the government\u2019s decision to  slash overseas development aid , from 0.7% to 0.5% of GDP, which will deprive vulnerable countries of planned assistance worth billions of pounds. The government is keen to point out that climate finance will be ringfenced, and that at \u00a311.6bn over the next five years, it will be more than many other rich nations are providing. However, developing countries at the Cop26 talks and senior officials are  concerned at the signals  the cut sends about rich-country obligations and solidarity with the developing world. Achim Steiner, administrator of the UN Development Programme, put it in diplomatic terms: \u201cIt sends a very mixed signal, and makes developing countries very concerned. It certainly does not enhance the confidence with which developing countries come to the table.\u201d Kyte was more forthright: \u201cThis decision is the single worst self-inflicted injury in this kind of diplomacy that most of us have seen for a very long time.\u201d Some fear other rich countries could use the UK\u2019s stance as an excuse to cut their own aid budgets. \u201cYou can\u2019t say this doesn\u2019t have an impact \u2013 it does,\u201d said Gu\u00e9rin. \u201cPeople are looking at this.\u201d Much progress has been made globally in the past year: countries responsible for two-thirds of global emissions now have a  target to reach net zero  mid-century. However, long-term plans are not enough \u2013 countries also need to come forward before Cop26 with  national plans for cutting emissions  in the next 10 years. \u201cThe science tells us that  what happens in the next decade will be crucial ,\u201d said Figueres. \u201cWe are getting perilously close to  tipping points .\u201d Positive things the government has done Net zero target, including a 10-point plan Commitment to reduce emissions by 68% by 2030  \u2013 a stiffer set of cuts than promised by comparable developed countries \u00a311.6bn in climate finance to developing countries between 2021 and 2025 Boost to offshore wind through licensing of sites, funding to improve ports and  Contracts for Difference \u00a31.3bn to local authorities for insulation and low-carbon heating to social housing and households on low incomes Funding for some low-carbon emerging technologies, including hydrogen, carbon capture and storage and nuclear fusion Integrated review of defence and foreign policy made climate change \u201cnumber one priority\u201d for international policy End to funding fossil fuels overseas Negative actions Slashing overseas aid from 0.7 % of GDP to 0.5% Cumbrian coalmine given green light, now subject to public inquiry Licensing of new oil and gas exploration sites in North Sea Scrapping the \u00a31.5bn green-homes grant for home insulation and low-carbon heating, leaving 20 million households without incentives to decarbonise Slashing incentives to buy electric cars, freeze on fuel duty and \u00a327bn road-building scheme, while emissions from transport fell only 1% in last decade Support for Mathias Cormann, a climate sceptic, to head the OECD despite concerns from many countries and campaigners Support for airport expansion Covid-19 stimulus money given to high-emitting companies without green strings attached Missing tree planting targets Thousands of green jobs lost"}, "210705_news_468172.txt": {"page_id": "210705_news_468172.txt", "text": "In the last months, we travelled around, and with this release, we tried to implement some improvements based on our experience with the daily application of the TorBox. The most significant improvement is abolishing\u00a0 wicd \u00a0and introducing our new TorBox Wireless Manager (TWM). Not only is the TWM much easier to use, but it also doesn\u2019t need so much power. Another pleasant novelty is the support of  Azur-Meek and Snowflake , which should also work in China. During our travels, we have noticed incorrect DNS resolution regarding  torproject.org  in some countries. Probably, this is a kind of cheap censorship mechanism. For this reason, during the installation and updates,\u00a0 local DNS resolutions \u00a0are made through Google\u2019s and Cloudflare\u2019s Domain Name Servers instead of using the Internet Providers presetting delivered by\u00a0 DHCP .\u00a0 Important:\u00a0these settings are only for TorBox local traffic; all data from the clients are routed through Tor (including DNS requests).  Nevertheless, some user complained about using Google\u2019s and Cloudflare\u2019s DNS servers and requested to implement other DNS servers.  In the FAQ , we explain our decision in detail and how someone, who cannot live with it, has the possibility to change these settings.  TorBox Image (about 1 GB): \u00a0 v.0.4.0 (10.04.2021) \u00a0\u2013\u00a0 SHA-256 values TorBox Menu only: \u00a0 v.0.4.0 (10.04.2021) \u00a0\u2013\u00a0 SHA-256 values We strongly recommend using the new image\u00a0rather than updating an existing system.\u00a0 The new TorBox Wireless Manager, which replaces wicd. \u2022 \u2022 \u2022 Changelog: v.0.3.2 (24.08.2020) \u00a0 \u2013> v.0.4.0 (10.04.2021) Update:  The system is based on\u00a0 Raspberry Pi OS \u201cBuster\u201d Lite \u00a0with a\u00a0 Linux Kernel 5.10.17 \u00a0and\u00a0 Tor version 0.4.5.7 . The  Tor Project  fixed in this latest version two critical denial-of-service bugs:  TROVE-2021-001  and  TROVE-2021-002 , of which only the first one is relevant for clients. New : wicd has been replaced by the TorBox Wireless Manager (TWM).  We like to hear your feedback . New : Support for Meek-Azure and Snowflake implemented, which should also work in China. Meek uses a technique called \u201c domain fronting \u201d to send a message to a Tor relay in a way that is hard to block. Meek-Azure makes it look like you are browsing to  Microsoft\u2019s Azure  server\u00a0 instead of using Tor. Snowflake is an improvement upon  Flashproxy . It sends your traffic through  WebRTC , a peer-to-peer protocol with built-in  NAT punching . However, because Meek-Azure and Snowflake are slower, OBFS4 bridges should be used first. If not needed, the best is not to use bridges in the first place. Please,  tell us  about your experiences with the use of bridges to circumvent censorship. New : Based on several user requests, the  configuration sub-menu  (entry 11) comprises now an option to block all HTTP plain text traffic through Tor.\u00a0This should avoid unencrypted data traffic at the Exit Node, which could break your anonymity (see  here ). However, it is possible that not only http-requests but also other tools, such as VPN clients, will no longer work. Where possible, we recommend installing  HTTPS Everywhere  in the Browser.  We like to hear  your feedback on your experiences about that feature so that we can decide if we should block all HTTP plain text traffic by default, starting with one of the next releases. New : Based on several user requests, TorBox  can be configured  to be accessed with SSH from the Internet. New : Based on several user requests, support for additional network driver were added:  Realtek  8188eu, 8188fu, 8192eu, 8812au, 8814au, 8821au, 8821cu, and 8822bu. New : It is now possible to connect/disconnect the TorBox from a VPN using the  countermeasure sub-menu  without changing Tor\u2019s primary interface to the Internet. With this feature, the user can influence the route of the local network data from the command line and, for example, circumvent censorship measures that don\u2019t allow updating TorBox. Additionally, it gives the possibility to completely disconnect the TorBox from a VPN after finishing using  main menu entry 9 , which enables TorBox to use route Tor over VPN (for more information about Tor over VPN / VPN over Tor, see  here ). New : In the  main menu , in the top of the right corner, a message shows not only if Tor is working (meaning  https://check.torproject.org  returns a positive result), but also if the TorBox is connected to a VPN (meaning that local network data from the command prompt is routed through VPN). New : Installation script for  Debian 10 (Buster) and Debian 11 (Bullseye)  \u2013 for more information, see  here . Fixed : The user \u201ctorbox\u201d was not a member of the group \u201cnetdev\u201d, which causes a display error in the  entry 1 and 3 in the update and reset sub-menu . Fixed : During the installation of TorBox with the installation script, Tor will be compiled because the  the Tor Project  doesn\u2019t provide a binary version for the Raspberry Pi. We had this option before in the  update and reset sub-menu  but not in the installation script, which leads to missing tor packages. Fixed : Fixed the download path for the TorBox menu in the installation as well as in the  update and reset sub-menu . We also changed the GitHub download path for the  Raspberry Pi Framebuffer Copy  needed for  AdAfruits Pi TFT installation . GitHub is suddenly changing URLs, which is a pain in the ass. Fixed : Missing path to torbox.lib in some scripts, which use Bridges and prevented Tor from restarting automatically. Fixed : Wrong\u00a0 menu entry relating to the countermeasure against a disconnection when idle after a restart. Improved : During the installation and updates, local DNS resolutions are made through Google\u2019s and Cloudflare\u2019s Domain Name Servers to  avoid cheap censorship mechanism .  Important:\u00a0these settings are only for TorBox local traffic; all data from the clients are routed through Tor (including DNS requests).  For more information and an explanation of how it is possible to change it,  see here . Improved : The support for  Sixfab Shields/HATs for cellular connections  can now be installed offline. Improved : The script to install the Adafruit PI TFT is now locally stored and not fetched from the  Adafruit Github Repository  (Adafruit changed it, and it was broken). However, an Internet connection is still necessary for the installation. Improved : The support for installing TorBox on a\u00a0 Ubuntu 20.04 / 20.10 \u00a0or\u00a0 Debian Buster/Bullseye \u00a0system. TorBox\u2019s implementation on other systems and hardware is experimental because we do not have the resources to check all details on all different installations. You can help us with  reporting errors back to us . Improved : Cleaned up the code and outsourced more essential functions into the TorBox library or separate sub-scripts. This will help to maintain the code in future releases properly. Improved : The appearance of all menus has been streamlined, and in the files, we fixed some minor errors. The countermeasure sub-menu of TorBox v.0.4.0 with Snowflake and Meek-Azure. \u2022 \u2022 \u2022 Known problems and bugs LIMITATION : If HTTP plain text traffic is blocked ( configuration sub-menu entry 11 ), .onion addresses, which use \u201chttp://\u201ddoesn\u2019t work anymore directly with Chrome and Chromium. Both browsers will behave like all other browsers by default, because based on\u00a0 IETF RFC 7686 , applications that do not implement the Tor protocol generate an error upon the use of .onion and do not perform a DNS lookup. However, .onion addresses using \u201chttp://\u201d  can be used through SOCKS 5  even if the HTTP plain text traffic is blocked. Onion addresses using \u201chttp://\u201d can also be used with the Tor Browser \u2013  with or without its own Tor instance  \u2013 running on a client. \ud83d\ude42 In other words, blocking HTTP plain text traffic does not work if  SOCKS 5 proxy functionality or Tor Browser is used on a client. \ud83d\ude41  WARNING MESSAGE ADDED \u2714\ufe0e . PROBLEM : People running an  OBFS4 bridge relay  will probably encounter the following hourly error message: \u201cUnable to find IPv6 address for ORPort xxxx.\u201d It seems that with Tor version 0.4.5.* the Tor Project focuses on improving the IPv6 support (until now,  a Tor relay needs a public IPv4 address ). At the same time, they changed the address auto-discovery behaviour (see  here ,  here  and  here ), which probably leads to this hourly error message. Even, the Tor Project writes in the  Changelog for 0.4.5.7  that they removed \u201ca spammy log notice falsely claiming that the IPv4/v6 address was missing\u201d, it doesn\u2019t seem to work completely. However,  this error message has no negative on the operation and the status on  Metrics .   PROBLEM SOLVED \u2714\ufe0e . BUG : Entry 5 in the update and reset sub-menu, which should update the TorBox menu fails to remove the old  lib/__pycache__  directory. Even if saying yes to remove it, the update will be incompleted because it cannot replace the old lib directory. Unfortunatelly, all files in that directory except  lib/__pycache__  are deleted, so that the TorBox menu will not properly work anymore. It can be fixed with the following procedure: \u2013 Leave the TorBox menu by pressing ESC \u2013 Type  sudo chmod a+w -R lib   \u2013 Start TorBox menu again by typing  ./menu \u2013 Start the update and reset sub-menu and execute entry 5 . After this procedure and the successful update, the bug is fixed. The current image is updated.\u00a0  BUG FIXED \u2714\ufe0e . BUG : This affects only Bridge Relay operators: due to a bug in the main menu script, every second time when the main menu was started, the OBFS4 and ORPort was blocked, which set the Bridge Relay offline.  You can fix these bug by updating the TorBox menu ( update and reset sub-menu entry 5 ). The current image is updated.\u00a0  BUG FIXED \u2714\ufe0e . BUG : Already in TorBox v.0.3.2, main menu\u2019s start-up can be stuck on the message \u201cChecking connectivity to the Internet \u2013 please wait\u2026\u201d for an annoying amount of time if TorBox has no Internet connection. In TorBox v.0.4.0, the introduced timeout had no effect because we did it in a wrong way. You can fix these bug by updating the TorBox menu ( update and reset sub-menu entry 5 ). The current image is updated.  BUG FIXED \u2714\ufe0e . . BUG : Using entry 10 in the  configuration sub-menu  to enable the SSH access to TorBox  from the Internet was not permanent when chosen so, but was permanent when chosen temporary (for a description and a quick fix, see  issue #46 ). You can fix these bug by updating the TorBox menu ( update and reset sub-menu entry 5 ).  BUG FIXED \u2714\ufe0e . BUG : Entry 7 in the  update and reset sub-menu  did not erase all passwords in the TorBox Wireless Manager. To take effect, a reboot is needed. You can fix these bug by updating the TorBox menu ( update and reset sub-menu entry 5 ).  BUG FIXED \u2714\ufe0e  BUG : Because of a wrong variable name, the Snowflake and the Meek-Azure bridges got in the way (for details see  issue #48 ).  Nyxnor  fixed the bug with the pull request  #49  and  #51 . You can fix these bug by updating the TorBox menu ( update and reset sub-menu entry 5 ).  BUG FIXED \u2714\ufe0e . BUG : Since TorBox v.0.3.2, we introduced a new SOCKS v5, which supports\u00a0 destination address stream isolation . Unfortunately, we used the port number, which is reserved for the Tor control port. So far, this didn\u2019t have any adverse side effects. However, this is not the way it supposed to be. For that reason, we changed the SOCKS v5 port for destination address stream isolation to 9052.  You can fix these bug by changing in /etc/tor/torrc the following lines:  SocksPort 192.168.42.1:9051 IsolateDestAddr  ->  SocksPort 192.168.42.1:9052 IsolateDestAddr  and  SocksPort 192.168.43.1:9052 IsolateDestAddr  ->  SocksPort 192.168.42.1:9052 IsolateDestAdd  (with or without #) or by updating the TorBox menu ( update and reset sub-menu entry 5 ) and than copying the default torrc to /etc ( cp etc/tor/torrc /etc/tor/torrc ).   The proposed fix will most likely break tor because the menu script must also be adapted to the new port. For that reason, the fix will be included in TorBox v.0.4.1.   BUG NOT FIXED IN v.0.4.0 \ud83d\ude2c .   LOOK&FEEL : Because  we offer several install scripts , which dependent on the operating system, install Tor in different ways, we decided to put the repository for Tor\u2019s binaries  and  sources, knowing that, for example, on Raspberry Pi OS with apt-get update an error message is shown, which does not affect. However, inexperienced users might be discouraged by the error message. See also  issue #36 . You can fix these bug by updating the TorBox menu ( update and reset sub-menu entry 5 ). The current image is updated.  CLOSED \u2714\ufe0e"}, "210705_news_468176.txt": {"page_id": "210705_news_468176.txt", "text": "Could Japan see an increase in the presence of U.S. military capabilities and personnel in the years ahead? It\u2019s looking possible. Soon after he took office, Secretary of Defense Lloyd Austin  ordered  a posture review to ensure that the U.S. global footprint is the right size to support U.S. strategy. The results of the review will inform the Biden administration how best to allocate military forces in pursuit of U.S. interests. As part of the review, the United States will  consult with its treaty allies  and consider its alliance commitments. In the Indo-Pacific, the U.S. presence is concentrated in Japan, with about  56,000 active military personne l and all four services represented. Despite agreements between the United States and Japan years ago that have led to a gradual reduction in the U.S. presence in Japan, such as the number of U.S. Marines in Okinawa, Japan\u2019s continued importance to U.S. strategy and Tokyo\u2019s increased willingness to be more proactive in the security domain mean that Japan could see an uptick in U.S. military presence \u2014 more ships, more troops, even long-range strike missiles \u2014 after this review is complete. \u00a0 \u00a0 The current U.S. force presence in Japan, including its capabilities and disposition, is a product of history and U.S. vital interests. Immediately after World War II, U.S. occupying forces used many of the same bases and airfields that Imperial Japanese forces had used. Then, guided by the logic inherent in  NSC-68  \u2014 the historic presidential directive issued in 1950 that called for a military buildup to compete with the Soviet Union \u2014 the U.S. presence grew after the end of the American occupation of Japan in 1952. The exigencies of the Korean War also shaped the U.S. presence in Japan. Although the Korean conflict was confined to the peninsula, because North Korea (and, by extension, China) had no power-projection capabilities beyond their immediate shores, Japan was a sanctuary for the United States. As such, Japan and Okinawa (which, unlike the rest of Japan, remained under  U.S. occupation until 1972 ) served as a power-projection platform for U.S. operations and a sustainment and logistics hub. Both served a similar role in the Vietnam War. Collectively, unchallenged U.S. air and sea control in the region became the foundation for U.S. regional presence. This enabled the United States to project force when, where, and how it wished from its secure bases in Japan. While the shape of the U.S. presence in Japan has changed over time, its contribution to U.S. and allied interests has been reaffirmed by every administration from President Harry S. Truman through  President Joe Biden . Sanctuary No More Japan is no longer the sanctuary for U.S. forces that it once was, and this has been true for several decades. First and foremost, all U.S. (and Japanese) bases are well within range of adversaries\u2019 air, sea, and missile platforms.  China , for example, has invested heavily in missile and rocket forces to achieve sea control over the East and South China Seas from an arsenal spread throughout its vast continental hinterland.  North Korea , too, has developed significant missile capabilities. The fact that weapons today are much more accurate at distance further sharpens the challenge. For example, China\u2019s development of a variant of its  DF-21 missile , dubbed a \u201ccarrier killer,\u201d threatens to keep the U.S. Navy at a considerable distance from any potential operation. Finally, emerging technologies provide U.S. adversaries with ubiquitous, pervasive, detailed, and accurate surveillance of all of Japan, thereby improving their ability to strike with greater accuracy and lethality. For example, more sophisticated satellites will provide near continuous coverage of the Japanese archipelago and the waters around it,  hypersonic weapons  with evasive trajectories will make defending against them more difficult, and  AI combined with autonomy  will provide faster decision-making and greater domain awareness, thereby disadvantaging militaries that rely solely on the speed of humans. Gone are the days of the United States being able to surreptitiously move forces to and within the region without an adversary taking note or holding the forces at risk. Taken together, these developments mean that U.S. force posture in the Western Pacific, long a strong bulwark of U.S. conventional deterrence, has become increasingly vulnerable. The United States understands this. China projects power on almost a daily basis in campaigns designed to intimidate Taiwan, Japan, and other nations. China\u2019s forces are  routinely present  in the air and seas surrounding Japan and Taiwan, presumably to test opposing forces\u2019 reactions and possibly coerce a response. In addition to other reasons, Washington\u2019s continuing attention to protecting U.S. vital interests, fulfilling its alliance commitments, and protecting the territory and lives of U.S. allies and partners has meant that the United States has not simply pulled back from its overseas presence and let other countries fend for themselves. What Could Change? Being within range of adversary weapons does not sound the end of U.S. forward presence, particularly given the global range of some types of weapons. Nor can U.S. allies change their geography. If U.S. withdrawal is not an option, and long-range precision weapons have made every U.S. base in Japan a target, there are several areas that potentially could see change after the Department of Defense\u2019s posture review. One area could be air and missile defenses. Commandant of the U.S. Marine Corps Gen. David Berger recently  commented  that it is imperative to think anew about overseas installations, acknowledging that U.S. forward bases and infrastructure are vulnerable to adversaries. In the Indo-Pacific, for example,  Chinese ballistic missiles  pose a considerable threat to U.S. air bases. A  2015 RAND report  examined the effect of ballistic missile salvos targeted at Kadena Air Force Base in Okinawa and found that a large salvo could close the runway for days or even weeks. Because abandoning U.S. forward bases is not realistic, Berger suggests that more efforts are needed to raise the costs of launching an attack on these bases, to reduce operational dependence on them, and to improve their resilience across dimensions and domains. With such a heavy U.S. presence in Japan, the United States may look to pursue improvements in both passive and active defense measures. Some would be invisible to the general public, such as the hardening of critical infrastructure like munitions and fuel depots or airplane shelters. Others would be more visible, such as dispersing and distributing forces across greater areas. Spreading out U.S. capabilities as well as fuel, apron space, runways, and prepositioned munitions would require physical space, which would likely mean increasing the U.S. footprint in Japan. Similarly, if the United States seeks to improve active defense capabilities in Japan, perhaps by installing more American-operated Patriot systems or Terminal High Altitude Area Defense batteries, this too could necessitate a greater U.S. presence. Related to improved defensive measures is a second possible change: offensive long-range ground fires. Adm. Phil Davidson, head of U.S. Indo-Pacific Command, recently outlined his requests to Congress for the  Pacific Deterrence Initiative , which included calls for ground-based, long-range fires. These precision-strike fires, meant to support air and maritime maneuver at great distances, would help to hold Chinese assets at risk, including those at sea, in the air, and at considerable distance from the coast on the Chinese mainland. Ever since the United States withdrew from the Intermediate-range Nuclear Forces Treaty in 2019, there have been persistent rumors of the United States wanting to secure bases in the Pacific for missiles with ranges formally prohibited by that treaty, anywhere between 500 to 5,500 km. In  August 2019 , then-Secretary of Defense Mark Esper said he was in favor of placing such missiles in the region relatively soon. With the threat posed by China not going away, it would not be surprising if, as part of the posture review, the United States approaches Japan with requests to host these types of strike capabilities. This is particularly appealing not just because of Japan\u2019s geography, but because of  Japan\u2019s own incremental movements  toward achieving stand-off missile capabilities. If the United States and Japan are concerned about trying to offset the threat posed by China, including eliminating the missile gap they currently face with China, then fielding long-range ground fires in robust numbers would make sense as one possible option. In addition to strengthening base defenses and an offensive strike element, a third possible change could be the United States seeking to increase its \u201csealift\u201d capabilities \u2014 ships that can carry soldiers, weapons, or supplies. Berger\u2019s vision for the U.S. Marine Corps a decade hence,  Force Design 2030 ,  proposes new operational concepts to protect U.S interests from a widely distributed, mobile, operationally resilient network throughout the  First Island Chain . Maneuvering agile, mobile, compact forces, such as squads and platoons, throughout the littoral among islands calls for small, speedy, agile ships. Specifically needed are relatively small ships with enough square footage for vehicles mounting long-range weapons to support sea and air control, and troop capacity of around 40 people. In Japan, this could mean the need for finding homeports for more of these types of ships. Surface combatants, carrier strike groups, and established amphibious group forces may also need reinforcement. When we consider that  China\u2019s three maritime forces  \u2014 the People\u2019s Liberation Army Navy, the China Coast Guard, and the\u00a0People\u2019s Armed Forces Maritime Militia \u2014 outnumber the forces that the United States and Japan can bring to bear against them, the demand for naval forces in an emergency or conflict will likely far exceed the supply of what the U.S. Navy, U.S. Marine Corps, and Japan\u2019s Maritime Self-Defense Force could provide. The United States and Japan have not been growing their fleets at the same pace as China, meaning that they will likely have fewer available ships than China in the future. Fewer ships mean reduced ability to deliver force by sea. Should the United States want to try to reverse this, it may choose to move naval combatants currently based in Guam, Hawaii, and California to Japan. Back in 2015, the  Center for Strategic and Budgetary Assessments  found that basing an additional aircraft carrier at Yokosuka would meet the entire demand for carrier coverage in the Pacific without having to build more ships to fulfill the U.S. Navy\u2019s regional commitments. If the same logic holds true, it is possible that the United States may seek a second carrier strike group to be homeported in Japan. Or, understanding that a carrier strike group may be impractical due to space concerns, a second option could be another  amphibious ready group . One is already homeported at Sasebo. Deploying a second would enable one group to stay in port and one to be deployed, thereby permitting nearly continuous deployed amphibious capability without demanding any large space requirement. A fifth possible posture change could come in the form of a joint warfighting U.S. command element in Japan. Since the Korean War, the United States has maintained a combined headquarters in the Combined Forces Command that allows a U.S. commander in Seoul to maintain a posture prepared to fight at a moment\u2019s notice should North Korea resume hostilities, which the Army refers to as a \u201cfight tonight\u201d posture. The United States has nothing close to that in Japan, even though Japan is home to the largest number of U.S. regional forces. China\u2019s force projection capability puts Japan in a similar \u201cfight tonight\u201d situation where hostilities could break out at a moment\u2019s notice, which could demand similar structure and readiness to that in place on the Korean peninsula. Instead, the individual service components report directly back to their service boss in Hawaii and, in a contingency, the U.S. Indo-Pacific Commander would exercise operational command. Maintaining the type of command and control necessary in an environment where U.S. equipment and personnel have difficulty communicating with one another, as well as with the commanders overseeing an operation, would challenge any type of U.S. operation run out of Hawaii. And in peacetime, as long as command and control resides at the Indo-Pacific headquarters in Hawaii it will be difficult to foster the type of daily interaction and training that is needed in Japan to build a coherent combined response. Given that China has moved to unify its forces, there is always the possibility that the United States may seek to counter this with a U.S. joint command element in Japan to drive joint and combined contingency plan development and planning for a combined fight. There are other possible options not reviewed here that the force posture review may also recommend. One is increasing the size of U.S. Forces Japan through an increase in permanent staff to perform the duties of an operational command element. Another is the periodic reinforcement of either the U.S. Seventh Fleet or III Marine Expeditionary Force with certain skills and expertise to create a joint command element from the nucleus of a service command element. Yet another would be increasing the U.S. Army contribution through the addition of a Multi-Domain Task Force including air and missile defense forces. None of these changes would be easy or cost-free for the United States or for Japan.\u00a0It is also possible that they could elicit local Japanese opposition. While there are many cases of local communities accepting an expanded U.S. or Japanese presence, such as Yamaguchi prefecture\u2019s support of the expansion of Marine Corps Air Station Iwakuni or Okinawa\u2019s acceptance of the expansion of the Air Self-Defense Force base in Naha, there are also examples of local opposition to new or relocated U.S. presence. The most well-known example is the ongoing effort to relocate the Marine Corps Air Station Futenma from its current location in southern Okinawa to a new location in northeast Okinawa. While objections to changing the footprint of the U.S. presence in Japan will differ depending on the community, one can imagine possible reasons ranging from noise concerns, environmental issues, and opposition to plans for greater pier space or expanded fencelines that infringe upon existing fishing grounds or agricultural lands. And if  Japan\u2019s cancellation  of Aegis Ashore last summer teaches us anything, it is that one can never rule out the possibility that the public may oppose the introduction of something new out of fear of safety issues it might pose to the local community. The possibility of local opposition suggests that the development and implementation of any force structure recommendations should be an all-of-government effort in both nations.\u00a0Through close and continuous consultation, the allies could better position themselves to show their publics and relevant government agencies the purpose of the proposed force posture changes to enhance their shared security. Close consultation may also lead the Japanese government to consider valuable force structure changes on their part, thereby further enhancing overall alliance effectiveness. Such an alliance effort is much more powerful than just the sum of its parts. While it is still unknown what results the posture review will bring, the recent  2+2 meeting  in Tokyo demonstrated that the alliance is stronger than ever given the common positions shown on China and the need to find ways to bolster the alliance. That strength, combined with an increased proactiveness by Japan, means that it is possible that Japan could see an uptick in U.S. military presence after this review is complete. One thing is clear. If the words of the late  Sen. Mike Mansfield  were true in the past, that the \u201cU.S.-Japan relationship was the cornerstone of stability in the Far East and in the world, bar none,\u201d the regional challenges that the alliance faces now may make that sentiment even truer today. \u00a0 \u00a0 Lt. Gen. (ret.) Wallace C. Gregson, Jr. is a former commander of III Marine Expeditionary Force in Japan, and former assistant secretary of defense for Asian and Pacific security affairs. Jeffrey \u00a0 W .  Hornung is a political scientist at the nonprofit, nonpartisan RAND Corporation.\u00a0 Image:  Sgt. Maj. Michael Cato"}, "210705_news_468211.txt": {"page_id": "210705_news_468211.txt", "text": "Proposed Hypothetical Arcology The  X-Seed 4000  was a  concept   megatall   skyscraper . [1]  The idea was initially created and developed by Martin Pascoe.  Its proposed 4-kilometre (2.5\u00a0mi) height, 6-kilometre-wide (3.7\u00a0mi) sea-base, and 800-floor capacity could accommodate 500,000 to 1,000,000 inhabitants. This structure would be composed of over 3,000,000 tons of   steel .  \n It was designed for  Tokyo ,  Japan  by the  Taisei Corporation  in 1995 as a futuristic environment combining ultra-modern and technological living and interaction with wildlife and nature. [2] [3]  Methods of transportation within the X-seed would most likely include  MagLev  trains.\n The X-Seed 4000 \"is never meant to be built,\" says Georges Binder, managing director of Buildings & Data, a firm which compiles data banks on buildings worldwide. \"The purpose of the plan was to earn some recognition for the firm, and it worked.\" [4] Unlike conventional skyscrapers, to remain habitable the X-Seed 4000 would be forced to actively protect its occupants from considerable internal air pressure and external air pressure gradations and weather fluctuations that its massive elevation would cause. Its design calls for the use of  solar power  to maintain internal environmental conditions. [1]  As the proposed site for the structure is located in the  Pacific Ring of Fire , the most active volcano range in the world, the X-Seed 4000 would be vulnerable to  earthquakes  and  tsunamis .\n A sea-based location and a  Mount Fuji  shape are some of this building's other major design features\u2014the real Mount Fuji is land-based and is 3,776 metres (12,388\u00a0ft) high, making it 224 metres (735\u00a0ft) shorter than the X-Seed 4000. \n The X-Seed 4000 is projected to be twice the height of the  Shimizu Mega-City Pyramid  at 2,004 metres (6,575\u00a0ft). \nThe Shimizu Mega-City Pyramid (proposed in 2007, also planned for  Tokyo ,  Japan ) faces most of the same problems as the X-Seed. Other projects that may be in the top five man made structures are the  Ultima Tower  3,218 metres (10,558\u00a0ft) in  San Francisco ,  Dubai City Tower  2,400 metres (7,900\u00a0ft) and the  Bionic Tower  1,228 metres (4,029\u00a0ft) in either  Hong Kong  or  Shanghai .\n See also [ edit ] References [ edit ] External links [ edit ]"}, "210705_news_468251.txt": {"page_id": "210705_news_468251.txt", "text": "The British cybersecurity firm Darktrace has announced plans for a \u00a33bn listing on the  London Stock Exchange , providing a shot in the arm for the City after Deliveroo\u2019s disastrous debut damaged the capital\u2019s reputation for big tech \u201cunicorn\u201d flotations. Poppy Gustafsson, the company\u2019s 38-year-old chief executive who holds a stake that will be worth a reported \u00a320m when it floats in about a month, said that London was the \u201cnatural choice\u201d despite  Deliveroo shares plunging  after the food delivery company\u2019s debut last month. \u201cThis is an exciting landmark for us, a great British tech company,\u201d Gustafsson said. \u201cOur intention to list on the London Stock Exchange marks a major milestone in Darktrace\u2019s history of rapid growth, and a historic day for the UK\u2019s thriving technology sector.\u201d Founded in 2013 by mathematicians from the University of Cambridge, artificial intelligence (AI) experts and cyber specialists from GCHQ, Darktrace creates digital security products that \u201cself-learn and self-heal\u201d to enable businesses to stay one step ahead of hackers and viruses. Since signing up the Drax power plant as its first client, Darktrace has secured 4,700 customers, from Rolls-Royce to the NHS, which it helped fight the WannaCry virus in 2016. The company\u2019s advisory board includes the former M15 director-general Lord Evans of Weardale, the former home secretary Amber Rudd, and Alan Wade, who spent more than three decades working for the CIA. Autonomy\u2019s co-founder Mike Lynch\u2019s Invoke Capital was Darktrace\u2019s first shareholder, and remains its largest, with a 39.5% stake set to be worth almost \u00a31.2bn at flotation. However, Lynch is  fighting extradition to the US , where he is accused of  fraudulently inflating the value of Autonomy  before its \u00a38.4bn sale to Hewlett-Packard in 2011. Lynch, who could face a maximum prison sentence of 25 years if found guilty, denies any wrongdoing. Sushovan Hussain, the former Autonomy finance chief who is also a Darktrace shareholder, was sentenced to five years in a US prison for fraud relating to the HP deal in 2019. Gustafsson, who also worked at Autonomy, said that Lynch\u2019s desire to remain in the UK played no part in Darktrace\u2019s decision to reject flotation on the New York Stock Exchange or Nasdaq, both of which are popular with tech businesses. \u201cLondon was a very logical place for us,\u201d said Gustafsson, who received an OBE for services to cybersecurity last year. \u201cThe management team is here within the UK, and also the UK has its significant heritage in both cybersecurity, with GCHQ, and the history at Bletchley Park, but also in terms of AI innovation, with the likes of Charles Babbage and Ada Lovelace, all the way to Alan Turing being the father of AI.\u201d Darktrace, which  began offering its services to the NHS  free of charge at the start of the pandemic, said that demand for some of its products had surged over the past year as companies struggled to keep up with security risks after the boom in homeworking. \u201cDarktrace\u2019s services are certainly in demand as companies around the world fight cybersecurity issues,\u201d said Russ Mould, the investment director at AJ Bell. \u201cHowever, there is still some controversy to accompany its stock market debut. Mike Lynch is battling extradition to the US and his former colleague Sushovan Hussain is serving time in prison for fraud. \u201cIt will be interesting to see if prospective investors view association with these two individuals as merely unfortunate or actually not a reason to not buy the shares.\u201d There were reports in February that the Swiss banking firm UBS had pulled out of plans to sponsor the float, owing to the company\u2019s links to Lynch, who stepped down from Darktrace\u2019s board in 2018. Darktrace, which is co-headquartered in Cambridge and San Francisco, employs about 1,500 staff globally. About 40% of staff are female, including at top executive level, an unusual situation in the male-dominated tech industry where the average is closer to 15%. Gustafsson has previously said that she gets a reminder of the gender diversity issue on panels at tech events, where she is used to a \u201csea of men staring back at me\u201d. Darktrace says it is tapping a market worth about $40bn (\u00a329bn), and its revenues have grown from $79m in 2018 to $199m in 2020. Adjusted earnings swung from a $27m loss to a $9m profit over the same period, helped by a sharp decline in travel costs because of restrictions imposed during the Covid pandemic. The company plans to float at least 20% of its shares, with an option to release a further 15% of shares to the market. The money raised will help speed up product development and strengthen the balance sheet, Darktrace said. Unlike Deliveroo, Darktrace has decided to avoid adopting a dual-class share structure, which gives founders and existing shareholders greater voting rights than new investors, and which some analysts believe contributed to the former\u2019s poor stock market debut. \u201cThere is a lot riding on Darktrace\u2019s forthcoming IPO after Deliveroo\u2019s stock market flop,\u201d Mould said. \u201cIf Darktrace manages to float its shares successfully and see them rise in value once trading begins then sentiment may improve towards London as a listing venue.\u201d"}, "210705_news_468292.txt": {"page_id": "210705_news_468292.txt", "text": "I n 2009, in the midst of the global financial crisis,  Paul Volcker , the former Federal Reserve chair, famously  observed  that the only socially productive financial innovation of the preceding 20 years was the automated teller machine. One wonders what Volcker would make of the tsunami of digitally enabled financial innovations today, from mobile payment platforms to internet banking and peer-to-peer lending. Volcker might be reassured: like the humble ATM, many of these innovations have tangible benefits in terms of lowering transactions costs. But as a critic of big financial firms, Volcker presumably also would worry about the entry of some very large technology companies into the sector. Their names are as familiar as their services are ubiquitous: Amazon in the US, the messaging company Kakao in Korea, the online auction and commerce platform Mercado Libre in Latin America, and the Chinese technology companies  Alibaba  and Tencent. These entities now do virtually everything related to finance. Amazon extends loans to small and medium-sized businesses. Kakao offers the full range of banking services. Alibaba\u2019s Ant Financial and Tencent\u2019s WeChat provide a cornucopia of financial products, having expanded so rapidly that they recently became targets of a Chinese government  crackdown . The challenges for regulators are obvious. Where a single company channels payments for the majority of a country\u2019s population, as does M-Pesa in Kenya, for example, its failure could crash the entire economy. Regulators must therefore pay close attention to operational risks. They must worry about the protection of customer data \u2013 not just financial data but also other personal data to which big tech companies are privy. Moreover, the big tech firms, because of their ability to harvest and analyse data on consumer preferences, have an enhanced ability to target their customers\u2019 behavioural biases. If those biases cause some borrowers to take on excessive risk, big tech will have little reason to care if it is merely providing technology and expertise to a partner bank. This moral hazard is why Chinese regulators now require the country\u2019s big techs to  use their own balance sheets  to fund 30% of any loan extended via co-lending partnerships. Governments also have laws and regulations to prevent providers of financial products from discriminating on the basis of race, gender, ethnicity and religion. The challenge here is distinguishing between price discrimination based on group characteristics and price discrimination based on risk. Traditionally, regulators require credit providers to list the variables that form the basis for lending decisions so that the regulators can determine whether the variables include prohibited group characteristics. And they require lenders to specify the weights attached to the variables so that they can establish whether lending decisions are uncorrelated with ethnic or racial characteristics once conditioned on those other measures. But as big tech companies\u2019 artificial intelligence-based algorithms replace loan officers, the variables and weights will be changing continuously with the arrival of new data points. It\u2019s not obvious that regulators can keep up. In algorithmic processes, moreover, the  source of bias can vary . The data used to train the algorithm may be biased. Alternatively, the training itself may be biased, with the AI algorithm \u201clearning\u201d to use the data in biased ways. Given the black-box nature of algorithmic processes, the location of the problem is  rarely clear . Finally, there are risks to competition. Banks and fintechs rely on cloud computing services operated by the big tech firms, rendering them dependent on their most formidable competitors. Big techs can also cross-subsidise their financial businesses, which are only a small part of what they do. By providing a range of interlocking services, they can prevent their customers from switching providers. Regulators have responded with open banking rules requiring financial firms to share their customer data with third parties when customers consent. They have authorised the use of application programming interfaces that allow third-party providers to plug directly into financial websites to obtain customer data. It is not clear that this is enough. Big techs can use their platforms to generate large amounts of customer data, employ it in training their AI algorithms, and identify high-quality loans more efficiently than competitors lacking the same information. Customers may be able to move their financial data to another bank or fintech, but what about their non-financial data? What about the algorithm that has been trained up using one\u2019s data and that of other customers? Without this, digital banks and fintechs won\u2019t be able to price and target their services as efficiently as the big techs. Problems of consumer lock-in and market dominance won\u2019t be overcome. In an old parable about banks and regulators, the banks are greyhounds \u2013 they run very fast. The regulators are bloodhounds, slow afoot but faithfully on the trail. In the age of the platform economy, the bloodhounds are going to have to pick up the pace. Given that only three central banks report having  dedicated fintech departments , there is reason to worry that they will lose the scent. Barry Eichengreen is a professor of economics at the University of California, Berkeley, and a former senior policy adviser at the IMF. \u00a9 Project Syndicate"}, "210705_news_468296.txt": {"page_id": "210705_news_468296.txt", "text": "Warning: this article contains images of pixelated male genitalia. On December 9, 1993, members of the United States Senate\u2019s Subcommittee on Regulation and Government Information and its Subcommittee on Juvenile Justice held a joint hearing on the topic of violence and sex in videogames. Educators, social scientists, activists, and several prominent figures from the videogame industry itself spoke there for almost three hours. More heat than light was on display for much of that time: the middle-aged politicians often displayed a comprehensive ignorance of the subject at hand, the supposed experts often treated nuanced issues with stubborn stridency, and the industry figures often proved more interested in attacking each other than mounting a coordinated defense against the charge of being the corruptors of America\u2019s youth. But history sometimes moves in surprising ways. The hearing prompted far-reaching changes in gaming out of all proportion to its worthiness as a good-faith debate about a significant social concern. The first and to-date only industry-wide standard for rating the content in videogames \u2014 the same system that is still in use today \u2014 was one outcome. And another, much stranger result was the splashy trade show that has since come to dominate the industry\u2019s public-relations calendar. One might say that December 9, 1993, was the day that the games industry began to wake up to a sense of itself as a distinct mass-media entity in its own right. This is the story of how those things came to be.   \nVideogames have been causing intermittent moral panics for almost as long as they\u2019ve existed. The first of them to ignite public ire dates all the way back to 1976 and a small company called Exidy. The year before, Exidy had made a standup-arcade game called  Destruction Derby , about the time-honored American motorsports pastime of the  demolition derby , a staple of county fairs and other rural gatherings. When Chicago Coin, the company who had agreed to distribute the game to arcades, failed to pay them their royalties, Exidy revamped it into something called  Death Race  and released it on their own. Instead of other cars, you were now expected to collide with stick figures, called \u201cgremlins\u201d or \u201cmonsters\u201d in Exidy\u2019s official terminology, in order to score points. When you hit one, it was replaced with a little gravestone. As it happened, though, a recent B-movie called  Death Race 2000  was generating enraged headlines at the very same time. Starring a pre- Rocky  Sylvester Stallone, it dealt with a cross-country road race of the dystopian future where the drivers were rewarded with bonus points for mowing down pedestrians en route. It\u2019s very difficult to say what the connection between the film and the game actually was. The programmer who created the latter has insisted to this day that he was unaware of the movie at the time he did so. Still, the shared title remains quite a coincidence. Perhaps a marketer at Exidy belatedly elected to capitalize on the film\u2019s notoriety by giving the already finished game the same name? Death Race , with its onscreen tombstones to mark dead pedestrians. At any rate, the shared title certainly wasn\u2019t lost on the media at the time. Several television-news programs, including the highly respected nationwide  60 Minutes , ran segments about the game after receiving a flood of complaints from parents and other concerned adults, and many or most arcade owners removed it from their floor. Nolan Bushnell, the founder and chief executive of industry leader Atari, was very displeased with the negative attention  Death Race  brought to a burgeoning new form of entertainment: \u201cWe had an internal rule that we wouldn\u2019t allow violence against people. You could blow up a tank or you could blow up a flying saucer, but you couldn\u2019t blow up people. We felt that was not good form.\u201d But Pete Kaufman, the founder of Exidy, was unrepentant. Those arcade owners who weren\u2019t scared away by the controversy, he noted, did a booming business with  Death Race . The young industry was already learning an important lesson: that extreme violence in a videogame is dangerous because of the unwanted attention it can attract, but that it also has the potential to be very, very profitable. The industry\u2019s future would be marked by a delicate dance between these two realities, as it attempted to be outrageous enough to attract customers with a taste for violence without going so far as to bring the heavy hand of government down upon its head.   \nAtari and their American and Japanese competitors went from strength to strength in the years after  Death Race . First arcades became centerpieces of adolescent social life, and then, thanks to the Atari VCS home console, videogames took over American living rooms as well. The elder generation reacted to these things in much the same way that their parents had to such youth phenomena as Elvis and the Beatles: with a shrug of complete incomprehension, followed in many cases by concerns about the influence of this strange new pop-culture development on their children\u2019s mental and even moral well-being. The city council of the Dallas, Texas, suburb of Mesquite went so far as to ban children from visiting arcades without an adult escort. A legal challenge raised by the American Civil Liberties Union in response made it all the way to the Supreme Court, which struck the law down as unconstitutional in 1982. Undaunted, Dr. C. Everett Koop, President Ronald Reagan\u2019s unusually prominent surgeon general, became a vocal critic of videogames and an advocate for laws to limit their pernicious influence, claiming that they were consciously engineered to addict children, \u201cbody and soul.\u201d It\u2019s an odd truism of American culture that, while violence in media may upset various people at various times, nothing brings out the censors in the body politic like a little sex. In October of 1982, a company called Mystique, with ties to the pornography industry, proved this once again with an Atari VCS game called  Custer\u2019s Revenge , which combined violence and sex, then added a concluding flourish of racism. In it, you played a reincarnation of the benighted general. His most prominent onscreen feature was his outlandishly long penis, which he used to rape the Native American women he found scattered about the battlefield, already helpfully tied to stakes. Custer\u2019s Revenge . Be careful of the cacti when you\u2019re waving that thing around\u2026 Controversy had clearly been the whole point of the game, and it was rewarded with its full measure, managing to unite the American Indian Community House, the National Organization for Women, and Women Against Pornography for a shared protest outside the New York City venue where it was shown to the press for the first time. Robin Quinn of the last-named organization proclaimed, accurately enough, that the game \u201csays that rape is not only a legitimate form of revenge but a legitimate form of entertainment.\u201d George Armstrong Custer III came out of the woodwork to complain that his great-granduncle\u2019s reputation was being \u201cmaligned,\u201d while Atari filed a dubious lawsuit claiming that the very existence of the game on their console created a \u201cwrongful association\u201d in the minds of the public. Arnie Katz, the founding editor of  Electronic Games  magazine, remembers telling the leadership of the protest movement that \u201cthe best way to keep the game from selling is to ignore it.\u201d In the absence of a willingness to heed that perhaps wise advice,  Custer\u2019s Revenge  wound up selling about 80,000 copies, at $50 a pop. Two other, similarly tasteless \u201cadult\u201d games from Mystique attracted less attention from groups who largely spent their outrage on  Custer\u2019s Revenge , and, just as Katz had predicted, proved much less commercially successful. Still, the arrival of games of this ilk would surely have led to more controversy and eventually to serious calls for legislation, if only what struck many as the passing fad for videogames hadn\u2019t ended abruptly the following year, in the series of events that have gone down in history as the Great Videogame Crash. By the beginning of 1984, the arcade market was greatly diminished, the home-console market effectively destroyed. For the next few years, for the first and only time in the history of digital gaming, computers rather than consoles became the most popular way to play games in the home; the  Commodore 64  home computer became the new heart of the gaming mass market. But even that machine, ultra-popular though it was as a computer model, wasn\u2019t a patch on what the Atari VCS had been. Likewise, the market for floppy-disk-based entertainment software was a small fraction of the size of the former market for console cartridges \u2014 so small that it existed out of the sights and minds of the sort of public agencies that had raised concerns about the videogames of the earlier era. Thus software publishers felt little or no compunction about including whatever content struck their fancy and seemed most likely to appeal to their primarily young and male audience.  Strip-poker games , many featuring digitized photographs of real models, were a dime a dozen; casual profanity was everywhere; the CRPG  Wasteland  gave you the option of visiting a house of ill repute (and catching \u201c Wasteland  herpes\u201d as a reward for your effort). Sometimes the lack of condemnation from the fuddy-duddy set could be downright frustrating. When Steve Meretzky of the text-adventure maker Infocom failed to generate any controversy with  A Mind Forever Voyaging , his brutal take-down of the Reagan administration\u2019s conservative politics, he decided to make a sex comedy called  Leather Goddesses of Phobos . He confidently expected that, as he wrote in the game\u2019s self-congratulatory opening text, people would soon be \u201cindignantly huffing toward their dealer, their lawyer, or their favorite repression-oriented politico.\u201d The actual result? Crickets \u2014 and a bunch of other adventure games, such as Sierra\u2019s  Leisure Suit Larry  series, that were even naughtier, and included graphics to boot. Sex Vixens from Space , one of many risque games that were eagerly played by adolescent boys during the games industry\u2019s equivalent of pre-Hays Code Hollywood. The return of concerns about videogame content to the public consciousness unsurprisingly coincided with the return of console systems, and the vastly greater number of players they\u2019ve always tended to attract, to the center of the mass market. The  Nintendo Entertainment System  was first imported to North America from Japan in a rather quiet and cautious fashion in late 1985. But by 1987, it was gaining steam quickly, and by decade\u2019s end its market penetration exceeded even that of the Atari VCS in its heyday. The fact was, the executives at Nintendo, both those in Japan and in the United States, had made a careful study of what Atari had gotten right and wrong back in the day, and developed a plan for how they could do things in a better, more sustainable way. Nintendo exercised complete control over the NES and everything associated with it. They created an ironclad legal framework which allowed them to decide who was allowed to make NES games, what sort of games these were allowed to be down to the very last detail, and even how many cartridges their software \u201cpartners\u201d were allowed to manufacture and sell. Then, as the icing on the cake, Nintendo took a cut of every NES game anyone sold. Not only did this approach make the company extraordinarily profitable, but it ensured that they wouldn\u2019t have to contend with any examples of a  Custer\u2019s Revenge  and the ensuing public-relations nightmare. Nintendo hewed to a firm \u201cfamily-friendly\u201d policy. Anecdotes about their censorship regime abound, from the swimsuit calendar which they forced LucasFilm Games to pull down from a wall inside  Maniac Mansion  to the gravestone crosses which Capcom had to remove from  DuckTales  \u2014 for, in Nintendo\u2019s zeal not to offend, religious symbols of even the most understated stripe were strictly prohibited. Nevertheless, plenty of Americans found plenty of room in their hearts to be offended by Nintendo\u2019s success. In many cases, their concerns about the heavy-handed tactics which the company used to control both the medium and the message of the NES were perfectly reasonable. Still, a distinct whiff of xenophobia and/or outright racism clung to many of the criticisms, manifested in dark mutterings about the latest Pearl Harbor, couched in stereotypes about the shifty Oriental character. When Nintendo introduced the  Game Boy  handheld console in 1989 and saw it blow up as big as the NES, the mutterings threatened to become a chorus. Believing that the winds of public opinion were at their back, Atari Games and Atari Corporation, the two halves into which the old king of American videogames  had been split  back in 1984, launched a series of legal challenges that attempted to tear down the barriers around Nintendo\u2019s walled garden. These would drag on for years, but would never provide the decisive victory the deposed kings of gaming were looking for; they soon learned that Nintendo could afford good lawyers too. Ditto a probe by the Justice Department and the Federal Trade Commission; the smoking gun these would-be trust busters were looking for either didn\u2019t exist or was  very  well-hidden. But there was also another reason that the government investigation fizzled out anticlimactically in 1992, two years after its beginning: Nintendo had some genuine competition in the console space by that point, making it hard for the agencies to stick them with the monopoly tag. The Sega Genesis console, another product of a Japanese company, had first reached American shores in August of 1989. It thoroughly outclassed the NES in technical terms, with a 16-bit rather than an 8-bit processor and far better graphics and sound. Justifiably alarmed, Nintendo did everything they could to snuff out Sega\u2019s North American operation, pressuring everyone from game publishers to retail stores to shun the alternative platform or face the consequences. Their efforts kept Sega on the ropes for quite some time, but Nintendo never could completely finish the job. A turning point came when  Electronic Arts , one of the largest American game publishers,  chose to make  Sega rather than Nintendo their platform of first choice. By 1992, following years of dogged effort, Sega had brought their brand to a place of near commercial parity with Nintendo, despite the appearance in 1991 of a new Super NES which made up for most of the NES\u2019s failings in comparison to the Genesis and then some. Sega owed their success at least partially to their willingness to embrace edgier and often more violent content, pitched to a slightly older adolescent demographic than the stereotypical Nintendo fanatic. The differences in corporate personality were vividly illustrated by the two companies\u2019 de-facto mascots. Nintendo\u2019s Mario was cute and sweet and harmless; Sega\u2019s Sonic the Hedgehog was manic and a little unhinged \u2014 a little bit more  dangerous  than the cuddly Italian plumber. Sega didn\u2019t hesitate to call out their target by name: \u201cSega Genesis does what Nintendon\u2019t,\u201d ran one of their most-used slogans. But it could just as easily have read, \u201cSega Genesis does what Nintendo  won\u2019t ,\u201d in terms of content. The two companies\u2019 North American management absolutely loathed one another. Soon they would parade their antipathies before no less august a body than the United States Senate. Although that landmark hearing would purport to examine questionable videogame content in general, its story is inextricably bound up with that of two games in particular, as different from one another as they could be in their genres, format, and to some extent even the audiences they attempted to reach. One was notable for its extreme level of violence, while the other was notable for its combination of sex and violence \u2014 or rather was  made  notable by politicians and others who convinced themselves that it contained far more of both than was actually the case. We\u2019ll take the two suspects one at a time.   \nArcades were still blundering along at this late date, sustained by the impressive audiovisuals that were made possible by their specialized hardware, which not even the likes of the SNES could match. By far the biggest arcade hit of 1992 was a game called  Mortal Kombat , the latest in what was already a long line of so-called \u201cfighting games.\u201d (\u201cAren\u2019t most videogames fighting games?\u201d says the na\u00efve observer\u2026) The premise was simplicity itself: you and an opponent \u2014 in the form of either the computer or, for maximum entertainment, your human buddy \u2014 controlled avatars who stood face to face on the screen and beat the ever-loving crap out of one another.  Mortal Kombat  won special favor in a crowded field for the variety of fighters you could choose to control, each with his or her own strengths and weaknesses; for its many moves, counter-moves, and power-move combinations; for its rambunctiously over-the-top depiction of the action, including copious amounts of blood; and for the so-called \u201cfatalities\u201d that finished a match, where a fighter\u2019s heart might get pulled right out of his chest or his head ripped off his shoulders. Jeff Greeson, a student of the game and its lore, notes that \u201c Mortal Kombat  not only shocked anyone who had ever played the game, but those who simply walked by the game were mesmerized by its gore.\u201d No arcade game had ever been as extreme as this. How could it not become a hit? A  Mortal Kombat  \u201cfatality.\u201d The life cycle of a hit arcade game in those days was much like that of a hit movie: it would remain an arcade exclusive for nine to twelve months in order to maximize that revenue stream, then come home in a version for consoles and/or computers. Midway Games, the maker of the original  Mortal Kombat  arcade cabinet, placed its home ports in the hands of the software publisher Acclaim Entertainment, who had contracts with both Nintendo and Sega. True to form, Sega encouraged Acclaim to put in as much of the arcade edition\u2019s lurid violence as would fit within the more limited audiovisual capabilities of the Genesis. But Nintendo was different: while they certainly wanted the game on the SNES, they insisted that Acclaim tone it down \u2014 for example, by replacing flying blood with flying sweat, and by removing the gory fatalities entirely. Howard Lincoln, a Nintendo of America executive who is widely and justly regarded as one of the two principal architects of the brand\u2019s success, remembers an extended back-and-forth with Acclaim over the issue: \u201cLook, we\u2019re going to make the Sega version, and it\u2019s going to be right in line with the coin-op game. Having a toned-down version for Nintendo\u2026 Do you guys really want us to do that? Does that really make sense?\u201d But Nintendo held firm to the family-friendly standards that had gotten them this far. Versions of  Mortal Kombat  for the SNES, the Game Boy, the Genesis, and the Game Gear \u2014 the last being Sega\u2019s handheld competitor to the Game Boy \u2014 shipped simultaneously on September 13, 1993, on the back of a marketing budget that was higher than the combined cost of creating them. Just as Acclaim had intended, \u201cMortal Monday\u201d became a major event in the lives of countless young fans, who greeted the game the way their parents might have a new Led Zeppelin album. The merchandising manager of Electronics Boutique, one of the country\u2019s biggest videogame retailers, called it \u201cthe largest new release we\u2019ve ever had.\u201d Later that week, the  New York Times  could already report that the Sega versions were handily outselling the Nintendo versions. Whether you were into videogames or not,  Mortal Kombat  was an inescapable mass-media presence during the autumn of 1993. Over the next two months, 1 million SNES  Mortal Kombat  cartridges were sold. This was an impressive showing \u2013 except that 2 million Genesis cartridges were sold over the same period. It was a triumphant moment for Sega, who had struggled so long and hard to reach this point, even as it struck Nintendo\u2019s management as the most palpable sign yet that they were in danger of being dismissed as a kiddie company by the teenagers who were now flocking to Sega, bringing along with them their greater reserves of precious disposable income. For the first time, a serious internal debate began at Nintendo over the commercial sustainability of their family-friendly approach. Despite or because of its outrageous violence,  Mortal Kombat  was and is a good game in the estimation of most connoisseurs of its genre. Even if it had never prompted a public controversy, it would probably still be fondly remembered by them today; it proved the starting point of a franchise that has encompassed thirteen more games to date. But the other game destined to take center stage before the United States Senate was not so good, and would almost certainly be completely forgotten today if not for its strange moment of infamy in the halls of government.   \nIf nothing else, the game in question does have a fascinating origin story. It begins with Tom Zito, a journalist and music critic for the  Washington Post ,\u00a0 Rolling Stone , and the  New Yorker , who in 1984 was assigned by the last of these to profile Nolan Bushnell of Atari fame. He parlayed that meeting into a job with the Sunnyvale, California-based Axlon, one of the legendary technologist\u2019s several companies, marketing baby monitors and talking Teddy bears which were distributed by the toy giant Hasbro. But Bushnell always encouraged his proteges to think expansively rather than narrowly. Thus early in his tenure with Axlon, Zito allowed himself to become intrigued by the  new video technology of the laser disc , and by the possibility of  overlaying conventional computer graphics  onto its pre-recorded random-access imagery. In 1986, he stumbled upon the NES and the burgeoning excitement around it during a routine visit to a department store. Deciding that a laser-disc-powered videogame console was just the ticket, he hired a small team to cobble together a Rube Goldberg contraption they called the Nemo. When the limitations of laser discs began to bite \u2014 they could fit only 30 minutes of video onto a side, and the hardware was expensive to boot \u2014 they tried to make the concept work with the even blunter instrument of a videotape player under the control of an attached computer. \u201cWhat I truly believed was that interactive television could be something akin to today\u2019s casual gaming,\u201d says Zito. \u201cI really believed it could be something very, very big.\u201d But Bushnell, alas, displayed more and more skepticism as the technical challenges to the concept became more and more clear. So, Zito secured support directly from Hasbro to develop the gadget further, and he and his team of programmers and engineers split from Bushnell to work on it independently. They decided that the best way to proceed was to create a full-length, playable game to demonstrate the potential of the Nemo. But what kind of game could they hope to make, given all the limitation of their prototype hardware? As it happened, a game destined to go down in history as one of the schlockiest of all time was inspired by a much more high-brow piece of artistry. An experimental theatrical play called  Tamara  was enjoying an extended run at the time in a grand old American Legion mansion in Hollywood. Instead of sitting in one place and watching the show unfold on one stage, the audience could move around the mansion\u2019s three floors on the trail of equally mobile actors; each spectator was encouraged to decide for herself which of the play\u2019s many characters and sub-plots were most interesting and to see them through for herself, as it were. Two of Zito\u2019s associates, by the names of Rob Fulop and Jim Riley, went to see the play in question one evening. Then they saw it again, and then again. This was not atypical in itself: with so much happening simultaneously, the only way to piece together anything like the complete picture was to attend multiple performances. Yet the precise nature of Fulop and Riley\u2019s curiosity  was  unusual: rather than trying to piece together the full plot, they were trying to understand how the play really  worked , and how its approach might be adapted to interactive video. When they thought they had an understanding of those things, they produced a design document for something called  Night Trap . Night Trap  was a bizarre creation by any standard, being the (interactive) story of a group of vampires in training who attack a mansion full of college girls having a weekend sleepover party. Not yet having won their fangs, the vampires have to suck the girls\u2019 blood with a weird contraption of plastic tubing. These are unusually diffident \u2014 not to say nerdy \u2014 vampires: instead of overpowering the girls bodily, they\u2019ve installed a network of surveillance cameras in the house, along with traps which they can activate remotely to capture the girls for blood extraction. The player\u2019s role is that of a good Samaritan who has hacked into the surveillance system, with the goal of turning the tables on the vampires and catching them in their own booby traps. While by no means completely bereft of a certain creepy voyeuristic vibe \u2014 how could it be when it combined college girls in their pajamas, vampires, and a secret surveillance system? \u2014 the final script was far from sexually explicit, and likewise more silly than violent. The developers did, after all, envision the game someday being sold by Hasbro, a maker of children\u2019s toys. Indeed, they allowed that company\u2019s management to review the script and remove or change anything they found objectionable. Fulop, Riley, and Zito spent sixteen days in 1987 shooting the footage for the game with a Hollywood crew that included the future cinematographer of  Forrest Gump  and the former producer of  The Man from U.N.C.L.E.  The shoot wound up costing $1 million, several times the budget of even the most elaborate conventional videogames of the time. For all the richly deserved schlocky reputation which it would later earn,  Night Trap  was a genuinely pioneering effort in its way. The combination of real-world footage featuring real actors with conventional graphics would become one of the dominant trends of computer gaming during the early- and mid-1990s. Many of the dubious hallmarks of this so-called \u201cfull-motion-video\u201d era appeared for the first time in  Night Trap . There was, for example, the way that it tried to make up for the cheesiness that was an inevitable result of its ultra-low cinematic budget by affecting a knowing, ironic attitude \u2014 i.e., it\u2019s  supposed  to be terrible! That\u2019s the joke? Get it? Well then, what are you complaining about? This sort of thing  can work occasionally , but  most of the time  it just comes across as the cheaply disingenuous ploy it really is. And then there was the use of actors who were vaguely recognizable, but not \u2014 or no longer \u2014 truly sought-after. \u201cInteractive \u2018moviegames\u2019 were populated by performers either on their way up or on their way down the Hollywood ladder,\u201d says Rob Fulop. \u201cNobody aspired to appear in a moviegame.\u201d  Night Trap \u2018s big catch was Dana Plato, a young actress who had had a prominent role in the hit sitcom  Diff\u2019rent Strokes  from 1978 until 1986, but whose struggles with alcohol and drugs, and the erratic behavior they brought on, had now all but derailed her career. \u201cShe\u2019d come in late and never wanted to rehearse,\u201d remembers Fulop. \u201cHer doing this project was obviously a step down from her previous popularity, and she didn\u2019t make a great deal of effort to hide this fact.\u201d This sort of thing too would become all too typical of later interactive movies. When the shoot was complete, the developers returned to Sunnyvale to try to figure out how to turn their pile of videotapes into a playable game on the Nemo. In the best spirit of  Tamara , you were supposed to be able to switch between the video feeds from eight different cameras set up around the mansion; you would need to be in just the right place at just the right time to trigger a trap and catch each of the vampires. But making this random-access concept work using the fundamentally sequential medium of videotape was, needless to say, a tall order. Amazingly, Hasbro allowed Zito and company to shoot the footage for a second interactive movie while they were still struggling to implement their first one. Zito conceived  Sewer Shark  as a visual-effects extravaganza, and therefore gave the director\u2019s chair to John Dykstra, the effects supervisor for such films as  Silent Running ,  Star Wars , and  Star Trek: The Motion Picture . He spent most of his time setting up shots of the tunnels down which the player would fly a spacecraft; think of an interactive-movie version of the later 3D action game  Descent , if your imagination can encompass such a thing. Any way you look at it,  Sewer Shark  is a well-nigh ludicrous technological stew. Just as Hollywood was beginning to embrace computer-generated imagery in place of many physically-constructed special effects,  Sewer Shark  flipped that formulation on its head; it was filmed using old-fashioned physical scale models, which were then digitized and displayed on a computer. Shot in exotic Hawaii for reasons no one can seem to explain, the  Sewer Shark  footage wound up costing $2 million. When not supervising film shoots, Zito was spending a lot of time hobnobbing with the Hollywood set, trying to interest them in a concept that still had no practical delivery device. He talked to Jane Fonda about an interactive workout video; talked to Jerry Bruckheimer about an interactive  Top Gun ; talked to Paramount about an interactive  Star Trek ; talked to the rock band Yes about an interactive music video; talked to George Miller about an interactive  Mad Max ; talked to ESPN about interactive sports broadcasting. He even flew to London for a meeting with Stanley Kubrick. None of it went anywhere. It isn\u2019t clear how much progress his technical team made on the task of turning  Night Trap  and  Sewer Shark  into playable games on the Nemo while he was away. We can say for sure, however, that their progress wasn\u2019t fast enough for Hasbro\u2019s taste. The latter came to suspect, by no means entirely unreasonably, that Zito was more interested in enjoying his Hollywood jet-setter lifestyle than buckling down and delivering the finished product he had promised them. They finally pulled the plug on the Nemo in 1989 \u2014 ironically, just as the evolution of computer technology, especially the onset of  CD-ROM , was beginning to make what Zito had first proposed to do some three years before seem at least potentially practical. But Zito, for his part, was well aware that the science-fictional was slowly moving into the realm of the possible. He convinced Hasbro to sell him the rights and all of the footage earmarked for  Night Trap  and  Sewer Shark  for a song. Two years later, what had once seemed so pie-in-the-sky was now striking many people who weren\u2019t named Tom Zito as gaming\u2019s necessary future. That year, there appeared  Sherlock Holmes: Consulting Detective , the first published game to make extensive use of filmed live-action footage. It did very, very well. Suddenly afraid that his five-year-old brainstorm was about to take off without him, Zito founded a company called Digital Pictures. Its first objective would be to make a pair of interactive movies built around the live-action footage which he had carried away from the Nemo project.\u00a0 His rhetoric, once so bizarre, was now right in line with the emerging conventional wisdom: \u201cUltimately, I believe the [videogame] business will be more like traditional Hollywood stuff than what\u2019s coming out of Silicon Valley today: some dinky animated guys running around the screen. We\u2019ll be doing interactive game shows, talk shows, dramas, sitcoms.\u201d \u201cWhy watch a movie where you can\u2019t have any effect over it?\u201d asked the Digital Pictures artist Josh Solomon. \u201cWhy not be able to put your own stamp on it?\u201d There was one important difference to separate Digital Pictures from most of the others jumping on the full-motion-video bandwagon. These others tended to focus on the high-end personal-computer marketplace, where CD-ROM drives were slowly but steadily winning acceptance, and where the hardware in general dramatically outclassed that of the consoles. But Zito was a mass-media populist by instinct; he wanted to bring his interactive movies to the living rooms of everyone, not just to the dens, offices, and bedrooms of a privileged few. Both Nintendo and Sega were also aware of CD-ROM, and both were contemplating whether and how they could use the technology. But the former, after first partnering with Sony to make a CD-ROM add-on for the SNES, abruptly pulled out of the deal; an optical drive wouldn\u2019t finally make it to a Nintendo console until the release of the GameCube in 2001. Nintendo\u2019s abandonment of the field left only Sega, who planned to make a CD add-on of their own for the Genesis. So, Zito signed on with them. Re-purposing the aged footage wasn\u2019t easy. First it had to be digitized, then downgraded dramatically to fit a venerable console that in all truth was thoroughly unsuited to the task it had been assigned: it could display just 61 colors at a time from a palette of just 512. Compared to the full-motion-video productions on personal computers \u2014 not exactly marvels of high-fidelity in themselves \u2014  Sewer Shark  on the Genesis was a bad joke. Digital Pictures programmer Ken Melville: All our video had to be tortured, kicking and screaming, into the most horrifying, blurry, reduced-color-palette mess imaginable. I shudder to think about it. The audio, the video, the accessing of data on the sloooow-crawling 10 K per second bandwidth CD was all torturous and disastrous. The limitations presented were enormous. The actual gameplay that was shoehorned in on top of the video was as simplistic as could be, consisting of little more than cross-hair and some grainy targets to shoot at. Sewer Shark . Sega\u2019s CD add-on shipped on September 15, 1992; the two-and-a-half minute television advertisement that was rolled out to mark the occasion had cost more to make than three or four typical videogames. The gadget had sold 1.5 million units by the time anyone managed to complete the first tally. As one of the first games to be made available for Sega CD,  Sewer Shark  did very well. In 1993, it was bundled with the add-on for a period of time, thereby making a lot more money for Digital Pictures. Night Trap  appeared soon after  Sewer Shark . It was more formally ambitious than the simple rail shooter that was  Sewer Shark  \u2014 the original,\u00a0 Tamara -inspired gameplay concept had traveled the long and winding road to the Genesis intact \u2014 but it was no more attractive to look at and no more fun to play, being in the end an exercise in trial and error and rote timing. Predictably enough, the magazine reviews fixated on the novelty of its use of video and the nubile girls it featured so prominently, and especially on Dana Plato\u2019s starring role. Over the five years since the footage had been shot, she had become one of Hollywood\u2019s most infamous burnouts, having recently been arrested twice: once for robbing a liquor store (\u201cI\u2019ve just been robbed by the girl who played Kimberly on  Diff\u2019rent Strokes ,\u201d said the clerk when he phoned the police), then again for forging a drug prescription. But even her involvement constituted a paltry \u2014 not to mention rather mean-spirited \u2014 ground for playing a game, as some of the more perceptive or less beholden reviewers reluctantly acknowledged. Night Trap . Dana Plato stands to the viewer\u2019s left. She died of a drug overdose in 1999 at age 34, after an intensely troubled life. Night Trap  didn\u2019t sell in particularly big numbers in comparison to its predecessor. Had it never come to a certain senator\u2019s attention, it would doubtless have become no more than a minor footnote to gaming history, like the rest of Digital Pictures\u2019s underwhelming output. As it was, though, it got to join  Mortal Kombat  as the public face of videogame depravity.   \nAccording to his own account, Joseph A. Lieberman, a United States Senator for the Democratic Party from the state of Connecticut, first heard about  Mortal Kombat  when his chief of staff Bill Andresen told the senator in casual conversation how his nine-year-old son had asked for a copy, and how he had refused because he had read in the newspaper that the game was \u201cincredibly violent.\u201d His curiosity kindled, Lieberman suggested that the two of them have a look at the game themselves. Lieberman: I was startled. It was very violent, and rewarded violence. At the end, if you really did well, you\u2019d get to decide how to kill the other guy, how to pull his head off. And there was all sorts of blood flying around. Then we started to look into it, and I forget how I heard about Night Trap. I looked at that game too, and there was a classic. It ends with this attack scene on this woman in lingerie, in her bathroom. I know that the creator of the game said it was all meant to be a satire of Dracula, but nonetheless, I thought it sent out the wrong message. Of course, the player\u2019s objective in  Night Trap  was to protect the girls rather than attack them, and the nerdy trainee vampires were unusually  non- violent by the traditional standards of their kind. Yet Lieberman would continue to spout misleading statements like these for months to come \u2014 before, during, and after the Senate hearing on videogame content which he instituted and oversaw. The scene from  Night Trap  that got Joe Lieberman\u2019s dander up. In light of his manifest ignorance, many have questioned the senator\u2019s own professed origin story of his investigation; did he and his chief of staff really have the wherewithal to go out and buy  Mortal Kombat , buy or otherwise procure a Sega Genesis to play it on, and then get far enough into it to see its trademark fatalities? Tom Zito, for his part, claims that the investigation began in a very different way: that Nintendo, or one of their Washington lobbyists, arranged to show the good senator what sorts of filth their rival Sega was peddling. And indeed, the bad blood between the two companies was so pronounced that this conspiracy theory sounds more plausible than it perhaps ought to. We can say for sure only that, if Nintendo did touch off the affair in an attempt to stick it to their arch-rival, it would soon snowball hopelessly out of their control as well. Naturally, we cannot hope to know what was really in Senator Lieberman\u2019s mind in the midst of all this \u2014 whether he simply saw it as an easy way to win favor with his constituents (videogame players were not a large voting bloc in comparison to nervous parents and grandparents), or whether he really, truly felt the deep-seated concern he expressed on numerous occasions. In Lieberman\u2019s defense, however, it should be noted that violent crime in the real world and its causes constituted a big part of Washington\u2019s agenda that year and the next, in the midst of a spate of well-publicized incidents. For example, on October 1, 1993, a twelve-year-old girl named Polly Klaas was abducted from a slumber party in rural California at knife point, then murdered and buried in a shallow grave. Although the connection was never explicitly made during the Senate hearings, it isn\u2019t a huge leap to presume that the slumber-party aspect of  Night Trap  may have been what tipped the balance and singled it out for so much overheated condemnation. Whatever his motivation or combination thereof, Joseph Lieberman, chairman of the Senate Governmental Affairs Committee\u2019s Subcommittee on Regulation and Government Information, reached out to his friend Herbert Kohl, chairman of the Judiciary Committee\u2019s Subcommittee on Juvenile Justice. The two announced a joint hearing on the subject of videogame content and its effects on the psychology of children and adolescents, advertising it as the first step toward an eventual law that would require videogame publishers to mark any of their products which contained violent and/or sexual content on their boxes. The videogame industry was about to get its day in a decidedly hostile court, with  Mortal Kombat  and  Night Trap  in the role of its two most flagrant offenders. The games made for quite the odd couple.  Mortal   Kombat  was, for all its envelope-pushing violence, traditionalist in spirit, engineered to appeal to the teenage boys who had always been the biggest market for videogames;  Night Trap , despite its manifestly clumsy execution, was an attempt to do something genuinely new in games, with the potential to appeal to new types of players.  Mortal Kombat  would later be remembered as a very good game;  Night Trap  as a very, very bad one.  Mortal Kombat  was a game whose content a reasonable person could reasonably object to in at least some contexts;  Night Trap  was most offensive in its sheer ineptness, and was hardly the grisly interactive slasher flick which Lieberman apparently believed it to be. Nevertheless, here they both were. December 9, 1993, would change the games industry forever. ( Sources:  the books  Dungeons and Dreamers : The Rise of Computer Game Culture from Geek to Chic  by Brad King and John Borland ,  The Ultimate History of Video Games  by Steven L. Kent,  Generation Xbox: How Video Games Invaded Hollywood  by Jamie Russell, and  Game Over: How Nintendo Conquered the World  by David Sheff;  Edge  of February 1994;\u00a0 New York Times  of October 15 1982 and September 16 1993;  Retro Gamer  54; the article \u201cRegulating Violence in Video Games: Virtually Everything\u201d by Alex Wilcox in the  Journal of the National Association of Administrative Law Judiciary , Volume 31, Issue 1. Online sources include Kevin D. Impellizeri\u2019s  look back at the videogame hearings ,  \u201cWhen Two Tribes Go to War: A History of Video Game Controversy\u201d  at  GameSpot ,  \u201cThe 25 Dumbest Moments in Gaming\u201d  at  GameSpy , and  Shannon Symonds\u2019s blog post about  Death Race  at the Strong Museum of Play\u2019s website.) \u00a0"}, "210705_news_468313.txt": {"page_id": "210705_news_468313.txt", "text": "Das organisierte Verbrechen breitet sich laut einem Bericht von  Europol  immer weiter in der Europ\u00e4ischen Union aus. Noch nie sei die EU und seine B\u00fcrger so bedroht gewesen, warnte die europ\u00e4ische Polizeibeh\u00f6rde in einer in  Lissabon   vorgelegten Analyse . Die Gefahr sei gro\u00df, dass Kriminelle die Coronakrise mit der wirtschaftlichen Rezession sowie die \u00c4ngste der Menschen ausnutzen w\u00fcrden. Dies k\u00f6nnten \u00bbideale Bedingungen\u00ab f\u00fcr Verbrecher sein, B\u00fcrger, Unternehmen und \u00f6ffentliche Instanzen ins Visier zu nehmen, hei\u00dft es in dem 87 Seiten langen Bericht. Zun\u00e4chst habe sich dies im Handel mit gef\u00e4lschten Masken und Desinfektionsmitteln gezeigt. \u00bbNun nimmt der Handel mit gef\u00e4lschten Impfstoffen und Tests zu\u00ab, sagte Europol-Direktorin Catherine De Bolle. Gef\u00e4lschte Vakzine stellten eine Gefahr f\u00fcr die Gesundheit dar und d\u00fcrften auf keinen Fall in den Umlauf kommen. Mit der Coronapandemie nahm laut Europol auch die Cyberkriminalit\u00e4t zu. Im Januar sei die Malware Emotet stillgelegt worden, durch die die bislang \u00bbgef\u00e4hrlichste\u00ab Cyberattacke der Welt durchgef\u00fchrt worden war. Milliardengewinne im Kokainhandel Die umfassende Analyse basiert auf Tausenden F\u00e4llen und Daten, die Ermittler und Sicherheitsdienste in der EU erhoben haben. 70 Prozent der Banden und Netzwerke seien in mindestens drei EU-Staaten aktiv, hei\u00dft es in dem Bericht. Gut 40 Prozent der Banden sind demnach im Drogenhandel aktiv, das weitaus gr\u00f6\u00dfte kriminelle Gesch\u00e4ft in der EU. Dem Bericht zufolge wurde eine \u00bbnoch nie dagewesene Mengen von Kokain\u00ab aus  Lateinamerika  in die EU geschmuggelt. Daraus erl\u00f6sten Kriminelle in  Europa  und  S\u00fcdamerika  Milliardenbetr\u00e4ge. Die Reinheit des nach Europa geschmuggelten Kokains sei auf dem \u00bbh\u00f6chsten jemals festgestellten Niveau\u00ab, hei\u00dft es bei Europol weiter. Die Beh\u00f6rde wies zudem darauf hin, dass allein bei einer Razzia der niederl\u00e4ndischen und deutschen Polizei  Ende Februar 23 Tonnen Kokain beschlagnahmt worden seien . Kriminelle Netzwerke infiltrieren legale Wirtschaft Die Analyse legt offen, wie sehr die Unterwelt mit der legalen Welt verwoben ist. Mehr als 80 Prozent der kriminellen Netzwerke nutzen demnach legale Gesch\u00e4ftsstrukturen und sind selbst wie Wirtschaftsunternehmen organisiert, mit verschiedenen Managementebenen. Die organisierte Kriminalit\u00e4t kontrolliert direkt legale Gesch\u00e4ftsstrukturen oder infiltriert sie. Dem Bericht zufolge sind alle legalen Wirtschaftsstrukturen potenziell anf\u00e4llig daf\u00fcr, durch kriminelle Organisationen \u00fcbernommen zu werden. Mehr als 60 Prozent der Banden nutzten auch Korruption als Mittel. \u00bbParalleles Untergrund-Finanzsystem\u00ab Die organisierte Kriminalit\u00e4t unterwandert gezielt europ\u00e4ische Volkswirtschaften. \u00bbDas Ausma\u00df und die Komplexit\u00e4t der Geldw\u00e4sche-Aktivit\u00e4ten in der EU sind untersch\u00e4tzt worden\u00ab, hei\u00dft es in dem Bericht. Professionelle Geldw\u00e4scher h\u00e4tten ein \u00bbparalleles Untergrund-Finanzsystem\u00ab geschaffen, um Transaktionen und Zahlungen fernab des legalen Finanzsystems und damit jeder Kontrolle und Nachverfolgbarkeit durchzuf\u00fchren. Das organisierte Verbrechen macht innerhalb der EU und pro Jahr laut Sch\u00e4tzungen von Europol einen Profit von 140 Milliarden Euro. Nur ein Prozent der Verm\u00f6genswerte aus kriminellen Handlungen w\u00fcrde von Ermittlern aufgesp\u00fcrt und konfisziert. Brutale und willk\u00fcrliche Gewalt Besorgniserregend f\u00fcr alle B\u00fcrger in der EU: Gewaltsame \u00dcbergriffe scheinen h\u00e4ufiger vorzukommen \u2013 und sie werden dem Bericht zufolge auch immer brutaler. Immer h\u00e4ufiger k\u00e4men Waffen oder Sprengstoff in der \u00d6ffentlichkeit zur Anwendung: \u00bbKriminelle wenden wahllos Gewalt an und nehmen Opfer unabh\u00e4ngig von ihrer Bedeutung oder eine m\u00f6gliche Verwicklung ins Visier. Dabei nehmen sie oft in Kauf, dass unbeteiligte Zuschauer Schaden erleiden.\u00ab"}, "210705_news_468323.txt": {"page_id": "210705_news_468323.txt", "text": "Warum betreibt das Planungsamt der Bundeswehr einen\u00a0 \u201e360-Grad-Ansatz\u201c bei der  \u201estrategischen Planung\u201c? ES&T hatte die M\u00f6glichkeit, mit  Generalleutnant Christian Badia,  dem Abteilungsleiter Planung im Bundesministerium der Verteidigung, ein  Interview zu f\u00fchren.\u00a0 Foto: Bundeswehr/Neumann ES&T:  Herr General Badia, Planungsentscheidungen m\u00fcssen oft in einem noch unsicheren Umfeld vorbereitet werden. Welches sind die gr\u00f6\u00dften Risiken f\u00fcr die Planung der Bundeswehr? Badia : Es ist genau die Leistung strategischer Planung, Unsicherheit so weit wie m\u00f6glich beherrschbarer zu machen. In der Planung wird es immer Risiken und Unsicherheiten geben, mit denen wir umgehen m\u00fcssen. Hierzu ist eine Strukturierung mit Blick auf das Ganze notwendig. Welche M\u00f6glichkeiten oder Instrumente habe ich, um diese Beherrschbarkeit zu erreichen? Hierzu betrachten wir systematisch verschiedenste Szenare mit sicherheits- und verteidigungspolitscher Relevanz: Zukunftsanalysen, Kurzanalysen, z. B. \u00fcber neue Technologien oder auch \u00fcber m\u00f6gliche zuk\u00fcnftige Konflikte. Am Schluss tritt dann der Zeitfaktor als weiterer Risikofaktor hinzu, der mich umtreibt: Bin ich aufgrund meiner Analysen und Planungen zeitgerecht in der Lage, den Herausforderungen begegnen zu k\u00f6nnen? ES&T:  Was unterscheidet den Planungsprozess in einem Unternehmen wie der Bundeswehr von Wirtschaftsunternehmen? Badia:  Nun, der gro\u00dfe Unterschied zwischen der Bundeswehr und einem Gro\u00dfunternehmen ist, dass Letzteres gewinnorientiert arbeitet und dann auch entsprechend plant. Dort ist Planung darauf ausgerichtet, Produkte und deren Herstellung so effizient zu gestalten, wie es nur irgendwie geht, um eine Gewinnmaximierung zu erzielen. Beim Planungsprozess der Bundeswehr dagegen muss eine F\u00e4higkeitsmaximierung erzielt werden, um mit den zur Verf\u00fcgung stehenden Ressourcen in einer 360-Grad-Betrachtung die besten F\u00e4higkeiten erlangen zu k\u00f6nnen. Hier muss man potenziellen Herausforderungen, die in der Zukunft entstehen, m\u00f6glichst erfolgreich begegnen. Das ist der gro\u00dfe Unterschied. Melden Sie sich unkompliziert zu einem Tageszugang an, um sofort von allen ESUT Digital Vorteilen zu profitieren.\u00a0 Zum Tageszugang   Digital Tageszugang  f\u00fcr 1,50 \u20ac / 1 Tag   Bestellen   Digital Halbjahresabo  f\u00fcr 30 \u20ac / 6 Monate   Bestellen Digital Jahresabo \nf\u00fcr 60 \u20ac / 12 Monate Bestellen Genie\u00dfen Sie die Premium-Inhalte und weitere Vorteile von\u00a0 ESUT   Digital : Zugang zu allen Online-Inhalten Umfassende Suche im News-Archiv Individualisierbarer Newsbereich Hintergr\u00fcnde, Analysen und technische Fachartikel komplett und exklusiv aus der Europ\u00e4ischen Sicherheit und Technik und den Wehrtechnischen Reports Tagesaktuelle News aus den Rubriken Industrie / Innere Sicherheit / International / Land / Luft / Politik / R\u00fcstung / See / Streitkr\u00e4fte uvm. \u00a0"}, "210705_news_468336.txt": {"page_id": "210705_news_468336.txt", "text": "Shares in  Alibaba  surged on Monday after the e-commerce company said that a record $2.8bn (\u00a32bn) fine handed down by Chinese regulators marked the end of an investigation into anti-competitive practices at the company. Top executives at the company, founded by the billionaire  Jack Ma , told investors that while Chinese regulators continued a wider investigation into the sprawling conglomerates in the country\u2019s tech industry, they believed the multibillion dollar fine announced at the weekend marked the end of the focus on Alibaba. The company is listed in Hong Kong and its shares climbed as much as 9% on the management\u2019s comments. \u201cWe are pleased we can put this matter behind us,\u201d said Joe Tsai, the executive vice-chair of Alibaba Group. \u201cWith this penalty decision we\u2019ve received good guidance on some of the specific issues under the anti-monopoly law. Other than the mergers review, we\u2019re not aware of any other [antitrust] issues.\u201d China\u2019s market regulators imposed the fine on Alibaba, worth 4% of its domestic revenues in 2019, for restricting merchants that use its website from doing business or running promotions on rival e-commerce platforms. Alibaba said that it will spend \u201cbillions of dollars\u201d to improve the experience of merchants on its site. While the $2.8bn fine is a record by Chinese regulators, it is also far less than the maximum 10% of revenues that Alibaba could have faced. The billionaire Jack Ma founded Alibaba.  Photograph: Philippe Lopez/AFP/Getty Images While Alibaba no longer faces any further investigations, it still has to comply with a \u201ccomprehensive rectification\u201d programme as well as scrutiny of past mergers and acquisitions. Alibaba has become the lightning rod in the crackdown on big tech after Ma, one of China\u2019s most popular, outspoken and wealthiest entrepreneurs, gave a blunt speech in 2020 criticising national regulators that reportedly infuriated the president, Xi Jinping. After the comments, Chinese regulators  blocked the $34bn stock market flotation  of the Alibaba online payments subsidiary Ant Group, which would have been the biggest share offering in history, and Ma  disappeared from the public eye for three months . Last month, Beijing ordered Alibaba  to sell off some of its media assets , which include Hong Kong\u2019s South China Morning Post, as the Chinese government cracks down on the growing public influence held by the country\u2019s sprawling tech conglomerates. In a separate development on Monday, China\u2019s central bank announced that Ant Group will restructure as a financial holding company, a move expected to curb its profitability and valuation. The People\u2019s Bank of China said that under a \u201ccomprehensive and feasible restructuring plan\u201d, Ant would cut the \u201cimproper\u201d linkage between the payments service Alipay, the virtual credit card business Jiebei and the consumer loan unit Huabei. The central bank also asked Ant to break its \u201cmonopoly on information, and strictly comply with the requirements of credit information business regulation\u201d. The company has agreed to improve corporate governance and \u201crectify illegal financial activities in credit, insurance and wealth management\u201d, the central bank said. The central bank said it had also asked Ant to control its leverage and product risks, to control the liquidity risk of its flagship fund products and to \u201cactively lower\u201d the size of its massive Yu\u2019E Bao money market fund. The measures \u201cset an example\u201d for financial regulation of the platform economy\u201d, the state-backed Economic Daily newspaper said. \u201cAnt Group attaches great importance to the seriousness of the rectification,\u201d the company said. It added that it planned to set up a personal credit reporting business and fold its two flagship lending businesses into its consumer finance company."}, "210705_news_468393.txt": {"page_id": "210705_news_468393.txt", "text": "O n Thursday 8 April, Emmanuel Macron  announced the closure  of the prestigious \u00c9cole Nationale d\u2019Administration, France\u2019s elite school for turning out senior civil servants and politicians. The president\u2019s announcement sounded familiar \u2013 he had already pledged to reform the ENA, a school renowned for its conservatism and aversion to change, back in 2019 \u2013 but this time it\u2019s final: Macron said that the time had come to abolish an institution that is widely regarded as a symbol of elitism and inequality. With just a year until the next presidential election, Macron is neck and neck  in the polls  with Marine Le Pen. The ENA abolition looks, therefore, as if it\u2019s part of a strategy to reconnect with \u201cthe people\u201d. It\u2019s easy to forget, given the pandemic, but before France entered lockdown in March 2020, it had been experiencing the most sustained anti-elite movement for generations in the form of the  gilets jaunes  (yellow vests) protests. Macron has certainly has not forgotten this. The president does not want to dispense with the idea of an elite school altogether but to build something that allegedly works better. A new school called the Institut du Service Public, a kind of \u201cpublic management school\u201d, will replace the ENA. Unsurprisingly, Jean-Louis Debr\u00e9, once a close ally of Jacques Chirac, declared that this was a \u201cpopulist\u201d measure (by which he meant it was pandering to public opinion). His father, Michel Debr\u00e9, the first of Charles de Gaulle\u2019s prime ministers, founded the ENA in 1945. Its aim was to train students drawn from all walks of life through entrance exams so jobs in the French civil service could be assigned on merit rather than wealth and personal background.  In reality, the school turned out to be a close-knit club for the upper-class, rather than a force for democratisation. The elitist recruitment pattern has worsened over the years: by 2014, 70% of students came from the upper classes, as opposed to  45% in the 1950s . ENA graduates \u2013 called  \u00e9narques  \u2013 land the best jobs in the civil service, but also in business and frontline politics. Alumni include several presidents, the past eight prime ministers and current CEOs of top business and banking firms. There is even a term,  pantouflage   (from the word for slippers), referring to the practice by which civil servants find lucrative work in the private sector \u2013 the  \u00e9narques  are emblematic of this tendency, which accentuates the public perception of an incestuous old boys\u2019 network. One may think of  \u00e9narques  as the French counterparts of  Oxford PPE graduates . Insofar as they are both effectively finishing schools for the ruling class, the comparison makes sense, although the specifics are quite different. In the UK, private education and elite universities are a fact of life. In France, the ideology of republicanism \u2013 which comes from the French Revolution \u2013 insists on the notion of equality of treatment for all and on the delivery of state-of-the-art public services owned and run by the state. The ENA is a public institution whose funding almost wholly comes from the state. This makes the way it favours students with high economic and cultural capital not just embarrassing, but a seeming contradiction of the state\u2019s republican ideals. To get in, candidates spend a year prior to applying to the school in a  classe pr\u00e9paratoire , an extremely demanding course. The written exam ( concours externe ) tests a wide range of subjects and disciplines. Only a minority of candidates make it to the second round, which includes oral exams that essentially test their elocutionary skills. Then comes the dreaded  grand  oral , a long ordeal in front of a jury during which candidates are asked all kinds of puzzling and provocative questions designed to test their capacity to think on the spot. It all has the effect of giving richer candidates who have the right social skills the edge over candidates from more modest backgrounds. Once in the ENA, it is essential to graduate in the top 10%, if graduates want to be able to choose from the most prestigious roles in the French state. The rest may be assigned to mediocre positions. Former graduates often complain about the teaching, which has a reputation  for being dull  and conservative, and describe the institution as a  bastion of upper-class snobbery . Back in 2006, Nicolas Sarkozy (who did not attend the ENA) mocked the \u201csadist or idiot\u201d who had seen fit to include exam questions for ENA candidates about  Madame de la Fayette\u2019s Princess of Cl\u00e8ves , a 17th-century novel. Sarkozy\u2019s mockery alluded to the idea that  \u00e9narques  are trained to discuss subjects as varied as politics, economics, history, arts or literature but can do so in only a superficial manner. The modernisation of the ENA should therefore involve a comprehensive overhaul of the school\u2019s recruitment process and exam procedures, as well as a dramatic modernisation of the curriculum so it reflects the challenges of the real world. But does Macron really want this? Only he knows. Would it appease the yellow vests and people angered by his economic policies? Though the reform may be welcome and necessary,   it will probably not make the slightest difference to them:  \u00e9narques  and ordinary citizens would carry on living on different planets."}, "210705_news_468399.txt": {"page_id": "210705_news_468399.txt", "text": "A leading sight charity has stressed the need for inclusive web design after rail websites switched to black and white to mark Prince Philip\u2019s death, leaving partially sighted people struggling. Network Rail and National Rail websites turned from colour to greyscale in a tribute to the Duke of Edinburgh. The gesture backfired after customers highlighted accessibility issues and complained they could no longer use the website. Other train operators, including CrossCountry and Northern rail, had also removed colour from their websites. One Twitter user said: \u201cNational Rail have coloured their entire website grey to \u2018mourn Prince Philip\u2019, rendering the whole website completely useless to people with visual impairments. The UK has completely lost the plot.\u201d Robin Spinks, the innovation lead for the Royal National Institute of Blind People, said: \u201cAs someone who is registered severely sight impaired, good colour contrast on a website is incredibly important. A lack of this makes it difficult for me to read the content and causes headaches and eye strain. It leaves me feeling unwelcome as a customer. \u201cAlthough I can understand why an organisation might make a change to its website in circumstances such as this, any change should be inclusive and accessible so that all customers can continue to use the site as normal.  \u201dAdherence to inclusive design standards should remain the most important aspect for all digital design, regardless of any changes made.\u201d Mikey Stillwell, a designer at the research, design and user experience agency Verj, who is colour-blind, said familiarity rather than accessibility was the issue.  He said: \u201cIn terms of [an] accessibility standpoint, I can\u2019t really see too much of an issue because there\u2019s loads of contrast on the website. But from a UX [user experience] there is. When you have grey call to actions, for example, they\u2019re normally seen as disabled or inactive. There is a total loss of hierarchy for what is important on a website as well if everything\u2019s the same colour.\u201d Buckingham Palace announced that  Prince Philip died on Friday morning aged 99 . Many institutions across Britain marked their own tributes in respect. BBC broadcasters changed their ties to black, while there were also scheduling changes made to radio and television programming across the network for special coverage of the Prince\u2019s death. A spokesperson for National Rail said: \u201cThe National Rail Enquiries website has been temporarily greyscaled as a mark of respect following the death of HRH Duke of Edinburgh on Friday. We are listening to feedback about how people are using the website and are making further changes today to make it more accessible to all our customers.\u201d A Network Rail spokesperson said: \u201cWe temporarily made our website greyscale as a mark of respect following the death of HRH the Duke of Edinburgh. We\u2019ve been made aware this has caused problems for people accessing the content so it\u2019s now back to its usual look. We\u2019re sorry it\u2019s caused issues and we thank everyone for their feedback.\u201d"}, "210705_news_468404.txt": {"page_id": "210705_news_468404.txt", "text": "Acquisition will combine solutions and expertise to deliver new cloud and AI capabilities across healthcare and other industries REDMOND, Wash., and BURLINGTON, Mass. \u2013 April 12, 2021 \u2013  Microsoft Corp (Nasdaq: MSFT) and Nuance Communications, Inc. (Nasdaq: NUAN) today announced they have entered into a definitive agreement under which Microsoft will acquire Nuance for $56.00 per share, implying a 23% premium to the closing price of Nuance on Friday, April 9, in an all-cash transaction valued at $19.7 billion, inclusive of Nuance\u2019s net debt. Nuance is a trusted cloud and AI software leader representing decades of accumulated healthcare and enterprise AI experience. Mark Benjamin will remain CEO of Nuance, reporting to Scott Guthrie, executive vice president of Cloud & AI at Microsoft. The transaction is intended to close this calendar year. Microsoft has accelerated its efforts to provide industry-specific cloud offerings to support customers and partners as they respond to disruption and new opportunities. These efforts include the  Microsoft Cloud for Healthcare , introduced in 2020, which aims to address the comprehensive needs of the rapidly transforming and growing healthcare industry. Today\u2019s acquisition announcement represents the latest step in Microsoft\u2019s industry-specific cloud strategy. Nuance is a pioneer and a leading provider of conversational AI and cloud-based ambient clinical intelligence for healthcare providers. Nuance\u2019s products include the Dragon Ambient eXperience, Dragon Medical One and PowerScribe One for radiology reporting, all leading clinical speech recognition SaaS offerings built on Microsoft Azure. Nuance\u2019s solutions work seamlessly with core healthcare systems, including longstanding relationships with Electronic Health Records (EHRs), to alleviate the burden of clinical documentation and empower providers to deliver better patient experiences. Nuance solutions are currently used by more than 55% of physicians and 75% of radiologists in the U.S., and used in 77% of U.S. hospitals. Nuance\u2019s Healthcare Cloud revenue experienced 37% year-over-year growth in Nuance\u2019s fiscal year 2020 (ended September 2020). Microsoft\u2019s acquisition of Nuance builds upon the successful existing partnership between the companies that  was announced in 2019 . By augmenting the Microsoft Cloud for Healthcare with Nuance\u2019s solutions, as well as the benefit of Nuance\u2019s expertise and relationships with EHR systems providers, Microsoft will be better able to empower healthcare providers through the power of ambient clinical intelligence and other Microsoft cloud services. The acquisition will double Microsoft\u2019s total addressable market (TAM) in the healthcare provider space, bringing the company\u2019s TAM in healthcare to nearly $500 billion. Nuance and Microsoft will deepen their existing commitments to the extended partner ecosystem, as well as the highest standards of data privacy, security and compliance. \u201cNuance provides the AI layer at the healthcare point of delivery and is a pioneer in the real-world application of enterprise AI,\u201d said Satya Nadella, CEO, Microsoft. \u201cAI is technology\u2019s most important priority, and healthcare is its most urgent application. Together, with our partner ecosystem, we will put advanced AI solutions into the hands of professionals everywhere to drive better decision-making and create more meaningful connections, as we accelerate growth of Microsoft Cloud for Healthcare and Nuance.\u201d Beyond healthcare, Nuance provides AI expertise and customer engagement solutions across Interactive Voice Response (IVR), virtual assistants, and digital and biometric solutions to companies around the world\u00a0across\u00a0all industries. This expertise will come together with the breadth and depth of Microsoft\u2019s cloud, including Azure, Teams, and Dynamics 365, to deliver next-generation customer engagement and security solutions. \u201cOver the past three years, Nuance has streamlined its portfolio to focus on the healthcare and enterprise AI segments, where there has been accelerated demand for advanced conversational AI and ambient solutions,\u201d said Mark Benjamin, CEO, Nuance. \u201cTo seize this opportunity, we need the right platform to bring focus and global scale to our customers and partners to enable more personal, affordable and effective connections to people and care. The path forward is clearly with Microsoft \u2014 \u00a0who brings intelligent cloud-based services at scale and who shares our passion for the ways technology can make a difference.\u00a0At the same time, this combination offers a critical opportunity to deliver meaningful and certain value to our shareholders who have driven and supported us on this journey.\u201d The transaction has been unanimously approved by the Boards of Directors of both Nuance and Microsoft. The deal is intended to close by the end of this calendar year and is subject to approval by Nuance\u2019s shareholders, the satisfaction of certain regulatory approvals, and other customary closing conditions. Upon closing, Microsoft expects Nuance\u2019s financials to be reported as part of Microsoft\u2019s Intelligent Cloud segment. Microsoft expects the acquisition to be minimally dilutive (less than 1 percent) in fiscal year 2022 and to be accretive in fiscal year 2023 to non-GAAP earnings per share, based on the expected close timeframe. Non-GAAP excludes expected impact of purchase accounting adjustments, as well as integration and transaction-related expenses. The acquisition will not impact the completion of its existing share repurchase authorization. Nadella, Benjamin, Guthrie and Microsoft Chief Financial Officer Amy Hood will host a webcast for investors and media on April 12, 2021, at 8 a.m. Pacific Time/11 a.m. Eastern Time regarding this transaction. The presentation is available via webcast at\u00a0 https://aka.ms/MS-Investor-Call  or to international callers at +1 (201) 689-8023 (no password required), or to U.S. callers at (877) 407-0666 (no password required),\u00a0at that time. Goldman Sachs & Co. LLC is acting as exclusive financial advisor to Microsoft, while Simpson Thacher & Bartlett LLP is acting as its legal advisor. Evercore is acting as exclusive financial advisor to Nuance, while Paul, Weiss, Rifkind, Wharton & Garrison LLP is acting as its legal advisor. About Microsoft \nMicrosoft (Nasdaq \u201cMSFT\u201d @microsoft) enables digital transformation for the era of an intelligent cloud and an intelligent edge. Its mission is to empower every person and every organization on the planet to achieve more. About Nuance Communications, Inc. Nuance Communications \u00a0(NASDAQ: NUAN) is a technology pioneer with market leadership in conversational AI and ambient intelligence. A full-service partner trusted by 77 percent of U.S. hospitals and 85 percent of the Fortune 100 companies worldwide, Nuance creates intuitive solutions that amplify people\u2019s ability to help others. For more information, press only: Microsoft Media Relations, WE Communications, (425) 638-7777,\u00a0 [email\u00a0protected] Note to editors:\u00a0For more information, news and perspectives from Microsoft, please visit the Microsoft News Center at\u00a0 http://news.microsoft.com . Web links, telephone numbers and titles were correct at time of publication but may have changed. For additional assistance, journalists and analysts may contact Microsoft\u2019s Rapid Response Team or other appropriate contacts listed at\u00a0 https://news.microsoft.com/microsoft-public-relations-contacts . Trademark reference: Nuance and the Nuance logo are registered trademarks or trademarks of Nuance Communications, Inc. or its affiliates in\u00a0the United States\u00a0and/or other countries. All other trademarks referenced herein are the property of their respective owners. Additional Information and Where to Find It In connection with the transaction, Nuance Communications, Inc. (the \u201cCompany\u201d) will file relevant materials with the Securities and Exchange Commission (the \u201cSEC\u201d), including a proxy statement on Schedule 14A. Promptly after filing its definitive proxy statement with the SEC, the Company will mail the definitive proxy statement and a proxy card to each stockholder entitled to vote at the special meeting relating to the transaction. INVESTORS AND SECURITY HOLDERS OF THE COMPANY ARE URGED TO READ THESE MATERIALS (INCLUDING ANY AMENDMENTS OR SUPPLEMENTS THERETO) AND ANY OTHER RELEVANT DOCUMENTS IN CONNECTION WITH THE TRANSACTION THAT THE COMPANY WILL FILE WITH THE SEC WHEN THEY BECOME AVAILABLE BECAUSE THEY WILL CONTAIN IMPORTANT INFORMATION ABOUT THE COMPANY AND THE TRANSACTION. The definitive proxy statement, the preliminary proxy statement and other relevant materials in connection with the transaction (when they become available), and any other documents filed by the Company with the SEC, may be obtained free of charge at the SEC\u2019s website (http://www.sec.gov) or at the Company\u2019s website (http://investors.nuance.com) or by writing to Nuance Communications, Investor Relations, 1 Wayside Road, Burlington, Massachusetts, 01803. The Company and certain of its directors and executive officers and other members of management and employees may be deemed to be participants in the solicitation of proxies from the Company\u2019s stockholders with respect to the transaction. Information about the Company\u2019s directors and executive officers and their ownership of the Company\u2019s common stock is set forth in the Company\u2019s proxy statement on Schedule 14A filed with the SEC on December 17, 2020. To the extent that holdings of the Company\u2019s securities have changed since the amounts printed in the Company\u2019s proxy statement, such changes have been or will be reflected on Statements of Change in Ownership on Form 4 filed with the SEC. Information regarding the identity of the participants, and their direct or indirect interests in the transaction, by security holdings or otherwise, will be set forth in the proxy statement and other materials to be filed with SEC in connection with the transaction. Forward-Looking Statements This press release contains certain forward-looking statements within the meaning of the \u201csafe harbor\u201d provisions of the United States Private Securities Litigation Reform Act of 1995 with respect to the proposed transaction and business combination between Microsoft and Nuance, including statements regarding the benefits of the transaction, the anticipated timing of the transaction and the products and markets of each company. These forward-looking statements generally are identified by the words \u201cbelieve,\u201d \u201cproject,\u201d \u201cpredicts,\u201d \u201cbudget,\u201d \u201cforecast,\u201d \u201ccontinue,\u201d \u201cexpect,\u201d \u201canticipate,\u201d \u201cestimate,\u201d \u201cintend,\u201d \u201cstrategy,\u201d \u201cfuture,\u201d \u201copportunity,\u201d \u201cplan,\u201d \u201cmay,\u201d \u201ccould,\u201d \u201cshould,\u201d \u201cwill,\u201d \u201cwould,\u201d \u201cwill be,\u201d \u201cwill continue,\u201d \u201cwill likely result,\u201d and similar expressions (or the negative versions of such words or expressions). Forward-looking statements are predictions, projections and other statements about future events that are based on current expectations and assumptions and, as a result, are subject to risks and uncertainties. Many factors could cause actual future events to differ materially from the forward-looking statements in this press release, including but not limited to: (i) the risk that the transaction may not be completed in a timely manner or at all, which may adversely affect Nuance\u2019s business and the price of the common stock of Nuance, (ii) the failure to satisfy the conditions to the consummation of the transaction, including the adoption of the merger agreement by the stockholders of Nuance and the receipt of certain governmental and regulatory approvals, (iii) the occurrence of any event, change or other circumstance that could give rise to the termination of the merger agreement, (iv) the effect of the announcement or pendency of the transaction on Nuance\u2019s business relationships, operating results, and business generally, (v) risks that the proposed transaction disrupts current plans and operations of Nuance or Microsoft and potential difficulties in Nuance employee retention as a result of the transaction, (vi) risks related to diverting management\u2019s attention from Nuance\u2019s ongoing business operations, (vii) the outcome of any legal proceedings that may be instituted against us or against Nuance related to the merger agreement or the transaction, (viii) the ability of Microsoft to successfully integrate Nuance\u2019s operations, product lines, and technology, and (ix) the ability of Microsoft to implement its plans, forecasts, and other expectations with respect to Nuance\u2019s business after the completion of the proposed merger and realize additional opportunities for growth and innovation. In addition, please refer to the documents that Microsoft and Nuance file with the SEC on Forms 10-K, 10-Q and 8-K. These filings identify and address other important risks and uncertainties that could cause events and results to differ materially from those contained in the forward-looking statements set forth in this press release. Forward-looking statements speak only as of the date they are made. Readers are cautioned not to put undue reliance on forward-looking statements, and Microsoft and Nuance assume no obligation and do not intend to update or revise these forward-looking statements, whether as a result of new information, future events, or otherwise. \u00a0"}, "210705_news_468432.txt": {"page_id": "210705_news_468432.txt", "text": "Vodafone hat in Deutschland Teile seines 5G-Mobilfunknetzes auf \"5G Standalone\" mit eigenem Kernnetz umgestellt, das ohne LTE-Anker auskommt. Der Netzbetreiber stellt alle seine Antennen um, die auf den 5G-Frequenzen im 3,5 Gigahertz-Bereich funken, das sind rund 1000 Antennen im in 170 St\u00e4dten und Gemeinden. Das erste Rechenzentrum f\u00fcr das 5G-Kernnetz steht in Frankfurt. Bei den vorangegangenen Tests und der Umstellung hat Vodafone mit den Partnern Ericsson, Oppo, Qualcomm und Nokia zusammengearbeitet. \"Als erster Netzbetreiber legen wir bei 5G die LTE-St\u00fctzr\u00e4der beiseite und starten mit einem 5G-Kernnetz\", sagte der Deutschland-Chef Hannes Ametsreiter am Montag. \"5G Standalone\" zeichnet sich vor allem durch \u00e4u\u00dferst kurze Datenlaufzeiten aus. Diese geringe Latenz ist zum einen f\u00fcr kommerzielle Anwender interessant, aber auch Anwendungen der \"Augmented Reality\" und Gamer k\u00f6nnen profitieren. Auch andere bauen aus Vodafone ist nicht der einzige Provider, der sich mit dem Thema auseinandersetzt: Die Deutsche Telekom testet \"5G Standalone\" bereits seit Februar in Garching bei M\u00fcnchen, hat bislang aber noch keinen gr\u00f6\u00dferen Live-Betrieb in der Fl\u00e4che angek\u00fcndigt. Wettbewerber Telef\u00f3nica (O2) hat den  Start des \"reinen\" 5G-Betrieb f\u00fcr dieses Jahr  in Aussicht gestellt. \"5G Standalone\" erm\u00f6glicht aber auch ganz neue Einsatzm\u00f6glichkeiten. Dazu geh\u00f6rt die Funktion des \"Network Slicings\". Dabei kann ein physisches Netz in mehrere virtuelle Netze mit unterschiedlichen Anforderungen, garantierten Bandbreiten und Latenzen unterteilt werden. So k\u00f6nnte beispielsweise einem TV-Team in einem voll besetzten Bundesliga-Stadion ein eigenes virtuelles Mobilfunknetz zugewiesen werden, mit dem ein kabelloser Kamera-Einsatz m\u00f6glich w\u00e4re. Entsprechende  Feldversuche laufen zwischen Vodafone und dem TV-Sender Sky . W\u00e4hrend bei \"5G Standalone\" die Datenlaufzeiten gemessen in Millisekunden signifikant besser ausfallen, spielt die neue Ausbaustufe bei der Bandbreite (gemessen in Megabit pro Sekunde) dagegen bislang keine Rolle. In bestimmten Szenarien f\u00e4llt die Bandbreite im \"reinen\" 5G-Netz sogar geringer aus als im \"5G Non-Standalone\"-Netz. Noch h\u00f6here \u00dcbertragungsraten sind dann f\u00fcr weitere 5G-Ausbaustufen vorgesehen. Kunden mit 5G-Tarifen k\u00f6nnen bei Vodafone selbst entscheiden, ob sie im bestehenden 5G-Netz mit LTE surfen wollen oder in dem neuen Netz. Handyhersteller ziehen mit Mit dem Ausbau des 5G-Netzes bei allen 3,5-GHz-Stationen geht Vodafone in Vorleistung, denn zu Beginn werden nur wenige Anwender davon profitieren k\u00f6nnen. Bislang unterst\u00fctzen nur wenige Smartphones wie das  Oppo Find X3 Pro  den neuen Standard. Allerdings werden auch Modelle von Samsung, Huawei und anderen Herstellern erwartet, die \"5G Standalone\" nutzen k\u00f6nnen. Vodafone-Kunden mit 5G-Vertragstarif und einem geeigneten Handy k\u00f6nnen ab kommender Woche eine kostenfreie \"5G Core\"-Option hinzubuchen. Mit der neuen Technik r\u00fccken auch die verteilten Rechenzentren in den Blickpunkt. Um die Datenlaufzeiten m\u00f6glichst gering zu halten, bauen die Provider diese Kapazit\u00e4ten nicht zentral aus, sondern die Daten werden immer n\u00e4her beim Kunden verarbeitet. Dieses Konzept wird auch als \"Edge Computing\" bezeichnet, weil sich die Rechner \"am Rande des Netzes\" befinden. Bis 2023 will das Unternehmen insgesamt zehn 5G-Rechenzentren in Betrieb nehmen. Noch in diesem Jahr werde das zweite 5G-Rechenzentren in Berlin ans Netz, sagte Vodafone-Technikchef Gerhard Mack. Kurze Zeit sp\u00e4ter werde ein drittes 5G-Rechenzentrum in M\u00fcnchen live gehen. \"Die weiteren 5G-Rechenzentren folgen bis 2023. Dann ist in ganz Deutschland der Datenaustausch nahezu in Echtzeit m\u00f6glich.\" ( vbr )"}, "210705_news_468436.txt": {"page_id": "210705_news_468436.txt", "text": "Apple richtet sein Angebot an Unterhaltungselektronik offenbar neu aus: Das Unternehmen entwickelt eine neue Ger\u00e4tekategorie, die Funktionen der Settop-Box Apple TV mit dem vernetzten Lautsprechersystem HomePod kreuzt und um eine Kamera erg\u00e4nzt, wie die Finanznachrichtenagentur  Bloomberg  unter Berufung auf informierte Personen berichtet. Das Ger\u00e4t diene als Streaming-Box f\u00fcr den Fernseher, lasse sich \u00fcber das Sprachassistenzsystem Siri steuern, auch f\u00fcr die Musikwiedergabe nutzen sowie f\u00fcr Videochats einsetzen. Parallel sei ein vernetzter Lautsprecher mit Touchscreen in Entwicklung, der durch eine integrierte Kamera ebenfalls Videotelefonate erm\u00f6glicht,  wie  Bloomberg  ausf\u00fchrt . \u00c4hnlich wie bei  Amazons Echo Show 10  erw\u00e4ge Apple, das Display mit Kamera auf einem motorisierten Arm zu befestigen, das den Nutzer beim Videochat im Raum verfolgen k\u00f6nne. Apple ohne klare Heim-Strategie Beide Produkte seien noch in einem sehr fr\u00fchen Stadium der Entwicklung und k\u00f6nnten deshalb letztlich auch mit anderen Funktionen auf den Markt kommen oder auch vor einer Produktreife verworfen werden, schr\u00e4nkt die Finanznachrichtenagentur ein.  Apple hat im vergangenen Jahr offenbar die mit der Entwicklung von Apple TV und HomePod vertrauten Teams zusammengelegt und die zugrundeliegende Software zusammengef\u00fchrt \u2013 auch die  Lautsprecher setzen inzwischen auf tvOS . Unterhaltungselektronik und Smart-Home-Ger\u00e4te spielten bei Apple in den letzten Jahren nur eine weit untergeordnete Rolle:  WLAN-Router, bei denen das Unternehmen einst mit seinen AirPort-Basisstationen Vorreiter war, gibt es von Apple schon l\u00e4nger nicht mehr . Apple TV wird weiter verkauft, wurde zuletzt aber 2017 aktualisiert. Den  gro\u00dfen HomePod hat Apple j\u00fcngst eingestellt , mit dem HomePod mini bleibt das Unternehmen aber im Markt der vernetzten Lautsprecher aktiv \u2013 die Kugel dient zugleich als Steuerzentrale f\u00fcr Smart-Home-Hardware anderer Hersteller. ( lbe )"}, "210705_news_468447.txt": {"page_id": "210705_news_468447.txt", "text": "BEIJING \u2014 China\u2019s government on Friday criticized the Biden administration\u2019s curbs on access to U.S. technology for its supercomputer developers and said sanctions \u201conly strengthen China\u2019s determination\u201d to invent its own. The sanctions announced Thursday block access to U.S. technology for researchers and manufacturers the Commerce Department said build supercomputers used by the Chinese military in weapons development. They can be used to simulate nuclear explosions and the aerodynamics of high-speed or stealth aircraft and missiles. The penalties are the latest sign President Joe Biden is sticking to the tough line taken by his predecessor, Donald Trump, toward Chinese tech industries seen by Washington as potential threats. The step adds to conflict over the ruling Communist Party\u2019s industrial plans, access to American technology and accusations of computer attacks and theft of business secrets. A foreign ministry spokesman, Zhao Lijian, accused Washington of misusing phony security warnings to \u201cmaliciously suppress\u201d Chinese industry. \u201cContainment and suppression by the United States cannot stop the pace of China\u2019s scientific and technological progress, but will only strengthen China\u2019s determination and will to innovate independently,\u201d Zhao said. Zhao said Beijing would protect its companies, echoing Chinese warnings after previous U.S. trade penalties that often are followed by no action. Biden has said he wants better relations with Beijing but has given no indication he will roll back sanctions imposed by Trump on Chinese telecom equipment giant Huawei and other companies. The Communist Party has responded by declaring that accelerating efforts to transform China into a self-reliant \u201ctechnology power\u201d will be this year\u2019s top economic priority. Chinese supercomputers have set speed records but use U.S.-supplied processor chips and other hardware. \u201cThe United States has implemented a technical blockade of China in the supercomputer field for a long time, but China\u2019s supercomputers still lead the world due to independent innovation,\u201d Zhao said. The U.S. penalties apply to National Supercomputing Centers in the cities of Jinan, Shenzhen, Wuxi and Zhengzhou, Tianjin Phytium Information Technology, Shanghai High-Performance Integrated Circuit Design Center and Sunway Microelectronics. Meanwhile, American telecom regulators are in the process of stripping three Chinese phone carriers of the right to operate in the United States. Trump also tried to force the Chinese owner of video service TikTok to sell its U.S. unit and issued an order barring Americans from investing in securities of companies deemed by the Pentagon to be linked to China\u2019s military. More In IT/Networks Palantir global defense lead Doug Philippone explains where his company fits into the military's JADC2 plans. The U.S. Army wants access to research on autonomous systems and data analytics. Two vendors were named to the Next Generation Load Device-Medium after the Army delayed the contract to reassess its acquisition strategy. Retired Maj. Gen. Peter Gallagher will oversee the contractor's national security technology portfolio. The Naval Research Lab sees zinc-based batteries as a reusable replacement for fire-prone and single-use batteries. In Other News Space Systems Command is the second of three planned field commands, and it will be in charge of acquiring most of the Space Force's systems. Submarines of Naval Submarine Force Pacific and their tenders did not receive the required internal and external cybersecurity inspections in recent years, raising the specter of cyber vulnerability among some of the sea service\u2019s most potent platforms. The U.S. Navy is \u201cwell on its way\u201d to delivering replacement small and medium unmanned underwater vehicles that will support the submarine and the expeditionary mine countermeasures communities. The demonstration will help the agency reduce background noise and detect hypersonic weapons from space. The service is laser-focused on bringing the Integrated Battle Command System through an initial operational test set to begin next month. Load More"}, "210705_news_468460.txt": {"page_id": "210705_news_468460.txt", "text": "More than two million devotees bathed in the River Ganges on Monday despite soaring rates of new Covid-19 cases across  India , a second wave that has struck down Bollywood stars, sent migrant workers fleeing from cities and contributed to the slowing of vaccination programmes around the world. The largest bathing day of the Hindu religious festival Kumbh Mela in the northern Indian city of Haridwar highlighted the immense challenge facing officials in trying to implement social distancing as the daily rate of new Covid-19 cases crossed 160,000 over the weekend, India\u2019s highest point since the beginning of the pandemic. Police in Haridwar said they had been trying to keep worshippers apart but that it was not practical to issue fines on Monday, an auspicious day when the largest crowds of the festival thronged the banks of the river waiting for their opportunity to bathe in waters they believe can cleanse sins and free them from the cycle of death and rebirth. \u201cA stampede-like situation may arise if we would try to enforce social distancing at ghats [the steps leading down to the water], so we are unable to enforce social distancing here,\u201d the inspector general of the local police force, Sanjay Gunjyal, told the news agency ANI. Footage from the riverside showed large, mainly unmasked crowds of men and women jostling for space to stand. \u201cThere is no coronavirus,\u201d a pilgrim told NDTV from the bathing site on Monday. \u201cThe Ganga will protect you. There is nothing to worry about.\u201d The scenes jarred with other video, broadcast on Indian television, of people lying in the streets outside some hospitals, unable to find a bed, and warnings from doctors in some states that supplies of oxygen, ventilators and the drug remdesivir, used to treat serious cases, were starting to run low. Several states have reintroduced evening or weekend lockdowns in response to the record increases, while others including the worst-hit, Maharashtra, are considering more drastic measures. But officials are conscious of the economic impact of quarantines in a country where about 90% of the workforce is informal, and reliant on day-to-day work to earn a living. The country\u2019s first lockdown, last March, triggered an exodus of migrant workers on a scale that rivalled the partition of the subcontinent more than 70 years ago. Workers are starting to leave again and the rate is likely to increase if harsh lockdowns are reinstated in big cities. India\u2019s experience has been a showcase of the unpredictability of Covid-19. Early predictions that the subcontinental country \u2013 with poor healthcare in many rural areas and dense living conditions in cities \u2013 would be swamped by the virus were confounded by a relatively low death rate and a steep fall in new cases through winter, when cases elsewhere in the world surged. By February, the first wave that peaked six months earlier appeared to have vanished, and scientists were speculating that parts of India\u2019s population of almost 1.4bn had acquired a form of natural herd immunity, with seroprevalence studies suggesting around 22% of Indians had been infected by December. Then cases started grow faster and in greater numbers than ever, reaching more than 168,000 new infections on Sunday. The death toll is still relatively low among India\u2019s disproportionately young population, but is likely to soon eclipse the previous wave\u2019s peak of more than 1,200 a day. Indian health experts say the resurgence is being driven by an infectious new  \u201cdouble mutant\u201d variant , but have also blamed lax attitudes even among those who can afford to maintain social distance, including Bollywood actors such as Akshay Kumar, Alia Bhatt and Katrina Kaif, all of who have tested positive in recent weeks. \u201cDuring the earlier peak, one patient could spread the disease to 30-40% of his or her contacts,\u201d Randeep Guleria, a physician and member of the country\u2019s national taskforce on the virus, said at the weekend. \u201cThis time, 80-90% of people who come into contact with a patient turn positive.\u201d Narendra Modi, the prime minister, launched a \u201cfestival of vaccines\u201d on Sunday, urging Indians aged over 45 to apply for doses as soon as possible, but pressed on with a programme of several large rallies on Monday in the state of West Bengal, where his party is involved in a bitter election fight with a regional rival. India has administered more than 100m vaccine doses, but even with the largest manufacturer in the world, the Serum Institute of India, directing most of its supply for domestic use, shortages are being  reported in parts of the country . The scale of the task is also vast: if a rate of about 3m vaccines administered a day was maintained it would  take 21 months  to inoculate the 75% of the country thought to be required to achieve herd immunity, according to one estimate. Delivering the  10m a day that some health experts  say is required would dwarf the capacity of the country\u2019s vaccine manufacturers, a maximum of 112m doses a month according to government data. An expert panel on Monday approved the emergency use of Russia\u2019s Sputnik V, one of seven vaccines that the government hopes will be produced locally by the end of the year. The resurgence of the virus in India has had knock-on effects for global vaccination efforts. When rates were low at the beginning of this year, the Indian government allowed the export of nearly 65m doses as gifts or sales to foreign governments. It has reduced those export approvals as case numbers have grown, leaving fewer doses for customers including the UN-backed global vaccine-sharing mechanism Covax, a key source for developing countries."}, "210705_news_468464.txt": {"page_id": "210705_news_468464.txt", "text": "This article is part of the Technology Insight series, made possible with funding from Intel. As we create more content, deploy more sensors at the network\u2019s edge, and replicate more data for AI to contextualize, the demand for compute bandwidth  roughly doubles every three years . Keeping up is becoming increasingly difficult as modern computing architectures get closer and closer to the theoretical performance limits of electrical connections linking their processors, storage, and networking components. Silicon photonics technology\u2014a combination of silicon integrated circuits and semiconductor lasers\u2014may help\u00a0 overcome the bottlenecks imposed by electrical I/O, replacing copper connections with optical ones at the board and package level. According to James Jaussi, senior principal engineer and director of Intel\u2019s PHY research lab, miniaturized silicon photonics components open the door to architectures that are more disaggregated. That could look like pools of compute, memory, and peripheral functionality distributed throughout the system; connected over long distances with optical links, software-defined infrastructure, and high-speed networking. For now, integrated photonics is still the stuff of lab experiments. But a number of breakthroughs introduced during Intel\u2019s recent Labs Day show that the technology is capable of lower power, higher performance, and greater reach than today\u2019s server interconnects. KEY POINTS: Silicon photonics technology is already bringing down costs and improving availability of high-speed optical transceivers. The miniaturization of silicon photonics components opens the door to board-to-board and package-to-package optical I/O. Recent research promises to overcome impending performance and power scaling issues facing electrical I/O. Silicon photonics is already pervasive in the datacenter Today, silicon photonics technology is used in datacenters for connecting switches that might be miles apart. On one end, transceivers (devices able to transmit and receive) convert electrical signals to light, which is then sent across optical fiber. At the other end, those optical signals are changed back into electrical. What makes the conversion from electrical to optical a worthwhile endeavor? In short, higher bandwidth, coverage over greater distances, and an immunity to electromagnetic interference. But traditional optical transceivers are expensive. Their transmitter and receiver sub-assemblies must be carefully constructed and hermetically sealed for protection, which makes it difficult for manufacturers to keep up with demand. And the myriad of components that go into a transceiver take up significant space. Silicon photonics packs many of the optical and electronic pieces used to build a transceiver into highly integrated chips. These chips are manufactured in advanced fabs by the same machines that produce the latest CPUs, GPUs, and FPGAs. They enjoy the benefits of cutting-edge lithography, automation, and economies of scale, making them much smaller and less expensive than the technology they replace. Intel introduced its own family of 100 Gb/s transceivers based on semiconductor lasers back in 2016, ten years after demonstrating the technology alongside researchers from UC Santa Barbara. It quickly scored wins with performance-sensitive customers like  Microsoft\u2019s Azure cloud computing service . Since then, it has shipped more than four million 100 Gb/s modules, according to  Labs Day presentations . Above: Intel\u2019s hybrid silicon laser generates light at four wavelengths from a single laser cavity. Intel has its sights set on scaling optical I/O volumes several orders of magnitude higher though\u2014into the billions of devices. That would take optical beyond rack-to-rack communications in the datacenter and down to the board level, right onto the compute engines where electrical I/O currently dominates. Intel calls this research integrated photonics. Miniaturized silicon photonics as an electrical interconnect alternative If electrical I/O works so well between the server boards and processing packages, why look to silicon photonics as a replacement? Unfortunately, electrical interconnects are struggling to keep those resources fed, and every bit of speed-up comes at the cost of disproportionately more power consumption. There\u2019s a wall in sight, and that\u2019s making optical I/O an appealing alternative. \u00a0 Above: Over time, the power requirements of electrical I/O are trending larger than the total available socket power, leaving nothing for compute engines. Although silicon photonics transceivers offer notable advantages over traditional optical designs, their components are still too large, too expensive, and too power-hungry to displace electrical I/O within servers. The breakthroughs announced at Labs Day 2020 change this. Jaussi says there are six ingredients in the company\u2019s recipe for integrated photonics: light generation, amplification, detection, modulation, CMOS interface circuits, and package integration. Intel already has a hybrid silicon laser in its portfolio, which is used on its silicon photonics transceivers for converting electrical signals into light. So, it\u2019s focusing on the other five building blocks. What will it take to enable integrated photonics on compute packages? In a basic transmitter, the laser creates light onto which data is encoded by a modulator. Existing silicon modulators are large, and therefore expensive in the context of integrated photonics. New micro-ring modulators announced during Labs Day shrink this component\u2019s footprint by more than 1000x. Voltage supplied by a circuit above the modulator either traps light in the ring or allows it to travel down its waveguide. A detector at the other end interprets the absence or presence of light as zeroes and ones. The photodiodes in existing silicon photonics optical transceivers rely on materials like Germanium or Indium Phosphide to \u201csee\u201d light in the wavelengths used to move data. Silicon, it was thought, had no light detection capability in that range. Intel showed otherwise by using its all-silicon micro-ring structure as  a photodetector operating at 112 Gb/s . \u201cA major advantage of this development is processing and material cost reduction,\u201d says Jaussi. Intel multiplies the bandwidth through each fiber by capturing multiple wavelengths (or colors) of light from one laser. This technology is called wavelength division multiplexing. In his Labs Day demo, Jaussi showed four micro-rings trapping four separate wavelengths from a single optical channel to convey four bits of data. In the early days of silicon photonics research, this would have taken four different lasers, plus a multiplexer. Doing it with one is key to moving data fast enough in\u00a0 a space-constrained application like on-package I/O, where there isn\u2019t room for lots of laser firing next to each other. The addition of a semiconductor optical amplifier helps optimize integrated photonics systems for power consumption, since an amplifier provides light power more efficiently than the laser. These amplifiers are made from the same materials as the multi-wavelength laser\u2014an important consideration for manufacturing at volume. Above: Intel\u2019s integrated photonics prototype features an electronic CMOS IC stacked on top of a photonics IC in a 3D package, combining the benefits of silicon integrated circuits and semiconductor lasers. Combining cutting-edge photonics and cost-effective fabrication As part of Intel\u2019s Labs Day demonstration, Haisheng Rong, principal engineer at Intel Labs, showed off a photonic IC with the hybrid silicon laser, micro-ring modulators, an optical amplifier, and micro-ring photodetectors integrated together and manufactured in a high-volume CMOS fab. He was joined by fellow principal engineer Ganesh Balamurugan, who described the electrical IC responsible for driving and controlling Intel\u2019s micro-ring modulators. The two ICs are stacked, one on top of the other, and connected with copper pillars. \u201cThis is an example of how we can tightly integrate energy-efficient CMOS circuits with silicon photonics using 3D packaging,\u201d says Balamurugan. \u201cSuch cointegration is key to delivering performance and cost-optimized optical transceivers.\u201d By integrating silicon photonics building blocks with compute resources, Intel believes it can break the current trend of larger processors with more I/O pins, which are needed to satisfy growing bandwidth requirements. Silicon photonics makes it possible to achieve lower power consumption, greater throughput between compute elements, and reduced pin counts, all in a smaller footprint. Above: Back in March 2020, Intel announced the successful integration of its 1.6 Tb/s silicon photonics engine with its 12.8 Tb/s programmable Ethernet switch, putting optical I/O on the same package as a Barefoot Tofino 2 ASIC for the first time. The company is already showing off  high-performance Ethernet switch silicon co-packaged with silicon photonics engines , designed to address the power and cost/complexity issues posed by electrical I/O scaling limitations within two switch generations. It\u2019ll be longer before we see integrated photonics inside of servers\u2014Intel acknowledges that the technology isn\u2019t on the product implementation path yet. However, over time, the company hopes to scale its silicon photonics platform up to 1 Tb/s per fiber at 1pJ of energy consumed per bit, reaching distances of up to 1 km. With electrical I/O facing an impending power wall and silicon photonics already a successful component of Intel\u2019s networking catalog, this is a technology you\u2019ll want to keep an eye on. \u00a0  "}, "210705_news_468488.txt": {"page_id": "210705_news_468488.txt", "text": "als Vodafone und die Telekom im Sommer 2019  den Start ihrer 5G-Netze verk\u00fcndeten , klang das, als h\u00e4tte die Mobilfunk-Zukunft begonnen. Klar, um von den neuen Netzen zu profitieren, brauchte man die richtige Hardware, das richtige Smartphone und einen Mobilfunkvertrag, in dem 5G enthalten war. Und ja, 5G war damals eher die Stecknadel im Mobilfunkheuhaufen: F\u00fcr einen Praxistest musste ich durch die halbe Stadt fahren,  um eine 5G-Antenne zu finden . Aber es war 5G! Oder etwa nicht? Die Antwort auf diese Frage formuliert Vodafone an diesem Montag so: \u00bbWir l\u00fcften das gr\u00f6\u00dfte Mobilfunk-Geheimnis in Deutschland und machen den Weg frei f\u00fcr die Echtzeit: 5G steht in Deutschland zum ersten Mal auf eigenen Beinen.\u00ab Illustriert wird die Aussage mit dem Bild eines Kinderfahrrads, dem jemand die St\u00fctzr\u00e4der abschraubt. Die kleinen R\u00e4der sollen das LTE-Netz symbolisieren, auf das sich 5G bisher sozusagen gest\u00fctzt hat, und das nun nicht mehr als Hilfe gebraucht wird. 5G wird damit von der \u00e4lteren LTE-Technik unabh\u00e4ngig, die Rede ist nun von \u00bb5G Standalone\u00ab (5G SA). H\u00f6here Daten\u00fcbertragungsraten schafft 5G dadurch nicht, im Gegenteil. Vorerst ist die Geschwindigkeit von 5G Standalone auf 700 Megabit pro Sekunde (Mbit/s) limitiert, w\u00e4hrend in den LTE-gest\u00fctzten 5G-Netzen bis zu 1 Gigabit pro Sekunde m\u00f6glich ist. Trotzdem wird 5G schneller, denn die sogenannte Latenz, die Zeit, die Daten brauchen, um durchs Netz geschickt zu werden, wird deutlich k\u00fcrzer. Davon profitieren Endnutzer beispielsweise bei Onlinespielen oder Augmented Reality (AR). Wichtiger ist diese Verbesserung aber f\u00fcr industrielle Anwendungen, wie etwa die Fernsteuerung von Maschinen, die nun quasi in Echtzeit erfolgen kann. Die andere Neuerung wird als \u00bbNetwork Slicing\u00ab bezeichnet. Sie erm\u00f6glicht es, das 5G-Netz wie einen Laib Brot in mehrere unterschiedlich dicke Scheiben zu schneiden und jeder dieser Scheiben bestimmte Attribute zuzuschreiben. Vodafone nennt als Beispiel das 5G-Netz in einem Fu\u00dfballstadion, das man so aufteilen kann, dass f\u00fcr die Live\u00fcbertragung von TV-Bildern in Echtzeit ein garantiertes Datenbudget freigehalten wird, w\u00e4hrend f\u00fcr die Zuschauer eine andere Scheibe des Netzes bereitsteht. F\u00fcr Normalnutzer d\u00fcrfte im Alltag allerdings wichtiger sein, dass 5G Standalone die Akkulaufzeit von Smartphones potenziell um 20 Prozent verl\u00e4ngern kann, weil diese sich nur noch in ein Netz einbuchen und nicht parallel Kontakt zu 5G und LTE halten m\u00fcssen. Wie beim ersten Start von 5G wird man von der neuen 5G-Version, die zun\u00e4chst an tausend Mobilfunkantennen scharf geschaltet wurde, nur profitieren k\u00f6nnen, wenn man ein Smartphone besitzt, das diese Technik auch unterst\u00fctzt. Laut Vodafone ist dazu bislang nur das Oppo Find X3 Pro ( unseren Testbericht finden Sie hier ) in der Lage \u2013 und das auch erst nach einem Softwareupdate. Weitere kompatible Ger\u00e4te werden im Laufe des Jahres erwartet. Anders als Vodafone hat die Telekom noch keinen Starttermin f\u00fcr 5G Standalone genannt. Das Unternehmen  testet die Technik derzeit in Garching bei M\u00fcnchen  und meldete von dort Anfang M\u00e4rz die  erste erfolgreiche 5G-Standalone-Datenverbindung in Deutschland . Telefonica O2 hat den Start von 5G Standalone bisher nur grob  f\u00fcr 2021 angek\u00fcndigt . Fremdlinks: drei Tipps aus anderen Medien \u00bbApples CEO is making very different choices from Mark Zuckerberg\u00ab  (Englisch, 36 Minuten) Nur selten gibt Apple-Chef Tim Cook Interviews und schon gar nicht als Podcast. F\u00fcr die US-Journalistin Kara Swisher und ihren \u00bbSway\u00ab-Podcast hat er jetzt eine Ausnahme gemacht. Im Interview spricht er \u00fcber den Sturm auf das Kapitol, die Chat-App Parler, soziale Medien, Apples Anstrengungen im Bereich Automobilit\u00e4t und dar\u00fcber, wie lange er seinen Posten noch ausf\u00fcllen will. \u00bbDas Google i/o 2021 Puzzle\u00ab , (Englisch, viele Knobelminuten) Nach einer coronabedingten Pause 2020 wird Google seine Entwicklerkonferenz i/o in diesem Jahr wieder veranstalten. Als reines Onlineevent, versteht sich. Zwar ist die kostenlose Registrierung mittlerweile freigeschaltet, doch das sollte Sie nicht davon abhalten, sich an dem R\u00e4tsel zu versuchen, hinter dem der Konzern Informationen zu der Veranstaltung zun\u00e4chst versteckt hatte. \u00bbW\u00e4chter, Teil 1\u00ab  (Russisch, 50 Minuten) Zugegeben, dieser Tipp ist nur etwas f\u00fcr Hardcore-Tolkien-Fans, die vorzugsweise auch noch Russisch verstehen. Es handelt sich um eine verschollen geglaubte Verfilmung von J.R.R. Tolkiens \u00bbHerr der Ringe\u00ab aus den Neunzigerjahren, die jetzt wiederentdeckt wurde und so ganz anders ist, als der mehrere Hundert Millionen Dollar teure Dreiteiler von Peter Jackson. Genie\u00dfen Sie das langsam nahende Fr\u00fchlingswetter \u2013 und bleiben Sie gesund! Matthias Kremp"}, "210705_news_468510.txt": {"page_id": "210705_news_468510.txt", "text": "Americans know our nation has been shaken by the events of the past year or so. COVID-19 is only the start of what threw us off track. But reports from the U.S. intelligence community should serve as a warning that we are not alone in having been disrupted, and we\u2019d better be prepared for the consequences. In its annual Global Trends report, the National Intelligence Council said the pandemic is  \u201cthe most significant singular global disruption since World War II, with health, economic, political and security implications that will ripple for years to come.\u201d It is important for ordinary Americans to understand the weight of such a statement, as in many ways the nations of the world are not done experiencing the repercussions of the events of World War I and its aftermath, let alone World War II. All over the planet, people are experiencing  \u201cnew uncertainties about the economy, governance, geopolitics, and technology,\u201d  according to the report. Think that has nothing to do with us? Think again. \u201cState and nonstate rivals will vie for leadership and dominance in science and technology with potentially cascading risks and implications for economic, military, and society security,\u201d  the report said. We are going to be pre-occupied with our own recovery for quite some time, particularly given the massive domestic spending efforts to rebuild (should they come to fruition). Many eyes will be turned inward, and that is important. It must not be at the expense of our safety from threats created by turmoil being experienced around the globe.  Our intelligence and military communities will need to double their efforts, for many years to come. NEWSLETTER Today's breaking news and more in your inbox"}, "210705_news_468514.txt": {"page_id": "210705_news_468514.txt", "text": "When life gives you lemons, make lemonade \u2013 so the saying goes. In Belgium, the wording is a little different: when life gives you poop, grow flowers. The Belgium-born cleaning brand Ecover took this instruction almost literally. After experiencing strong sales during the Covid-19 pandemic, the brand decided to \u201cfertilise the future\u201d, by pledging to fund three eco pioneers of tomorrow, whose ideas are founded on nature-based solutions. \u201cWhen the pandemic hit, we were really influenced by people like Arundhati Roy, who talked about the pandemic as a \u2018portal \u2013 an opportunity for a reset moment\u2019\u201d, says Tom Domen, Ecover\u2019s head of long-term innovation. \u201cWe challenged ourselves to think how we could use this crisis to identify new ways to address the challenges posed by the climate and ecological emergency. While many businesses were struggling, sales of cleaning products were increasing. Could we \u2018fertilise\u2019 a more positive and hopeful beginning out of a time of crisis?\u201d Several brainstorms later, the Fertilise the Future fund was born: a competition to find innovative organisations across Europe that promote nature-based solutions \u2013 concepts that work with, and enhance, nature to help mitigate the impact of the climate crisis on people. \u201cCrucially, nature-based solutions deliver additional benefits,\u201d Domen says. \u201cSuch as biodiversity, carbon capture and socio-economic benefits. The concept is grounded in the knowledge that healthy, natural and managed ecosystems produce a diverse range of services on which human wellbeing depends.\u201d Ecover created four categories for entries, which mirrored the key areas in need of scale-up, as set out by the Nature-based Solutions (NBS) Coalition following the UN Climate Action summit. These were: a) the conservation and restoration of forest and other terrestrial ecosystems; b) the conservation and restoration of freshwater resources; c) sustainable agriculture and food systems; and d) ensuring nature\u2019s systemic role in sustainable development. \u201cWe had never done anything like this before,\u201d Domen says. \u201cSo we spoke to sustainability experts outside the business and our creative partners, who helped us shape the idea into what we launched in June last year.\u201d The response was beyond Ecover\u2019s expectations, with more than 770 entries across Europe. The entries were whittled down to 21 finalists across three countries \u2013 the UK, Belgium and Germany \u2013 with one winner in each. \u201cOur initial response to seeing the entries was just how many ideas and organisations are in need of support,\u201d says Domen. \u201cAnd also how many great, innovative ideas and companies are out there. A number of times throughout the judging we wished our funding pot was greater to support more causes.\u201d Of the shortlisted finalists, nine were based in the UK, including the University of Leeds\u2019 opportunity to transform Hardknott Forest in the Lake District to wild native woodland; the climate charity, Possible, and its community hedgerow planting; and the Wildlife Trust for Birmingham and the Black Country\u2019s project to restore the River Stour for people and wildlife. On the subject of waterways, other finalists included Kent Wildlife Trust\u2019s ambition to introduce water buffalo to restore an area of ancient fenland; Norfolk Rivers Trust\u2019s mission to build wetlands to filter water and counteract pollution; the Marine Conservation Society\u2019s project to prove the ocean can capture carbon at sea at up to 10 times the rate as the same area on land; and the Floodplain Meadows Partnership\u2019s restoration of floodplain meadows. Somerset Wildlands pledged to bring back lost wildlife through rewilding, while Action for Conservation\u2019s Penpont project, led by 20 teenagers, aimed to restore woodlands and waterways, and implement sustainable food systems. \u201cWe spoke to every finalist and we were impressed with a number of elements,\u201d Domen says. \u201cFirst, the passion and dedication behind each project. Second, the scientific rigour embedded into a number of the projects. And third, the human element of each project.\u201d After the first phase of judging was completed by the Ecover team, external experts were brought in. In the UK, the judges were Nathalie Seddon, professor of biodiversity at the University of Oxford and director of the Nature-based Solutions Initiative; and Natalie Fee, award-winning environmentalist, author of How to Save the World for Free, and founder of City to Sea, a UK-based organisation running campaigns to stop plastic pollution at source. \u201cBoth judges encouraged us to think about where we could make the biggest difference with our funding. They reviewed all the finalists and were very supportive of the impact of each of them,\u201d Domen says. \u201cBut ultimately, they encouraged us to go where they thought nature needed us the most \u2013 which led us to our final decision.\u201d The  Floodplain Meadows Partnership project  \u2013 formed of Long Mead Wildlife Site\u2019s Thames Valley Wildflower Meadow Restoration Project, Berks, Bucks and Oxon Wildlife Trust and The Open University \u2013 was chosen to restore floodplain meadows along the River Thames and to promote their benefits. \u201cThe Floodplain Meadows Partnership project brings a welcome sigh of relief to anyone who\u2019s concerned about the catastrophic loss of biodiversity in the UK,\u201d says Fee. \u201cWith 97% of wildflower meadows lost since the 1930s, our bee and butterfly populations have also diminished sharply, with many species facing extinction. This project will not only restore some of that habitat, but will provide important and pioneering data on whether floodplain meadows store even more carbon than woodlands.\u201d Both Fee and Seddon praised the winner\u2019s collaborative approach. \u201cIt is well planned, based in strong local partnerships, and involves collaboration across science, policy and practice,\u201d says Seddon. \u201cThe project also has a scientifically robust methodology for evaluating the multiple societal and ecological benefits of floodplain restoration over time, for example in terms of carbon, biodiversity and flood abatement, as well as for local communities, and has great potential for scaling in the UK and beyond.\u201d The fight against the climate crisis is not one we can win alone. But so much can be done when we work together \u2013 from individual action through to policymakers. \u201cWe know that as one brand there\u2019s a limit to what we can achieve,\u201d says Domen. \u201cWe are working on reducing our own carbon and plastic footprints, and our formulas are designed to reduce their impact on the environment. But we know that\u2019s not enough. We have to go further and work in partnership with others if we\u2019re going to help tackle the crisis we\u2019re facing.\u201d Ecover was founded 40 years ago in Belgium with the ambition to pioneer cleaning products that reduce their impact on the environment. Find out more at  ecover.com/fund"}, "210705_news_468521.txt": {"page_id": "210705_news_468521.txt", "text": "The apparent attack by Israel  on Iran\u2019s nuclear enrichment facility  appears to be the latest episode in an increasing tit-for-tat cyberwar. Both sides have already targeted so-called industrial control systems [ICS], which have emerged as a key weakness for countries across the globe. While  Iran  described the latest attack as \u201csabotage\u201d, Israeli media called it a cyber-attack. The vulnerability of ICS systems, and similar so-called \u201coperating technology\u201d used in industrial processes and large infrastructure plants \u2013 from electrical grids, to steel, chemical and water treatment plants \u2013 was demonstrated more than a decade ago by revelation of the  US-Israeli Stuxnet malware attack  on the Natanz plant. Since the emergence of the Stuxnet virus, attempts to hack and exploit ICS systems have emerged as one of the most dangerous and contested frontlines in cyberwarfare around the world, with officials in the Biden administration last week revealing a planned executive order to beef up US defences. The appeal of cyber-attacks via operational technology is that \u2013 unlike more conventional hacking to steal data \u2013 they are aimed at a physical impact, whether a power blackout, water contamination or causing systems to overrun and become damaged, even explode. Iran, whose nuclear efforts have historically relied on Siemens industrial control technology, one of the gateways attacked by Stuxnet, is particularly vulnerable to these kinds of attack because of embargos on the transfer of technology that could be used in the programme to protect control systems. In 2010, Stuxnet was among the most sophisticated malware ever detected, reportedly damaging as many as one-fifth of the nuclear centrifuges in Iran. That attack, however, led Iran to develop its own ability for cyber-attacks on critical infrastructure led by a group of hackers known as APT33, which has recently shifted its interest from IT networks to ICS. Last year, Israeli media blamed Iranian hackers for two attacks on water treatment plants in the country. While attacks like the latest at Natanz have inevitably grabbed the headlines, they are only the most obvious evidence of a continuing cyber conflict. Last autumn, an Iranian news agency reported cyber-attacks had hit the electronic infrastructure of the country\u2019s ports. An official said: \u201cSworn enemies have been trying for some time to carry out cyber-attacks.\u201d While a report by researchers at Tenable two years ago suggested that countries had become better at protecting infrastructure against the threat of attacks similar to Stuxnet, they still found numerous vulnerabilities in ICS systems."}, "210705_news_468557.txt": {"page_id": "210705_news_468557.txt", "text": "Der chinesische Huawei-Konzern stellt sich auf noch lange andauernde US-Sanktionen ein und setzt verst\u00e4rkt auf neue Gesch\u00e4ftsbereiche wie selbstfahrende Autos. Allein in diesem Jahr werde mehr als eine Milliarde US-Dollar, etwa 840 Millionen Euro in die Entwicklung der Roboterwagen-Technologie investiert, sagte Huawei-Chef Eric Xu am Montag. \"Huawei Inside\" Zugleich halte Huawei an der 2018 getroffenen Entscheidung fest, kein eigenes Auto zu bauen, sondern als Partner ausgew\u00e4hlter Hersteller aufzutreten, betonte er. Bisher gebe es Vereinbarungen mit drei chinesischen Unternehmen, die Untermarken mit Huawei-Technik an Bord auf den Markt bringen wollen. Ein Logo \u2013 \"Huawei Inside\" \u2013 werde darauf hinweisen, dass in dem gemeinsam entwickelten Auto Technik zum autonomen Fahren von Huawei stecke. Es werde nur wenige Kooperationen dieser Art geben, sagte Eric Xu. Zugleich werde sich die Investition f\u00fcr Huawei rentieren, selbst wenn man sich auf den chinesischen Heimatmarkt mit aktuell 30 Millionen verkauften Fahrzeugen pro Jahr beschr\u00e4nken sollte. US-Sanktionen Mit Blick auf die US-Sanktionen sagte Xu, die Strategie von Huawei sei darauf ausgelegt, unabh\u00e4ngig von der Politik Washingtons zu \u00fcberleben und zu wachsen. Huawei gehe davon aus, noch lange Zeit  auf der schwarzen Liste der US-Regierung zu stehen . Der damalige US-Pr\u00e4sident Donald Trump hatte den chinesischen Smartphone-Anbieter und Netzwerk-Ausr\u00fcster 2019 mit Sanktionen belegt \u2013 und die Ma\u00dfnahmen im vergangenen Jahr auf den Chip-Bereich ausgeweitet. Huawei verlor dadurch den Zugang  zu wichtigen Techniken aus dem Westen . Seit der Konzern keine neuen Smartphones mit Google-Diensten verkaufen kann, st\u00fcrzten die Verk\u00e4ufe der Ger\u00e4te au\u00dferhalb Chinas ab. Auf die Chip-Blockade reagierte Huawei mit Vorratsk\u00e4ufen, die nach Einsch\u00e4tzung von Experten zur aktuellen weltweiten Halbleiter-Knappheit beitrugen. Eric Xu machte am Montag die US-Sanktionen f\u00fcr die Engp\u00e4sse verantwortlich. Trump begr\u00fcndete seine Ma\u00dfnahmen mit der Gefahr von Spionage und Sabotage durch Huawei, der Konzern weist die Vorw\u00fcrfe zur\u00fcck. Der neue US-Pr\u00e4sident Joe Biden stellte Trumps China-Ma\u00dfnahmen auf den Pr\u00fcfstand, es gab bisher aber keine Hinweise darauf, dass die Sanktionen gegen Huawei aufgehoben werden k\u00f6nnten. ( olb )"}, "210705_news_468560.txt": {"page_id": "210705_news_468560.txt", "text": "\nGlobale Gesellschaft\n In Reportagen, Analysen, Fotos, Videos und Podcasts berichten wir weltweit \u00fcber soziale Ungerechtigkeiten, gesellschaftliche Entwicklungen und vielversprechende\u00a0Ans\u00e4tze\u00a0f\u00fcr die L\u00f6sung globaler Probleme. Alle Artikel Im Bundesstaat Meghalaya im Nordosten Indiens lassen die Khasi, ein indigenes Volk, Br\u00fccken wachsen, die lebendig sind: Sie lenken \u00fcber der Erde wachsende Wurzeln des Gummibaums so, dass sie \u00fcber die Jahrzehnte hinweg \u00fcber Fl\u00fcsse und Schluchten ranken, stabilisieren sie mit ausgeh\u00f6hlten St\u00e4mmen und anderem Material, bis die Wurzeln auch auf der anderen Seite fest im Boden verwachsen. Anders als klassische Br\u00fccken trotzen die flexiblen \u00dcberg\u00e4nge auch St\u00fcrmen, \u00dcberschwemmungen und Erdbeben in einer der regenreichsten Regionen der Welt \u2013 manche Konstrukte w\u00e4hren jahrhundertelang. Bild vergr\u00f6\u00dfern Wurzelbr\u00fccken k\u00f6nnen auch starke Unwetter \u00fcberstehen \nFoto: Barcroft Media\u00a0/ Getty Images\n Von indigenen Baumeistern, die mit der Natur arbeiten, statt sie zu unterwerfen, lassen sich heute auch westliche Architekten und St\u00e4dteplaner inspirieren. Neue Studieng\u00e4nge wie \u00bbGreen Technologies in Landscape Architecture\u00ab an der Technischen Universit\u00e4t M\u00fcnchen  treiben die Baubotanik voran . Die Forscher und Architekten versuchen Elemente wie Stahl und Beton mit B\u00e4umen zu verbinden \u2013 und entwerfen etwa Stege, die von Weiden gest\u00fctzt werden. Der Corona-Zeitgeist f\u00f6rdert die R\u00fcckbesinnung auf Natur,  Gemeinschaft  und lokalen Lebensraum: Lockdowns haben den Bewegungsradius vieler Menschen auf ihre unmittelbare Umgebung beschr\u00e4nkt. St\u00e4dte weltweit wollen wieder gr\u00fcner werden, sie  d\u00e4mmen den Verkehr  ein und  bauen Fahrradrouten aus . Zoonosen  wie das Coronavirus, aber auch  Folgen des Klimawandels  wie Fluten und Br\u00e4nde offenbaren zudem, wie hoch der Preis f\u00fcr die Zerst\u00f6rung der Umwelt ist. Auf viele moderne Herausforderungen k\u00f6nnten indigene Konzepte Antworten liefern \u2013 und die St\u00e4dte der Zukunft nicht nur lebenswerter, sondern auch resilienter gestalten. Julia Watson, die an den Universit\u00e4ten Harvard und Columbia Urban Design lehrt, beschreibt in ihrem Buch \u00bbLo-TEK. Design by Radical Indigenism\u00ab, wie indigene Innovation im Einklang mit der Natur aussehen kann \u2013 von schwimmenden Inseln aus Schilf in  Peru  oder im  Irak  bis hin zu nat\u00fcrlichen Recycling- oder Bew\u00e4sserungssystemen in  Indien  oder  Indonesien . Die australische Designerin und Landschaftsarchitektin beobachtet derzeit \u00bbeine Art Renaissance indigenen Denkens\u00ab: Neue Initiativen w\u00fcrden entstehen, die dieses Wissen in Nachhaltigkeitsdebatten einbringen, und auch indigene Expertinnen und Experten selbst f\u00e4nden zunehmend Geh\u00f6r. Bild vergr\u00f6\u00dfern Die Designerin Julia Watson erforscht indigene Techniken weltweit \nFoto: Aria Isadora\n Lange galt das traditionelle Wissen als primitiv, wurde von offiziellen Stadt- und Landschaftsplanern ignoriert: \u00bbIn der Zeit der Aufkl\u00e4rung und der industriellen Revolution ist eine westliche Vorstellung von Technologie und Fortschritt entstanden, die mit dem Kolonialismus Zivilisationen weltweit aufgezwungen wurde\u00ab, sagt Watson. \u00bbHightech-Einheitsl\u00f6sungen widersprechen aber der Vielfalt und Komplexit\u00e4t von \u00d6kosystemen.\u00ab Indigene Techniken w\u00fcrden Watson zufolge auf ortsbasiertem Wissen beruhen \u2013 die Gemeinschaften \u00bbh\u00f6ren der Natur zu\u00ab. Detaillierte Kenntnisse \u00fcber lokale Flora und Fauna, Erde, Feuer oder Klima flie\u00dfen in diese Techniken ein, die \u00fcber Tausende von Jahren hinweg verfeinert und weitergegeben werden, von Generation zu Generation. Bild vergr\u00f6\u00dfern Hohe Kunst: Die \u00bbSubak\u00ab-Bew\u00e4sserungssysteme in Indonesien sind mittlerweile Weltkulturerbe \nFoto: Athanasios Gioumpasis\u00a0/ Getty Images\n In asiatischen L\u00e4ndern filtern Farmer in treppenf\u00f6rmigen Reisterrassen etwa seit Tausenden von Jahren Regenwasser \u2013 die Anlagen erm\u00f6glichen ein komplexes Wasser- und N\u00e4hrstoffmanagement, zugleich entstehen so Biotope, in denen sich V\u00f6gel und andere Tiere ansiedeln. Das Prinzip hat die thail\u00e4ndische Landschaftsarchitektin Kotchakorn Voraakhom in die Gro\u00dfstadt gebracht \u2013 und die Thammasat-Universit\u00e4t in  Bangkok  in eine  gigantische Rooftop-Farm verwandelt . Das Dach der Universit\u00e4t ist heute Outdoor-Klassenzimmer, Park und Anbaufl\u00e4che zugleich. Solarzellen speichern Sonnenenergie, die Terrassen absorbieren Regen und sch\u00fctzen das Geb\u00e4ude vor \u00dcberschwemmungen. Mit ihrem Netzwerk Porous City Network will die Architektin weitere \u00f6ffentliche Fl\u00e4chen in solche produktiven Orte verwandeln, die dem Klimawandel trotzen, die Luft verbessern und die Artenvielfalt erh\u00f6hen. Bild vergr\u00f6\u00dfern Die Thammasat-Universit\u00e4t in Bangkok \u00e4hnelt einem Berg mit Reisterrassen \nFoto: Landprocess\n Auch die indigene Technik der Aquaponik, die symbiotische Zucht von Pflanzen und Fisch, hat sich weltweit verbreitet. Indonesische Farmer legen Wasserbassins an, in denen Reispflanzen durch die n\u00e4hrstoffreichen Ausscheidungen der Fische besser wachsen. Die Pflanzen bieten wiederum den Fischen Schutz und Schatten \u2013 und ziehen Insekten an, die als Fischfutter dienen. In  Berlin  produziert die ECF Farm auf \u00e4hnliche Weise  Buntbarsch und Basilikum  und verkauft die Erzeugnisse an Supermarktketten. Und das Wissen der Aborigines k\u00f6nnte k\u00fcnftig dabei helfen, Br\u00e4nde einzud\u00e4mmen \u2013 die australischen Ureinwohner bek\u00e4mpfen Feuer mit Feuer. Mit rituellen, kleineren Feuern zu k\u00fchleren und feuchteren Jahreszeiten verbrennen sie kontrolliert trockenes Gras und Unterholz, um zu verhindern, dass das Material sich im hei\u00dfen Sommer entz\u00fcndet und Fl\u00e4chenbr\u00e4nde ausl\u00f6st.  Firesticks , eine indigene Nichtregierungsorganisation, versucht, das traditionelle Feuermanagement bekannter zu machen. Bild vergr\u00f6\u00dfern Aborigines verhindern Gro\u00dfbr\u00e4nde mit traditionellen Techniken \u2013 je nach Boden, Bepflanzung und Jahreszeit \nFoto: Rafael Ben-Ari/ Chameleons Eye\u00a0/ ddp images/Newscom\n St\u00e4dte und Regionen von  Portugal  bis in die  USA  k\u00f6nnten theoretisch von den Pr\u00e4ventionsans\u00e4tzen gegen Br\u00e4nde profitieren. \u00bbEs geht aber nicht darum, zwingend jede indigene Idee aus  Australien ,  Afrika  oder  Lateinamerika  nach New York oder in europ\u00e4ische St\u00e4dte zu transferieren\u00ab, schr\u00e4nkt Watson ein. \u00bbMan muss immer analysieren, ob sich Ans\u00e4tze innerhalb von \u00e4hnlichen Klimazonen oder \u00d6kosystemen \u00fcbertragen lassen.\u00ab Doch indigenes Denken ist mehr als ein Tool, das sich nur in einem bestimmten Kontext verwenden l\u00e4sst. F\u00fcr die Zukunftsforscherin und Designerin Monika Bielskyte stellt indigenes Wissen das bisher g\u00e4ngige Gestaltungsprinzip des \u00bbHuman-centered Design\u00ab, das den Menschen in den Mittelpunkt stellt, radikal auf den Kopf \u2013 hin zu einem \u00bbLife-centered Design\u00ab. Nicht menschliche Organismen sind bei dieser Herangehensweise ebenso wichtig wie Menschen; Erde, Bakterien, Pilze oder Algen spielen als Infrastruktur eine zentrale Rolle bei der Gestaltung von k\u00fcnftigen St\u00e4dten. \u00bbDas indigene Denken kann uns helfen, unser Verh\u00e4ltnis zu unserer Lebenswelt und auch zueinander in den St\u00e4dten der Zukunft neu zu gestalten\u00ab, sagt Bielskyte, die in  S\u00fcdafrika  lebt. \u00bbMan wird aufh\u00f6ren, einzelne Geb\u00e4ude und voneinander getrennte R\u00e4ume zu entwerfen \u2013 und anfangen, in \u00d6kosystemen zu denken.\u00ab Bild vergr\u00f6\u00dfern Neue Lebensentw\u00fcrfe statt neue Wolkenkratzer: Indigenes Denken ver\u00e4ndert die Herangehensweise an Architektur und St\u00e4dtebau \nFoto: Liu Qinglin\u00a0/ VCG / Getty Images\n Bielskyte erforscht derzeit, wie organische Materialien wie Pilze als Bauelemente eingesetzt werden k\u00f6nnen. Sie beobachtet auch, dass die Dringlichkeit von \u00bbRewilding\u00ab deutlicher wird \u2013 Stadtplaner verwandeln zunehmend Teile von Betonw\u00fcsten zur\u00fcck in nat\u00fcrliche Korridore, indem sie zum Beispiel einheimische Pflanzen auss\u00e4hen, St\u00e4dte also wieder verwildern lassen. In ihrem neuen Kollektiv  Protopia Futures  will Bielskyte Kreative, Wissenschaftler und Praktiker aus aller Welt zusammenbringen, um alternative, inklusive Zukunftsvisionen zu schaffen, die queere und indigene Perspektiven sowie Erfahrungen von Menschen mit Behinderungen vereinen \u2013 als Gegenentwurf zur Mainstream-Science-Fiction. Bild vergr\u00f6\u00dfern Fliegendes \u00bbBaykar\u00ab-Auto: Nicht alle Zukunftsvisionen, die technisch m\u00f6glich sind, sind wirklich sinnvoll \nFoto: Anadolu / Getty Images\n \u00bbDie Zukunft beginnt mit Fantasie\u00ab, sagt sie. \u00bbIch bin gespannt, was passieren w\u00fcrde, wenn jemand aus der jungen, indigenen Generation die alte Riege der Stararchitekten wie  Zaha Hadid , Rem Koolhaas oder  Frank Gehry  abl\u00f6sen w\u00fcrde und die Macht h\u00e4tte, die Stadt neu zu gestalten.\u00ab Die bisherigen Zukunftsvisionen seien haupts\u00e4chlich von wei\u00dfen, privilegierten, heterosexuellen M\u00e4nnern gepr\u00e4gt worden. \u00bbDie Visionen von Wolkenkratzern oder fliegenden Autos haben uns in unserem Denken dar\u00fcber, was futuristisch ist und was nicht, fehlgeleitet \u2013 wir m\u00fcssen hinterfragen, was wir wirklich brauchen\u00ab, glaubt Bielskyte. \u00bbEin Himmel, der mit fliegenden Autos \u00fcbers\u00e4t ist, w\u00e4re ein Albtraum.\u00ab Dieser Beitrag geh\u00f6rt zum Projekt Globale Gesellschaft Bereich \nWas ist das Projekt Globale Gesellschaft?\n aufklappen Unter dem Titel \u00bbGlobale Gesellschaft\u00ab berichten Reporterinnen und Reporter aus  Asien, Afrika, Lateinamerika und Europa  \u2013 \u00fcber Ungerechtigkeiten in einer globalisierten Welt, gesellschaftspolitische Herausforderungen und nachhaltige Entwicklung. Die Reportagen, Analysen, Fotostrecken, Videos und Podcasts erscheinen im Auslandsressort des SPIEGEL. Das Projekt ist langfristig angelegt und wird \u00fcber drei Jahre von der Bill & Melinda Gates Foundation (BMGF) unterst\u00fctzt. Ein ausf\u00fchrliches FAQ mit Fragen und Antworten zum Projekt finden Sie  hier . Bereich \nWie sieht die F\u00f6rderung konkret aus?\n aufklappen Die Bill & Melinda Gates Foundation (BMGF) unterst\u00fctzt das Projekt \u00fcber drei Jahre mit einer Gesamtsumme von rund 2,3 Mio. Euro. Bereich \nSind die journalistischen Inhalte unabh\u00e4ngig von der Stiftung?\n aufklappen Ja. Die redaktionellen Inhalte entstehen ohne Einfluss durch die Gates-Stiftung. Bereich \nHaben auch andere Medien \u00e4hnliche Projekte?\n aufklappen Ja. Gro\u00dfe europ\u00e4ische Medien wie \u00bbThe Guardian\u00ab und \u00bbEl Pa\u00eds\u00ab haben mit \u00bbGlobal Development\u00ab beziehungsweise \u00bbPlaneta Futuro\u00ab \u00e4hnliche Sektionen auf ihren Nachrichtenseiten mit Unterst\u00fctzung der Gates-Stiftung aufgebaut. Bereich \nGab es beim SPIEGEL bereits \u00e4hnliche Projekte?\n aufklappen Der SPIEGEL hat in den vergangenen Jahren bereits zwei Projekte mit dem European Journalism Centre (EJC) und der Unterst\u00fctzung der Bill & Melinda Gates Foundation umgesetzt: die \u00bb Expedition \u00dcberMorgen \u00ab \u00fcber globale Nachhaltigkeitsziele sowie das journalistische Fl\u00fcchtlingsprojekt \u00bb The New Arrivals \u00ab, in deren Rahmen mehrere preisgekr\u00f6nte Multimedia-Reportagen zu den Themen Migration und Flucht entstanden sind. Bereich \nWo finde ich alle Ver\u00f6ffentlichungen zur Globalen Gesellschaft?\n aufklappen"}, "210705_news_468588.txt": {"page_id": "210705_news_468588.txt", "text": "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      > I'll take \"isn't the worst\". ;) You should! It was definitely a compliment. > I'd love to get more of your thoughts On a technical level, i would say PowerShell is a breakthrough because it democratized the concept of structured data REPL as a shell. This pattern was well-known to GNU (and other LISP) hackers but not very popular otherwise, so thank you very much for that. Despite that, having telemetry in a shell is a serious problem in my view. That, and other technical criticisms  others have mentioned (see previous HN discussions about PowerShell) is why i don't use PowerShell more. On a more meta level, i'd say the biggest missing feature of the software is self-organization (or democracy if you'd rather call it that). The idea is great but the realization is far from perfect. Like most products pushed by a company, PowerShell is being developed by a team who has their own agenda/way and does not take time/energy to gather community feedback on language design. I believe no single group of humans can figure out the best solutions for everyone else, and that's why community involvement/criticism is important. For this reason, despite being much more modest in the current implementation, i believe NuShell being the child of many minds has more potential to evolve into a more consistent and user-friendly design in the future. Beyond that, i have a strong political criticism of Microsoft as part of the military industrial complex, as a long-standing enemy to free-software (still no Github or Microsoft XP source code in sight despite all the ongoing openwashing) and user-controled hardware (remember when MS tried to push for SecureBoot to not be removable in BIOS settings?), as an unfair commercial actor abusing its monopoly (forced sale of Windows with computers is NOT ok, and is by law illegal in many countries) and more generally as one among many corporations in this capitalist nightmare profiting from the misery of others and contributing its fair share the destruction of our environment. This is not a personal criticism (i don't even know you yet! :)) so please don't take it personally. We all make questionable ethical choices at some point in life to make a living (myself included), and i'm no judge of any kind (i'll let you be your own judge if you let me be mine). In my personal reflection about my own life, I found some really good points in this talk by Nabil Hassein called \"Computing, Climate Change, and All our Relationships\", about the human/political consequences of our trade as global-north technologists. I strongly recommend anyone to watch it:  https://nabilhassein.github.io/blog/computing-climate-change... > how PowerShell might be more useful for the kinds of scenarios you're thinking about I don't think i've seen any form of doctests in PowerShell. I think that would be a great addition for many people. A test suite in separate files is fine when you're cloning a repo, but scripts are great precisely because they're single files that can be passed around as needed. > Structured shells have so much potential outside of that, though. Indeed! If they're portable enough, have some notion of permissions/capabilities and have a good type system they'd make good candidates as scripting languages to embed in other applications because these applications usually expose structured data and some form of DSL, so having a whole shell ecosystem to develop/debug scripts would be amazing. I sometimes wonder what a modern, lightweight and consistent Excel/PowerShell frankensteinish child would look like. Both tools are excellent for less experienced users and very functional from a language perspective. From a spreadsheet perspective, a structured shell would for example enable better integration with other data sources (at a cost of security/reproducibility but the tradeoff is worthwhile in many cases i think). From a structured shell perspective, having spreadsheet features to lay data around (for later reuse, instead of linear command history) and graph it easily would be priceless. > I'd love to get to a place one day where we could treat arbitrary datasets like that, sort of like a generalized SQL/JSON/whatever REPL. Well that's precisely what nushell's \"from\" command is doing, supporting CSV, JSON, YAML, and many more!  https://www.nushell.sh/book/command_reference.html  no SQL there yet ;-) PS: I wish you the best and hope you can find some time to reflect on your role/status in this world. And i hope i don't sound too condescending, because if you'd asked me yesterday what i would tell a microsoft higher-up given the occasion, it would have been full of expletives :o... so here's me trying to be friendly and constructive as much as possible, hoping we can build a better future for the next generation. Long live the Commune (150th birthday this year)!                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        "}, "210705_news_468638.txt": {"page_id": "210705_news_468638.txt", "text": "Yuriko Nakao | Getty Images Governments around the world may start to clamp down on the use of  bitcoin  and other cryptocurrencies, the CEO of a top crypto exchange has warned. A number of officials \u2014 from U.S. Treasury Secretary  Janet Yellen  to European Central Bank President  Christine Lagarde  \u2014 have sounded the alarm about the use of bitcoin for money laundering, terrorist financing and other illegal activities. \"I think there could be some crackdown,\" Jesse Powell, CEO of Kraken, told CNBC in an interview. Cryptocurrencies have surged in value lately, with bitcoin hitting a record high price of more than $61,000 last month. The world's most valuable digital coin was last trading Monday at around $60,105. Kraken is the world's fourth-largest digital currency exchange in terms of trading volume. The firm is considering going public through a direct listing \u2014 similar to Coinbase \u2014 next year after achieving record trading volumes in the first quarter,  CNBC reported  last week. Coinbase is  set to go public  on Wednesday, and could be valued at as much as $100 billion \u2014 more than major trading venue operators like  Intercontinental Exchange , owner of the New York Stock Exchange. Crypto investors are hailing the company's stock market debut as a major milestone for the industry after years of skepticism from Wall Street and regulators. Still, Kraken's chief thinks regulatory uncertainty around crypto isn't going away anytime soon. A recent  anti-money laundering rule  proposed by the U.S. government would require people who hold their crypto in a private digital wallet to undergo identity checks if they make transactions of $3,000 or more. \"Something like that could really hurt crypto and kind of kill the original use case, which was to just make financial services accessible to everyone,\" Powell said. Cryptocurrencies like bitcoin have often been associated with illicit activities due to the fact that people transacting with it are pseudonymous \u2014 you can see where funds are being sent but not who sent or received them. There are signs that the use of crypto for nefarious purposes may be falling. Illicit activity accounted for  just 0.34% of all crypto transaction volume  last year, according to blockchain analysis firm Chainalysis. That was down from roughly 2% a year earlier. \"I hope that the U.S. and international regulators don't take too much of a narrow view on this,\" Powell said. \"Some other countries, China especially, are taking crypto very seriously and taking a very long-term view.\" Kraken's CEO said he feels the U.S. is more \"shortsighted\" than other nations and \"susceptible\" to the pressures of incumbent legacy businesses \u2014 in other words, the banks \u2014 that \"stand to lose from crypto becoming a big deal.\" \"I also think it might be too late,\" Powell added. \"Maybe the genie's out of the bottle and just trying to ban it at this point would make it more attractive. It would certainly send a message that the government sees this as a superior alternative to their own currency.\" The U.S. isn't the only country considering strict new rules on crypto. In India, for example, the government is  considering a law that would ban cryptocurrencies  and penalize anyone holding or trading them."}, "210705_news_468639.txt": {"page_id": "210705_news_468639.txt", "text": "Noncombatant \u00a0\ud83d\ude1a\n About \u00a0\ud83e\udd13\n Other Writing \u00a0\ud83e\uddd0\n Bandcamp \u00a0\ud83c\udfb5\n Prioritizing Memory Safety Migrations 9 April 2021 Update 11 April: Please also see  Long Live Sandboxing! . Sandboxing\nis not dead, despite what you might have heard. With all the talk of using Rust to reduce memory unsafety bugs,  such\nas Android using Rust in the Android Open Source Project , there\u2019s a lot of\nextremely reasonable concern about the high cost of \u201crewriting it all in Rust\u201d\n(or any other safer language), as it\u2019s often phrased. Operating systems, web\nbrowsers, complex online services, and so on can be implemented with tens of\nmillions of lines of C/C++ code. ( Sometimes\nmore .) Rewriting all that seems prohibitively expensive, and exacerbates  what\nAlex Gaynor aptly calls grief \u200a\u2014\u200apeople stay in the denial stage longer when\nstruck by the enormity of the memory unsafety problem. Thankfully,  replacing C/C++ with\ncode in a safer language  is not an all-or-nothing task. We can do it\ngradually; some parts we might never need to replace. Most safer languages can\nlink in the same address space as C and/or C++, and call into and be called by\nC/C++. You can also normalize data structures such that the safe code handles\narbitrary inputs, and the C/C++ code can focus on a single, simpler grammar. For\nexample:   You can accept arbitrary\nimage (e.g.) formats from the internet, use a safer language to normalize them\ninto Skia\u2019s simple  SkBitmap  format, and then handle the bitmaps in\nSkia in C++. This simplifies the C++ code (reducing its attack surface), and\nprovides a simple cross-language interface. But how do you tell where to start replacing C/C++ with safer code, and where\nto stop? Although security is certainly not the only benefit of a safe language\u200a\u2014\u200athe\nAndroid team\u2019s post starts out stressing correctness\u200a\u2014\u200amy perspective is\nsecurity. And from that starting point, we can use what I think is a pretty\nclear method to prioritize our efforts. Even if you have, say, 20 million lines of C++ code, not all of it is\ndirectly or indirectly exposed to attackers. You can start hardening the most\nexposed code first, and you can rank exposure by how long the path to the code\nis. Consider  Ian\nBeer\u2019s epic radio pyrotechnics , in which he compromised iPhones by sending\nthem mean-spirited packets by radio. We can model the attack surface exposure\nsomething like this:   An attack pathway from the internet to the\nkernel. That\u2019s a bit of an oversimplification, but it lets us see that the attacker\u2019s\ncall graph is not very deep\u200a\u2014\u200athat is, that driver is pretty exposed. Additionally, as the title of Ian\u2019s post points out, the attacker\u2019s cost to\ntraverse the first few edges is 0. We can model that by assigning \u2018weight\u2019 or\n\u2018cost\u2019 to the edges in the graph. The higher the cost, the less likely it is\nthat the attacker will succeed. Assuming the radio is fairly simple and does\nlittle normalization or filtering before passing what it got to the kernel, we\nmight draw something like this:   An attack pathway from the\ninternet to the kernel, with estimated costs for each edge\ntraversal. On a scale from 0 < low < medium < high, we might generously\nestimate the cost to exploit the vulnerable driver to be maybe medium. If the\ndefender is lucky, maybe ASLR is working, or something. Ian explains everything in full detail in his post, but in general we should\nnot think of C/C++ code as defensible. If an attacker is able to get at C/C++\nattack surface, we must assume they can win with an exploit based on memory\nunsafety. As an additional example, consider your web server\u2019s or browser\u2019s TLS\nimplementation. Should we consider it exposed? We can model it something like\nthis:   An attack pathway from the internet to the\napplication\u2019s TLS implementation. In this case, the attacker is interested only in the application\u2019s TLS\nimplementation, and is just using the kernel as a way to get there\u200a\u2014\u200athey are\nnot attacking the device driver or the TCP implementation. (Although those are\nalso exposed attack surfaces, of course.) The kernel typically does not do\nanything with the application layer traffic, passing it verbatim to the userland\napplication. So the kernel is not creating a security boundary in this case. The attacker has a pretty straight shot to your application\u2019s TLS\nimplementation; the only attack precondition is whether the attacker can send\nmalicious TLS traffic to the application. Obviously, servers listen to the\ninternet and process whatever they get; that\u2019s 0 cost. If attacking a client, an\nattacker may have to get the target to contact their server or may have to be on\nthe same network as the target. We might say that is up to medium cost. So, things like device drivers and HTTP, TCP, and TLS implementations are all\nfine candidates for (re)implementing in a safer language. They\u2019re unavoidably\nexposed. Consider an example where the C/C++ attack surface is not as directly\nexposed.   An attack pathway from the\ninternet to a client\u2019s CSP evaluator. In this example, we have an HTTP client that is going to parse and evaluate a\n Content Security\nPolicy  (CSP) header. Each of the network interface, device driver, TCP\nimplementation, TLS client implementation, HTTP client implementation, and CSP\nparser are fairly exposed attack surface. For example, if the attacker wants to\nexploit some bug in the CSP parser, they can likely rely on all of the previous\ncomponents to pass the header value through verbatim to the CSP parser. Thus,\nthey probably do not create a security boundary. But if the attacker wants to exploit a likely bug class, mis-evaluation of\nCSP policy, they must first get past the CSP parser. Although it is vulnerable\nattack surface, it does also provide some security boundary: the policy must be\nwell-formed according to the grammar the parser accepts. Another bug class is\nthat the parser\u2019s grammar is not necessarily the same as the grammar in the\nspec. Thus, we\u2019d be speaking of logic bugs in the CSP parser and/or evaluator. This\nis the kind of code that can be buggy in any language; this is not memory\nunsafety that can be resolved at scale with a safer language. These examples suggest that you have to get fairly deep into the call graph\nbefore memory unsafety becomes less of a concern. That\u2019s consistent with  the\nfindings that memory unsafety accounts for anywhere from \u2154 to \u00be of\nvulnerabilities . The problem is that bad. Models like those above can be step 1 in a process of repair triage. You\nmight order a set of constraints when filtering through what code to rewrite,\napply mitigations or testing to, or even get rid of first: Select the most exposed code\n ...of that code, start with the highest-privilege code\n ...of that code, start with the code that has the highest observed bug count\n Or you might triage differently, depending on your situation: Select the most exposed code\n ...of that code, start with the code that has the highest observed bug count\n ...of that code, start with the highest-privilege code\n Or even: Select the code that has the highest observed bug count\n ...of that code, start with the most exposed code\n ...of that code, start with the highest-privilege code\n Which approach is appropriate depends on your system. For example, if you are\nworking entirely in the kernel, all your code runs at the same level of\nprivilege so you can\u2019t use that as a filter. Or if you are in userland, but your\napplication is not making use of process sandboxing, consider exploring that\nfirst before starting a rewrite effort. In any case, we don\u2019t have to \u201crewrite everything in Rust\u201d to significantly\nimprove memory safety, and we are not lost in a sea of undifferentiated attack\nsurface. There are ways we can prioritize in a somewhat systematic way\u200a\u2014\u200awe\ndon\u2019t have to fix random things ad hoc. Thanks to Jacob Hoffman-Andrews, Andrew Dunham, and Dev Akhawe for reading\ndrafts of this post and suggesting helpful improvements!"}, "210705_news_468641.txt": {"page_id": "210705_news_468641.txt", "text": "Experiments to find Majorana signals are performed by loading a nanowire into a dilution refrigerator capable of cooling it down to close to absolute zero. Credit: HGA Architects and Engineers A shadow has fallen over the race to detect a new type of quantum particle, the Majorana fermion, that could power quantum computers. As someone who works in this area, I\u2019ve become concerned that, after a series of false starts, a significant fraction of the Majorana field is fooling itself. Several key experiments claiming to have detected Majorana particles, initially considered as breakthroughs, have not been confirmed. One recent case ended in a high-profile retraction from  Nature  (see  Nature   591 , 354\u2013355; 2021 ), which I initiated with my colleague Vincent Mourik, a physicist at the University of New South Wales in Sydney, Australia. We raised concerns after obtaining additional data from the original experiments that were not included with the published paper. Much is at stake. Majorana particles are in theory their own antiparticles, and were predicted in 1937 by Italian physicist Ettore Majorana. Computer giant Microsoft hopes to use Majorana particles to build a reliable quantum computer: the particles should make for exceptionally stable quantum bits. The scientific excitement around them is on a par with gravitational waves and the Higgs boson. Experimentally, researchers are at loggerheads over whether Majoranas have been detected at all, let alone whether they\u2019re an asset for quantum computing. As scepticism of the claims creeps beyond the cognoscenti, the field is at risk of getting a bad reputation, despite its untapped promise. Challenging science Producing Majoranas in the laboratory is very hard. Experiments combine cutting-edge fields such as nanotechnology, superconductivity, device engineering and materials science. In the most developed approach, researchers must first grow a nanowire crystal \u2014 a feat in itself \u2014 to produce a column of atoms 100 nanometres (one-thousandth the width of a human hair) across. Then they must connect the wire to a circuit sensitive enough to measure single electrons travelling through it. The whole experiment must be done at about one-hundredth of a degree above absolute zero, in a magnetic field 10,000 times that of Earth\u2019s. Under those extremes, when all the electrons in the wire are magnetized, Majorana particles should emerge from the two wire ends. In theory. More than 100 groups have tried this. Two dozen have reported Majorana manifestations. These usually appear in the form of a characteristic electronic signal: a narrow peak in current as voltage is varied across the nanowire. I was a member of one of the first teams to observe this, in 2012 1 . More papers soon appeared. Detections of a quantized value of the current, first predicted in theory and then reported in experiments published in  Science 2  in 2017 and  Nature 3  in 2018, were interpreted by many to be the ultimate evidence of Majoranas. In 2020, these observations came under scrutiny after replication experiments were conducted.  Science  published an experiment led by researchers at Pennsylvania State University in University Park contradicting the 2017 report 4 . My group reproduced patterns from the 2018  Nature  study, but demonstrated that they need not originate from Majorana 5 . We did a cross-check on both ends of the same nanowire, but found a current peak on only one end. This violated the basic expectation from the theory that Majoranas always come in pairs. The rate of rebuttal is speeding up: researchers have not been able to confirm the findings 6 , 7  of two separate papers claiming to have found Majorana regimes in nanowires 8 , 9 . And reports of current peaks in a new iron-based superconductor, Fe(Te,Se), that were attributed to Majoranas 10 \u2013 12  in  Science  and  Nature Communications  will need to become more nuanced after a  Physical Review Letters  publication this year 13 . The lesson: Majorana particles aren\u2019t necessary to produce the current peak signals. At least since 2014, we have known of more-mundane explanations, such as other quantum states that are not Majoranas 14 , accidental signals caused by imperfections in the nanowire, or fascinating but previously explored cooperative behaviour of numerous electrons (see \u2018Mixed signals\u2019). Yet, affirmative papers kept coming out without even mentioning alternative explanations, creating the impression that a debate is raging between Majorana optimists and pessimists. Source: Peng Yu/Frolov lab Reflection needed As someone who has published and reviewed positive and negative Majorana claims, I sense a wider problem. The controversy has already begun to erode confidence in the basic experimental method of passing current through quantum objects, even though this powerful technique has been used in many great discoveries, including Nobel-prizewinning observations in superconductivity, the quantum Hall effect and tunnelling. It has already begun to affect me. Prospective graduate students ask whether I\u2019m stopping Majorana research. Grant reviewers assume it is the methodology, not selective data reporting, that causes confusion in the field. In my view, nothing is wrong with the basic method often dubbed \u2018quantum transport\u2019. I feel that selective data presentation is the main problem. If all papers included full or at least appropriately selected sets of data, quantum physicists could assign correct explanations, Majorana or not. But I think that researchers are cherry-picking \u2014 focusing on data that agree with the Majorana theory and sidelining those that don\u2019t. A case in point: a 2020  Science  paper on Fe(Te,Se) reported quantized behaviour of current, which the authors saw in a single vortex, out of 60 assessed 10 . I contend that data-selecting researchers can be enabled by some journals and reviewers who might be insufficiently stringent. (When asked about the 2020 paper, a spokesperson for  Science  said that results and conclusions, including alternative mechanisms to explain the observed quantization, were presented carefully.) Time and time again, I and other reviewers argued for journals not to publish papers based on selective data presentation, only to see them appear in other (or sometimes the same) journals. Sometimes there really is no need to present all of your data, if a single graph tells the whole story. But for Majorana particles, simply searching through the data to identify peaks of the right height is not enough to stake a detection claim, especially when alternative theories exist. It is all too easy for selection bias to take over in hypothesis-driven experimental research. The \u2018best\u2019 data are often considered to be those that fit the theory. So deviations are too readily dismissed as experimental or human error that can thus be discarded. Another problem is the breadth of peer review needed to check Majorana claims. Scrutiny is hard in any multidisciplinary field. Referees tend to be expert in one subject and struggle to judge others, and that leaves gaps. For example, a theoretical physicist might be comfortable assessing the calculations but not the experimental process, and a materials scientist, who understands how to grow nanowires, might skip the theory part. But a holistic view of the whole study is needed to properly assess it. It is an all too familiar story. In a  Nature  survey of the \u2018reproducibility crisis\u2019 across chemistry, biology, physics, engineering and medical sciences (see  Nature   533 , 452\u2013454; 2016 ), selective reporting of results was a top culprit. We\u2019ve seen this for decades. Physicist Robert Millikan\u2019s oil-drop experiments of more than a century ago famously omitted some data points. He did get close to the actual value of the electron charge \u2014 but science cannot depend on these sorts of fluke. Some Majorana papers are turning out to be unreliable because of how data are selected. Ways forward The behavioural norms across the condensed-matter physics community need updating. There is only one solution, and it is more accountability across the board. The following steps will help both Majorana research and fields far beyond. Open data. Scientists should disclose all data in a repository and comply with sharing standards, such as FAIR (findability, accessibility, interoperability and reusability) 15 . Some curation is unavoidable. The volume of data collected in a modern physics laboratory is high: computer scripts control the equipment, which might run 24 hours a day. A remedy is to clearly explain the protocol that is used to perform any data selection \u2014 so others might reuse or scrutinize it. Remember, data selection is a form of data processing. Journals, funders (including corporations), research labs and universities should demand such open data practices, as they do in clinical trials, genomics, Earth sciences and a handful of other disciplines. Sharing data improves reliability, fosters collaborations and speeds up progress. The high-energy physics community, for example, could teach others how to share study protocols so that each paper is repeatable or reproducible 16 . Although it is not widely known, access to further data is already required by many publication policies and government codes of research conduct. Notably, the United States does not have a national code, in contrast with other countries investing heavily in research. Further efforts are needed to make such sharing automatic and not \u2018upon request\u2019. As the case of the Majorana paper recently retracted from  Nature  showed, seeing full data can be crucial for evaluating an experiment. Critics will counter that simply sharing data does not capture all that goes on in the lab, that experience and insight \u2014 craft \u2014 have value that cannot be described in a protocol. I argue that robust, useful science is built on reliable processes that can be revisited, verified and re-examined as many times as necessary. Open process. Reviewers need to be more questioning of extraordinary claims. Are the results too good to be true? Have enough data been presented? Have other explanations been considered? Cross-checks should be conducted, making it harder to stake an unreliable claim. For Majorana physics, this is as basic as comparing the magnetic- and electric-field dependence of current peaks with what would be expected theoretically. If done consistently, this would thwart many false claims. But even the most rigorous reviews can be ignored. If the paper is rejected, the authors are free to disregard all input they were given and send their manuscript to another journal. I have seen Majorana papers that have received multiple negative reviews and rejections on scientific grounds published with only minor changes in another high-profile journal. Opening up the notoriously opaque publication process is key to cutting down on the proliferation of bad research. Editors should take responsibility: it is they who decide, even if they lack in-depth expertise for that particular paper\u2019s topic. Each accepted paper should have its editor\u2019s name published alongside it. For each retraction, the editors should provide their view on what happened. All journals, especially high-impact ones, need to have community oversight. Editorial retraction should be applied widely, because waiting on authors to retract papers on their own can take an eternity. At the moment, most journals do not even have the capacity to run their own investigations into claims of mistakes in their papers. They should build this capacity, with the help of the research community. Journals deserve praise for publishing negative results and for normalizing verification studies in physics. Researchers willing to share their results should receive well-deserved attention. For example, the American Physical Society ran an invited session on negative Majorana results at its virtual 2021 March meeting. What of Majorana research? It remains viable and important. But, in my view, the key discoveries have yet to be made. A concentrated effort is now needed to improve our nanowire materials, experimental techniques and data analysis, as well as to tease out alternative explanations. Reliable proof is needed that the particles are indeed their own antiparticles \u2014 with our eyes on the full data. Only then will we be ready to develop Majorana quantum computers."}, "210705_news_468642.txt": {"page_id": "210705_news_468642.txt", "text": "NVIDIA introduces GRACE CPU NVIDIA GRACE is a next-generation ARM-based CPU designed for giant-scale AI and HPC applications. Today at GTC 2021 NVIDIA announces its first CPU called GRACE. The CPU is designed for AI and servers, it is not a consumer product. NVIDIA confirmed that GRACE features the highest memory bandwidth of 500 GB/s thanks to LPDDR5X technology with ECC correction. It also attaches to GPU through Cache Coherent NVLINK to GPUs at 900 GB/s. A CPU to CPU connection is up to 600 GB/s. The CPU is based on ARM architecture. It features Neoverse cores, the slide we obtained confirms. \u201cLeading-edge AI and data science are pushing today\u2019s computer architecture beyond its limits\u2014processing unthinkable amounts of data. Using licensed Arm IP, NVIDIA has designed Grace as a CPU specifically for giant-scale AI and HPC. Coupled with the GPU and DPU, Grace gives us the third foundational technology for computing, and the ability to re-architect the data center to advance AI. NVIDIA is now a three-chip company.\u201d   \u2014 said Jensen Huang, founder and CEO of NVIDIA NVIDIA Grace CPU, Source: NVIDIA NVIDIA Grace CPU, Source: NVIDIA   NVIDIA Announces CPU for Giant AI and High Performance Computing Workloads   \u2018Grace\u2019 CPU delivers 10x performance leap for systems training giant AI models, using energy-efficient Arm cores   Swiss Supercomputing Center and US Department of Energy\u2019s Los Alamos National Laboratory First to Build NVIDIA CPU-Powered Supercomputers SANTA CLARA, Calif., April 12, 2021 \u2014 GTC \u2014 NVIDIA today announced its first data center CPU, an Arm-based processor that will deliver 10x the performance of today\u2019s fastest servers on the most complex AI and high performance computing workloads. The result of more than 10,000 engineering years of work, the NVIDIA Grace\u2122 CPU is designed to address the computing requirements for the world\u2019s most advanced applications \u2014 including natural language processing, recommender systems and AI supercomputing \u2014 that analyze enormous datasets requiring both ultra-fast compute performance and massive memory. It combines energy-efficient Arm CPU cores with an innovative low-power memory subsystem to deliver high performance with great efficiency. \u201cLeading-edge AI and data science are pushing today\u2019s computer architecture beyond its limits \u2013 processing unthinkable amounts of data,\u201d said Jensen Huang, founder and CEO of NVIDIA. \u201cUsing licensed Arm IP, NVIDIA has designed Grace as a CPU specifically for giant-scale AI and HPC. Coupled with the GPU and DPU, Grace gives us the third foundational technology for computing, and the ability to re-architect the data center to advance AI. NVIDIA is now a three-chip company.\u201d Grace is a highly specialized processor targeting workloads such as training next-generation NLP models that have more than 1 trillion parameters. When tightly coupled with NVIDIA GPUs, a Grace CPU-based system will deliver 10x faster performance than today\u2019s state-of-the-art NVIDIA DGX\u2122-based systems, which run on x86 CPUs. While the vast majority of data centers are expected to be served by existing CPUs, Grace \u2014 named for Grace Hopper, the U.S. computer-programming pioneer \u2014 will serve a niche segment of computing. The Swiss National Supercomputing Centre (CSCS) and the U.S. Department of Energy\u2019s Los Alamos National Laboratory are the first to announce plans to build Grace-powered supercomputers in support of national scientific research efforts. NVIDIA is introducing Grace as the volume of data and size of AI models are growing exponentially. Today\u2019s largest AI models include billions of parameters and are doubling every two-and-a-half months. Training them requires a new CPU that can be tightly coupled with a GPU to eliminate system bottlenecks. NVIDIA built Grace by leveraging the incredible flexibility of Arm\u2019s data center architecture. By introducing a new server-class CPU, NVIDIA is advancing the goal of technology diversity in AI and HPC communities, where choice is key to delivering the innovation needed to solve the world\u2019s most pressing problems. \u201cAs the world\u2019s most widely licensed processor architecture, Arm drives innovation in incredible new ways every day,\u201d said Arm CEO Simon Segars. \u201cNVIDIA\u2019s introduction of the Grace data center CPU illustrates clearly how Arm\u2019s licensing model enables an important invention, one that will further support the incredible work of AI researchers and scientists everywhere.\u201d Grace\u2019s First Adopters Push Limits of Science and AI  CSCS and Los Alamos National Laboratory both plan to bring Grace-powered supercomputers, built by Hewlett Packard Enterprise, online in 2023. \u201dNVIDIA\u2019s novel Grace CPU allows us to converge AI technologies and classic supercomputing for solving some of the hardest problems in computational science,\u201d said CSCS Director Prof. Thomas Schulthess. \u201cWe are excited to make the new NVIDIA CPU available for our users in Switzerland and globally for processing and analyzing massive and complex scientific datasets.\u201d \u201cWith an innovative balance of memory bandwidth and capacity, this next-generation system will shape our institution\u2019s computing strategy,\u201d said Thom Mason, director of the Los Alamos National Laboratory. \u201cThanks to NVIDIA\u2019s new Grace CPU, we\u2019ll be able to deliver advanced scientific research using high-fidelity 3D simulations and analytics with datasets that are larger than previously possible.\u201d Delivering Breakthrough Performance  Underlying Grace\u2019s performance is fourth-generation NVIDIA NVLink\u00ae interconnect technology, which provides a record 900 GB/s connection between Grace and NVIDIA GPUs to enable 30x higher aggregate bandwidth compared to today\u2019s leading servers. Grace will also utilize an innovative LPDDR5x memory subsystem that will deliver twice the bandwidth and 10x better energy efficiency compared with DDR4 memory. In addition, the new architecture provides unified cache coherence with a single memory address space, combining system and HBM GPU memory to simplify programmability. Grace will be supported by the NVIDIA HPC software development kit and the full suite of CUDA\u00ae and CUDA-X\u2122 libraries, which accelerate more than 2,000 GPU applications, speeding discoveries for scientists and researchers working on the world\u2019s most important challenges. Availability is expected in the beginning of 2023.   \u00ab end of the press release \u00bb    "}, "210705_news_468644.txt": {"page_id": "210705_news_468644.txt", "text": "Kicking off another busy Spring GPU Technology Conference for NVIDIA, this morning the graphics and accelerator designer is announcing that they are going to once again design their own Arm-based CPU/SoC. Dubbed Grace \u2013 after Grace Hopper, the computer programming pioneer and US Navy rear admiral \u2013 the CPU is NVIDIA\u2019s latest stab at more fully vertically integrating their hardware stack by being able to offer a high-performance CPU alongside their regular GPU wares. According to NVIDIA, the chip is being designed specifically for large-scale neural network workloads, and is expected to become available in NVIDIA products in 2023. With two years to go until the chip is ready, NVIDIA is playing things relatively coy at this time. The company is offering only limited details for the chip \u2013 it will be based on a future iteration of Arm\u2019s Neoverse cores, for example \u2013 as today\u2019s announcement is a bit more focused on NVIDIA\u2019s future workflow model than it is speeds and feeds. If nothing else, the company is making it clear early on that, at least for now, Grace is an internal product for NVIDIA, to be offered as part of their larger server offerings. The company isn\u2019t directly gunning for the Intel Xeon or AMD EPYC server market, but instead they are building their own chip to complement their GPU offerings, creating a specialized chip that can directly connect to their GPUs and help handle enormous, trillion parameter AI models. NVIDIA SoC Specification Comparison \u00a0 Grace Xavier Parker \n\t\t\t(Tegra X2) CPU Cores ? 8 2 CPU Architecture Next-Gen Arm Neoverse \n\t\t\t(Arm v9?) Carmel \n\t\t\t(Custom Arm v8.2) Denver 2 \n\t\t\t(Custom Arm v8) Memory Bandwidth >500GB/sec \n\t\t\tLPDDR5X \n\t\t\t(ECC) 137GB/sec \n\t\t\tLPDDR4X 60GB/sec \n\t\t\tLPDDR4 GPU-to-CPU Interface >900GB/sec \n\t\t\tNVLink 4 PCIe 3 PCIe 3 CPU-to-CPU Interface >600GB/sec \n\t\t\tNVLink 4 N/A N/A Manufacturing Process ? TSMC 12nm TSMC 16nm Release Year 2023 2018 2016 More broadly speaking, Grace is designed to fill the CPU-sized hole in NVIDIA\u2019s AI server offerings. The company\u2019s GPUs are incredibly well-suited for certain classes of deep learning workloads, but not all workloads are purely GPU-bound, if only because a CPU is needed to keep the GPUs fed. NVIDIA\u2019s current server offerings, in turn, typically rely on AMD\u2019s EPYC processors, which are very fast for general compute purposes, but lack the kind of high-speed I/O and deep learning optimizations that NVIDIA is looking for. In particular, NVIDIA is currently bottlenecked by the use of PCI Express for CPU-GPU connectivity; their GPUs can talk quickly amongst themselves via NVLink, but not back to the host CPU or system RAM. The solution to the problem, as was the case even before Grace, is to use NVLink for CPU-GPU communications. Previously NVIDIA has worked with the OpenPOWER foundation to get NVLink into POWER9 for exactly this reason, however that relationship is seemingly on its way out, both as POWER\u2019s popularity wanes and POWER10 is skipping NVLink. Instead, NVIDIA is going their own way by building an Arm server CPU with the necessary NVLink functionality. The end result, according to NVIDIA, will be a high-performance and high-bandwidth CPU that is designed to work in tandem with a future generation of NVIDIA server GPUs. With NVIDIA talking about pairing each NVIDIA GPU with a Grace CPU on a single board \u2013 similar to today\u2019s mezzanine cards \u2013 not only does CPU performance and system memory scale up with the number of GPUs, but in a roundabout way, Grace will serve as a co-processor of sorts to NVIDIA\u2019s GPUs. This, if nothing else, is a very NVIDIA solution to the problem, not only improving their performance, but giving them a counter should the more traditionally integrated AMD or Intel try some sort of similar CPU+GPU fusion play. By 2023 NVIDIA will be up to NVLink 4, which will offer at least 900GB/sec of cummulative (up + down) bandwidth between the SoC and GPU, and over 600GB/sec cummulative between Grace SoCs. Critically, this is greater than the memory bandwidth of the SoC, which means that NVIDIA\u2019s GPUs will have a cache coherent link to the CPU that can access the system memory at full bandwidth, and also allowing the entire system to have a single shared memory address space. NVIDIA describes this as balancing the amount of bandwidth available in a system, and they\u2019re not wrong, but there\u2019s more to it. Having an on-package CPU is a major means towards increasing the amount of memory NVIDIA\u2019s GPUs can effectively access and use, as memory capacity continues to be the primary constraining factors for large neural networks \u2013 you can only efficiently run a network as big as your local memory pool. CPU & GPU Interconnect Bandwidth \u00a0 Grace EPYC 2 + A100 EPYC 1 + V100 GPU-to-CPU Interface \n\t\t\t(Cummulative, Both Directions) >900GB/sec \n\t\t\tNVLink 4 ~64GB/sec \n\t\t\tPCIe 4 x16 ~32GB/sec \n\t\t\tPCIe 3 x16 CPU-to-CPU Interface \n\t\t\t(Cummulative, Both Directions) >600GB/sec \n\t\t\tNVLink 4 304GB/sec \n\t\t\tInfinity Fabric 2 152GB/sec \n\t\t\tInfinity Fabric And this memory-focused strategy is reflected in the memory pool design of Grace, as well. Since NVIDIA is putting the CPU on a shared package with the GPU, they\u2019re going to put the RAM down right next to it. Grace-equipped GPU modules will include a to-be-determined amount of LPDDR5x memory, with NVIDIA targeting at least 500GB/sec of memory bandwidth. Besides being what\u2019s likely to be the highest-bandwidth non-graphics memory option in 2023, NVIDIA is touting the use of LPDDR5x as a gain for energy efficiency, owing to the technology\u2019s mobile-focused roots and very short trace lengths. And, since this is a server part, Grace\u2019s memory will be ECC-enabled, as well. As for CPU performance, this is actually the part where NVIDIA has said the least. The company will be using a future generation of Arm\u2019s Neoverse CPU cores, where the initial N1 design has already been  turning heads . But other than that, all the company is saying is that the cores should break 300 points on the SPECrate2017_int_base throughput benchmark, which would be comparable to some of AMD\u2019s second-generation 64 core EPYC CPUs. The company also isn\u2019t saying much about how the CPUs are configured or what optimizations are being added specifically for neural network processing. But since Grace is meant to support NVIDIA\u2019s GPUs, I would expect it to be stronger where GPUs in general are weaker. Otherwise, as mentioned earlier, NVIDIA big vision goal for Grace is significantly cutting down the time required for the largest neural networking models. NVIDIA is gunning for 10x higher performance on 1 trillion parameter models, and their performance projections for a 64 module Grace+A100 system (with theoretical NVLink 4 support) would be to bring down training such a model from a month to three days. Or alternatively, being able to do real-time inference on a 500 billion parameter model on an 8 module system. Overall, this is NVIDIA\u2019s second real stab at the data center CPU market \u2013 and the first that is likely to succeed. NVIDIA\u2019s  Project Denver , which was originally announced just over a decade ago, never really panned out as NVIDIA expected. The family of custom Arm cores was never good enough, and never made it out of NVIDIA\u2019s mobile SoCs. Grace, in contrast, is a much safer project for NVIDIA; they\u2019re merely licensing Arm cores rather than building their own, and those cores will be in use by numerous other parties, as well. So NVIDIA\u2019s risk is reduced to largely getting the I/O and memory plumbing right, as well as keeping the final design energy efficient. If all goes according to plan, expect to see Grace in 2023. NVIDIA is already confirming that Grace modules will be available for use in HGX carrier boards, and by extension DGX and all the other systems that use those boards. So while we haven\u2019t seen the full extent of NVIDIA\u2019s Grace plans, it\u2019s clear that they are planning to make it a core part of future server offerings. First Two Supercomputer Customers: CSCS and LANL And even though Grace isn\u2019t shipping until 2023, NVIDIA has already lined up their first customers for the hardware \u2013 and they\u2019re supercomputer customers, no less. Both the Swiss National Supercomputing Centre (CSCS) and Los Alamos National Laboratory are announcing today that they\u2019ll be ordering supercomputers based on Grace. Both systems will be built by HPE\u2019s Cray group, and are set to come online in 2023. CSCS\u2019s system, dubbed Alps, will be replacing their current Piz Daint system, a Xeon plus NVIDIA P100 cluster. According to the two companies, Alps will offer 20 ExaFLOPS of AI performance, which is presumably a combination of CPU, CUDA core, and tensor core throughput. When it\u2019s launched, Alps should be the fastest AI-focused supercomputer in the world. An artist's rendition of the expected Alps system Interestingly, however, CSCS\u2019s ambitions for the system go beyond just machine learning workloads. The institute says that they\u2019ll be using Alps as a general purpose system, working on more traditional HPC-type tasks as well as AI-focused tasks. This includes CSCS\u2019s traditional research into weather and the climate, which the pre-AI Piz Daint is already used for as well. As previously mentioned, Alps will be built by HPE, who will be basing on their previously-announced Cray EX architecture. This would make NVIDIA\u2019s Grace the second CPU option for Cray EX, along with AMD\u2019s EPYC processors. Meanwhile Los Alamos\u2019 system is being developed as part of an ongoing collaboration between the lab and NVIDIA, with LANL set to be the first US-based customer to receive a Grace system. LANL is not discussing the expected performance of their system beyond the fact that it\u2019s expected to be \u201cleadership-class,\u201d though the lab is planning on using it for 3D simulations, taking advantage of the largest data set sizes afforded by Grace. The LANL system is set to be delivered in early 2023."}, "210705_news_468746.txt": {"page_id": "210705_news_468746.txt", "text": "KCP - A Fast and Reliable ARQ Protocol   Introduction KCP is a fast and reliable protocol that can achieve the transmission effect of a reduction of the average latency by 30% to 40% and reduction of the maximum delay by a factor of three, at the cost of 10% to 20% more bandwidth wasted than TCP. It is implemented by using the pure algorithm, and is not responsible for the sending and receiving of the underlying protocol (such as UDP), requiring the users to define their own transmission mode for the underlying data packet, and provide it to KCP in the way of callback. Even the clock needs to be passed in from the outside, without any internal system calls. The entire protocol has only two source files of ikcp.h, ikcp.c, which can be easily integrated into the user's own protocol stack. You may have implement a P2P, or a UDP-based protocol, but are lack of a set of perfect ARQ reliable protocol implementation, then by simply copying the two files to the existing project, and writing a couple of lines of code, you can use it. Technical Specifications TCP is designed for traffic (the amount of kilobits per second of data that can be transmitted), which focuses on the full use of bandwidth. While KCP is designed for the flow rate (the amount of time it takes to send a single packet from one end to the other), with 10% -20% bandwidth waste in exchange for transmission speed 30%-40% faster than TCP. TCP channel is a grand canal with very slow flow rate, but very large flow per second, while KCP is a small torrent with the rapid flow. KCP has both normal and fast modes, achieving the result of flow rate increase by the following strategies: RTO Doubled vs Not Doubled: TCP timeout calculation is RTOx2, so three consecutive packet losses will make it RTOx8, which is very terrible, while after KCP fast mode is enabled, it is not x2, but x1.5 (Experimental results show that the value of 1.5 is relatively good), which has improved the transmission speed. Selective Retransmission vs Full Retransmission: When packet loss occurs in TCP, all the data after the lost packet will be retransmitted, while KCP is selective retransmission, and only re-transmits the data packets that are really lost. Fast Retransmission: The transmitting terminal sends 1, 2, 3, 4 and 5 packets, and then receives the remote ACK: 1, 3, 4 and 5, when receiving ACK3, KCP knows that 2 is skipped 1 time, and when receiving ACK4, it knows that 2 is skipped 2 times, at this point, it can consider that 2 is lost, without waiting until timeout, it will directly retransmit packet 2, which can greatly improve the transmission speed when packet loss occurs. Delayed ACK vs Non-delayed ACK: In order to make full use of the bandwidth, TCP delays sending an ACK (Even NODELAY does not work), so that the timeout calculation will come out with a relatively high RTT, which has extended the judgment process when packet loss occurs. While for KCP, it is adjustable whether to delay sending an ACK. UNA vs ACK+UNA\uff1a There are two kinds of ARQ model responses: UNA (All packets before this number received, such as TCP) and ACK (The packet with this number received). Using UNA alone will result in full retransmissions, and using ACK alone has too much cost for packet loss, hence in the previous protocols, one of the two has been selected; while in KCP protocol, all packets have UNA information except for a single ACK packet. Non-concessional Flow Control: KCP normal mode uses the same fair concession rules as TCP, i.e., the send window size is determined by: four factors including the size of the send cache, the size of the receive buffer at the receiving end, packet loss concession and slow start. However, when sending small data with high timeliness requirement, it is allowed to select skipping the latter two steps through configuration, and use only the first two items to control the transmission frequency, sacrificing some of the fairness and bandwidth utilization, in exchange for the effect of smooth transmission even when BT is opened. Quick Install You can download and install kcp using the  vcpkg  dependency manager: git clone https://github.com/Microsoft/vcpkg.git\ncd vcpkg\n./bootstrap-vcpkg.sh\n./vcpkg integrate install\n./vcpkg install kcp\n The kcp port in vcpkg is kept up to date by Microsoft team members and community contributors. If the version is out of date, please  create an issue or pull request  on the vcpkg repository. Basic Usage Create KCP object: //  Initialize the kcp object, conv is an integer that represents the session number,  //  same as the conv of tcp, both communication sides shall ensure the same conv,  //  so that mutual data packets can be recognized, user is a pointer which will be  //  passed to the callback function. \nikcpcb *kcp = ikcp_create(conv, user); Set the callback function: //  KCP lower layer protocol output function, which will be called by KCP when it  //  needs to send data, buf/len represents the buffer and data length.  //  user refers to the incoming value at the time the kcp object is created to  //  distinguish between multiple KCP objects int   udp_output ( const   char  *buf,  int  len, ikcpcb *kcp,  void  *user)\n{\n  ....\n}\n //  Set the callback function \nkcp->output = udp_output; Call update in an interval: //  Call ikcp_update at a certain frequency to update the kcp state, and pass in  //  the current clock (in milliseconds). If the call is executed every 10ms, or  //  ikcp_check is used to determine time of the next call for update, no need to  //  call every time; ikcp_update (kcp, millisec); Input a lower layer data packet: //  Need to call when a lower layer data packet (such as UDP packet)is received: ikcp_input (kcp, received_udp_packet, received_udp_size); After processing the output / input of the lower layer protocols, KCP protocol can work normally, and ikcp_send is used to send data to the remote end. While the other end uses ikcp_recv (kcp, ptr, size) to receive the data. Protocol Configuration The protocol default mode is a standard ARQ, and various acceleration switches can be enabled by configuration: Working Mode: int   ikcp_nodelay (ikcpcb *kcp,  int  nodelay,  int  interval,  int  resend,  int  nc) nodelay  : Whether nodelay mode is enabled, 0 is not enabled; 1 enabled. interval  \uff1aProtocol internal work interval, in milliseconds, such as 10 ms or 20 ms. resend  \uff1aFast retransmission mode, 0 represents off by default, 2 can be set (2 ACK spans will result in direct retransmission) nc  \uff1aWhether to turn off flow control, 0 represents \u201cDo not turn off\u201d by default, 1 represents \u201cTurn off\u201d. Normal Mode: ikcp_nodelay(kcp, 0, 40, 0, 0); Turbo Mode\uff1a ikcp_nodelay(kcp, 1, 10, 2, 1); Window Size: int   ikcp_wndsize (ikcpcb *kcp,  int  sndwnd,  int  rcvwnd); The call will set the maximum send window and maximum receive window size of the procotol, which is 32 by default. This can be understood as SND_BUF and RCV_BUF of TCP, but the unit is not the same, SND / RCV_BUF unit is byte, while this unit is the packet. Maximum Transmission Unit: Pure algorithm protocol is not responsible for MTU detection, the default mtu is 1400 bytes, which can be set using ikcp_setmtu. The value will affect the maximum transmission unit upon data packet merging and fragmentation. Minimum RTO: No matter TCP or KCP, they have the limitation for the minimum RTO when calculating the RTO, even if the calculated RTO is 40ms, as the default RTO is 100ms, the protocol can only detect packet loss after 100ms, which is 30ms in the fast mode, and the value can be manually changed: Document Indexing Both the use and configuration of the protocol is very simple, in most cases, after you read the above contents, basically you will be able to use it. If you need further fine control, such as changing the KCP memory allocator, or if you need more efficient large-scale scheduling of KCP links (such as more than 3,500 links), or to better combine with TCP, you can continue the extensive reading: Related Applications kcptun : High-speed remote port forwarding based (tunnel) on kcp-go, with ssh-D, it allows smoother online video viewing than finalspeed. dog-tunnel : Network tunnel developed by GO, using KCP to greatly improve the transmission speed, and migrated a GO version of the KCP. v2ray \uff1aWell-known proxy software, Shadowsocks replacement, integrated with kcp protocol after 1.17, using UDP transmission, no data packet features. HP-Socket : High Performance TCP/UDP/HTTP Communication Component. frp : A fast reverse proxy to help you expose a local server behind a NAT or firewall to the internet. asio-kcp : Use the complete UDP network library of KCP, complete implementation of UDP-based link state management, session control and KCP protocol scheduling, etc. kcp-cpp : Multi-platform (Windows, MacOS, Linux) C++ implementation of KCP as a simple library in your application. Contains socket handling and helper functions for all platforms. kcp-java \uff1aImplementation of Java version of KCP protocol. kcp-netty \uff1aJava implementation of KCP based on Netty. java-kcp : JAVA version KCP, based on netty implementation (including fec function) csharp-kcp : csharp version KCP, based on dotNetty implementation (including fec function) kcp-go : High-security GO language implementation of kcp, including simple implementation of UDP session management, as a base library for subsequent development. kcp-csharp : The csharp migration of kcp, containing the session management, which can access the above kcp-go server. kcp2k : Line-by-line translation to C#, with optional Server/Client on top. kcp-rs : The rust migration of KCP lua-kcp : Lua extension of KCP, applicable for Lua server node-kcp : KCP interface for node-js nysocks : Nysocks provides proxy services base on libuv and kcp for nodejs users. Both SOCKS5 and ss protocols are supported in the client. shadowsocks-android : Shadowsocks for android has integrated kcptun using kcp protocol to accelerate shadowsocks, with good results kcpuv : The kcpuv library developed with libuv, currently still in the early alpha phase. xkcptun : C language implementation of kcptun, embedded-friendly for  LEDE  and  OpenWrt  projects. yasio : A cross-platform asynchronous socket library focus on any client application with kcp support, easy to use, API same with UDP and TCP, see  benchmark-pump . gouxp : Implementing a callback-based KCP development package with Go, with decryption and FEC support, is easy to use. Protocol Comparison If the network is never congested, KCP/TCP performance is similar; but the network itself is not reliable, and packet loss and jitter may be inevitable (otherwise why there are various reliable protocols). Compared in the intranet environment which is almost ideal, they have similar performance, but on the public Internet, under 3G / 4G network situation, or using the intranet packet loss simulation, the gap is obvious. The public network has an average of nearly 10% packet loss during peak times, which is even worse in wifi / 3g / 4g network, all of which will cause transmission congestion. Thanks to  zhangyuan  the author of  asio-kcp  for the horizontal evaluation on KCP, enet and udt, and the conclusions are as follows: ASIO-KCP  has good performace in wifi and phone network(3G, 4G) . The kcp is the  first choice for realtime pvp game . The lag is less than 1 second when network lag happen.  3 times better than enet  when lag happen. The enet is a good choice if your game allow 2 second lag. UDT is a bad idea . It always sink into badly situation of more than serval seconds lag. And the recovery is not expected. enet has the problem of lack of doc. And it has lots of functions that you may intrest. kcp's doc is in both chinese and english. Good thing is the function detail which is writen in code is english. And you can use asio_kcp which is a good wrap. The kcp is a simple thing. You will write more code if you want more feature. UDT has a perfect doc. UDT may has more bug than others as I feeling. For specifics please refer to:  Reliable Udp Benchmark  and  KCP-Benchmark , for more guidance to the hesitant users. MMO Engine  SpatialOS  has a benchmark report on KCP/TCP/RakNet: for more details, please see the report itself: KCP is used by See  Success Stories . Donation Donation is welcome by using alipay, the money will be used to improve the protocol and documentation. twitter:  https://twitter.com/skywind3000 \nblog:  http://www.skywind.me zhihu:  https://www.zhihu.com/people/skywind3000"}, "210705_news_468780.txt": {"page_id": "210705_news_468780.txt", "text": "Enlarge   /  In this 2011 photo, Dr. Michael A. Lee uses Dragon Medical voice-recognition software to enter his notes after seeing a patient. Earlier today, Microsoft announced its plans to purchase Nuance for $56 per share\u201423 percent above Nuance's closing price last Friday. The deal adds up to a $16 billion cash outlay and a total valuation for Nuance of about $19.7 billion, including that company's assumed debt. Who is Nuance? Enlarge   /  In this 2006 photo, Rollie Berg\u2014who has extremely limited use of his hands due to multiple sclerosis\u2014uses Dragon NaturallySpeaking 8 to interact directly with his PC. Nuance is a well-known player in the field of natural language recognition. The company's technology is the core of Apple's Siri personal assistant. Nuance also sells well-known personal speech-recognition software  Dragon NaturallySpeaking , which is invaluable to many people with a wide range of physical disabilities. Dragon NaturallySpeaking, originally released in 1997, was one of the first commercially available continuous dictation products\u2014meaning software that did not require the user to pause briefly between words. In 2000, Dragon Systems was acquired by ScanSoft, which acquired Nuance Communications in 2005 and rebranded itself as Nuance. Earlier versions of Dragon software used  hidden Markov models \u00a0to puzzle out the meaning of human speech, but this method had serious limitations compared to modern AI algorithms. In 2009, Stanford researcher Fei-Fei Li created ImageNet\u2014a massive training data set that spawned a boom in deep-learning algorithms used for modern, core AI tech. After Microsoft researchers Dong Yu and Frank Seide successfully  applied  deep-learning techniques to real-time automatic speech recognition in 2010, Dragon\u2014now Nuance\u2014applied the same techniques to its own speech-recognition software. Fast forward to today, and according to both Microsoft and Nuance, medically targeted versions of Dragon are in use by 77 percent of hospitals, 75 percent of radiologists, and 55 percent of physicians in the United States. Microsoft\u2019s acquisition play Microsoft and Nuance began a partnership in 2019 to deliver  ambient clinical intelligence \u00a0(ACI) technologies to health care providers. ACI technology is intended to reduce physician burnout and increase efficiency by offloading administrative tasks onto computers. (A 2017  study  published in the Annals of Family Medicine documented physicians typically spending two hours of record-keeping for every single hour of actual patient care.) Acquiring Nuance gives Microsoft direct access to the company's entire health care customer list. It also gives Microsoft the opportunity to push Nuance technology\u2014currently, mostly used in the US\u2014to Microsoft's own large international market. Nuance chief executive Mark Benjamin\u2014who will continue to run Nuance as a Microsoft division after the acquisition\u2014describes it as an opportunity to \"superscale how we change an industry.\" The move doubles Microsoft's total addressable market in the health care vertical to nearly $500 billion. It also marries what Microsoft CEO Satya Nadella describes as \"the AI layer at the healthcare point of delivery\" with Microsoft's own massive cloud infrastructure, including Azure, Teams, and Dynamics 365. The acquisition has been unanimously approved by the boards of directors of both Nuance and Microsoft and it is expected to close by the end of 2021."}, "210705_news_468781.txt": {"page_id": "210705_news_468781.txt", "text": "Enlarge   /  Rochelle Walensky, director of the US Centers for Disease Control and Prevention (CDC), adjusts her protective mask during a Senate Health, Education, Labor, and Pensions Committee hearing in Washington, DC.  Highly effective COVID-19 vaccines are simply too slow to stop surges  like the one underway in Michigan , Rochelle Walensky, director of the Centers for Disease Control and Prevention, said Monday. Dr. Walensky\u2019s explanation during the White House COVID-19 press briefing comes amid mounting requests and calls for federal authorities to flood Michigan with vaccine supply. The state has seen a 400 percent spike in cases since March 5, when state officials eased restrictions on residential gatherings and occupancy limits for bars, restaurants, venues, and stores. Since then, the highly transmissible B.1.1.7 coronavirus variant has also increased in prevalence. Now, the state\u2019s seven-day average for new daily cases is over 7,377, and hospitals are filling up. On March 30, when the surge was already in full swing,  Michigan Gov. Gretchen Whitmer appealed to the White House  for additional vaccine shipments. However, the White House declined, opting to stick to its largely population-based strategy for doling out vaccine supply to each state and jurisdiction. Experts quickly decried the move, saying that an increase in vaccine doses could help bring an end to the surge in cases. In an interview on CBS\u2019s  Face The Nation Sunday  April 11, former Food and Drug Administration commissioner and Pfizer board member Dr. Scott Gottlieb echoed his calls for the White House to boost Michigan\u2019s vaccine allotment. \u201cIt\u2019s a request that\u2019s been made for weeks now, and I think we should have done it weeks ago,\u201d he said. \u201cWe need to get in the habit of trying to surge resources into those hotspots to put out those fires of spread.\u201d But Walensky pushed back on that idea Monday, noting that it takes weeks for people to build up protective immune responses from vaccines. With the mRNA vaccines made by Pfizer-BioNTech and Moderna\u2014the two most common vaccines used in the US right now\u2014people don\u2019t reach full protection until  two weeks after their second dose,  which occurs  three and four weeks after their first , respectively. Reality check \u201cWe know that if vaccines go in arms today, we will not see an effect of those vaccines, depending on the vaccine, for somewhere between two to six weeks,\u201d Walensky said. She continued: So when you have an acute situation, [an] extraordinary number of cases like we have in Michigan, the answer is not necessarily to give vaccine. In fact, we know that the vaccine will have a delayed response. The answer to that is to really close things down, to go back to our basics, to go back to where we were last spring, last summer and to shut things down, to flatten the curve. Walensky emphasized the use of lockdowns, social distancing measures, testing, and contact tracing as the primary responses to surges like Michigan\u2019s. As before, she noted that the CDC has surged other resources to Michigan, including teams to help deal with specific outbreaks and hasten the use of the vaccine doses already distributed to the state. Meanwhile, the B.1.1.7 variant is continuing to spread in states across the country, Andy Slavitt, a senior White House adviser on the pandemic, said during the press briefing. It is now the predominant lineage of the virus nationwide. The White House doesn\u2019t want to prioritize Michigan, potentially compromising vaccination campaigns in others states, Slavitt explained. The current strategy provides the \u201cability to vaccinate people quickly in each of those states,\u201d Slavitt said, \u201crather than taking vaccines and shifting it to play whack-a-mole, [which] isn\u2019t the strategy that public health leaders and scientists have laid out.\u201d The points were largely echoed in  a press briefing by the World Health Organization \u00a0on Monday, which emphasized that globally, the pandemic is growing exponentially. There have now been seven consecutive weeks with increases in cases and four weeks of increasing deaths. Several concerning variants are driving outbreaks in various regions. \u201cThis is not the situation we want to be in 16 months into a pandemic, where we have proven control measures,\u201d WHO technical lead Maria Van Kerkhove said in the briefing. Vaccinations are underway, she noted, but their full impact is not yet in effect. For now, she and the WHO director-general echoed Walensky, imploring people to get back to the basics of physical distancing, mask-wearing, ventilation, hand hygiene, and avoiding crowds\u2014until vaccination efforts are farther along. \u201cWe need headlines around the tools that we have right now that can prevent infections and save lives,\u201d Van Kerkhove said. \u201cWe are in a critical point... It is time right now where everyone has to take stock and have a reality check about what we need to be doing.\u201d"}, "210705_news_468790.txt": {"page_id": "210705_news_468790.txt", "text": "This article will take a closer look at network loops and how they can be abused as part of DDoS attacks. Network loops combined with existing reflection-based attacks can create a traffic amplification factor of over a thousand. In this article, we\u2019ll see how an attacker will only need 50mb/s to fill up a 100gb/s link. I'll demonstrate this in a lab environment. This blog is also a call to action for all network engineers to clean up those lingering network loops as they aren\u2019t just bad hygiene but a significant operational DDoS risk. Network Loops All network engineers are familiar with network loops. A network loop causes an individual packet to bounce around in a network while consuming valuable network resources (bandwidth and PPS). There are various reasons IP networks can have loops, typically caused by configuration mistakes. The only real \u201cprotection\u201d against network loops is the Time To Live (TTL) check. However, the Time To Live check is less of protection against loops, and more protection against them looping forever. The Time to Live (TTL) refers to the amount of time or \u201chops\u201d that a packet is set to exist inside a network before being discarded by a router. The TTL is an 8-bit field in the IP header, and so it has a maximum value of 255. The typical TTL value on most operating systems is 64, which should work fine in most cases. Most network engineers know loops are bad hygiene. I think it\u2019s much less understood (or thought about) what the operational risk of a loop is. In my experience, most loops are for IP addresses that aren\u2019t actually in use (otherwise, it would be an outage), so typically, solving this ends up on the backburner. A Simple Loop example Let\u2019s look at a simple example. The diagram below shows two routers; imagine those being your core or edge routers in your datacenter.  Typical loop For whatever reason, they both believe 192.0.2.0/24 is reachable via the other router. As a result, packets for that destination will bounce between the two routers. Depending on the TTL value, the bandwidth used for that one packet will be: packet size in bytes x 8 x TTL. Meaning for a 512 byte packet and a TTL of 60, the amount bandwidth used is 245Kbs. In other words, the 512 byte packet turned into 307,20 bytes (60x) before it was discarded. You could think of this number 60 as the amplification factor. But.. but.. Network loops are rare That depends on your definition of rare. The good folks at  Qrator  monitor for loops on the Internet. According to the Qrator measurements, there are roughly twenty million unique loops (measured as unique router pairs). source: Qrator Radar. Number of routing loops\u00a0globally Combining loops and common Amplification attacks. Now that we\u2019ve covered the risk of loops and their potential > 100x (the TLL) amplification factor, this may remind you of the traditional DDoS attacks. The record high DDOS attacks you read about in the news are now hitting hundreds of Gigabits per second and peaking into the Terabits. All these large volume metric attacks are mostly the same type of attack and are commonly known as amplification or reflection attacks. They rely on an attacker sending a small packet with a spoofed source IP, which is then reflected and amplified to the attack target. The reflectors/amplifiers are typically some kind of open UDP service that takes a small request and yields a large answer. Typical examples are DNS, NTP, SSDP, and LDAP. The large attacks you read about in the news typically combine a few of these services. By now, it may be clear what the danger of combining these two types of scenarios can be. Let\u2019s look at an example. A typical DNS amplification query could be something like this, and RRSIG query for irs.gov dig -t RRSIG irs.gov @8.8.8.8 This query is 64 bytes on the wire. The resulting answer is large and needs to be sent in two packets; the first packet is 1500 bytes, the second packet 944 bytes. So in total, we have an amplification factor of (1500+944)/64 = 38. IP (tos 0x0, ttl 64, id 32810, offset 0, flags [none], \nproto UDP (17), length 64)\n    192.168.0.30.57327 > 8.8.8.8.53: 42548+ [1au] RRSIG? irs.gov. (36)\n\nIP (tos 0x0, ttl 123, id 15817, offset 0, flags [+], \nproto UDP (17), length 1500)\n    8.8.8.8.53 > 192.168.0.30.57327: 42548 8/0/1 irs.gov. RRSIG, irs.gov. \n    RRSIG, irs.gov. RRSIG, irs.gov. RRSIG, irs.gov. RRSIG[|domain]\n    \nIP (tos 0x0, ttl 123, id 15817, offset 1448, flags [none], \nproto UDP (17), length 944)\n    8.8.8.8 > 192.168.0.30: ip-proto-17 Note: There are many different types of amplification attacks. This is just a modest and straightforward DNS example. Also, note that common open reflectors, such as Public DNS resolvers typically have smart mechanisms to limit suspicious traffic to reduce the negative impact these services could have. The tcpdump output above shows that when the answer arrives back from Google\u2019s DNS service, the TTL value is 123; this is higher than most other public DNS resolvers (most appear to default to 64). If we combine this attack with the \u2018loop\u2019 factor we looked at previously (determined by the TTL value), we have the total amplification factor. Adding up the numbers Ok, so let\u2019s continue to work on the DNS amplification example. The amplification number of 38 and a TTL of 123, would result in a total amplification number of : 38 * (123 / 2) = 2,337 Note that I\u2019m dividing the TTL number by two so that we get a per receive (RX) and transmit(tx) number. For now, let\u2019s use 2,337 as a reasonable total amplification number. What kind of traffic would an attacker need to generate 10G or 100G of traffic? One would need just about 5Mbs/s to saturate a 10g link and say ~50Mbs to saturate a 100Gbs link! These numbers are low enough to generate from a simple home connection. Now imagine what an attacker with bad intentions and access to a larger botnet could do\u2026 Let\u2019s double-check this with a Demo To make sure this is indeed all possible and the math adds up, I decided to build a lab to reproduce the scenario we looked at above. Sequence of\u00a0events The lab contains four devices: An  attacker:  initiating a DNS-based reflection attack. The (spoofed) source IP is set to 192.0.2.53 A  DNS resolver:  receiving the DNS queries (with a spoofed source IP) and replying with an answer that is 38 times larger than the original question. The IP TTL value in the DNS answer is 123. A  router pair  (rtr1\u200a\u2014\u200artr2): Both routers have a route for 192.0.2.0/24 pointing to each other. As a result, the DNS answer with a destination IP of 192.0.2.53 will bounce back and forth between the two routers until the TTL expires. In the screenshot above, we see the attacker on the top left sending queries at a rate of 5.9Mbs. On the bottom left, we see the DNS resolver receiving traffic at 5.9Mbs from the client and answering the queries at a rate of ~173mbs. The IP packets with the DNS responses have a TTL of 123. We see the router pair on the right-hand side: rtr1 on the top right and rtr2 on the bottom right. As you can see, both devices are sending and receiving at 10Gb/s. So, in this case, we observe how the client (attacker) turned 6mb/s into 10Gb/s. Wrapping up In this blog, we looked at the danger of network loops. I hope it\u2019s clear that loops aren\u2019t just a hygiene or cosmetic issue but instead expose a significant vulnerability that should be cleaned up ASAP. We saw that loops are by no means rare and that there are millions of router pairs with network loops. In fact, according to Qrator data, over 30% of all Autonomous Systems (ASns), including many of the big cloud providers, have networks with loops in them. We observed that an attacker can easily saturate a 10G link with 85Mbs (at a TTL of 240) without any UDP amplification. Or if combined with a typical UDP amplification attack, 6Mbs of seed traffic will result in 10G on a looped path, or 60Mb/s could potentially fill up a 100Gbs path! Not all loops are the same Most loops happen between two adjacent routers; quite a few of those appear to occur between an ISP\u2019s router and the customer router. I have also seen loops happen involving up to eight hops (routers) spanning various metro areas while looping between Europe and the US. These transatlantic loops are expensive and hard to scale up quickly. As a result, loops on links like these will have a more significant impact. Call to Action I hope this article convinced you to check your network for loops and make sure you won\u2019t be affected by attacks like these. Consider signing up for the free  Qrator  service, and you\u2019ll get alerted when new loops (or other issues) are detected in your network."}, "210705_news_468805.txt": {"page_id": "210705_news_468805.txt", "text": "Notbremse ausgehebelt, \u00d6ffnungen trotz britischer Variante, Modellregionen - mit diesem \"Mosaik an faulen Kompromissen\" habe die Politik Deutschland in eine neue Pandemiewelle gesteuert. In einem Gastbeitrag f\u00fcr \" ZEIT Online \" haben die Wissenschaftler und Mediziner Melanie Brinkmann,\u00a0Denise Feldner,\u00a0Clemens Fuest,\u00a0Maximilian Mayer,\u00a0Elvira Rosert\u00a0und\u00a0Matthias F. Schneider neben Kritik allerdings auch sieben Ma\u00dfnahmen zur erfolgreichen Pandemie-Bek\u00e4mpfung pr\u00e4sentiert, f\u00fcnf von ihnen betreffen die Ausweitung der Testaktivit\u00e4ten. Die Autorinnen und Autoren geh\u00f6ren zu den Wissenschaftlerinnen und Medizinern, die die sogenannte\u00a0No-Covid-Strategie\u00a0entworfen haben. \"Wir werden nicht m\u00fcde, auf einen besseren Weg aus der Pandemie hinzuweisen. Resignieren ist nicht. Wir m\u00fcssen den Blick gemeinsam nach vorne richten\", schreibt Virologin Brinkmann dazu auf Twitter.   Surftipp: Alle Neuigkeiten zur Corona-Pandemie finden Sie im News-Ticker von FOCUS Online \u00a0 Top-Forscher um Brinkmann erkl\u00e4ren: 7 Ma\u00dfnahmen f\u00fchren uns aus der Pandemie Das Ziel: eine dauerhafte Niedriginzidenz. Denn, so erkl\u00e4ren sie, auch die angestrebte Herdenimmunit\u00e4t durch eine Impfung von 60 bis 70 Prozent der Bev\u00f6lkerung werde aufgrund der ansteckenderen britischen Variante nicht mehr ausreichen. Statt nun auf die notwendige Beschleunigung beim Impfen zu warten, empfehlen die Wissenschaftler einen raschen und konsequenten Lockdown, um die Inzidenzen kurzfristig zu senken, und folgende Ma\u00dfnahmen: eine enorme Ausweitung der Testaktivit\u00e4ten (in f\u00fcnf Schritten), den Ausbau der digitalen Kontaktnachverfolgung und ein klares Signal an die Bev\u00f6lkerung, wann ein Lockdown-Ende erreicht ist.\u00a0\u00a0 Ausweitung der Testaktivit\u00e4ten: 5 Schritte Die Wissenschaftlerinnen und Wissenschaftler setzen auf den Einsatz von Corona-Tests. Dabei z\u00e4hlen sie f\u00fcnf Punkte auf: Erstens: In  bundesweiten Test-Wochen sollten sich alle Menschen im Abstand von zwei bis drei Tagen mittels Antigen-Schnelltests selbst testen oder in Testzentren per PCR testen lassen , schreiben die Autorinnen und Autoren in der \"Zeit\". In Hotspots sollten zudem mobile Teams konsequent alle Bewohnerinnen und Bewohner durchtesten. So k\u00f6nnten versteckte Infektionen erkannt werden. Zweitens:  Schulen, Kitas und Betriebe  sollen unterst\u00fctzt werden um  individuelle Testsysteme  entwickeln und aufbauen zu k\u00f6nnen. Ein wichtiger Schritt sei dabei die  Kopplung von Test- und Pr\u00e4senzpflicht . Hei\u00dft im Klartext: Zutritt nur mit (negativem) Test. Weitere Forderung: \"Alle Personen, darunter insbesondere unvermeidbare Pendler, m\u00fcssen dauerhaft Zugang zu mindestens drei Schnelltests pro Woche bekommen \u2013 je nach Infektionslage auch noch h\u00e4ufiger\", schreiben die Autoren. Drittens: Die Wissenschaftler setzen bei ihrer Strategie auch auf die  Aufkl\u00e4rung der Bev\u00f6lkerung . Das Gesundheitsministerium soll in einer mehrw\u00f6chigen Kampagne \u00fcber das Testen informieren. Landr\u00e4te und B\u00fcrgermeisterinnen sollen Info-Materialien zu Tests und Quarant\u00e4ne in unterschiedlichen Sprachen erhalten. Viertens: Auch  an den   Grenzen m\u00fcsse anders getestet  werden. \"Einreisende nach Deutschland m\u00fcssten vor ihrer Abreise einen PCR-Test machen lassen, nach ihrer Ankunft in Deutschland sofort in Quarant\u00e4ne gehen und sich nach f\u00fcnf Tagen erneut testen lassen\", schreiben die Wissenschaftler. F\u00fcr Berufspendler, Lieferanten und in H\u00e4rtef\u00e4llen k\u00f6nnten Ausnahmeregelungen gelten. Die aktuelle Regelung, die lediglich einen negativen Test vor der Abreise und eine Quarant\u00e4ne nur f\u00fcr Einreisende aus Risikogebieten vorsieht, sei aus ihrer Sicht \"nicht wirksam und daher untauglich\". Einreisende sollten zudem keine \u00f6ffentlichen Verkehrsmittel nutzen. Man k\u00f6nne dem Beispiel Taiwans folgen, die ein spezielles Taxi-System verf\u00fcgten. F\u00fcnftens: F\u00fcr eine potentielle  Isolation  m\u00fcssten ebenfalls Vorkehrungen getroffen werden. Daf\u00fcr m\u00fcssten \"die  arbeitsrechtlichen Rahmenbedingungen  wie Lohnfortzahlung und Arbeitsplatzsicherheit so ver\u00e4ndert werden, dass sich positiv getestete Personen und ihre Kontakte umgehend selbst isolieren k\u00f6nnen\". Die Isolierten sollten zudem durch eine  \"Quarant\u00e4nebegleitung\" psychologisch unterst\u00fctzt  werden. Kirchengemeinden oder Vereine k\u00f6nnen hierbei helfen, so die Autoren. Zudem br\u00e4uchte es externe Unterbringungsangebote in Form von Quarant\u00e4nehotels, wie sie in anderen L\u00e4ndern l\u00e4ngst \u00fcblich sind. Nur dann k\u00f6nnen sich auch Menschen, die in beengten Wohnverh\u00e4ltnissen leben, sicher isolieren. Digitalisierung der Kontaktnachverfolgung vorantreiben und klares Lockdown-Ende festlegen Zwei weitere Ma\u00dfnahmen empfehlen die Wissenschaftler dar\u00fcber hinaus. So m\u00fcsse die  digitale Kontaktnachverfolgung und Quarant\u00e4neanordnung drastisch verbessert  werden. Zwar h\u00e4tten mehr als 300 Gesundheits\u00e4mter die Software Sormas zum Kontaktpersonenmanagement installiert, doch nur ein Drittel der \u00c4mter w\u00fcrde sie tats\u00e4chlich nutzen, kritisieren die Autoren. Man m\u00fcsse das digitale Angebot entsprechend ausbauen und breiter vernetzen. Und um der Lockdown-M\u00fcdigkeit der Bev\u00f6lkerung entgegenzuwirken, empfehlen die Wissenschaftler noch einen wichtigen Aspekt: das  Ende des Lockdowns  sollte klar bestimmt werden. Die Verfechter der No-Covid-Strategie empfehlen hierf\u00fcr  eine Sieben-Tages-Inzidenz von unter 10 . Es sollte zudem keine Corona-F\u00e4lle unbekannten Ursprungs geben. Schulen k\u00f6nnten zwar schon fr\u00fcher \u00f6ffnen, allerdings nur mit systematischen Tests und einer deutlich niederigeren Inzidenz als der aktuellen. \u00a0 "}, "210705_news_468808.txt": {"page_id": "210705_news_468808.txt", "text": "It looks like any other wireless security camera you could buy on Amazon.\u00a0 But this one is made by Dahua \u2014 one of a few Chinese surveillance providers the U.S. Defense Department and NASA are\u00a0 banned from doing business \u00a0with. The\u00a0 government says \u00a0the companies support China\u2019s military \u2014 meaning whatever the camera sees could end up there.\u00a0 \"The idea that the footage can be taken by the Chinese is not even theoretical. It is absolutely verified.\" That\u2019s retired Brigadier Gen. Robert Spalding, who previously led strategy for the National Security Council at the White House. Having a device in your home is the same as if you had a tunnel that had been dug from an enemy's location up through your house, coming in through your living room.\u00a0 But there\u2019s more. The U.S. blacklisted Dahua for human rights abuses: An estimated one million people have been sent to internment camps in China \u2014 the Biden administration\u00a0 calling it genocide . Dahua is accused of contributing by sharing its technology to monitor minorities, especially an ethnic group called Uyghurs. \"Now an investigation into Dahua\u2019s own software shows it tracks specifically whether someone is a Uyghur.\" With a few clicks, Newsy found Dahua cameras being sold in plain sight on Amazon \u2014 and there are no warnings alerting customers of government ties or human rights violations.\u00a0 That\u2019s also true for China\u2019s Hikvision, a company accused by the U.S. of the same transgressions, and that bills itself as the world\u2019s \"largest manufacturer of video surveillance products.\" Hikvision\u2019s digital\u00a0 storefront \u00a0still stands \u2014 along with a subsidiary whose products include a baby monitor compatible with Amazon Alexa. Other shops sell the companies\u2019 products \u2014 and have names like \u201c DC Security \u201d even though they\u2019re based in China.\u00a0 Selling their products isn\u2019t illegal, but it does raise ethical questions for tech analysts like Lindsay Gorman.\u00a0 \"Companies like Amazon need to have robust due diligence practices and do a scan of all their products, where they source them, and understand really deeply the connections between those products and some of the abuses we've seen.\" Officials from Hikvision and Dahua tell Newsy they aren\u2019t connected to China\u2019s military or the repression of civilians. A Hikvision spokesperson says it\u2019s engaging with governments globally to clarify \u201cmisunderstandings.\u201d\u00a0 Amazon says it aspires to be the \"Earth\u2019s most customer-centric company\" and tells us it's still in compliance with the law. But an employee speaking on condition of anonymity says there\u2019s \"too much\" to keep track of. Candidly, Amazon is \"too big.\" The company also says it hastily purchased Dahua thermal cameras in 2020 to protect employees from COVID-19 but doesn\u2019t intend on buying more. Trending stories at  Newsy.com"}, "210705_news_468887.txt": {"page_id": "210705_news_468887.txt", "text": "Die brasilianische Regierung hat erstmals eine nationale Strategie f\u00fcr k\u00fcnstliche Intelligenz (KI) ver\u00f6ffentlicht. Das Dokument stellt Ma\u00dfnahmen vor, mit deren Hilfe Forschung, Innovation und die Entwicklung entsprechender Technik zur Bew\u00e4ltigung der gr\u00f6\u00dften Herausforderungen des Landes vorangetrieben werden sollen. Auch eine KI-Ethik geh\u00f6rt dazu. Laut dem Sekret\u00e4r f\u00fcr Unternehmertum und Innovation des Ministeriums f\u00fcr Wissenschaft, Technologie und Innovation (MCTI), Paulo Alvim, wurde die Strategie durch das Zusammenstellen verschiedener und sektoraler Ansichten erstellt, einschlie\u00dflich der Ber\u00fccksichtigung internationaler Erfahrungen. Ein gro\u00dfer Schritt f\u00fcr Brasilien \"Die gro\u00dfe Relevanz und die Bandbreite der Auswirkungen von KI haben unz\u00e4hlige L\u00e4nder dazu veranlasst, Richtlinien, Strategien oder Pl\u00e4ne zu entwickeln, die sich mit diesem Thema befassen. In Brasilien arbeiten wir seit 2009 verst\u00e4rkt an diesem Thema. Zun\u00e4chst haben wir eine Umfrage mit Experten durchgef\u00fchrt. Parallel dazu haben wir zwischen Dezember 2019 und M\u00e4rz 2020 eine \u00f6ffentliche Konsultation der Gesellschaft \u00fcber die elektronische Plattform der Bundesregierung durchgef\u00fchrt, bei der wir rund tausend Beitr\u00e4ge erhalten haben\", freut sich der Regierungssekret\u00e4r. Laut seinem Chef, dem brasilianischen Minister f\u00fcr Wissenschaft, Technologie und Innovation und Oberstleutnant der Luftwaffe, Marcos Pontes, ist  die Ver\u00f6ffentlichung  \"die Erf\u00fcllung eines Traums\" und ein gro\u00dfer Schritt f\u00fcr Brasilien, da die Regierung KI als \"essentiell\" f\u00fcr die Entwicklung vieler anderer Technologien, wie z. B. Innovationen rund um das Internet der Dinge, ansieht.  Netzwerk der KI-Forschungszentren Pontes wies auch darauf hin, dass die brasilianische Regierung auch Fortschritte beim Netzwerk der nationalen KI-Forschungszentren mache. Ohne ein konkretes Datum zu nennen, sagte der Minister, dass vier der acht KI-Zentren fertiggestellt werden sollen.  Das Dokument analysiert  den aktuellen Stand der KI in der Welt und vergleicht die globalen und lokalen Szenarien -- laut der Publikation haben die USA im Jahr 2019 mehr als 224 Millionen US-Dollar in KI-Startups investiert, Brasilien hingegen nur eine Million Dollar. Es enth\u00e4lt aber auch den Hinweis, dass Brasilien vor erheblichen Herausforderungen stehe, um seine KI-Industrie zu entwickeln, wie z. B. fehlende F\u00e4higkeiten, eine hohe Steuerlast f\u00fcr Unternehmen und B\u00fcrokratie.  OECD-Standards gelten in Brasilien Brasilien h\u00e4lt sich an die menschenzentrierten KI-Prinzipien der Organisation f\u00fcr wirtschaftliche Zusammenarbeit und Entwicklung (OECD), die Empfehlungen zu Bereichen wie Transparenz und Erkl\u00e4rbarkeit enthalten. Basierend auf den OECD-Prinzipien diskutiert die Strategie sechs vertikale Themen: Qualifizierung f\u00fcr eine digitale Zukunft, Schaffung von Arbeitskr\u00e4ften, Forschung, Entwicklung, Innovation und Unternehmertum, staatliche Anwendung von KI, Anwendung in den produktiven Sektoren und \u00f6ffentliche Sicherheit. Dar\u00fcber hinaus werden drei gemeinsame Themen f\u00fcr alle Bereiche diskutiert: Gesetzgebung, Regulierung und ethische Nutzung sowie internationale Aspekte und KI-Governance. Die Ziele des Plans, um die verschiedenen Themen voranzutreiben, sind: die F\u00f6rderung kontinuierlicher Investitionen in die KI-Forschung und -Entwicklung; die Beseitigung von Barrieren f\u00fcr KI-Innovationen; die Ausbildung von Fachkr\u00e4ften f\u00fcr das KI-\u00d6kosystem; die Stimulierung von Innovationen und die Entwicklung der brasilianischen KI in einem internationalen Umfeld und die Etablierung von Kooperationen zwischen \u00f6ffentlichen und privaten Organisationen, der Industrie und Forschungszentren f\u00fcr die KI-Entwicklung.  Kooperation mit der Privatwirtschaft Innerhalb der Ziele skizziert der Plan verschiedene strategische Ma\u00dfnahmen, um die KI-Entwicklung in Brasilien voranzutreiben. Dazu geh\u00f6ren die Notwendigkeit von Forschung rund um ethische KI, die Aktualisierung des nationalen Lehrplans, um Elemente in Bezug auf Computer und Programmierung zu integrieren, die F\u00f6rderung von Vielfalt in Teams, die KI-Technologien entwickeln, und die Schaffung von Governance-Systemen f\u00fcr die KI-Nutzung im \u00f6ffentlichen und privaten Sektor. (tol)"}, "210705_news_468909.txt": {"page_id": "210705_news_468909.txt", "text": "Heroku has long been held up as the gold-standard  platform as a service (PaaS)  for software developers to easily deploy their code without having to worry about the underlying infrastructure, while others see it as akin to a  magical fallen civilization  with a limited future. \u201cThe history of IT is littered with platforms people thought were fantastic that don\u2019t exist anymore,\u201d said James Governor, a founder of the developer-focused analyst firm RedMonk. \u201cIt had a good run and a huge influence, but nothing lasts forever.\u201d Heroku\u2019s architectural limitations and the high cost of running a business on the platform have historically hindered its ability to truly scale beyond a core set of web 2.0 customers, but there is still hope that Heroku is setting itself up for a glorious second act. Heroku\u2019s revolutionary legacy Founded in 2007 by three  Ruby  developers\u2014James Lindenbaum, Adam Wiggins, and Orion Henry\u2014Heroku was  bought  just three years later, when the SaaS giant Salesforce eventually beat out VMware to pick the company up for $212 million when it still had only 30 people on staff and supported only the Ruby programming language. \u201cI believe Heroku was one of the most revolutionary products of its generation and pushed web development further forward than it gets credit for,\u201d said Jason Warner, head of engineering at Heroku between 2014 and 2017. \u201cIt is also one of the most confounding, because it was so ahead of its time. It looked like magic at the time, and people were blown away by it, but it started to calcify under Salesforce. It should never have been a PaaS; it should have been a multilayered cake of PaaS with various escape hatches to build out with Kubernetes or go multicloud, but that wasn\u2019t what was to be.\u201d Today, Heroku is part of the broader  Salesforce Platform  of developer tools, but it remains a successful business in its own right, accounting for hundreds of millions of dollars in annual revenues and supporting a wide range of languages and thousands of developers who run applications on it. \u201cSalesforce has made it more stable, scalable, and support new languages. The core idea of taking an app and pushing to the cloud without having to think about servers, with a beautiful developer experience, is the same today, and I know that because I am a customer,\u201d cofounder Adam Wiggins said. In practice, using Heroku typically involves a common runtime of deploying to a unique domain, which routes HTTP requests to a virtualized Linux container\u2014or dyno, as Heroku calls them\u2014spread across a \u201cdyno grid\u201d of AWS servers. Heroku\u2019s  Git  server handles application repository pushes from permitted users. There is also the option for dedicated, single-tenant  Private Spaces  for premium enterprise customers. \u201cHeroku was one of the first real cloud-native development environments, and they essentially invented the widespread model of container-based computing,\u201d said Yefim Natis, a distinguished vice president at Gartner. \u201cThe thing that blew people\u2019s mind was the Git push to deploy, which is the core idea people take away from Heroku, to take away all of this other stuff people thought they had to do,\u201d said Heroku cofounder Lindenbaum, now a partner at the startup accelerator Heavybit. \u201cOur vision wasn\u2019t to put lipstick on a pig, but to rethink how this problem isn\u2019t a problem anymore.\u201d Heroku\u2019s popularity has always hinged on its simplicity, elegance, and usability, pioneering the focus on the developer experience and aiming to make deployment as seamless as the development process. \u201c[Heroku] was magical and everyone that saw it freaked out,\u201d said Adam Jacob, Chef cofounder and now CEO of the System Initiative. Technologically, Heroku has stagnated Ten years on, none of the original cofounders are still at Heroku. Meanwhile, under Salesforce, the company has steadily grown its revenues but left the core product largely alone, while broad industry shifts occurred around it. \u201cHeroku is like a fallen civilization of elves. Beautiful, immortal, beloved by all who encountered it\u2014but still a dead end,\u201d Jacob  tweeted .    \u201cWhen I joined Heroku, the vision had been fulfilled, but it is also static and has been for some time, which is the frustrating thing for some people,\u201d Warner said. Although Heroku helped pioneer simplified, cloud-native software development techniques, it took too long to adapt to the emerging industry standards of  Docker containers  orchestrated by Kubernetes, said Gartner\u2019s Natis. \u201cAs far as its architecture and its pioneering character, I think that stopped with the acquisition [by Salesforce]. I think they got frozen in time.\u201d Tod Nielsen, who was Heroku\u2019s CEO from 2013 to 2016, said from a business perspective, \u201cSalesforce did a great job of expanding Heroku within corporates.\u201d But technologically, \u201cwhat they gave up was all the \u2018cool kid\u2019 innovation.\u201d Built on AWS EC2 instances, Heroku\u2019s underlying dyno grid system naturally trades off complexity and customizability for simplicity and speed. These trade-offs make the platform elegant and easy to use, but also somewhat inflexible. For a certain set of companies\u2014namely those building  12-factor web applications \u2014Heroku has and always will be a piece of technical wizardry. \u201cIt was very powerful as a developer workflow that was highly productive for a certain class of application, which a lot of startups were building at the time,\u201d said RedMonk\u2019s Governor. However, as Heroku expanded into other languages, issues cropped up. \u201cI think we were possibly too early in wanting everything to be simple, which becomes difficult when you turn around and try to go to the  Java  community, with its immense amount of tooling and deeply embedded ways of working,\u201d said Blake Mizerany, who was the first full-time engineering hire at Heroku in 2008. \u201cThat would bite us a little bit when we spoke to companies that wanted to build on Heroku, because they always needed something way off the happy path with Heroku.\u201d For organizations that wanted a little more flexibility to run applications where they needed, the rival PaaS  Cloud Foundry  from VMware offered a more palatable route, by allowing for on-premises deployments and the sort of complex customizations required to hook into an enterprise environment. VMware also invested in a consulting arm,  Pivotal Labs , tasked with evangelizing the platform approach for more traditional organizations like Orange or Bank of America in the early 2010s. Heroku, by comparison, has been slow to allow for enterprise customers to operate in  hybrid and multicloud modes , something Salesforce has looked to address with the addition of Private Spaces in 2016, which allows customers to run in a dedicated environment, connect to on-premises systems, and select from one of six geographic regions. Similarly,  Salesforce\u2019s recently launched Hyperforce  should eventually allow all Salesforce customers more choice over where their services run in the public cloud. Where Heroku and other PaaS options thrive is in their ability to lasso complexity for developer teams to better focus on delivering new features for customers. The problem is, most organizations have built-in tech debt and ways of working that must be accounted for, making something as opinionated as Heroku too constraining. \u201cThere end up being too many pieces for people to assemble and maintain themselves, in which case we see people wanting something like Heroku and that ability to just focus on writing the application,\u201d said Stephen O\u2019Grady, the other cofounder of RedMonk. \u201cWe hear this a lot, where customers are spending like 40% of their time fighting Jenkins, for example. The trick is to do this with enough flexibility to meet a wide range of use cases, and that is where things like Heroku have proved to be too constrained or opinionated.\u201d Moving beyond Heroku\u2019s limits with build-your-own internal platforms Camille Fournier, head of platform engineering at the hedge fund and financial services firm Two Sigma, describes Heroku as the \u201cgold standard\u201d for the deploy side of the software development process. However, in her experience, \u201cdevelopers will start to meet the limits of what a platform like Heroku can provide and start to veer off of that path.\u201d Fournier believes that any quickly growing engineering organization will confront these limits eventually. \u201cIt tends to become obvious when you need to build your own platform. If you are using Heroku you will hit scaling limits and see teams peel off and do their own thing,\u201d she said. Many organizations that do decide to break away from Heroku\u2014like the streaming platform Hulu\u2014are looking to  build their own internal platform , working countless hours to chase the vision of a platform that resembles the Heroku experience, but meets specific requirements of their business. \u201cThe modern tech industry is basically folks just endlessly remaking remakes of Heroku,\u201d RedMonk analyst Governor has  tweeted . \u201cWhen something is that beautiful, it is not surprising that it spawned its own subgenre,\u201d Jacob said. It is often said that while not many people bought Velvet Underground records, those who did went out and started a band. For software developers of a certain era, Heroku carries a similar legacy. Every developer who came into contact with Heroku continues to chase some version of that legendary developer experience today. \u201cIt absolutely is the Velvet Underground of developer platforms,\u201d Jacob said. But there\u2019s a cost, Jacob noted: \u201cEveryone who touched it has an opinion. The problem is those opinions aren\u2019t just opinions, they are hard constraints when you run a business on software. It\u2019s not fungible and, contrary to popular belief, those constraints are in fact unique.\u201d That being said, for many early Heroku engineers like Mizerany, imitation really is the highest form of flattery. \u201cFor me, the fact that we built something that everyone finds themselves having to build today, is the biggest possible compliment,\u201d he said. Is Heroku too expensive? Pricing  often comes up as a key blocker for organizations who quickly feel like they are outgrowing Heroku, even if they really love the developer experience. \u201cPricing has always been a bugaboo and we never solved it,\u201d Warner said. \u201cAt Salesforce you had to make up margin on pricing. I think you can scale Heroku\u2014it runs some of the top 20 websites in the world\u2014but you have to think about it differently.\u201d Heroku is generally priced per dyno, with a bunch of premium add-ons and high-performance options for enterprise customers, so the cost goes up pretty quickly as your business grows. The highest performing, 14GB dyno costs $500 per dyno per month, and that\u2019s just the start. \u201cSome are willing to pay for that incredible experience, but for many that became challenging,\u201d RedMonk\u2019s Governor said. Take the software testing company Rainforest, which  moved from Heroku  to  Google\u2019s managed Kubernetes service  (GKE) in 2019 after it started to reach the limits of its database plan and costs started to spiral. \u201cUntil late last year, Rainforest ran most of our production applications on Heroku ... it allowed us to scale and remain agile without hiring a large ops team, and the overall developer experience is unparalleled. But in 2018 it became clear that we were beginning to outgrow Heroku,\u201d Rainforest\u2019s former senior architect, Emanuel Evans,  wrote in a company blog post . Furthermore, Evans wrote, Heroku is expensive, even with the savings the company made from being able to run everything through a small operations team. But Heroku tipped from expensive into too expensive, at least for certain compute-intensive workloads, when Rainforest added some important security-related features, such as virtual private cloud. Then there is the fintech PensionBee, which built its monolithic Node.js application from the ground up on Heroku in 2015, underpinned by Salesforce, with all data synced by a premium add-on product called Heroku Connect. PensionBee CTO, Jonathan Lister Parsons, sees the price concerns around Heroku as overblown when total cost of ownership is accounted for. \u201cI think about all the shit you don\u2019t need to do with Heroku and it is a list with 20 operational things on it,\u201d he said. \u201cYes, it is expensive compared to AWS, but you are getting a team of a thousand people who are there to run a service that runs your code very well.\u201d That being said, \u201cHeroku Connect is still unacceptably expensive and, as we grow and scale, it goes past the point where using that solution makes sense\u2014and they know that,\u201d Lister Parsons added. A Salesforce spokesperson acknowledged Heroku\u2019s cost but said, \u201cCloud operations are expensive, and we need to be sure we\u2019re adding all the costs up. If someone is comparing IaaS costs to Heroku\u2019s PaaS offering, they may be overlooking the staffing of devops, pipelines, integrations, and IaaS substrate impacts to operational load.\u201d"}, "210705_news_468933.txt": {"page_id": "210705_news_468933.txt", "text": "The Zig project has a non-profit foundation that needs to be managed, a community to shape, and the actual language to develop. These are all big, complex problems that don\u2019t default to a positive outcome without active care.  Andrew did an amazing job laying the foundations, from deliberately choosing a  non-profit corporate model , to picking clear values based on respect and empathy for the community to crystallize around, and finally by leading development by example both when it comes to exploring new ideas and when doing the methodical part of the work required to go from a proof of concept to a reliable tool,  like zig cc , for example. This is already a huge amount of good work that has been done and I\u2019m happy to have been contributing to it for almost a year by now, but I know that this is still not enough and,  as the project grows, we\u2019ll have new, harder puzzles to solve . One increasingly relevant puzzle is how to exist in the extended cinematic universe of open source development, where the influence of big players can have devastating effects on smaller projects. The importance of this last point cannot be overstated because, of all the various playing fields, this is the one where players enact meticulously planned adversarial strategies that can undo any kind of good work if you\u2019re not attentive enough. In the post where I  announced joining  the Zig Software Foundation I mentioned the importance of acknowledging how  growing a successful project is not an entirely peaceful process , and now I want to talk about how recent experiences have further developed my understanding of the open source game, and how that relates to the future of Zig. The end of Redis As a very funny coincidence, my last day at Redis Labs was also the same day Antirez resigned from the open source project and handed over both the official repository on GitHub and the main website ( redis.io ) to the company.  Redis Labs has always been frenemies with the cloud vendors, and especially AWS. The reason is simple: Redis Labs and all the companies that have formed behind OSS databases have to accept AWS as a partner, but they also want to be the primary beneficiary of the value that  their  respective OSS projects generate, while AWS  begs  to disagree.  This is a battle where neither party is the \u201cgood guy\u201d and the discussion that ensues reduces open source to a set of legal constraints for companies to collab-compete around . More importantly, AWS recently dealt a big blow to its opposition by taking the  license change  made by Elastic and transforming it into a casus belli to legitimize AWS\u2019 own hard fork of Elasticsearch, painting Elastic (and implicitly any company that dares to follow the same path) as a greedy bad actor that forced AWS to \u201c Step[\u2026] up for a truly open source Elasticsearch \u201d.  On the other side of the collab-compete barbed fence, with a hard-fork gun pointed at its head, Redis Labs has opted to play nice with the clouds and to fill the void left by Antirez with a  governance committee  composed of representatives from Redis Labs, Alibaba and AWS.\nAll the people in the committee are long-time contributors to Redis and new decisions are taken collaboratively through proposals on GitHub. This is a great start on paper and yet I\u2019m already extremely disappointed with where they\u2019re taking Redis. While the core team itself might be fine , we can\u2019t forget that all those developers have hierarchies above them and  they lack the ultimate power that Antirez had : copyright ownership and undisputed control over the codebase which allowed him to raise a big, fat middle finger to pressure coming from Redis Labs or any of the clouds.  Another thing that might hint at the sad mess everything is turning into, is what\u2019s at the top of  redis.io . It used to be that all commercial content would be relegated to redislabs.com,  but now apparently a \u201cTry Free\u201d button has found its way to the top menu of the open source website . So now you have to be careful because if you press the wrong (big, red, well positioned) button you might end up signing up for a Redis Enterprise Cloud account instead of getting a copy of Redis. Disgusting. It's not even the same damn product! The direction set by Antirez, imperfect as it might have been, was based on having a sustainable approach to development and making Redis a useful, somewhat minimalistic tool for solving coordination problems in a distributed system.  When I joined Redis Labs I used to think that these people were sitting on a golden goose and that they just needed to realize it . As time went by I came to the bitter realization that nobody in command was interested in doing Redis any justice and that the company,  addicted  to venture capital and utterly incapable of correcting its crippling deficiencies, wanted instead to corrupt the design of the product to compensate. This is the same kind of barren mentality that put a \u201cTry Free\u201d button on the open source website, and that is now indirectly driving the development of Redis. To be fair to some of my ex-colleagues, some people were genuinely good at their job and really tried their best to find harmony between the open source project and the company. Unfortunately their talent got completely wasted because every high level detail of the whole story made that impossibly hard, from the company structure, to its financial strategy, up to external forces like toxic (and effective) marketing and sales tactics from AWS and other players. The Rust Software Foundation The Rust project recently  announced  the creation of a dedicated non-profit organization. I\u2019m very happy for this development because Rust is in many ways like an older cousin that has a lot of things figured out, while we are just getting started (although the Zig Software Foundation predates Rust\u2019s by almost a year). Starting from this premise, I always keep an ear out for new developments to better understand how and when the challenges Rust is facing today are going to show up in our project tomorrow. One of such challenges was recently mentioned by  Ashley Williams , Interim Executive Director of the Rust Software Foundation, in an interview on the  Context Free YouTube Channel . I warmly recommend you check out both video and channel. In the interview Ashley mentions that the Rust software foundation has decided to reserve 5 board seats to \u201cfounding member companies\u201d and among the reasons for this choice she lists: to a certain extent take responsibility for the privilege of being able to use open source technology by giving back to the organization [\u2026] One of the biggest threats when you see an open source project like Rust start getting adopted by a lot of organizations, is that all of these fantastic engineers who are currently spending their time working in open source, on the technology, suddenly get scooped up into fantastic engineering jobs at these organizations but they\u2019re no longer working  on  the technology, they\u2019re asked to work  with  the technology, and this can lead, unless you\u2019re incredibly disciplined, to a type of brain drain in the project. I\u2019m not ecstatic at the idea of the 3 letter smirk being present in yet another board of directors but, at least in the case of Rust, we can see that the chain of command is properly aware of the negative effects that big tech can have on open source projects and, as long as they\u2019re attentive, I\u2019m sure everything will go well. The Zig Software Foundation might be in a bit of a pickle though! How are we going to stop big organizations from stealing our talent? I\u2019m certainly not in a position to have one Zoom call with today\u2019s Google CEO to  strike a cartel deal  over employees and in fact I \u2019 ve  been   busy   antagonizing  these people more than anything else. More importantly,  the Zig Software Foundation does not intend to give up board seats to any big tech company . Obviously we can\u2019t compete with big tech companies when it comes to salary. Since we\u2019re a small organization, we can offer a flat hierarchy and a lot of flexibility, but so can startups, and the salary problem remains. So what can we offer that they can\u2019t? Working in big tech While I\u2019m sure there must be some thoroughly enjoyable, albeit rare, jobs that you can get in big tech, my impression is that the vast majority falls into two categories: Work at a megacorp and either you rot working on menial things, or learn to play the politics game to get a shot at working on the one interesting project\u2026 until it gets canceled when the CEO decides to \u201crefocus\u201d, or when Monday comes if you work at Google. Work at a startup and tackle interesting problems, but know that the whole thing is inevitably meant to be a user acquisition trick to build a moat and get to a quick exit, which more often than not means selling everything to a megacorp that will progressively compromise the few good things about the product, while slowly turning everything into [1].  If you have any appreciation for good software that respects the end user and tries to find earnest solutions to problems, you are SOL when it comes to big tech jobs. Personally, I\u2019m tired of this, both as a user and as an engineer.  As a user, I\u2019m tired of having video drivers that require me to give up my email address before I can download an update, of garage openers that  require a monthly subscription fee , of seeing Oculus requiring a Facebook login after being bought,  Waze being sold to Google , Discord to Microsoft or whoever it will be.  People used to say \u201cwhen you don\u2019t pay for a service, you\u2019re the product\u201d, but now you always are the product and sometimes you even have to pay for the privilege . As an engineer,  I\u2019m tired of working on systems that are openly hostile to the end user, where the best and most elegant solution is rejected in the pursuit of an extravagant business model . I used to think that working in open source would save me from all this, but the truth is that this is not the case in practice nor in theory. It pains me to admit this publicly, but I fell for the marketing and only realized what open source really was once I started working at Redis Labs. Now you might think that since I\u2019m disillusioned with open source, I should look into the free software movement, but in my opinion free software is a disaster on too many fronts and its leadership has failed so badly that I don\u2019t even want to waste words discussing it past quoting this tweet by Steve Klabnik, with which I wholeheartedly agree. As far as I\u2019m concerned,  neither open source nor free software are able to represent my ideals when it comes to software  and I feel the need to come up with a new way of articulating what is missing from both of these movements. It\u2019s not easy to come up with a full fledged description of whatever this thing should be, but as a first approximation I came up with \u201d software you can love \u201d. It\u2019s very vague, but it perfectly captures the good parts about open source and free software, and filters out many of their flaws.  There\u2019s a limit to how much you can love software with terrible UX, just as much as there\u2019s a limit to how much you can love software that has good UX, but that keeps nagging you about enabling notifications because it  really needs  more engagement, or software that is bloated, janky and that has short shelf life because of bad engineering choices. It also captures the fact that having the source code available is nice for learning and \u201cright to repair\u201d purposes, but that there is more to software you can love and that sometimes a reasonably priced, rocksolid, proprietary tool can be preferable to a janky OSS project connected to a murky business model. Another good thing is that  I\u2019m not the only one thinking about these ideas: there\u2019s a galaxy of projects out there that have independently rediscovered most of them . The Zig project is one, but there\u2019s also the  Handmade Network , Andreas Kling\u2019s  SerenityOS  and more. While I don\u2019t speak for other projects, their respective philosophies can be learned by looking at the  Handmade Manifesto  or by watching  Andreas Kling\u2019s YouTube channel , and I\u2019ve started reaching out to them, to learn more about what they have to say. For example I recently interviewed  Abner Coimbre , ex-NASA engineer and cofounder of the Handmade Network on these same topics. A few days from the moment of writing I will interview  Jonathan Turner , who also recently left his cushy big tech job to fly solo, and hopefully I\u2019ll soon get a chance to interview  Andreas Kling , who in many ways seems to share the same collaborative spirit that has has been a major strength of the Zig project. With this lengthy premise, I\u2019m finally able to answer the brain drain question. Software you can love Big tech has been increasingly unable to create software you can love , and that\u2019s the ability you gain by refusing to work for the silicon valley. It\u2019s not for everyone and I can understand if somebody prefers the security of a stable job in a company too big to fail, where they can just go home after work, forget about software, and live a very comfortable life but  big tech jobs are not the absolute best choice for those who really love software craftsmanship  and, as it turns out, some of these people have taken a liking to Zig. A few days ago  Jakub Konka  left his job at Microsoft to become our first full-time core contributor. Jakub is a long time Rust user, ex-Wasmtime core contributor, and member of the Bytecode Alliance who started by contributing WASI bits to the Zig standard library and ended up reverse engineering the aarch64 MachO executable format in order to write a linker that  made Zig the first C cross compiler ever able to target Apple Silicon from any other supported platform . Here you can read more  about Jakub\u2019s involvement and what it means to work for the Zig Software Foundation, but to answer the high level question about our strategy, I think  it\u2019s big tech that should be worried, not us .  The Zig Software Foundation plans to remain a small and nimble organization so we\u2019re never going to be a threat to any big tech company when it comes to poaching \u2014  too much  \u2014 talent, but there\u2019s a growing movement of people who, just like us, are fed up with unlovable software and that want to do things differently.  I want these projects to grow and I\u2019ll be happy to share with them every bit of expertise I have when it comes to playing the open source game . Abner is running  the Handmade Seattle conference , which should return to being an in-person event later this year, and in the meantime  you can support all the out-of-conference content he produces by subscribing for a small monthly fee . Jonathan  wants to bring Nushell to v1.0 and is working on learning materials  for the Rust community that you can find on  his YouTube channel . He even tried out Zig, which allowed me to capture  this funny clip . Serenity OS is looking  to allow Andreas to work full-time on the project  and, who knows, an OS is a big project, especially one that is writing its own IDE, web browser and everything, so it might want to be able to pay major contributors one day. The Zig Software Foundation is looking for more donations  to pay two more full-time core contributors  in order to get faster to v1.0. Finally, if you run a project of comparable size and goals, I\u2019d love to get in touch. We\u2019re building software you can love, and big tech can\u2019t compete with us ."}, "210705_news_468948.txt": {"page_id": "210705_news_468948.txt", "text": "Last week, the Biden administration published its \u201cskinny budget\u201d proposals \u2014 essentially a wish list of the most important discretionary expenditures for the coming year. It\u2019s divided roughly evenly  between military and nonmilitary spending . Nonmilitary spending comes in at $769 billion, marking a 16 percent increase over Trump-era expenditures on items such as environmental investments and public health infrastructure (the nearly $9 billion request for the Centers for Disease Control and Prevention is the largest request in two decades). After the past four years of deliberate neglect of nonmilitary public infrastructure,  it will allow for a vital shoring up of a range of critical services , from health delivery systems to schools in low-income areas (the skinny budget request envisions a 40 percent increase in funding for the Department of Education). It requests billions of dollars to tackle climate change, a 20 percent increase in the dollars going to the Environmental Protection Agency and billions more to be channeled into research on cancers. Compare this budget with Trump\u2019s proposal for the year  \u2014 a more than 9 percent cut in nonmilitary spending compared to 2020 levels, and an ongoing assault against any and every part of the environmental and public health structures \u2014 and the budget becomes a study in contrasts with Trumpian priorities. And, while critics have pointed out this only takes nonmilitary spending back up to where it would have been had Trump followed the norms of the past three decades, it comes on the heels of passage of the multitrillion-dollar American Rescue Plan, and during negotiations over the $2.3 trillion infrastructure package that Biden is pushing \u2014 to be delivered over an eight-year period and funded by 15 years of increased taxes on wealthy individuals and corporations. Between these three financial packages, there\u2019s a lot of money in play \u2014 upwards of $6 trillion, or about one-quarter of what the  total U.S. GDP  was last year. All of these investments will, if enacted, put the U.S. on a dramatically different path than the one that Trump took the country down, though  far from a New Deal-type of transformation . And such change, in my book, can only be to the good. Less positive is that the defense request (for the Defense Department and miscellaneous other military- and security-related expenditures) comes in at $753 billion, a slight increase over Trump-era spending. Last summer, Sen. Bernie Sanders (I-Vermont) and other progressives in Congress proposed cutting defense spending by 10 percent and redirecting the dollars to domestic programs. It was the latest effort by the left to shift spending priorities away from weapons of war and toward social programs. And, as usual, it went down to defeat. By huge margins,  both houses of Congress rejected the idea , despite the fact that  opinion polls  show a growing number of Americans don\u2019t want to increase military spending and nearly a third of respondents say too much is spent on the military. This week, when it became clear that Biden\u2019s team wasn\u2019t about to cut defense spending, a range of progressives, including Sanders and Rep. Ro Khanna (D-California), expressed serious concern. Sanders pointed out that the  U.S. already spends more on its military than do the next 11 countries combined . Many progressive  advocacy groups also came out against the increased military spending . Biden\u2019s team in key ways buys into the ideology of the U.S. as the indispensable global policeman. Partly, Biden\u2019s problem here is that he is locked into a series of spending commitments negotiated long before he was elected president. Modernizing the country\u2019s nuclear arsenal \u2014 a terrifying stash of weapons that\u2019s already more than capable of ending most life on Earth if ever used \u2014 will cost an ungodly amount of money over the coming decades, and will only serve to make already astonishingly destructive weapons systems even more capable of inflicting catastrophic damage on the world. Yet the process over the past decades has basically been bought into by every president of the 21st century, and in  Congress, there\u2019s large bipartisan support for the upgrades . So, too, the  purchase of hundreds of F-35 aircraft , the most expensive weapons procurement contract in U.S. military history, will cost many hundreds of billions of dollars over the coming decades. But it\u2019s not just that the past is dictating the present here. It\u2019s also that Biden\u2019s team in key ways buys into the ideology of the U.S. as the indispensable global policeman. Defense Secretary  Lloyd Austin was on the board of directors for weapons manufacturer Raytheon  before he took his current job. Secretary of State  Antony Blinken has financial ties to a venture capital fund  with heavy investments in national security. In that sense, the proposal to increase spending on the military isn\u2019t in the least surprising. It is, in fact, in keeping with a bipartisan history that has prioritized military spending over much else for decades, and that has seen a constant churn of senior figures dividing their careers between positions of political power and lucrative private sector jobs in the defense industry. Indeed, since World War II, U.S. military spending levels have largely been at the whim of  what President Dwight D. Eisenhower termed a \u201cmilitary-industrial complex,\u201d  meaning that, regardless of need, there was, and remains, always extreme political pressure to increase the number of dollars channeled into weapons research and purchases, as well as the support systems that go into maintaining the world\u2019s largest military. Skinny budget requests are never the final word. They are the starting point to a conversation. At this moment, with the public supportive of big infrastructure investments and a reimagining and expansion of the social safety net, and with  a plurality of Democrats in favor of reducing military spending , there is a chance to shift the direction of federal expenditures. Representative Khanna and other progressives who have voiced concern at defense budgets that only ever go up have a chance, in the coming months, to help reshape the conversation on levels of military spending in this pandemic moment. It is a conversation long overdue and one that the Biden administration, with its progressive bent, ought to be ready to constructively engage in. \nCopyright \u00a9 Truthout. May not be reprinted without  permission .\n"}, "210705_news_468991.txt": {"page_id": "210705_news_468991.txt", "text": "Pablo Bonilla Ataides (left) with co-author Dr Ben Brown from the School of Physics.  Photo: Louise Cooper What started out as a second-year physics project is making its way into Amazon Web Service\u2019s (AWS) quantum computing program. University of Sydney science undergraduate  Pablo Bonilla Ataides  has tweaked some computing code to effectively double its capacity to correct errors in the quantum machines being designed in the emerging technology sector. The simple but ingenious change to quantum error correcting code has grabbed the attention of quantum researchers at the  AWS Center for Quantum Computing  in Pasadena, California, and the quantum technology programs at Yale University and Duke University in the United States. \u201cQuantum technology is in its infancy, partly because we haven\u2019t been able to overcome the inherent instability in the machines that produce so many errors,\u201d 21-year-old Mr Bonilla said. \u201cIn second-year physics I was asked to look at some commonly used error correcting code to see if we could improve it. By flipping half of the quantum switches, or qubits, in our design, we found we could effectively double our ability to suppress errors.\u201d The research is published today in  Nature Communications . The results of the study, co-authored by  Dr Steve Flammia  who has recently moved from the University of Sydney to AWS\u2019s quantum computing effort, are to feature\u00a0in the tech company\u2019s arsenal of error correction techniques as it develops its quantum hardware. Dr Earl Campbell  is a senior quantum research scientist at AWS. He said: \u201cWe have considerable work ahead of us as an industry before anyone sees real, practical benefits from quantum computers. \u201cThis research surprised me.\u00a0I was amazed that such a slight change to a quantum error correction code could lead to such a big impact in predicted performance. \u201cThe AWS Center for Quantum Computing team looks forward to collaborating further as we explore other promising alternatives to bring new, more powerful computing technologies one step closer to reality.\u201d Quantum errors Errors are extremely rare in the digital transistors, or switches, that classical computers use to run our phones, laptops and even the fastest supercomputers. However, the \u2018switches\u2019 in quantum computers, known as qubits, are particularly sensitive to interference, or \u2018noise\u2019, from the external environment. In order to make quantum machines work, scientists need to produce a large number of high-quality qubits. This can be done by improving the machines so they are less noisy and by using some capacity of the machines to suppress qubit errors below a certain threshold in order for them to be useful. That is where quantum error correction comes in. Assistant Professor Shruti Puri  from the quantum research program at Yale University said her team is interested in using the new code for its work. \u201cWhat amazes me about this new code is its sheer elegance. It\u2019s remarkable error-correcting properties are coming from a simple modification to a code that has been studied extensively for almost two decades,\u201d Assistant Professor Puri said. \u201cIt is extremely relevant for a new generation of quantum technology being developed at Yale and elsewhere. With this new code, I believe, we have considerably shortened the timeline to achieve scalable quantum computation.\u201d\u00a0 Co-author  Dr David Tuckett  from the School of Physics said: \u201cIt\u2019s a bit like playing battleships with a quantum opponent. Theoretically, they could place their pieces anywhere on the board. But after playing millions of games, we know that certain moves are more likely.\u201d Retrofit for industry"}, "210705_news_468996.txt": {"page_id": "210705_news_468996.txt", "text": "Microsoft will offenbar seine Einkaufstour  fortsetzen und greift nach seinem Kooperationspartner f\u00fcr Sprachsoftware Nuance. Volkswagens Versorgung mit Elektroauto-Batterien in den USA scheint gesichert, nachdem zwei s\u00fcdkoreanische Unternehmen ihre Streitigkeiten beigelegt haben. So schnell werden in der EU Verbrennermotoren indes aber nicht verboten. In der Abgasnorm 7 sollen sie weiter zugelassen sein. Neues hat indes das ESA-Teleskop Gaia jenseits des Firmaments entdeckt: Mehr Einsteinkreuze als erwartet. Das Wichtigste vom Tage in K\u00fcrze. US-Softwarekonzern  Microsoft  will mit dem  Zukauf von Nuance  sein Angebot f\u00fcr Sprachverarbeitungsdienste ausbauen. Nuance ist unter anderem f\u00fcr seine Diktier- und  Spracherkennungssoftware  mit dem Markennamen Dragon bekannt. Auch gilt das Unternehmen als ein f\u00fchrender KI- und Sprachverabeitungs-Spezialist. Zuletzt hatte Microsoft seine Gaming-Sparte mit dem milliardenschweren Zukauf von Zenimax Media gest\u00e4rkt, der Mutterfirma des Spielepublishers Bethesda. Zudem hatten US-Medien im M\u00e4rz \u00fcber Gespr\u00e4che \u00fcber eine \u00dcbernahme der US-Firma Discord f\u00fcr \u00fcber 10 Milliarden US-Dollar berichtet, einem Onlinedienst f\u00fcr Sprach-, Video- und Textchats, der sich anfangs vor allem an Computerspieler richtete.  Microsoft und Nuance arbeiten bereits seit 2019  im Bereich medizinischer Spracherkennung  zusammen . Aufatmen bei  Volkswagen  in den Vereinigten Staaten:  Aufgrund eines Rechtsstreites der s\u00fcdkoreanischen Unternehmen LG und SK  war die Versorgung mit Batterien f\u00fcr die Elektroautoproduktion gef\u00e4hrdet. Dadurch war die Erf\u00fcllung von Liefervertr\u00e4gen von Volkswagen und Ford mit SKI in Gefahr. Hintergrund war ein Streit der Muttergesellschaft von LG Energy Solution, LG Chem, mit SK Innovation (SKI). Im April 2019 hatte LG in den USA den Konkurrenten wegen des Vorwurfs verklagt, Mitarbeiter abgeworben und Gesch\u00e4ftsgeheimnisse gestohlen zu haben. Beide Unternehmen hatten sich au\u00dferdem gegenseitig mit Patentklagen \u00fcberzogen. Jetzt haben sie sich aber geeinigt. Dabei wird der Markt f\u00fcr Elektroautomobile in der Europ\u00e4ischen Union nun vielleicht doch nicht so schnell wachsen wie erwartet. Zumindest muss die deutsche Autoindustrie muss erst einmal nicht mehr akut bangen, dass Verbrenner-Autos in der EU faktisch verboten werden. Nach einem  Treffen der f\u00fcr die k\u00fcnftige Abgasnorm Euro 7 zust\u00e4ndigen Arbeitsgruppe  habe sich gezeigt, dass \"die EU-Kommission die Grenzen des technisch Machbaren akzeptiert\", sie habe sich von \"unerreichbaren Zielen verabschiedet\", sagte Hildegard M\u00fcller, Pr\u00e4identin des  Verbands der Automobilindustrie  (VDA). Der VDA ist an der Advisory Group on Vehicle Emission Standards (AGVES) der  EU-Kommission  beteiligt, die nun ihre Empfehlungen f\u00fcr  Euro 7  vorgestellt hat. Dabei habe sich ergeben, dass bisherige Pl\u00e4ne der EU-Kommission nicht umsetzbar gewesen seien. Gegen diese hatte der Automobilindustrieverband im November 2020 protestiert. Dank des ESA-Weltraumteleskops  Gaia  haben Astronomen und Astronominnen  12 neue Einsteinkreuze entdeckt . Damit erh\u00f6ht sich die Gesamtzahl der best\u00e4tigen Beispiele f\u00fcr diese visuellen Nachweise der Relativit\u00e4tstheorie um ein Viertel. Die Ph\u00e4nomene seien einzigartige M\u00f6glichkeiten, um Dunkle Materie und die Expansion des Universums zu untersuchen. Einsteinkreuze  sind spezielle Gravitationslinsen, bei denen das Licht ferner aktiver Galaxienkerne (Quasare) durch massereiche Objekte im Vordergrund so gekr\u00fcmmt wird, dass aus unserer Perspektive mehrere Abbilder ankommen. Aus deren Verteilung am Nachthimmel und der Differenz zur vorhergesagten Verteilung kann man beispielsweise etwas \u00fcber die mysteri\u00f6se Dunkle Materie erfahren. Das Weltraumteleskop Gaia war 2013 gestartet worden und fotografiert seitdem mit einer Gigapixelkamera kontinuierlich den Sternenhimmel. Inesbondere in den USA und in Brasilien  greifen staatliche Stellen zu Nutzerdaten in Apples iCloud , betroffen sind aber auch EU-Staaten. Im ersten Halbjahr 2020 hat Apple bei knapp 3400 Anfragen zu Apple-ID-Accounts \"Content Data\" an Beh\u00f6rden \u00fcbermittelt, wie aus dem j\u00fcngsten Transparenzbericht des Unternehmens hervorgeht. Unter \"Content Data\" k\u00f6nnen  iCloud -Inhalte wie Fotos, iPhone-Backups, E-Mails, Kontakte und Kalender fallen. iPhone-Backups enthalten zahlreiche Daten darunter etwa auch Textnachrichten. Welche der teils sensiblen Daten im Einzelnen herausgegeben wurden und aus welchem Grund, bleibt unklar. In dem neuen Transparenzbericht hat Apple erstmals best\u00e4tigt, dass auch bei Notfallanfragen von Beh\u00f6rden unter Umst\u00e4nden iCloud-Inhalte weitergegeben werden -- etwa zur Suche nach Vermissten. Von deutschen Beh\u00f6rden hat  Apple  im Zusammenhang mit Ermittlungen zu gestohlener Hardwar eine hohe Zahl an Anfragen zu Ger\u00e4ten erhalten. Der Telekom-Anbieter  Vodafone hat in Deutschland Teile seines 5G-Mobilfunknetzes  auf \"5G Standalone\" mit eigenem Kernnetz umgestellt, das ohne LTE-Anker auskommt. Der Netzbetreiber stellt alle seine Antennen um, die auf den 5G-Frequenzen im 3,5 Gigahertz-Bereich funken, das sind rund 1000 Antennen im in 170 St\u00e4dten und Gemeinden. Das erste Rechenzentrum f\u00fcr das 5G-Kernnetz steht in Frankfurt. Damit geht Vodafone in Vorleistung, denn zu Beginn werden nur wenige Anwender davon profitieren k\u00f6nnen. Bislang unterst\u00fctzen nur wenige Smartphones wie das Oppo Find X3 Pro den neuen Standard. Allerdings werden auch Modelle von Samsung, Huawei und anderen Herstellern erwartet, die \"5G Standalone\" nutzen k\u00f6nnen.  Vodafone-Kunden  k\u00f6nnen ab kommender Woche eine kostenfreie \"5G Core\"-Option hinzubuchen. Die Finanzaufsicht  Bafin  untersucht in zwei F\u00e4llen,  ob Mitarbeiter unerlaubt spekulative Aktiengesch\u00e4fte get\u00e4tigt haben . Die Gesch\u00e4fte waren bei einer Untersuchung von Mitarbeiterhandel mit Aktien der US-Unternehmen  Gamestop  und  AMC Entertainment  aufgefallen. Das geht aus einer Antwort der Bundesregierung auf eine Anfrage des Finanzexperten der Linken im Bundestag, Fabio de Masi, hervor. Die Kurskapriolen des US-Videospieleh\u00e4ndlers Gamestop hielten die B\u00f6rsen \u00fcber Wochen in Atem, ebenso die Spekulationen um Papiere der Kinokette AMC. Der Wert der Aktien hatte sich teils deutlich erh\u00f6ht. Auch noch wichtig: - Ein weiteres Kapitel im Streit um die R\u00fcckkehr von GNU-Guru Richard Stallman in den Vorstand der FSF. Seine umstrittene \u00c4u\u00dferung vom vergangenen Jahr versucht er in einer  sehr pers\u00f6nlichen Stellungnahme  zu erkl\u00e4ren. (tol)"}, "210705_news_469024.txt": {"page_id": "210705_news_469024.txt", "text": "I t was designed to make sharpening a pencil feel as thrilling as flying a jet. A gleaming chrome teardrop, tapered to a point and adorned with a bullet-like handle, Raymond Loewy\u2019s aerodynamic tail-fin pencil sharpener brought the glamour of the machine age to the humble office desk. As the godfather of American industrial design, Loewy gave his streamlined signature to trains, planes and Coca-Cola vending machines,  defining the sleek art deco look of the 1930s . But his go-faster pencil sharpener never made it into production, deemed one chrome-plated, deco-styled step too far. The design does survive in the form of its patent, filed in 1933 and now republished as one of 1,000 such protected inventions, brought together in a new book. \u201cDesign critics trashed Loewy\u2019s sharpener,\u201d says Thomas Rinaldi, author of the Patented: 1,000 Design Patents. \u201cA pencil sharpener doesn\u2019t have to fly through the air, they said, so why make it aerodynamic?\u201d But the design went on to become an icon of the streamlined era. The prototype sold for $85,000 at auction in 2001 and, despite the innumerable designs that Loewy realised throughout his career, it was the phantom pencil sharpener that was chosen to honour him on a  celebratory postage stamp  in 2011. Not cleared for take-off \u2026 the aerodynamic pencil sharpener by Loewy.  Photograph: Raymond Loewy/Phaidon Featured among everyday classics like the ring-pull can and the cassette tape, the sharpener is one of a number of designs in the book that never made it off the drawing board. From a flying automobile-cum-helicopter designed in 1959 to the  ill-fated Google Glass wearable display  of 2011, it seems designers\u2019 ambitions have often been ahead of what is technologically feasible \u2013 or practically desirable.  \u201cSifting through the archives,\u201d says Rinaldi, \u201cit became clear that design patents have often been used as a publicity tool. Someone would design a crazy thing, without having a manufacturer on board, so they could shop it around and get it published in magazines to raise their profile. For a lot of designers, patenting was a vanity exercise.\u201d Arranged chronologically from 1900 to the present day, one patent per page, the book presents a fascinating cross-section through more than a century of material culture, the simple black and white line drawings reflecting the changing tastes and technologies of the decades. Drawn from around 800,000 designs registered at the US Patent and Trademark Office, the catalogue of objects reflects not only the onset of mass manufacture, the rise of electrical appliances and the later ubiquity of personal electronics, but it also reveals a story of international trade and shifting global power. Causing a stir \u2026 the development of the food mixer.  Photograph: Phaidon We see how US-created designs for cars and appliances were overtaken by innovations from Japan in the 1970s and 80s and, more recently, an influx from China \u2013 with a sixfold increase in filings from Chinese companies and inventors over the last decade. On a stylistic level, we see how the rich ornamentation of the 1900s gives way to the streamlining of the 30s, and how the angular forms of the 70s melt into chubby postmodern products of the 80s. One surprise inclusion, among the scores of gadgets and appliances, is patented building designs \u2013 a phenomenon that coincided with the birth of roadside fast-food joints and gas stations, designed like 3D billboards to be easily recognisable from a moving car. Who could resist pulling in to refuel beneath one of Eliot Noyes\u2019 1966  UFO-like canopies for Mobil  (one of which remains on the A6 at Birstall, Leicestershire, safely listed in 2012)? Upper crust \u2026 1965 patents for a Pizza Hut building.  Photograph: Phaidon A section at the front of the book shows the evolution of particular products, which reads like Darwinian natural selection. The mobile phone arrives as an enormous brick in the 70s, before gradually shrinking towards the tiny palm-sized flip-phone of the early 00s, then swelling into vast touch-screen slabs too big for most pockets, before coming full circle and ending with a basic, compact model, with analogue buttons and a small LCD screen \u2013 part of a backlash against the always-connected smartphone lifestyle. The food mixer goes through similar convulsions, travelling from industrial-looking machine, through futuristic streamlined forms, and back again, concluding with a retro model patented in 2019 that doesn\u2019t look too far from the first 1927 version. One thing that remains eerily consistent is the visual style of the patent drawings. When design patents were introduced in the late 19th century, the illustrations had to withstand reproduction at reduced scale in periodically issued gazettes, and the standard has remained the same ever since. Whether depicting the design for a Victorian billiard table or  a Pizza Hut restaurant  building, a Rolodex card file or a Chinese drone, each item is drawn with the same objective black lines, floating against a white background. Stripped of all context, their very essence distilled, the objects take on an almost totemic quality. Even a novelty pig-shaped clock, with hands emerging from its rear, starts to look like a design classic. The book also shines a useful spotlight on unsung heroes behind the items we use every day. \u201cI\u2019d never heard of  Jean Reinecke  before I started this research,\u201d says Rinaldi. \u201cBut it turns out he probably designed three quarters of all the tape dispensers you\u2019ve ever touched. Or there\u2019s  Ray Patten , an in-house designer at General Electric who designed everything from kitchen timers to locomotives. Or Charles McLeod, who designed dozens of electric clocks.\u201d It was Rinaldi\u2019s own considerable collection of clocks, radios, lamps and other flea-market finds that first prompted him to find out more about the anonymous designers behind these products, beginning the years-long process of whittling down the selection for the book. UFO influenced \u2026 Grade II-listed Esso garage in Birstall.  Photograph: Rui Vieira/PA He traces the origins of design patents back to the birth of mass production, when industrialised processes of casting, stamping, weaving and cutting enabled objects to be produced at scale for the first time \u2013 as well as more easily copied. \u201cUtility patents\u201d had been around since the 1790s, but they regulated how an invention functioned, not how it looked, and proved ill-suited to protecting a new age of designed products. Introduced in 1842, the US design patent law saw just 14 designs registered in its first year, including a typeface, a bathtub and a \u201ccorpse preserver\u201d. By 1930, the patent office was issuing 3,000 design patents a year, and 6,500 by 1941, a figure that wasn\u2019t exceeded until 1989. That number has now mushroomed to around 35,000 \u2013 good news for lawyers, but maybe less so for innovators. The recent boom in design patents has mostly come from the electronics sphere, spurred on by a landmark supreme court case between Apple and Samsung, which began in 2011 and was finally settled in 2018, when  Apple was awarded $539m  in damages. The case revolved around Apple claiming Samsung had copied numerous elements of the iPhone, from its \u201cbounce-back scrolling\u201d interface to the \u201crectangular product shape with all four corners uniformly rounded\u201d. Patently absurd \u2026 designs for a combined automobile and helicopter registered in 1959.  Photograph: Phaidon Design patent lawsuits had never seen such vast sums awarded, so the shock ruling sparked an explosion in tech companies racing to patent every last detail of their devices, from the internal components of hardware to the user interface designs displayed on screen. As  Florian M\u00fcller , intellectual property activist and author of a patents blog, puts it: \u201cThe number of patents in a phone is so huge that nobody has ever been able to count.\u201d The unprecedented ruling  unleashed an arms race , with big tech companies amassing vast arsenals of pre-emptive patents, conceived as assets to be sold or traded, as well as providing an insurance policy against any potential litigation. If someone sues you for infringement, you are more likely to be able to countersue for one of the thousands of other patents in your possession. While providing protection on one hand, this thicket of patents also serves to stifle innovation. As patent law professor Michael A Carrier remarked in response to the Apple ruling: \u201cThere\u2019s always the trade-off between litigation and innovation, and in the time these companies spent in the courtroom, they weren\u2019t innovating.\u201d Still, in centuries to come, at least the inexorable archive of patent drawings will provide a useful insight into the lengths our species went to protect the monetisable details of smartphone packaging and poop emojis."}, "210705_news_469032.txt": {"page_id": "210705_news_469032.txt", "text": "More than 4.5 million people missed out on hospital treatment in  England  last year due to the disruption to the NHS caused by Covid, with growing numbers turning to crowdfunding to pay for cancer drugs and operations. The number of patients having planned surgery such as a joint replacement plummeted from 16.62 million in 2019 to just under 12 million last year \u2013 a drop of 4.64 million people \u2013 an analysis of NHS hospital activity by the  Health  Foundation reveals. The fall was mainly caused by hospitals suspending many of their normal services as they focused on the influx of people severely ill with coronavirus, which resulted in operating theatres being turned into  makeshift intensive care units  and surgical staff being repurposed to fight the pandemic. At the same time GPs referred 6 million fewer people to have diagnostic tests and treatment in hospital as a result of the disruption to care, patients\u2019 reluctance go to hospital in case they caught Covid and a desire not to add to the pressure on the overstretched  NHS . They referred 14 million patients in 2020, compared with 20 million in 2019. It has created millions of \u201cmissing patients\u201d who could send the overall NHS waiting list soaring from its already record high 4.6 million people to 9.7 million by 2024 if three-quarters of those people belatedly seek treatment now that the pandemic is easing, the Health Foundation estimates.  Data collected by the website GoFundMe shows that more and more people are turning to crowdfunding to finance urgent medical care as they battle delays, clinical trial cancellations and long waiting lists for NHS care. The number of people seeking donations from the public citing \u201cwaiting lists\u201d as a reason has gone up 87% between last year and this year and the number who mention \u201cclinical trials\u201d \u2013 medical research studies that aim to find better treatments, many of which were suspended during the pandemic \u2013 rose by 60%. There was also a 55% surge in people seeking public support mentioning cancer drugs between March 2020 and February 2021, GoFundMe said. Tim Gardner, a senior policy fellow at the Health Foundation, said a combination of long delays for care and the sheer number of people awaiting care could coalesce into a major political problem. The number of people forced to wait more than a year for their operation has rocketed from 1,613 before the pandemic to 304,044 in January this year, and more than 1 million people have been waiting at least six months, even though 92% of patients are supposed to be treated within 18 weeks under the referral to treatment scheme. \u201cThe waiting list is already at the highest level it\u2019s been since comparable records began in 2007, and if it did rise from 4.6 million now to 9.7 million by March 2024 as we estimate, that\u2019s more than double the waiting list now,\u201d said Gardner. \u201cThese \u2018missing millions\u2019 have the potential to become problematic for the government. So this \u2013 addressing the backlog of care and getting waiting times back on track \u2013 has got to be seen as the defining challenge between now and the next general election. \u201cHowever, doing that will take years. I think we are looking at well beyond the next election before patients needing care can access the care that they need within the 18-week commitment in the NHS constitution. NHS staff are exhausted, so I think progress towards tackling the backlog and getting things back on track will be slow.\u201d Rachel Power, the chief executive of the Patients Association, said: \u201cThe disruption to NHS services brought by the pandemic appears to have accelerated a trend that was already emerging for patients to use crowdfunding to seek the treatment they could not access on the NHS.\u201d She said the association was \u201cparticularly concerned by reports of treatments being cancelled that could be life-saving\u201d, adding that it was \u201cunderstandable the patients are exploring other avenues\u201d. The Health Foundation analysed the number of people who had hospital treatment in each month in 2019 and 2020. It found the NHS performed 712,620 fewer trauma and orthopaedic treatments, 396,107 fewer ear, nose and throat procedures and 205,918 fewer oral surgeries last year than the year before. The analysis also discloses that the biggest fall in the number of people who received planned care in hospital occurred in the north-west, where the number of patients treated fell from 222,741 to 154,487 \u2013 a 31% drop. The south-west recorded the smallest fall, but still treated 24% fewer patients. It also found that disruption to hospital treatment was much more severe in England\u2019s poorest areas compared with its richest. The number of \u201ccompleted treatment pathways\u201d fell by 9,162 per 100,000 population in the former but by 6,765 in the latter.  The former chief executive of the NHS in England Sir David Nicholson said  in a Guardian interview  last week that the scale of the backlog of care the NHS was facing was already \u201ctruly frightening\u201d and that delays could damage patients\u2019 health. The Health Foundation\u2019s findings come days after new  polling by Ipsos Mori  showed that people think \u201cimproving waiting times for routine operations\u201d is the most important task facing the NHS, even ahead of the Covid vaccination programme. Half of respondents cited shortening the wait times for surgery as the service\u2019s key priority, followed by increasing the number of NHS staff (43%), Covid vaccination (41%), NHS workers\u2019 mental wellbeing (38%) and mental health (36%), Health Service Journal (HSJ) reported. The \u201cgratitude bounce\u201d among the public towards the NHS during the pandemic would not last long now that Covid was in retreat, with people wanting the service to reduce waiting times for surgery as soon as possible, the pollster\u2019s chief executive, Ben Page, told HSJ. An NHS spokesperson said: \u201cAlongside treating around 400,000 seriously unwell patients with Covid since the pandemic began and rolling out the biggest vaccine programme in health service history, NHS staff also cared for more than 1.3 million patients without Covid during the peak of infections this winter and cut down waiting times by more than a third since last July. \u201cThe NHS has recently published a plan to accelerate the delivery of operations and other services with a \u00a31bn elective recovery fund, with every area of the country being asked to maximise their capacity to provide care for as many urgent and non-urgent patients as possible.\u201d"}, "210705_news_469086.txt": {"page_id": "210705_news_469086.txt", "text": "An increase in New Zealand\u2019s greenhouse gas emissions is a step in the wrong direction towards the country\u2019s goal of carbon neutrality by 2050, say experts, who have called on the government to bring in more radical action. The latest Greenhouse Gas Inventory, released by the minister for the environment on Tuesday, shows that both gross and net emissions increased by 2% in the 12 months to the end of 2019. The increase was predominantly driven by the energy sector and an increase in methanol production in the manufacturing industries. \u201cThe increase in emissions is not the direction we need to going in, as a country or globally,\u201d said a climate scientist, James Renwick, head of the school of geography, environment and earth sciences at Victoria University of Wellington. New Zealand\u2019s net emissions rose by 57% between 1990 and 2018, placing it among the  poorest performers  in the OECD. In late 2019 the prime minister, Jacinda Ardern,  ushered through legislation  that set a target of net zero by 2050 for CO2 emissions, including agriculture. The reported increase from 2018 to 2019 \u201chas taken us further away from meeting the targets we committed to in law\u201d, said the minister for climate change, James Shaw. He said the new figures did not take into account the impact of emissions reduction measures introduced by the government since 2019, such as a cap on the emissions trading scheme introduced last year; but they highlighted the need for further, urgent action. \u201cThe time for delay is over,\u201d Shaw said. Ralph Chapman, the director of Victoria University\u2019s environmental studies programme, told the Guardian that the inventory reporting year on year was less indicative than a longer-term view, as the 2% increase in 2018-19 probably reflected decisions made years earlier. \u201cThe government might be taking valuable action \u2013 in areas like banning new coal boilers in industry, and investing in public transport infrastructure \u2013 but each policy action takes a fairly long time to show up. \u2026 There will be reversals from time to time in the figures. But the trend in the greenhouse gas numbers has to be downward.\u201d The independent Climate Change Commission, tasked by the government with devising a strategy for how to decarbonise the economy by 2050, is in the process of finalising its proposal after a period of public consultation. Among the measures advised in its draft report, delivered in January, were accelerated renewable energy generation, climate-friendly farming practices and reduced livestock numbers, more permanent (and more native) forests, and a comprehensive move to electric vehicles. The commission is revising its report to reflect more than 15,000 submissions, with the final roadmap due to be tabled in parliament at the end of May. The government is then legally bound to adopt that, or devise its own plan by the year\u2019s end. Renwick \u2013 one of the six commissioners \u2013 said the latest figures could provide an \u201cextra impetus\u201d for action. \u201cWe have all the power, we know what we need to do, and we have most of the technologies we need,\u201d he said. \u201cIt is a question of getting on and not worrying about whether we can or can\u2019t achieve certain targets, to just go as far as we can and see how far we get.\u201d The upcoming UN Climate Change Conference, in Glasgow in November, is an opportunity for New Zealand to lead the world in setting more ambitious targets for carbon neutrality, according to Renwick. \u201cI think it\u2019s great that the commission\u2019s advice shows that it\u2019s doable and it\u2019s affordable, for this country at least.\u201d But Chapman, who co-wrote a submission on the Climate Change Commission\u2019s draft plan on behalf of the New Zealand Centre for Sustainable Cities, said the proposal did not go far enough to manage the risks around attaining net zero emissions by 2050. The slowness of policy actions to take effect meant more rapid, radical change was needed if New Zealand was to cut emissions by around 50% by 2030, in line with the Intergovernmental Panel on Climate Change\u2019s recommendation, said Chapman. \u201cThose concerned about future climate change should not be deflected from demanding further government policy action.\u201d Last week thousands of students  took part in protests  across New Zealand in the country\u2019s first School Strike 4 Climate protest since the Covid-19 pandemic hit."}, "210705_news_469136.txt": {"page_id": "210705_news_469136.txt", "text": "FOR CRYPTOCURRENCY purists the much-awaited listing of Coinbase, a cryptocurrency exchange, on April 14th must have been a disappointment. It started trading on a boring, conventional stock exchange, and not\u2014as might befit one of the world\u2019s biggest crypto firms\u2014on a buzzy blockchain, as the technology that powers the likes of bitcoin is called. The Economist Today Hand-picked stories, in your inbox A daily email with the best of our journalism Yet investors in Coinbase had nothing to complain about. The firm\u2019s initial valuation was nearly $100bn, putting it in the same league as Facebook, a social-media giant, which was valued at $104bn when it listed in 2012. Coinbase\u2019s first-quarter results, released on April 6th, no doubt helped generate excitement. It provisionally estimated a profit of $730m-800m on revenue of about $1.8bn, up from $179m and $585m, respectively, in the last three months of 2020. Impressive as all that sounds, does it justify the price tag? To be sure, compared with many cash-guzzling unicorns (tech startups worth more than $1bn), Coinbase looks mature. In the past quarter users traded about $335bn-worth of currencies on its platform. They also held $223bn in its accounts\u2014more than a tenth of the value of all cryptocurrencies. Founded in 2012, Coinbase always wanted to be more than a place where people buy and sell digital monies. It aimed instead to become a bridge between the anarchic cryptoworld and conventional finance. Though its history has been tumultuous at times, the firm is not far from its goal: users do not need a degree in cryptography to benefit from its services (though its customer support is notoriously wanting); it is on mostly good terms with regulators and banks; and, unlike other cryptoexchanges, it has so far avoided becoming the victim of a catastrophic hack. Yet in other respects the firm\u2019s prospects are uncertain. Although it has branched out somewhat, and now offers services to store and save cryptoassets, transaction fees still made up 96% of its revenue last year. This not only means that its fortunes rely heavily on the health of the cryptoeconomy, which can be volatile; it also means that its take could shrink if competition muscles in. Of the $335bn in trades in the first quarter of 2021 it kept about 0.5% in fees\u2014much more, for instance, than Nasdaq, the stock exchange on which Coinbase will list. VIDEO The Himalayan valuation could start to make sense if the cryptoeconomy continues to thrive and if conventional exchanges do not get religion\u2014admittedly two big ifs. Coinbase then might seem best placed to reap the rewards and become the centre of an \u201copen financial system for the world\u201d, as its IPO prospectus puts it. \u201cFor many of our customers, they simply think of us as their primary financial account in the cryptoeconomy,\u201d writes Brian Armstrong, the firm\u2019s boss. Like other promising startups that have recently gone public, Coinbase sees itself powered by an accelerating \u201cflywheel\u201d, tech-speak for a virtuous cycle: more customers means more liquidity, which allows the firm to accept more cryptoassets and offer more services, in turn attracting more customers, and so on. Even then, however, Coinbase would have to remedy some imbalances to really take off. One is getting so caught up in its flywheel that it can\u2019t do anything else, a malaise from which other big tech firms suffer. Both Google and Facebook, for instance, are still essentially advertising businesses. Coinbase, for all its ambitions, might get stuck being mainly an exchange. Another question-mark is management. Mr Armstrong is willing to learn from mistakes and eventually got things right, but he is no tech leader in the mould of a Steve Jobs or an Elon Musk. A self-described introvert, he cannot carry off his vision of bringing crypto to the masses by the sheer force of his personality, writes Jeff John Roberts, a journalist, in his recent book \u201cKings of Crypto\u201d, a profile of Coinbase. That helps explain why the firm\u2019s history has been one of delayed decisions and infighting. And then there is the inherent contradiction of trying to be a big, if not dominant, player in a world that by definition is meant to be fragmented (or \u201cdecentralised\u201d, in the lingo). If crypto becomes as successful as Coinbase wants it to be, there may be no need for a financial behemoth. In fact, the firm\u2019s most dangerous rivals may be neither its peers, such as Binance and Kraken, nor conventional financial institutions, but those without a big organisation behind them\u2014fully decentralised, like most cryptocurrencies themselves. Editor\u2019s note (April 14th 2021) : This article has been updated since it was first published. It first appeared in the Finance section of the print edition under the headline \u201cSquaring the coin\u201d."}, "210705_news_469141.txt": {"page_id": "210705_news_469141.txt", "text": "On March 24, fighters from Ansar al-Sunna Wa Jamma (ASWJ) \u2014 a U.S. State Department- designated  foreign terrorist organization that it calls \u201cISIS-Mozambique\u201d \u2014 launched a multi-day assault on Palma, a  coastal town  of roughly 75,000 people located in Mozambique\u2019s hard-pressed province of Cabo Delgado.  The operation  \u2014 which reportedly involved covert infiltration, multiple points of simultaneous attack, and maritime support \u2014 was well-coordinated, with clear evidence of prior planning and intelligence gathering. Over the course of the attack, ASWJ fighters targeted military personnel, banks, government buildings, a food warehouse, civil servants, and other civilians. On March 29, the Islamic State  claimed  the attack, emphasizing that ASWJ militants had killed Mozambican troops, local Christians, and foreigners. While security forces have largely reassumed control of the town, the total number of casualties is not yet known. The widespread  coverage  of ASWJ\u2019s attack in Palma \u2014 building on a year of operational success for the group \u2014 has likely elevated the profile of this conflict, including within the jihadist community, potentially increasing its appeal to regional and veteran foreign fighters. This risks further inflating the insurgents\u2019 growing corps of foreign recruits. While publicly available information on the exact number and role of foreign fighters in the group is limited, ASWJ \u2014 or \u201cal-Shabaab\u201d as it is called in Mozambique \u2014 has had a long history with the foreign fighter community. Early academic research into the group  revealed  the presence of youth from Africa\u2019s Great Lakes region, Uganda, and Tanzania. More recently, the Islamic State\u2019s media arm, AMAQ,  published  a video showing ASWJ fighters who appear to be foreigners alongside Mozambican fighters in Mocimboa da Praia, echoing claims that former ASWJ prisoners have made about foreigners present within the group\u2019s ranks. Tanzanian authorities last year  intercepted  multiple groups of young men who the authorities claim were en route to Mozambique, and South African officials a  claims  that South African nationals have joined the group. \u00a0 \u00a0 Mozambique, and Cabo Delgado specifically, boasts  many of the characteristics  that facilitate the entrance of foreign fighters: Poor border security, weak and declining state presence, and an ascendant insurgent movement. The presence of foreign fighters in ASWJ\u2019s ranks is already apparent. Less clear, however, is how foreign fighters might impact ASWJ as an organization and the trajectory of the conflict in Mozambique. Foreign fighters are often a boon to a nascent group, helping its local fighters to quickly develop the necessary military and technical skills to gain advantage on the battlefield. Over time, however, the presence of these foreigners within the ranks of an organization can become a liability, potentially sowing division within the group and shifting the strategic picture with the likely introduction of foreign assistance to the government combating the jihadist group. Towards this end, insight from al-Shabaab\u2019s experience with its foreign fighters in Somalia may serve as a valuable point of comparison. What We Know (and Do Not Know) About Foreign Fighters in ASWJ Foreign fighters are not new to ASWJ ranks. Reliable estimates of the number of fighters currently in the group\u2019s ranks \u2014 foreign or otherwise \u2014 are hard to come by, although one source recently estimated the  group\u2019s fighting corps  to be in the ballpark of 2,000 combatants. Based on  recent estimates  and the number of foreigners already arrested in connection to the insurgency in Cabo Delgado, it is likely that foreign combatants in ASWJ currently number in the low hundreds and  fill positions in both the leadership echelon and general rank-and-file . ASWJ\u2019s  sustained momentum  over the past few years does appear to have coincided with a notable  increase in size . While it is unclear how much of this recent growth is due to foreign recruits, what is clear is that foreign fighters have been active in the group for years. This is likely to  continue . The group\u2019s foreign fighters to date are mostly sourced from its regional neighbors. Despite ASWJ\u2019s\u00a0connection to the Islamic State, there is little evidence \u2014 such as a global call to arms \u2014 that foreign fighters are arriving  en masse  to ASWJ through Islamic State-related channels. Instead, the group\u2019s historical connections to various communities in Eastern, Southern and Central Africa \u2014 including in  Tanzania ,\u00a0 South Africa ,  Uganda ,  Democratic Republic of Congo , and  Somalia  \u2014 suggest that the influx of foreign fighters to the group is mostly generated by ASWJ itself. For example, from May 2017 to March 2018 \u2014 well before ASWJ\u2019s pledge to the Islamic State \u2014 the  Mozambican government prosecuted  370 persons accused of association with the militant group, 52 of whom were Tanzanian, three Ugandan, and one Somalian. Last month, the International Crisis Group  reported that the  \u201cbiggest cohort of foreigners fighting within the ranks of ASWJ \u2026 are from Tanzania.\u201d In March, the U.S. State Department identified Abu Yasir Hassan as the group\u2019s leader and named him as a  specially designated global terrorist . Hassan is reportedly  a Tanzanian national . These foreign fighters, particularly those emerging from ASWJ\u2019s connections to the \u201c militant milieu in Tanzania \u201d and  reported ties  to the  Allied Democratic Forces  in Democratic Republic of Congo, probably offered the group skillsets and training that they lacked prior to 2017. Despite the organization\u2019s demonstrated linkages to outside communities, ASWJ has  deep-set local roots . As  academic Eric Morier-Genoud  has put it, \u201cthe insurgency builds on a Mozambican religious sect whose leadership was primarily Mozambican.\u201d In the few public  statements  the group has made, it has spoken in local languages and focused on themes of corruption and abuse of the poor by FRELIMO, the party that has ruled Mozambique since independence. These messages likely resonate with a local audience that has seen little in the way of effective governance, particularly since the start of this conflict. Locals began calling the group al-Shabaab (\u201cthe youth\u201d) because they recognized them as local youth, a name the group seems to have embraced. Therefore, it is appropriate to think about the non-Mozambican nationals fighting in the ranks of ASWJ exactly as that \u2014 foreigners. This bears unique, and somewhat mixed, implications for the insurgency, its leaders, and local civilians. How Foreign Fighters Impact the Trajectory of Violence\u00a0 Comparative insights from the observed activity of foreign fighters in al-Shabaab in Somalia (not to be confused with ASWJ, which is commonly referred to as \u201cal-Shabaab\u201d by locals in Cabo Delgado) suggest that foreign fighters are not strictly a net gain for an insurgent group over time.\u00a0Like ASWJ, al-Shabaab recruited a significant number of foreign fighters in its formative years, even prior to  declaring its allegiance  to the leadership of a transnational terror organization (in al-Shabaab\u2019s case, al-Qaeda). The al-Shabaab case is well-suited to offer insight into how foreign fighters may influence ASWJ\u00a0and shape the broader trajectory of the conflict in Mozambique. We emphasize three of them here. First, foreign fighters offer a means of augmenting ASWJ\u2019s violent attacks and battlefield performance. In addition to serving as combatants, veteran fighters from foreign territories have provided fledgling domestic insurgents with the military experience and technical expertise needed to advance the local warfighting effort. For example, in the earlier phases of al-Shabaab\u2019s campaign in Somalia, foreign recruits significantly improved the group\u2019s  sniper operations  in Mogadishu as well as the group\u2019s use of  improvised explosive devices . Since 2008, foreign nationals from a variety of countries, including Sudan, Kenya, the United Kingdom, and the United States, have fought alongside al-Shabaab. Seasoned fighters from Yemen have been especially influential in the group. Analyst Katherine Zimmerman, for example,  stated in a U.S. congressional testimony  that al-Qaeda in the Arabian Peninsula \u201calmost certainly provided the equipment or the expertise for al-Shabaab\u2019s 2016 laptop bombing\u201d of a Daallo Airlines flight in February 2016. Over the past year, ASWJ has demonstrated a much-improved capacity for operational design and tactical execution, including  their maritime activities . In 2019, the  Armed Conflict and Location Event Dataset  recorded 38 armed attacks involving ASWJ and state forces, producing a total of 263 fatalities. In 2020, those numbers surged to 105 and 747, respectively. This increase in armed engagements between ASWJ and local security forces is only part of the whole picture. From 2017 to early 2019, the group  mostly relied on  crude weapons, such as machetes, and unsophisticated tactical methods. Since then, the group has significantly improved its material capabilities and the sophistication of its attacks.  In a recent article , journalist Tim Lister reported that by mid-2020 ASWJ had captured over 100 assault rifles, heavy machine guns, several mortars, and more than 20 RPG-7s from local security forces. The well-established smuggling network in the region offers an additional potential source of small arms. Moreover, ASWJ has shown itself capable of conducting complex operations. Last month\u2019s attack on Palma entailed a multi-stage operation and several points of simultaneous attack with operational maritime support. That the ASWJ militants are now capable of attacking \u2014 and even holding \u2014 strategic urban spaces clearly indicates that the group has matured from an adolescent insurgency into a formidable and entrenched force. Second, an influx of foreign fighters to ASWJ may prompt heightened pressure from regional and international counter-terrorism forces. In the case of al-Shabaab in Somalia, scholars Tricia Bacon and Daisy Muibu  convincingly argue  that foreign fighters have ultimately proved a long-term liability to the group\u2019s strategic mission, mostly by inviting more sophisticated and sustained\u00a0counter-terrorism efforts. More recently, this prompted al-Shabaab\u2019s leadership to undergo a self-imposed process of \u201cdomestication,\u201d returning its focus to the local drivers of the conflict and the group\u2019s local constituents. An influx of foreign fighters would create a similar dilemma for ASWJ, a group that, unlike al-Shabaab, has not blatantly tried to attract\u00a0international attention. Thus far, ASWJ has deftly\u00a0exploited local grievances and capitalized on the corruption and weakness of the Mozambique government to gain domestic recruits and at minimum keep the civilian population largely neutral. The organization has also purposefully maintained a low public profile, releasing only five propaganda videos since it first came on the scene, suggesting the group\u2019s audience is not the international community, but rather those closer to home. However, the Islamic State claims to credit for attacks in Mozambique will likely elevate ASWJ\u2019s profile among veteran foreign fighters, drawing in new recruits as well as increasing concern from the international community about the conflict. These external forces may ultimately run counter to the ASWJ leadership\u2019s apparent preference to stay out of the limelight and force the group to reevaluate the benefit of these foreign fighters within their ranks. Indeed, despite the Mozambican government\u2019s reluctance to collaborate with regional partners on this security issue, the United States recently  launched  a two-month training program for Mozambican marines and the Portuguese are set to  begin  a similar program this month. Finally, the sustained presence of foreign fighters poses a threat to ASWJ\u2019s internal cohesion.\u00a0Scholar  Thomas Hegghammer  aptly describes foreign fighters as \u201cinsurgents in every regard but their passports.\u201d Though perhaps trivial at face value, with different passports come a host of potential challenges that local commanders must address \u2014 and be prepared to adapt their patterns of management accordingly. Foreign nationals \u2014 especially those recruited into middle or upper management positions \u2014 will inevitably seek a say in ASWJ\u2019s strategic direction and, relatedly, its  theory of victory  (i.e., \u201cthe assumptions that group strategists make about how the execution of the military operations that they are planning will translate into the achievement of the political objectives that they are pursuing\u201d). This can lead to intra-group fractures. Scholar Kristin Bakke summarizes this dynamic succinctly: \u00a0 Although transnational insurgents may strengthen a domestic insurgent movement by contributing resources, fighters, and know-how, they can also weaken the movement by introducing new ideas about what the struggle is about and how it should be fought. Examples of this dynamic are legion. In 2011, after foreign fighters in Somalia \u201c ran afoul \u201d of al-Shabaab\u2019s leader, Ahmed Godane, the group systematically killed a number of its foreign recruits. In 2012, Omar Hammami, an American foreign fighter and al-Shabaab celebrity turned pariah, claimed that al-Shabab leaders  threatened his life  due to \u201cdifferences\u201d over matters of \u201cShari\u2019a and strategy.\u201d Hammami and other high-profile foreign fighters subsequently  broke away from the group  and continued to antagonize their previous comrades-in-arms. The feud culminated in September 2013 when al-Shabaab\u2019s intelligence branch managed to track down and kill Hammami. In other cases, foreign fighters pose a threat to insurgent cohesion by straining\u00a0sentiments of camaraderie within the rank-and-file. Language and cultural barriers, in particular, tend to pose a practical obstacle to coordination and socialization within the ranks. Academics Scott Gates and Sukanya Podder  argue that the  \u201ccombination of ideological motivation, non-parochialism, and detachment from local politics\u201d can generate a clash of preferences and interests within a rebel organization between foreign and local members. Force cohesion ( and effectiveness ) is directly affected by inter-combatant bonds \u2014 and the presence of foreign fighters can disrupt these ties. On this point, scholar Vera Mironova  records one  native member of the Syrian insurgent group Jabhat al-Nusra stating that \u201cThe only drawback of foreign fighters was they had their own communities and way of living. They weren\u2019t highly integrated into our society.\u201d After the attack in Palma, ASWJ leaders will likely consider the rate and method of territorial expansion, the nature of the group\u2019s engagement with the local civilian population, and the extent to which the group hopes to invite or avoid greater levels of foreign military support \u2014 issues that are tied to the group\u2019s vision for its success and long-term goals. The high-stakes nature of these choices has the potential to provoke disagreement between the group\u2019s leadership, local members, and foreign recruits. The political overtones to ASWJ\u2019s  attack  into southern Tanzania last October may already point to some Tanzanian influence in the group\u2019s decision-making, but one that distracts from the group\u2019s overarching grievances against the Mozambican state. The opportunity for entrenched factions to emerge along these lines is significant. In addition, while foreign fighters from the Swahili coast, such as Tanzania and southern Kenya, may be able to leverage a shared history to better integrate into the group, those coming from farther afield will likely find the transition more difficult, much like those joining al-Shabaab did. Foreign Fighters Complicate Regional and International Support \u00a0 An influx of foreign fighters will almost certainly exacerbate Mozambique\u2019s already-tense relationship with its neighbors, particularly South Africa and Tanzania. Specifically, foreign jihadists joining the ranks of ASWJ would provide the Mozambican government with a pretext to deflect attention away from its own difficulties in containing the group. Mozambique  has frustrated its neighbors  with its reluctance to request assistance from abroad despite the declining humanitarian and security situation in the north. Indeed, last year, in the wake of ASWJ\u2019s August attack on Mocimboa da Praia, Defense Minister Jaime Neto  said  that the only request that Mozambique had of its neighbors was \u201cvigilance at the borders, to keep these bandits out of our country,\u201d passing some of the responsibility for Mozambique\u2019s security onto regional partners. If foreign fighters are seen as having a key role in enhancing the insurgents\u2019 strength and effectiveness, the Mozambican government may try shifting the blame the international community places on it onto neighboring governments and organizations for failing to prevent the conflict from worsening. At the same time, Mozambique\u2019s neighbors may face increased insecurity as foreign fighters transit through their countries on the way to Cabo Delgado. According to  a U.N. study from 2018 , foreign fighters arrested in Malaysia on their way to Syria used the country as a place to plan attacks and collect funds. The recent designation of ASWJ as a foreign terrorist organization by the United States would likely increase pressure on countries if they found themselves in this situation to avoid looking like they were providing material support to the insurgents in Mozambique. To the extent that Mozambique accepts foreign security assistance, as it has with U.S. and Portuguese training missions to its marines, these partners will need to tread carefully to avoid creating a wave of \u201c accidental guerrillas , \u201d galvanizing greater jihadist engagement in Cabo Delgado. Academic  research  suggests that Ethiopia\u2019s entrance into Somalia in 2007 inspired foreign fighters to come to lend aid to al-Shabaab against this incursion. The African Union\u2019s Somalia mission was similarly viewed by jihadists as an occupation by infidels, again galvanizing foreign fighters to act. A similar phenomenon could come into play in Cabo Delgado, where the Islamic State\u2019s propaganda arm has already  accused  South Africa and Tanzania of meddling in the conflict. In addition to maintaining a low-key presence, discrete cooperation in law enforcement and intelligence sharing has the potential to transfer important skills to Mozambican counterparts while chipping away at the organization. The introduction of new foreign fighters, particularly those driven to respond to perceived Western security interference, will also likely exacerbate an already declining humanitarian situation. Fighters motivated to combat Western security assistance may introduce new threats in the battlespace with the likely introduction of new modes of warfare, such as improvised explosive devices and vehicle-born improvised explosive devices, further complicating the ability of internally displaced people to safely flee areas of combat and the ability of aid organizations to help them. Indeed, the arrival of foreign fighters to Mozambique bodes ill for the already beleaguered civilian population. Recent academic studies reveal that to better integrate foreign fighters, the indigenous leaders of a group will institutionalize or permit  sexual violence  and other  forms of civilian targeting , suggesting a potential uptick in the need for medical and psychological support to foreign fighters\u2019 victims. Looking Ahead Foreign fighters\u2019 influence on the ASWJ insurgency \u2014 both now and in the future \u2014 is likely to be multi-dimensional. In the immediate term, foreign recruits offer ASWJ a reliable means of boosting their battlefield effectiveness and regional prestige. Down the road, should ASWJ continue to target a significant number of foreign recruits, this decision could ultimately become a strategic mistake. The presence of foreign fighters could carry downstream consequences for the group including weakened internal cohesion, reduced support from the local population, and unwieldy mission creep should the group choose to expand its strategic ambitions beyond the northern regions of Mozambique. These consequences would threaten many of the  fundamentals  that have allowed the group to succeed, such as avoiding hubris in decision-making and keeping the civilian population neutral. An uptick in foreign fighter flows to Cabo Delgado may create new opportunities for the United States to work toward stabilizing security in the region, even in a scenario where the Mozambican government remains cautious toward external security support. Indeed, discretion in lending aid would be essential to reduce the risk of inspiring would-be foreign fighters to join the fight in Cabo Delgado. As Mozambique\u2019s neighbors grow more concerned over insecurity there, the United States could engage with these security services on intelligence collection and analysis and border security initiatives to better track fighters and prevent their entry into Mozambique. The recent foreign terrorist organization designation by the U.S. State Department, with its authorities to prosecute anyone lending material aid to ASWJ, would open the door to for greater law enforcement and judicial training programs and collaboration. Mozambique and its neighbors would likely benefit from joint training exercises that, while not expressly connected to combatting ASWJ, would still impart skills that would help Mozambique and its neighbors in that fight and create working-level relationships that would foster future collaboration. \u00a0 \u00a0 Emilia Columbo  is currently a senior associate (non-resident) with the Africa Program at the Center for Strategic and International Studies. She previously served as a senior Africa and Latin America security analyst at the Central Intelligence Agency. You can find her on Twitter  @EColumbo2019 . Austin C. Doctor \u00a0is an incoming assistant professor of political science at the University of Nebraska at Omaha and a member of the executive committee of the National Counterterrorism Innovation, Technology, and Education Center, a U.S. Department of Homeland Security Center of Excellence. He earned a Ph.D. in political science from the University of Georgia. He writes on militant organizations, terrorism, armed conflict, and political instability. You can find him on Twitter \u00a0@austincdoctor . Image:  U.S. Embassy in Mozambique"}, "210705_news_469182.txt": {"page_id": "210705_news_469182.txt", "text": "Gezielte Phishing-Attacken mit gesch\u00e4ftlichen E-Mails sind schwer zu erkennen. K\u00fcnstliche Intelligenz ist ein wichtiger Verb\u00fcndeter im Kampf gegen diese Masche. Phishing-Attacken haben meist das Ziel, den Adressaten zum Klick auf einen dubiosen Link oder das \u00d6ffnen einer Malware-verseuchten Datei zu bewegen. Bei der als Business E-Mail Compromise (BEC) bekannten Phishing-Variante kommen Angreifer dagegen ganz ohne Links und Schadsoftware aus. Stattdessen versuchen Sie, den Adressaten zu manipulieren und zu einer unbedachten Handlung zu motivieren. So gibt sich der Absender beispielsweise als Gesch\u00e4ftsf\u00fchrer aus und fordert die \u00dcberweisung einer gr\u00f6\u00dferen Summe auf ein ausl\u00e4ndisches Konto. Die Zahlungen werden als extrem dringend, vertraulich und existentiell f\u00fcr das Unternehmen dargestellt, was den betroffenen Mitarbeiter unter massiven Druck setzt. Eine andere beliebte Variante von BEC ist der Kontotrick. Gesch\u00e4ftspartner oder Dienstleister teilen mit, dass sich ihre Bankverbindung ge\u00e4ndert h\u00e4tte und bitten darum, die n\u00e4chste Anzahlung oder Rechnung auf das neue Konto zu \u00fcberweisen. Da die Nachrichten von bekannten Kontakten stammen und sich h\u00e4ufig sogar auf vorangegangene Konversationen beziehen, wird dieser Bitte oft ohne weitere Nachfrage nachgekommen. Enorme Sch\u00e4den Die durch BEC verursachten Verluste sind enorm. Das FBI berechnete allein f\u00fcr den Zeitraum Juni 2016 bis Juli 2019 einen Gesamtschaden von \u00fcber  26 Milliarden US-Dollar . Die Ermittler registrierten mehr als 160.000 F\u00e4lle in \u00fcber 170 L\u00e4ndern. Zu den bekanntesten Opfern in Deutschland z\u00e4hlt die \u00d6ko-B\u00e4ckereikette \u201eHofpfister\u201c. Eine Buchhalterin des Brotfabrikanten \u00fcberwies mehr als  1,9 Millionen Euro nach Hongkong , nachdem sie streng vertrauliche Anweisungen von ihrer Chefin, der Gesch\u00e4ftsf\u00fchrerin Nicole Stocker, erhalten hatte. Obwohl diese nur wenige B\u00fcrot\u00fcren weiter sa\u00df, sah die Angestellte von einer Nachfrage ab und \u00fcberwies das Geld. Nat\u00fcrlich war alles gef\u00e4lscht, inklusive der Unterschrift von Frau Stocker. \u00c4hnliche Dimensionen hatte ein BEC-Scam, den im vergangenen Jahr das britische National Cyber Security Centre  bekannt machte . Cyberkriminelle hatten sich in die Verhandlungen f\u00fcr einen Spielertransfer zwischen einem englischen Verein und einem Fu\u00dfball-Club auf dem europ\u00e4ischen Festland eingeklinkt, den Office-365-Account des Gesch\u00e4ftsf\u00fchrers auf britischer Seite \u00fcbernommen und Transferzahlungen von  einer Million Pfund  in letzter Minute auf ein anderes Konto umgelenkt. Anders als im Hofpfister-Fall stellte sich jedoch die ausf\u00fchrende Bank quer und verhinderte den Deal. Weniger Gl\u00fcck hatte dagegen die Bezirksverwaltung im US-amerikanischen  Cabarrus . Sie \u00fcberwies 2,5 Millionen US-Dollar f\u00fcr ein Bauprojekt auf das \u201eneue\u201c Konto der Baufirma. Als der Schwindel aufflog, konnte die Bank immerhin noch knapp 800.000 US-Dollar sicherstellen. Ma\u00dfgeschneidert und schwer zu entdecken Anders als bei Phishing-Kampagnen nach der \u201eSchrotschuss-Methode\u201c gehen BEC-Attacken oft monatelange gezielte Recherchen voraus. Die Angreifer informieren sich ausf\u00fchrlich \u00fcber die gesch\u00e4ftliche Lage, Gesch\u00e4ftspartner und Standorte eines Unternehmens. Um Vorlieben und Schw\u00e4chen herauszufinden, analysieren sie die Social-Media-Accounts von Gesch\u00e4ftsf\u00fchrern und Mitarbeitern in der Buchhaltung. Mit Spear-Phishing und Social Engineering versuchen sie dann, an die Passw\u00f6rter von E-Mail-Konten zu gelangen. Dabei adressieren sie entweder direkt Mitarbeiter in der Zielfirma oder nutzen Partner, Dienstleister und Kunden als Schl\u00fcssel. Die daraufhin \u00fcber scheinbar legitime Accounts versandten Nachrichten sind von Inhalt und Stil kaum von echten E-Mails des Absenders zu unterscheiden. F\u00fcr K\u00fcnstliche Intelligenz stellt die Erkennung solcher Betrugsversuche eine gro\u00dfe Herausforderung dar. Es gilt, nat\u00fcrliche Sprache in ihrer ganzen Komplexit\u00e4t und Vielschichtigkeit zu verstehen und zu analysieren. Dabei sind folgende Elemente zu ber\u00fccksichtigen: \u2013 Syntax:  Was sagen Struktur und Zeichensetzung eines Satzes \u00fcber die Absicht des Sprechers beziehungsweise Verfassers aus? \u2013 Semantik:  Was bedeuten die verwendeten W\u00f6rter im jeweiligen Kontext und in welcher Beziehung stehen sie zueinander? \u2013 Sentiment:  Welche emotionale F\u00e4rbung hat der Text? Ist er neutral, w\u00fctend, ironisch, sarkastisch oder drohend? CATBERT \u2013 ein Versuch, BEC zu erkennen Das  AI Team von Sophos  wollte herausfinden, ob sich BEC-Nachrichten mithilfe von KI zuverl\u00e4ssig erkennen lassen, selbst wenn sie ma\u00dfgeschneidert sind und sich in Stil und Tonfall an typischen E-Mails des echten Absenders orientieren. Dazu bildete ein vortrainiertes Machine-Learning-Modul namens  BERT  (Bidirectional Encoder Representations from Transformers) die Basis, das von Google entwickelt wurde. BERT erzielt bei der Sprachanalyse beeindruckende Ergebnisse. Es kann beispielsweise dem Handlungsverlauf einer Geschichte folgen und Emotionen erkennen. F\u00fcr den Einsatz in der Cybersecurity eignet sich das Modul allerdings nur bedingt, da es komplex und ressourcenhungrig ist. Eine Klassifizierung von E-Mails mit BERT w\u00fcrde daher viel zu lange dauern, um Ergebnisse in akzeptabler Geschwindigkeit zu liefern. Ein Machine-Learning-Tool f\u00fcr die E-Mail-Analyse sollte dar\u00fcber hinaus nicht nur Sprache an sich, sondern auch typische E-Mail-Kontextinformationen und -Metadaten wie Absender, Betreff und Header erkennen und auswerten k\u00f6nnen. Die Forscher entwickelten daher eine eigene, leichtgewichtige Variante von BERT, welche die Bedeutung typischer E-Mail-Komponenten ber\u00fccksichtigen konnte, und nannten sie \u201eCATBERT\u201c (Context-Aware Tiny BERT). CATBERT und eine weitere, ebenfalls leichtgewichtige Variante von BERT names  DistilBERT  wurden daraufhin an \u00fcber vier Millionen E-Mails und Metadaten trainiert, die von Sophos als BEC erkannt und abgefangen worden waren. 70 Prozent der Daten dienten zum Training, 30 Prozent wurden f\u00fcr die Validierung eingesetzt. Die Tests zeigten, dass CATBERT BEC-Nachrichten mit einer Genauigkeit von 90 Prozent erkennen kann, wobei die Falsch-Positiv-Rate nur 0,1 Prozent betr\u00e4gt. Das Modell ist dabei nicht nur 30 Prozent kleiner als DistilBERT, sondern auch doppelt so schnell. Ein Paper \u00fcber CATBERT befindet sich im Preprint und kann auf  arXiv.org  heruntergeladen werden. Fazit Die Leistung von KI bei der Abwehr von Business E-Mail Compromise ist beeindruckend. Wie das Sophos AI Team zeigen konnte, lassen sich selbst gezielte Angriffe mit individuell an den Adressaten angepassten Nachrichten anhand subtiler sprachlicher Merkmale mit hoher Genauigkeit identifizieren. Unternehmen tun daher gut daran, sich diese Expertise zunutze zu machen, und mit Herstellern wie  Sophos  zusammenzuarbeiten."}, "210705_news_469203.txt": {"page_id": "210705_news_469203.txt", "text": "V eins of the World sees Mongolian director Byambasuren Davaa, whose debut,  The Story of the Weeping Camel , earned an Oscar nomination for best documentary, again turn her evocative, naturalistic lens on to the lives of nomadic tribes in her home country. It\u2019s something of a pity that, having made its festival rounds, the film gets its release during the pandemic, as its arresting, expansive portraiture of Mongolian rural landscapes would have made an indelible impact on the big screen. Right from the beginning, an alluringly green vista takes over the frame; the potency of the land is breathtaking. Nevertheless, amid this poetic lushness, barren patches slowly appear. Not only the fertile soil but also a traditional way of life is in danger of being eaten away. Shattering changes are seen through the eyes of 11-year-old Amra (Bat-Ireedui Batmunkh), who is especially close to his mechanic father, Erdene (Yalalt Namsrai); Erdene leads his fellow nomads in their resistance against the encroachments of mining companies. Unfortunately, tragedy strikes and Amra\u2019s childlike innocence hardens into a resolute determination as he becomes a one-boy-army against the destructive forces of modern commerce. With a location as awe-inspiring as the Gobi desert, there\u2019s always a danger that the setting will overshadow the drama, but Batmunkh\u2019s committed performance is an emotional anchor that keeps the film clear of maudlin abstraction. Though dealing with overexploitation on a giant scale \u2013 these mining operations have even emptied rivers \u2013 Veins of the World is more interested in documenting the rituals of nomadic life. Amra visits the holy shaman tree, which bears witness not only to the child\u2019s familial trauma but also to the damage being wreaked daily on his ancestral land; this tenderness renders the desecration of the environment even more heartless and shocking. Though perhaps naive in its understanding of systematic exploitation, Veins of the World is intimate and sweeping all at once, and has at its core a sincerity that invites genuine empathy. Veins of the World is available on 13 April on YourScreen."}, "210705_news_469234.txt": {"page_id": "210705_news_469234.txt", "text": "Auf der Hausmesse GPU Technology Conference (GTC), die in diesem Jahr virtuell stattfindet, hat Nvidia einige Werkzeuge und Frameworks f\u00fcr Machine Learning angek\u00fcndigt. Allen voran dient das interaktive Conversational-AI-Framework Jarvis als Grundlage f\u00fcr Chatbots und virtuelle Assistenten. Daneben hat das Unternehmen  mit Megatron  ein Framework zum Trainieren von Sprachmodellen vorgestellt, das auf PyTorch aufbaut und auf das Training gro\u00dfer Sprachmodelle mit einer Transformer-Architektur abzielt.  Mit TAO  (Train, Adapt Optimize) zeigte Nvidia zudem ein GUI-basiertes Framework, mit dem Unternehmen vortrainierte ML-Modelle auf ihre spezielle Dom\u00e4ne anpassen k\u00f6nnen, um sie in ML-Anwendungen zu nutzen. Kommunikation auf mehreren Ebenen mit Jarvis Das Framework Jarvis zielt nicht nur auf Sprachverarbeitung, sondern bindet dar\u00fcber hinaus Computer Vision in die Verarbeitung ein. Nividia bezeichnet Jarvis als multimodales Conversational-AI-Framework. Zu den Kernfunktionen geh\u00f6ren automatische Spracherkennung (Automatic Speech Recognition, ASR), Natural Language Understanding (NLU) und Sprachsynthese (Text-to-Speech, TTS). Au\u00dferdem soll es beim Erkennen von Gesten, Objekten, Stimmungen und der Bewegung der Lippen sowie bei der Blickerfassung helfen.   Jarvis wertet diverse Aspekte der Kommunikation aus. \n      (Bild:\u00a0Nvidia)\n     Hinzu kommen dom\u00e4nenspezifische Services, um spezielle Szenarien abzudecken. Die Machine-Learning-Pipelines zum Verarbeiten der Informationen lassen sich individuell anpassen und mit eigenen Modellen nutzen. Dar\u00fcber hinaus bringt das Framework vortrainierte Modelle mit. Der Blogbeitrag verspricht, dass das mitgelieferte Spracherkennungsmodell eine Genauigkeit von \u00fcber 90 Prozent bietet. Insgesamt sollen die mitgelieferten Modelle ein Training von \u00fcber 100.000 Stunden auf Nvidia-DGX-Systemen hinter sich haben. Als Grundlagen dienen demnach gut eine Milliarde Textseiten und 60.000 Stunden Sprachdaten. Jarvis soll von Hause aus mit GPU-Beschleunigung Echtzeit\u00fcbersetzung in f\u00fcnf Sprachen mit einer Latenz unter 100 Millisekunden bieten.   Nvidias Promotion-Video zeigt die grundlegende Arbeitsweise von Jarvis. Jarvis befindet sich seit Mai 2020 im Early-Access-Programm. Seit Ende Februar ist die \u00f6ffentliche Beta von Jarvis 1.0 verf\u00fcgbar, und im Lauf des zweiten Quartals 2021 will Nvidia das Framework um zus\u00e4tzliche Funktionen erweitern. Weitere Details lassen sich  der Pressemitteilung  und  dem Entwicklerblog bei Nvidia  entnehmen. Zus\u00e4tzliche Ressourcen finden  sich auf der Projektseite . Partnerschaft mit Mozilla Dar\u00fcber hinaus k\u00fcndigt Nvidia eine Partnerschaft mit  Mozilla Common Voice  an. Die Open-Source-Sammlung von Sprachdaten war vergangenes Jahr im Zuge der Sparma\u00dfnahmen bei  Mozilla ins Wanken geraten , und der Projektleiter Kelly Davis hatte das Unternehmen verlassen. Insgesamt umfasst Common Voice gut 9000 Stunden Sprachaufzeichnungen in 60 Sprachen. Die Sammlung ist f\u00fcr jeden zug\u00e4nglich, und jeder kann eigene Daten beisteuern. Erkl\u00e4rtes Ziel ist es, durch ein breites Spektrum an Sprachdaten einem Bias in ML-Anwendungen entgegenzuwirken. Nvidia nutzt den Datensatz zum Entwickeln vortrainierter Modelle, die das Unternehmen wiederum kostenfrei der Allgemeinheit zur Verf\u00fcgung stellen willen. Laut einem  separaten Blogbeitrag von Mozilla  investiert Nvidia 1,5 Millionen US-Dollar in das Projekt. Common Voice l\u00e4uft ab sofort mit dem Ziel, vertrauensw\u00fcrdige KI zu schaffen, unter dem Dach der Mozilla Foundation. ( rme )"}, "210705_news_469243.txt": {"page_id": "210705_news_469243.txt", "text": "Enlarge   /  Udelv hasn't announced details about its forthcoming Mobileye-powered delivery robots\u2014such as who will help Udelv build them\u2014but they are expected to look something like this.  I've  written before  that Mobileye, an Intel subsidiary  since 2017 , is among the most formidable and underrated players in the self-driving sector. The Israeli company is the leading supplier of the chips, cameras, and software that power today's driver-assistance systems\u2014a couple of years ago, Mobileye claimed 70 percent market share. The company is hoping to enjoy a similarly dominant position in the emerging market for fully self-driving systems. On Monday, Mobileye  announced  that its self-driving technology stack would be branded Mobileye Drive. Mobileye says the technology will be \"a turn-key self-driving system ready for commercial deployment at scale.\" A Mobileye Drive system will have 13 cameras, three long-range lidars, six short-wave lidars, and six radars. It will be powered by Mobileye's EyeQ 5 processors. Mobileye Mobileye says the technology will be ready for commercial use by 2023. That would be a big deal if true, but I can't help being skeptical. Over the last five years, a number of major self-driving companies have announced optimistic launch dates and failed to meet them. If Mobileye can get the technology ready for market by 2023, it has the potential to become a major player in the emerging self-driving economy. Even if Mobileye misses its 2023 target, the company is still likely to be a significant player when it eventually enters the market. Mobileye already has strong relationships with major automakers,\u00a0and Mobileye plans to market Mobileye Drive to nonautomakers, too. On Monday, Mobileye  announced a deal  to sell Mobileye Drive to self-driving delivery startup Udelv. Udelv says it plans to deploy at least 35,000 delivery robots between 2023 and 2028 powered by Mobileye's technology. Mobileye hopes that this will be the first of many sales for the Mobileye Drive platform. Unique strengths Enlarge   /  Mobileye began testing in Munich in 2020. Mobileye Over the last five years, a number of companies have announced plans to commercially release self-driving technology in 2018, 2019, 2020, or 2021. Few (if any) have met their self-imposed deadlines. But Mobileye says it's different. The company says it hasn't made unrealistic promises in the past and is on track to deliver full self-driving technology to customers before the end of 2023. Mobileye does have a deep bench of engineering talent from its years of building hardware and software for driver-assistance systems. Mobileye is also in the process of rolling out one of the industry's most extensive testing programs. Mobileye tested its technology in Israel and Germany last year, and the company is\u00a0working to expand testing to Detroit, Tokyo, Paris, Shanghai, and possibly New York City this year. (Not coincidently, several of these cities are also the headquarters of major automakers.) Mobileye argues that testing in a variety of cities is important to ensure that its software doesn't overtrain on the quirks of one particular city. That's related to another of Mobileye's strategic advantages: access to gobs of map data. Mobileye has signed deals with several of its automaking partners to collect data from cameras on customer cars as they navigate cities around the world. Mobileye says it has largely automated the process of transforming this image data into highly accurate, three-dimensional maps that cover a large portion of the world's roads. These maps will not only contain information about road geometry\u2014they'll also provide valuable information about driving patterns that will help Mobileye's vehicles blend more naturally into existing traffic. A distinctive approach Most self-driving companies use a technique called sensor fusion: they take input from cameras, lidar, radar, and other sensors and combine them into a single unified model of the world. This model is then handed off to a planning module that figures out what the vehicle should do. By contrast, Mobileye is developing two completely independent self-driving systems\u2014one driven by cameras and the other by a combination of lidar and radar.\u00a0Once each of these systems has achieved a high level of performance separately, Mobileye will combine them into a single system. Mobileye believes this extra layer of redundancy will give the company higher confidence in the safety of its systems. There's plenty to like about this strategy, but it's far from obvious that it will be successful. Mobileye continues to rely on a head-scratching statistical argument to help establish the safety of its technology. Mobileye claims that\u2014if it can show its camera-based system can go 10,000 hours between crashes and its lidar-based system can separately go 10,000 hours between crashes\u2014its combined system will be able to go 100 million hours (10,000 times 10,000) without a crash. This argument implicitly assumes that the two systems' failure modes are statistically independent.\u00a0That\u00a0 doesn't seem like a plausible assumption :\u00a0situations that confuse one system are more likely to confuse the other one. So you can't just multiply the two probabilities together. Ultimately, it's hard to see how anyone\u2014even Mobileye insiders\u2014could be confident that the firm's technology will be ready by 2023. By the company's own admission, Mobileye's camera-based system is more mature than the lidar-based system. The company is still collecting the data it will need to convince regulators and the public that its system is safe. Mobileye might still be on track to deliver fully self-driving technology by 2023. But we won't know for sure until it happens. Mobileye inside Udelv Mobileye's relationships with automakers will\u00a0give it an advantage selling its technology to automakers. But experts expect that a lot of self-driving vehicles won't be customer-owned. Instead, they'll be in fleets used for on-demand taxi and delivery services. On Monday, Mobileye  announced a deal  to supply its forthcoming self-driving technology to Udelv, a startup that is building self-driving delivery robots. Under the deal, Mobileye will supply enough chips, sensors, and software to power 35,000 delivery robots between 2023 and 2028. Udelv is unusual among self-driving startups in that it doesn't see self-driving technology as its primary differentiator. Udelv CEO Daniel Laury believes that self-driving hardware and software will eventually be a commodity that a company like Udelv can license from multiple vendors. So Udelv has focused on other aspects of the delivery business. Udelv has been developing a new type of delivery truck that's optimized for self-driving operations. Rather than having discrete compartments, Udelv's truck features adjustable shelves and a two-dimensional iris-like door that gives customers access to one compartment at a time. This flexibility allows a single Udelv truck to carry a number of orders for different customers, with different item sizes, without worrying about customers taking the wrong items. Udelv is also focusing on the logistics of a delivery business. Udelv is already operating a delivery network using prototype vehicles with safety drivers behind the wheel. One of its biggest clients is a Houston auto-parts supplier called  XL Parts . Udelv hopes that XL Parts will be the first of many retailers who lease Udelv's trucks and technology to enable them to offer driverless delivery services to customers. Mobileye wants to work with everyone Enlarge   /  Nuro on Monday announced a pilot project to deliver Domino's pizzas in Houston. Domino's One of the most fundamental questions in any industry is how work is divided up among different companies. For example: most smartphone makers license the Android operating system from Google and buy chips from Qualcomm or one of its competitors. By contrast, Apple is vertically integrated, making its own operating system and many of its own chips for the iPhone. Self-driving companies have a wide range of theories about how the self-driving industry should be organized. Tesla is hewing to the Apple model: it's making its own cars and self-driving software, and it is planning to run its own ride-hailing network once the technology is ready. Waymo is planning to buy cars from traditional automakers, but it also hopes to develop most other components of its taxi service\u2014lidar sensors, self-driving software, ride-hailing app\u2014internally. By contrast, Mobileye hopes to sell its hardware, software, and sensors to as many partners as possible. Its deal with Udelv is nonexclusive, and Mobileye is presumably hoping to sell the same technology to Udelv's competitors. We expect Mobileye to also court companies creating taxi services, shuttle services, long-haul trucking services, and any other kind of self-driving service you can think of. There's an obvious attraction to this business model, since it lets Mobileye focus on what it does best\u2014develop chips and software\u2014and leave the messy business of operating delivery and taxi fleets to other companies. But the big question is whether Mobileye can actually build a self-driving stack that's suitable for use by a wide number of companies. A big advantage of the vertically integrated approach is that it sometimes enables a faster pace of innovation, since different parts of the product can be tailor-made for a specific purpose. That's the approach being taken by Nuro, one of Udelv's leading competitors. H-Town Founded by two ex-Google engineers, Nuro is also building self-driving delivery robots. Nuro just  announced  a deal to deliver pizza for a Domino's restaurant in the Houston area. If the pilot program goes well, it will presumably expand to many more Domino's and, eventually, to many other kinds of stores and restaurants. Nuro's vertically integrated approach means that it can carefully design its hardware and software for a robot designed for delivering pizzas and other short-range, low-speed trips. Nuro's robots don't need to go on the freeway. They also don't need to worry about keeping passengers safe\u2014since the vehicles will never have passengers. Developing a self-driving system that's customized for carrying goods at relatively slow speeds might just be an easier problem than developing a self-driving technology that works at all speeds and on all road types. Nuro can focus all of its energies on that task, while Mobileye is trying to design a system that works at freeway speeds and on a wide range of vehicle types. That might be more difficult and take longer. Or the opposite could be true: maybe Mobileye's vast data-collection efforts will enable the company to master the general self-driving problem more quickly than the focused efforts of a startup like Nuro. And Mobileye's open approach might let the company expand its market share rapidly by selling its technology to many partners at once."}, "210705_news_469263.txt": {"page_id": "210705_news_469263.txt", "text": "disclaimer: this is a technical post aimed at developers being somewhat aware of the problem space. There will be a concluding \u2018the day of\u2026\u2019 post aimed at end users where some of the benefits will be demonstrated in a stronger light. A few months back, I wrote a lighter\u00a0 post  about an ongoing effort towards reshaping the venerable Linux/BSD CLI to be free of the legacy cruft that comes with having to deal with the emulation of old terminal protocols, stressing the point that these protocols make the CLI less efficient, and hard to work with from both a user- and a developer- perspective. In this post, we\u2019ll recap some of the problems, go through the pending solution, update with the current progress and targets, and see what\u2019s left to do. To recap, some of the key issues to adress were: Split between terminal emulator and command line shell breaks desktop integration \u2013 \u00a0Visual partitions such as windows, borders and popups are simulated with characters that are unwanted in copy-paste operations and fail to integration with an outer desktop shell (if any). Code/data confusion  \u2013 both the terminal emulator and text-oriented user interfaces (TUIs) tries to separate content from metadata using a large assortment of encoding schemes, all being prone to errors, abuse, difficult to parse and ridden with legacy. Uncertain capabilities/feature-set  \u2013 basic things like color depth, palette, character encoding schemes and so on are all probed through a broken mishmash of environment variables, capability databases and the actual support varies with the terminal emulator that is being used. Confusion between user-input and data \u00a0\u2013 programs can\u2019t reliably distinguish between interactive (keyboard) input, pasted/\u201dIPC\u201d input and other forms of data entry. Lack of synchronisation.  This\u00a0makes it impossible for the terminal emulator to know when it is supposed to draw, and signal propagation contributes to making resize operations slow. Crazy encoding schemes for representing non-character data \u2013 \u00a0 such as  Sixel . This just scratches the surface and don\u2019t go into related issues when it comes to user-interaction, consistency, and it ignores the entire problem space of system interaction when it comes to tty devices, input modes, virtual terminal switching and so on. If you consider the entire feature-set of all protocols that are already around and in use, you get a very \u201c Cronenberg \u201c- take on a display server and I, at least, find the eerie similarities between terminal emulators and the insect typewriters from  Naked Lunch \u00a0both amusing, tragic and frightening at the same time; the basic features one would expect are there, along with some very unwanted ones, but pieced together in an outright disgusting way. If we also include related libraries and tools like  curses \u00a0and\u00a0 turbo vision \u00a0we get a clunky version of a regular point and click UI toolkit. Even though the scope is arguably more narrow and well-defined, these libraries are conceptually not far away from the likes of Qt, GTK and Electron. Study unicode and it shouldn\u2019t be hard to see that \u2018text\u2019 is mostly graphics, the largest difference by far is the smallest atom, and the biggest state-space explosion comes from saying \u2018pixel\u2019 instead of cell. So the first question is, why even bother to do anything at all within this spectrum instead of just maintaining the status quo? One may argue that we can, after all, write good CLI/TUIs using QT running on Xorg today, no change needed \u2013 it\u2019s just not the path people typically take; maybe it\u2019s the paradigm of favouring mouse or touch oriented user interaction that is \u201cat fault\u201d here, along with favouring style and aesthetics over substance. One counterpoint is that the infrastructure needed to support the toolkit+display server approach is morbidly obese into the millions of lines of code, when the problem space should be solvable within the tens-of-thousands, but \u201cso what, we have teraflops and gigabytes to spare!\u201d. Ok, how about the individual investment of writing software? accommodating for disabilities? attack surface? mobility and mutability of produced output? efficiency for a (trained) operator? or when said infrastructure isn\u2019t available? the list goes on. There is arguably a rift here between those that prefer the \u2018shove it in a browser\u2019 or flashy UIs that animate and morph as you interact, and those that prefer staring into a text editor. It seems to me that the former category gets all the fancy new toys, while the latter mutters on about insurmountable levels of legacy. What I personally want is many more \u201cone- purpose\u201d TUIs and for them to be much easier to develop. They need to be simpler, more consistent, obvious to use, and more configurable. That\u2019s nice and dreamy, but how are \u201cwe\u201d supposed to get there? First, lets consider some of the relevant components of the Arcan project as a whole, as the proposed solution reconfigures these in a very specific way. The following picture shows the span of current components: This time around, we\u2019re only interested in the parts marked  SHMIF ,  Terminal  and  TUI . Everything else can be ignored.\u00a0 SHMIF  is the structural glue/client IPC.  TUI  is a developer facing API built on top of SHMIF but with actual guarantees of being a forward/backwards compatible API. Terminal is a vtXXX terminal emulator/state machine built using a modified and extended version of libtsm. Inside the \u2018Arcan\u2019 block from the picture, we have something like this: From this, we take the  frameserver(ipc) \u2013 block and we put it into its own  shmif-server  library. We take the  platform  block and split out into its own,\u00a0 libarcan-abc . Terminal is extended to be able to use these two APIs along with optional Lua/whatever bindings for the TUI API so that the higher level shell CLI logic with all its string processing ickiness can be written in something that isn\u2019t C. This opens the door for two configurations. \u00a0Starting with the more complex one, we get this figure: Here, Arcan is used as the main display server or hooked up to render using another one (there are implementations of the platform layer for both low-level and high-level system integration). The running \u2018appl\u2019 acts as the window manger (which can practically be a trivial one that just works as fullscreen or the alt+fN VT switching style with only a few lines of code) and it may spawn one or many of the  afsrv_terminal . These can be run in \u2018compatibility mode\u2019 where the emulator state machine is activated and it acts just like xterm and friends. We can also run it in a simpler form: In this mode, the terminal works directly with the platform layer to \u00a0drive displays and sample input. It can even perform this role directly at boot if need be. An interesting property of shmif here is the support for different connection modes (which I\u2019ll elaborate on in another post) where you can both interactively\u00a0 migrate  and  delegate  connection primitives. This means that you can switch between these two configurations at runtime, without data loss \u2013 even have the individual clients survive and reconnect in the event of a display server crash. No matter the configuration,  you (the ghost behind the shell)  get access to all the  features in shmif\u00a0 and can decide which ones that should be used and which ones that should be rejected.  You  are in control over the routing via the choice in shell (and the appl- for the complex version). Recall that the prime target now is local text-oriented, command line interfaces \u2013 not changing or tampering with the awk | sed | grep | \u2026 flow, that\u2019s an entirely different beast. In contrast to curses and similar solutions, this approach also avoids tampering with  stdin ,  stdout ,  stderr  or  argv,  because \u00a0 connection primitives and invocation arguments are inherited or passed via env. This should mean that retrofitting existing tools can be done without much in terms of ifdef hell or breaking existing code. Anyhow, most of this is not just vapours from some hallucinogenic vision but has, in fact, already been implemented and been in testing for quite some time. What is currently being worked on now and for the near future is improving the quality in some of the existing stages and adding: Double buffering on the virtual cell screen level to add support for sub-cell \u201csmooth\u201d scrolling, text shaping, BiDi, and non-monospace, properly kerned, text rendering. API and structures for designating regions (alt-screen mode) or lines (normal mode) for custom input, particularly mixing/composing contents from other tui clients or frameservers. Then comes some more advanced refactoring: Shmif-server API still being fleshed out. Libarcan-abc platform split, as it depends on another refactoring effort. Lua bindings and possibly an example shell. And more advanced \u201csome time in the future\u201d things: Shmif-server-proxy tool that can convert to-/from- a network or pipe-passed \u2018line format\u2019 (protocol) to enable networking support and test high latency/packet loss behavior. CPU- only platform rasteriser (current form uses GL2.1+ or GLES2/3). Ports to more OSes (currently only Linux, FreeBSD, OSX). Should all these steps succeed, the last \u2018nail in the coffin\u2019 will be to provide an alternative platform output target that undoes all this work and outputs into a VT100 compliant mess again \u2013 all for the sake of backwards compatibility. That part is comparably trivial as it is the end result of \u2018composition\u2019 (merge all layers), it is the premature composition that is (primarily) at fault here as information is irreversibly lost. It is just made worse in this case as the feature scope of the output side (desktop computer instead of dumb terminal) and the capability of the input side (clients) mismatch because of the communication language. Like this: Like   Loading... Related  "}, "210705_news_469284.txt": {"page_id": "210705_news_469284.txt", "text": "Auf den ersten Blick sieht es so aus, als w\u00e4ren die Remote Carrier nur die \u201eJuniorpartner\u201c des New-Generation Fighter. Es k\u00f6nnte sich jedoch erweisen, dass sie gemeinsam mit der Combat Cloud zu den Elementen des Future Combat Air System geh\u00f6ren werden, von denen langfristig gesehen der gr\u00f6\u00dfere Innovationsschub und die h\u00f6here Wirkung ausgehen werden. Bekanntlich soll das Future Combat Air System (FCAS) alle vorhandenen und zuk\u00fcnftigen Elemente der Luftstreitkr\u00e4fte Frankreichs, Deutschlands und Spaniens zu einem \u201eSystem of Systems\u201c zusammenschlie\u00dfen, das dar\u00fcber hinaus f\u00e4hig ist, mit der Gesamtheit der eigenen Streitkr\u00e4fte (nach heutiger Lesart umfasst dies die \u201eDom\u00e4nen\u201c Land, Luft, See, Weltraum und Cyber) sowie der Streitkr\u00e4fte verb\u00fcndeter Nationen in einem bisher nicht erreichbaren Grad \u201enahtlos\u201c zusammenzuarbeiten. Eines der zuk\u00fcnftigen Elemente steht im Mittelpunkt der konzeptionellen Entwicklung des FCAS und war urspr\u00fcnglich synonym f\u00fcr das Gesamtsystem: ein Kampfflugzeug der n\u00e4chsten (dann wohl der sechsten) Generation, heute als New-Generation Fighter (NGF) bezeichnet. Zur Erweiterung seines Leistungsspektrums soll der New-Generation Fighter m\u00f6glicherweise selbst unbemannt eingesetzt werden k\u00f6nnen, in jedem Fall aber durch unbemannte Luftfahrzeuge, sogenannte Remote Carrier (RC), unterst\u00fctzt werden. Die Kombination von New-Generation Fighter und Remote Carrier f\u00fchrt die Bezeichnung Next Generation Weapon System (NGWS). Das Element des FCAS, das die Zusammenarbeit aller an einer Operation beteiligten Mittel und Kr\u00e4fte auf der Basis eines gemeinsamen, dynamischen Lagebilds sicherstellen soll, ist die sogenannte Combat Cloud. Dabei erstreckt sich die Wirkung der Cloud auf alle daf\u00fcr erforderlichen Vorg\u00e4nge der Informationsgewinnung, -verarbeitung und -\u00fcbertragung. Dies umfasst sowohl interne Vorg\u00e4nge wie z.B. das Zusammenwirken von New-Generation Fighter und Remote Carrier als auch die Zusammenarbeit mit externen Elementen aus anderen Dom\u00e4nen, weshalb man bei Airbus Defence and Space (Lead, Main Partner f\u00fcr die Combat Cloud) dazu \u00fcbergegangen ist, die Bezeichnung Multi-Domain Combat Cloud (MDCC) zu verwenden. Die auf der Paris Airshow 2019 gezeigten originalgro\u00dfen Modelle des New Generation Fighter und eines Remote Carrier (Foto: Ulrich Renn) Die im gesamten Wirkungsbereich der Cloud anfallenden Datenmengen werden so gro\u00df sein, dass das Ziel, die richtige Information zur rechten Zeit an den richtigen Nutzer zu bringen und ein umfassendes, dom\u00e4nen-\u00fcbergreifendes an die jeweilige F\u00fchrungsebene angepasstes Lagebild zu generieren, mit herk\u00f6mmlichen Mitteln nicht erreicht werden kann. Daher ruhen gro\u00dfe Hoffnungen auf dem Einsatz von k\u00fcnstlicher Intelligenz (KI), die unter anderem mit neuartigen f\u00fcr maschinelles Lernen und automatisierte Kooperation optimierten Algorithmen die Identifikation, Verarbeitung, Verteilung und Darstellung der f\u00fcr eine Operation relevanten Daten auf das erforderliche Niveau bringen soll. Melden Sie sich unkompliziert zu einem Tageszugang an, um sofort von allen ESUT Digital Vorteilen zu profitieren.\u00a0 Zum Tageszugang   Digital Tageszugang  f\u00fcr 1,50 \u20ac / 1 Tag   Bestellen   Digital Halbjahresabo  f\u00fcr 30 \u20ac / 6 Monate   Bestellen Digital Jahresabo \nf\u00fcr 60 \u20ac / 12 Monate Bestellen Genie\u00dfen Sie die Premium-Inhalte und weitere Vorteile von\u00a0 ESUT   Digital : Zugang zu allen Online-Inhalten Umfassende Suche im News-Archiv Individualisierbarer Newsbereich Hintergr\u00fcnde, Analysen und technische Fachartikel komplett und exklusiv aus der Europ\u00e4ischen Sicherheit und Technik und den Wehrtechnischen Reports Tagesaktuelle News aus den Rubriken Industrie / Innere Sicherheit / International / Land / Luft / Politik / R\u00fcstung / See / Streitkr\u00e4fte uvm. \u00a0"}, "210705_news_469337.txt": {"page_id": "210705_news_469337.txt", "text": "Enlarge   /  A nurse practitioner named Heidi Johnson administers a vaccine from Johnson & Johnson. On Tuesday morning, the Food and Drug Administration and the Centers for Disease Control and Prevention issued a release acknowledging that an extremely rare clotting disorder was associated with the use of the Johnson & Johnson COVID vaccine. The problem is actually less than a one-in-a-million issue; in data from the US, where 6.8 million doses of this vaccine have been used, there have only been six instances of the clotting problem detected. Because the clots call for an unusual treatment, however, the organizations are calling for a pause in administering the shot. This will provide them with time to ensure the medical community is aware of the appropriate treatment. This is not the first vaccine to create extremely rare clotting issues. They've also been seen following\u00a0 use of the AstraZeneca vaccine . The problem appears to be caused by the harmless virus (an adenovirus) that carries a single gene from SARS-CoV-2 in order to elicit an immune response. The leading hypothesis to explain the phenomenon is that, in very rare cases, the adenovirus triggers an immune response to factors found on the surface of platelets, which are an essential part of the clotting process. This activates platelets, causing clots, and at the same time reduces the total platelet count. These seemingly contradictory changes make treating the issue through the normal approach to excessive clotting dangerous. Typically, the appearance of clots would call for using a treatment that would reduce the probability of clots forming. But due to the low platelet counts in these individuals, those treatments can make it much less likely that clots form when they're needed. It's this difference between apparent patient needs and appropriate treatment that has caused the CDC and FDA to call for a pause in the use of the J&J vaccine. CDC will convene a meeting of the Advisory Committee on Immunization Practices on Wednesday to further review these cases and assess their potential significance. FDA will review that analysis as it also investigates these cases. Until that process is complete, we are recommending a pause in the use of this vaccine out of an abundance of caution. This is important, in part, to ensure that the health care provider community is aware of the potential for these adverse events and can plan for proper recognition and management due to the unique treatment required with this type of blood clot. The CDC and FDA highlight severe headache, abdominal pain, leg pain, or shortness of breath as potential symptoms of the clotting problem. So far, all six cases have occurred among women below the age of 50 and appeared between one and two weeks after vaccination. Because of the rarity of the clotting disorder compared to the very large risks posed by COVID-19, most countries that initially suspended the use of the AstraZeneca vaccine re-started it with added precautions. Germany, for example, is using it for patients over 60, since they are at lower risk of clots and much higher risk of severe COVID-19. It's likely that the US will also resume use of the J&J vaccine, which still has an excellent overall safety profile, typically has lower side effects than two-dose vaccines, and is extremely effective. Unfortunately, the US also has a large population of people who are hesitant about receiving any vaccines. Given their persistent worries, these events will likely feed into their paranoia. And their apparent resistance to facts will make it much harder for them to evaluate the risk calculation of an extremely rare side effect in a vaccine against a virus that's frequently deadly."}, "210705_news_469338.txt": {"page_id": "210705_news_469338.txt", "text": "ANN ARBOR, Mich.--( BUSINESS WIRE )--Prominent military and veteran service organizations, non-governmental organizations, corporations, foundations, and federal agencies are banding together in the  Partnership to FightCybercrime  to launch a national program that supports America\u2019s service members, veterans, spouses, survivors, and their families\u2014a community of 28 million that is disproportionately targeted for online scams, fraud, and identity theft.\n \nReports made to the Federal Trade Commission (FTC) show that U.S. military personnel, veterans and their families lost more than $420.5 million to cybercrime between 2016 and 2020. Additionally, the FTC found the median financial loss due to fraud is 44% higher for veterans than that of other civilians. 1 in 4 Americans is a victim of cybercrime but only 700,000 incidents were reported to the Internet Crime Complaint Center (IC3) in 2020.\n \n\u201cCybercriminals, online fraudsters and international crime syndicates are attacking our nation\u2019s military and veteran community at alarming rates,\u201d said Kristin Judge, CEO of the Cybercrime Support Network (CSN), the convener of the Partnership. \u201cBesides the emotional and financial toll on individuals and families, cybercrime can have a devastating impact on our military readiness and national security.\u201d\n \nJudge notes that scammers take advantage of multiple aspects of military life including the distance between family members, regular deployments and relocations, high use of social media networks, and accessing government programs and benefits online. Romance and sextortion scams, real estate rental scams, personal data phishing schemes, and charity fraud are common cybercrimes among this community.\n \n\"Service members devote their lives to protecting us, so it\u2019s our duty to help protect them. This program is a true partnership that brings the best military advocates together to facilitate a coordinated, actionable response to serve those targeted by cybercrime in a simple, effective manner,\u201d said Craig Newmark, whose charitable organization Craig Newmark Philanthropies is a founding partner of this program. \u201cExperiencing cybercrime is stressful enough. I\u2019m proud to support CSN and their groundbreaking work in helping those who have been harmed by cybercrime and taking the stress out of finding helpful resources.\u201d\n \nThe Partnership\u2019s Military & Veteran Program will educate and mobilize the community to recognize, report, and recover from cybercrime. The  FightCybercrime.org/military  website offers free prevention resources, online security tips, and guidance for service members and veterans to report cybercrime incidents to the appropriate authorities.\n \nThe  Partnership to FightCybercrime  Military & Veteran partner organizations are: 22Kill, Army Emergency Relief, DAV (Disabled American Veterans), Gold Star Wives - Arlington Chapter, Got Your Back Network, Iraq and Afghanistan Veterans of America, Links to Freedom, National Association of Black Veterans, National Organization for Victim Assistance, Patriot Boot Camp, Racing for Heroes, Tragedy Assistance Program for Survivors, Travis Manion Foundation, and VetSec.\n \nInterested nonprofit organizations can become a partner of the  Partnership to FightCybercrime  at no cost. Partners will support the program\u2019s outreach efforts, webinars, speaking engagements, and other initiatives. Please contact Ursula Palmer, CSN\u2019s Executive Director of Military and Veteran Programs, at  military@cybercrimesupport.org  to get involved.\n \nCSN will carry out a 12-month capital campaign to raise the $3.5 million needed to allow for the full implementation of the program. Corporations, foundations and other for-profit organizations interested in supporting our Nation\u2019s military and veteran community through this effort can contact Joan Giovanni, CSN\u2019s Chief Revenue Officer, at  engagement@cybercrimesupport.org .\n The  Partnership to FightCybercrime   is an initiative of the  Cybercrime Support Network (CSN).  CSN is a public-private, 501(c)(3) nonprofit collaboration whose mission is to serve individuals and businesses impacted by cybercrime. The organization is supported by  Craig Newmark Philanthropies ,  AT&T ,  Cisco Systems ,  Comcast ,  Google ,  Microsoft ,  NordVPN ,  Trend Micro ,  Verizon . For more information, visit  Cybercrimesupport.org  or  FightCybercrime.org .\n \nThe Military & Veteran program was co-founded by CSN, Craig Newmark Philanthropies and The Bob Woodruff Foundation.\n Craig Newmark Philanthropies'  goal is to support and connect people and drive broad civic engagement, working to advance grassroots organizations that are effective and getting stuff done in areas that include: Trustworthy journalism & the information ecosystem, voter protection, women in tech and veterans & military families. The Bob Woodruff Foundation  stands out from the sea of 46,000 nonprofits that serve veterans by the way in which they find, fund and shape innovative programs within that maze. It is by connecting communities with the brave heroes returning to them that BWF is best able to provide more than stopgap measures, but long-term solutions that take a holistic approach to recovery. "}, "210705_news_469402.txt": {"page_id": "210705_news_469402.txt", "text": "Whether it\u2019s Florida during hurricane season, California during fire season, or Texas during, erm\u2026 ice season, one thing is certain: Mother Nature\u2019s threats are growing in impact. They\u2019re no longer black swan events, either. Electricity infrastructure planners around the globe can learn from the U.S. military, whose mission requires planning for low-frequency, high-impact threats, and which has increasingly  focused on energy resilience  as foundational to their global peacekeeping and war-fighting missions. After the catastrophic power outages in Texas in February, politicized finger-pointing started immediately. Some blamed frozen wind turbines, others faulted the state\u2019s over-reliance on natural gas (which was stymied by frozen well-heads and pipelines), and yet others said the islanded transmission system and price-focused market structure used by the Electric Reliability Council of Texas (ERCOT), the independent operator of Texas\u2019 grid, were most culpable. Let\u2019s face it, though: failure to plan for extreme weather affects every type of power generation and any type of market structure. This is really a story about a widespread failure to take climate change-induced weather scenarios into account when planning for, recovering from, and rebuilding after extreme weather events. In the U.S. and around the world, every type of power system within every regulatory structure has faced catastrophic power failures due to increasingly severe weather. There have been many assessments of what, precisely, froze or failed in Texas, and since the full postmortem will take months to untangle, I\u2019ll spare you another recap. Suffice it to say that, end-to-end, there was a good-old-fashioned lack of infrastructure weatherization for the polar vortex freeze that pushed south into Texas in February, despite the  strong recommendations of US electricity regulators  following the 1989 and 2011 polar freezes. So, what are we to do? In many ways, it\u2019s simple. But it\u2019s not at all easy. Nor is it cheap. All of us \u2013 whether we are energy customers who depend on a reliable and resilient grid, or energy suppliers in charge of providing such \u2013 must change the way we think about (and what we\u2019re willing to pay for) high-impact, low-frequency attacks on our power grid. In the military,  mission assurance requires energy assurance , which is why planners often use a four-step approach to building robust and resilient energy systems. It\u2019s called D2R2: Deter. Defend. Recover. Rebuild. In the private sector, reliable energy is equally important, and the same approach can be used by infrastructure planners. This is P2R2. Prevent. Plan. Recover. Rebuild. Here\u2019s how it works: Prevention.  While we may have some ability to prevent human attacks on our power system (like cyber incursions or theft of substation components), preventing Mother Nature\u2019s increasingly volatile weather attacks may be impossible. It bears repeating that, according to the U.S.\u2019 most recent  Federal Government Climate Threat Assessment , released by the Trump Administration in 2019: \u201cMore frequent and intense extreme weather and climate-related events \u2026 are expected to continue to damage infrastructure, ecosystems, and social systems that provide essential benefits to communities.\u201d We need to reduce greenhouse gas pollution by accelerating the transition to zero-emission power sources to prevent further climate damage. At the same time, current and future extreme weather events need to be considered  grey rhinos  \u2014 a cousin to both the black swan and the elephant in the room \u2014 probable, low-frequency, high-impact (yet often neglected) threats. Planning.  Historical weather models are no longer accurate estimates of the future, yet most infrastructure planners still rely on them to determine the equipment necessary to provide vital services. Instead, planners need to use future-focused, climate-informed, predictive models to assess the \u201cnew normal\u201d of uncommon, but extreme, weather events. In addition to climate-informed models, planning requires assessing recovery capabilities through desktop exercises, system stress-testing, and real-world, pull-the-plug practices.  NERC \u2019s GridEx exercises and  Electric Infrastructure Security (EIS) Council\u2019s EarthEx exercises  are great examples; however, those scenarios are not developed locally and may be seen by system operators as black swan events. The U.S. Military has considered electricity to be critical to mission success since the cold war, when they hardened key substations for the possibility of high-altitude nuclear detonation that could cause  electromagnetic pulse outages.  In the last decade, the Pentagon has had a renewed focus, as cyberattacks and Mother Nature\u2019s weather have again threatened power infrastructure.  The Department of Defense now requires  a range of desktop simulations as well as physical pull-the-plug stress-testing of systems. Civilian electricity systems need a similar approach. The combination of climate-informed models and stress-testing will help grid operators determine how and where to shore up infrastructure to withstand extreme weather (or other risks to the power system). Likewise, given the significant costs associated with infrastructure upgrades and enhancements, as a society, we will need to decide how to pay for these improvements. Recovery.  Planning also requires recognizing that all generation sources fail sometimes, whatever the cause. Thus, how a power system recovers from a widespread outage must also evolve. An interconnected grid enables regions with excess power to serve regions in need. Micro-grids for critical facilities, rolling outage protocols that ensure sufficient warmth and water supply for everyone, and strong transmission will be necessary for 21st century recovery. Here, again, the private sector can take a lesson from the military. Rebuilding.  As pipelines and turbines thaw out and resume normal operations (it seems Texas was spared significant sustained damage in this regard), it may be easy to resume a business-as-usual approach and not see the next preventable grey rhino charging our way. Examining how New York and Puerto Rico are rebuilding after widespread, weather-induced damage to their grids is enlightening. Cascading blackouts during Tropical Storm Sandy resulted in the New York Power Authority re-building with much greater segmentation in their transmission system, allowing them to shut power to fewer residents in future storms. Building robust, resilient power systems for 21st century threats is the name of the game. This article by Miranda A. Ballentine, CEO, Renewable Energy Buyers Alliance, was published by the  World Economic Forum (Visited 101 times, 1 visits today)  "}, "210705_news_469423.txt": {"page_id": "210705_news_469423.txt", "text": "(Reuters) - U.S. federal health agencies on Tuesday recommended pausing the use of Johnson & Johnson's COVID-19 vaccine for at least a few days after six women under 50 given the shot developed rare blood clots, dealing a fresh setback to efforts to tackle the pandemic. Following the news, Johnson & Johnson (J&J) said it was delaying the rollout of the vaccine to Europe, a week after regulators there said they were reviewing rare blood clots in four recipients of the shot in the United States. Janet Woodcock, acting commissioner of the U.S. Food and Drug Administration (FDA) said the agency expected the pause to be a matter of days, and it was aimed at providing information to healthcare providers so they can diagnose, treat and report such blood clots. The moves come after European regulators said earlier this month they had found a possible link between AstraZeneca's COVID-19 vaccine and a similar rare blood clotting problem that led to a small number of deaths. J&J's single dose vaccine - most COVID-19 shots are delivered over two doses - and AstraZeneca's low-cost vaccine are seen as vital weapons in the fight against a pandemic that has claimed more than three million lives. Immunology experts echoed U.S. officials in underscoring that the risk posed by the J&J vaccine appeared extremely low, and it remained a valuable tool against the risks of COVID-19. But they acknowledged the need for health officials to proceed with caution to understand the best ways to mitigate any risk. \"Even if causally linked to the vaccine: 6 cases with about 7 million doses (lower than the risk of clots with oral contraceptives) is not something to panic about,\" Dr. Amesh Adalja, an infectious disease expert at the Johns Hopkins Center for Health Security in Baltimore, said in an email. \"People are asking me if they should cancel their J&J vaccine appointments and I have told them not to but I know many will and this will stall progress in controlling the pandemic.\" The FDA told a briefing there had been one reported death from the rare blood clotting condition among recipients of the J&J vaccine, while another person was in a critical condition. FDA official Peter Marks said it was \"plainly obvious\" the J&J cases were \"very similar\" to the AstraZeneca ones. However, officials said there had been no similar blood clot cases reported among recipients of the Moderna and Pfizer/BioNTech vaccines, which have accounted for most U.S. vaccinations so far. The White House said the pause in the J&J vaccine would not have a \"significant\" impact on its plan to administer about three million shots per day and a total of 200 million shots before President Joe Biden's 100th day in office. FDA's Marks said part of the reason for the pause was to warn doctors that administering the standard treatments for clots can cause tremendous harm in these cases, or be fatal. Vials with a sticker reading, \"COVID-19 / Coronavirus vaccine / Injection only\" and a medical syringe are seen in front of a displayed Johnson & Johnson logo in this illustration taken October 31, 2020. REUTERS/Dado Ruvic/Illustration Read More BALANCE OF RISKS Most of the available J&J vaccine has been used in the United States due to production issues that have limited the company's supply. As of April 12, more than 6.8 million doses of the J&J vaccine had been administered in the United States, representing about one reported case per million people. An advisory committee to the U.S. Centers for Disease Control and Prevention (CDC) will meet on Wednesday to review the cases, and the FDA will review the analysis, the agencies said in a statement. All six cases involved women between the ages of 18 and 48, and the symptoms occurred six to 13 days after vaccination. In the cases, a type of blood clot called cerebral venous sinus thrombosis (CVST) was seen in combination with low levels of blood platelets (thrombocytopenia). J&J said it was working closely with regulators and noted no clear causal relationship had been established between the cases and the COVID-19 vaccine made by its Janssen unit. \"To put this into perspective, it's similar to the chance of being struck by lightning in any given year in the UK. On the other hand, the risks from COVID-19 are substantial,\" said Ian Douglas, Professor of Pharmacoepidemiology at the London School of Hygiene & Tropical Medicine. \"If all 6.8 million people who've received the J&J vaccine in the U.S. were infected with the virus, several thousand would likely die and many more, including younger adults, would experience serious and long-lasting after effects.\" PROBLEMS The J&J and AstraZeneca vaccines both use an adenovirus vector - a harmless cold virus - that instructs human cells to produce a protein found on the surface of the coronavirus, thereby spurring the immune system to prepare an arsenal against the COVID-19-causing virus. Among leading global COVID-19 vaccine developers, China's CanSino Biological and Russia's Gamaleya Institute with its Sputnik V vaccine are also relying on this approach. The Pfizer/BioNtech and Moderna vaccines use mRNA technology. The European Medicines Agency (EMA) continues to recommend the use of AstraZeneca's COVID-19 vaccine, saying the benefits outweigh the risks. Several EU countries, however, have limited its use to certain age groups. As of April 4, the EMA said 169 cases of CVST and 53 of splanchnic vein thrombosis had been reported after vaccination with the AstraZeneca shot. About 34 million people had been given the shot in Europe by this date. J&J only began delivering its COVID-19 vaccine to European Union countries this week. Our Standards:  The Thomson Reuters Trust Principles."}, "210705_news_469450.txt": {"page_id": "210705_news_469450.txt", "text": "Die Corona-Pandemie hat aus Sicht von Regierungsberatern erhebliche R\u00fcckst\u00e4nde in der Digitalisierung in Deutschland \"schonungslos\" offengelegt. Sowohl beim Ausbau der digitalen Infrastruktur als auch beim Einsatz digitaler Technologien und Dienstleistungen sei das Land hinter viele andere OECD-Staaten zur\u00fcckgefallen, bem\u00e4ngelt der wissenschaftliche Beirat des Bundeswirtschaftsministeriums in einem am Dienstag ver\u00f6ffentlichten Gutachten. \"Im vergangenen Jahr hat die Corona-Krise zu einem erheblichen Digitalisierungsschub gef\u00fchrt. In einigen Bereichen, zum Beispiel dem Homeoffice, war es m\u00f6glich, in kurzer Zeit auf digitale Kommunikation und die Nutzung digitaler Prozesse umzustellen, in anderen Bereichen, so im Schul- und Gesundheitswesen, gelang dies nur m\u00fchsam oder so gut wie gar nicht\", sagte der Vorsitzende des Beirats, Klaus Schmidt, anl\u00e4sslich der Vorstellung des Gutachtens. Viele der Fortschritte im Bereich der Digitalisierung w\u00e4hrend der Corona-Pandemie h\u00e4tten aus Sicht der Berater auch schon lange vor der Krise unternommen werden k\u00f6nnen. \"Koordinations- und Organisationsversagen\" Von den Expertinnen und Experten wurden Entwicklungen in Bereichen wie Breitbandausbau, Homeoffice und digitale Kommunikation, bargeldlose Zahlung, Gesundheitssystem, allgemeinbildende Schulen, Hochschulen und \u00f6ffentliche Verwaltung untersucht. Dabei beruhe der R\u00fcckstand Deutschlands bei der Digitalisierung oftmals weniger auf fehlenden finanziellen Mitteln oder Marktversagen, sondern auf verschiedenen Formen von Organisationsversagen,  hei\u00dft es in dem Gutachten . Vor allem der \u00f6ffentlichen Hand stellt der Beirat schlechte Noten aus. Deutschland leiste sich in Schulen, Hochschulen, Landes- und Bundesministerien, kommunalen Verwaltungseinheiten und Gerichten \"Strukturen, Prozesse und Denkweisen, die teilweise archaisch anmuten\". Bereits vor der Pandemie habe sich das als ein wesentliches Hemmnis f\u00fcr eine effektive Digitalisierung erwiesen. Internationale Vergleiche h\u00e4tten das \"Koordinations- und Organisationsversagen der \u00f6ffentlichen Hand in Deutschland\" mehrfach aufgezeigt. Die administrative und politische F\u00fchrung m\u00fcsse hier vorangehen und die Dringlichkeit der Transformation deutlich machen. Keine Frage der Mittel Auch mit einer massiven Erh\u00f6hung der Mittel sei vorerst keine Beschleunigung zu erwarten, wenn nicht auch Abl\u00e4ufe in Planung und Umsetzung einfacher gestaltet und Aufgaben besser verteilt w\u00fcrden. Erkennbar sei das unter anderem am  Digitalpakt Schule, dessen Mittel bislang kaum an den Schulen angekommen seien . Entsprechend sollte die Politik b\u00fcrokratische Hindernisse minimieren und einen Regulierungsrahmen schaffen, der Digitalisierung erleichtert, wird im Gutachten gefordert. Dazu k\u00f6nnte etwa ein verbindlicher Staatsvertrag zwischen den Bundesl\u00e4ndern f\u00fcr Vereinfachungen sorgen. Ebenfalls spricht sich der Beirat f\u00fcr ein weniger absolutes Verst\u00e4ndnis des Datenschutzes aus. Im Falle des Breitbandzugangs sieht man vor allem den Staat als n\u00f6tigen Vorreiter, weil hier der Markt versage; ein staatlicher Ausbau k\u00f6nne die notwendige Infrastruktur z\u00fcgiger bereitstellen, insbesondere im l\u00e4ndlichen Raum. Bundeswirtschaftsminister Peter Altmaier (CDU) begr\u00fc\u00dfte die Empfehlungen. \"Der Beirat weist aber zu Recht darauf hin, dass Finanzmittel allein noch kein Allheilmittel sind. Genauso braucht es mehr Bereitschaft zu Ver\u00e4nderungen und verbesserte organisatorische Abl\u00e4ufe\", meinte Altmaier. Dies gelte f\u00fcr den Staat wie f\u00fcr Unternehmen. \"Unb\u00fcrokratische und dezentrale L\u00f6sungen, die der Beirat etwa f\u00fcr die Arbeit im Homeoffice bef\u00fcrwortet, k\u00f6nnen uns hierbei weiterbringen.\" ( axk )"}, "210705_news_469471.txt": {"page_id": "210705_news_469471.txt", "text": "  MKS  has announced the  Ophir\u00ae  LightIR 16-180mm Medium Wave Infrared (MWIR) f/3.6 continuous zoom lens, the newest addition to the LightIR family of lightweight, long-range, motorised continuous zoom lenses. This is a compact thermal imaging lens optimised for smaller size 10\u00b5m pitch VGA Focal Plane Array (FPA) detectors. It features a folded optics design that significantly reduces the length of the lens and therefore the overall size and weight of the optical system in which it is incorporated. The lens is 65 per cent smaller than existing MWIR 10\u00b5m VGA lenses. This makes it ideal for the demanding requirements of reduced size, weight, and power (SWaP) and small gimbal thermal imaging applications. These include drones and tactical UAV IR cameras as well as micro-/mini-tactical payloads in the commercial, homeland security, and defence markets.   The folded optics design enables long optical lengths, which produce a reduced sensitivity to tolerances in compact configurations that incorporate fewer optical elements. The innovative design also enables near diffraction-limit performance in harsh environments and addresses such challenges as line-of-sight (LOS) stabilisation and athermalisation. At only 460gr, it is one of the lightest continuous zoom lenses on the market and maintains sharp focus over the entire zoom range. The mechanical and electrical interface assures easy integration into camera systems."}, "210705_news_469472.txt": {"page_id": "210705_news_469472.txt", "text": "New York, April  13, 2021  (GLOBE NEWSWIRE) -- Reportlinker.com announces the release of the report \"Next Generation Military Power Supply Market Research Report by Type, by Component, by System Type, by End Use - United States Forecast to 2025 - Cumulative Impact of COVID-19\" -  https://www.reportlinker.com/p06014214/?utm_source=GNW   Market Statistics: The report provides market sizing and forecast across five major currencies - USD, EUR GBP, JPY, and AUD. This helps organization leaders make better decisions when currency exchange data is readily available. 1. The United States Next Generation Military Power Supply Market is expected to grow from USD 2,052.01 Million in 2020 to USD 3,178.31 Million by the end of 2025. 2. The United States Next Generation Military Power Supply Market is expected to grow from EUR 1,799.24 Million in 2020 to EUR 2,786.80 Million by the end of 2025. 3. The United States Next Generation Military Power Supply Market is expected to grow from GBP 1,599.53 Million in 2020 to GBP 2,477.47 Million by the end of 2025. 4. The United States Next Generation Military Power Supply Market is expected to grow from JPY 219,001.74 Million in 2020 to JPY 339,206.63 Million by the end of 2025. 5. The United States Next Generation Military Power Supply Market is expected to grow from AUD 2,979.79 Million in 2020 to AUD 4,615.33 Million by the end of 2025. Market Segmentation & Coverage: This research report categorizes the Next Generation Military Power Supply to forecast the revenues and analyze the trends in each of the following sub-markets: \"The Programmable is projected to witness the highest growth during the forecast period\" Based on Type, the Next Generation Military Power Supply Market studied across Non-Programmable and Programmable. The Non-Programmable commanded the largest size in the Next Generation Military Power Supply Market in 2020. On the other hand, the Programmable is expected to grow at the fastest CAGR during the forecast period. \"The Software is projected to witness the highest growth during the forecast period\" Based on Component, the Next Generation Military Power Supply Market studied across Hardware and Software. The Hardware further studied across AC/DC Converter, DC/AC Converter, DC/DC Converter, and EMI Filter. The Hardware commanded the largest size in the Next Generation Military Power Supply Market in 2020. On the other hand, the Software is expected to grow at the fastest CAGR during the forecast period. \"The Integrated Power Modules is projected to witness the highest growth during the forecast period\" Based on System Type, the Next Generation Military Power Supply Market studied across Discrete Power Supply System and Integrated Power Modules. The Discrete Power Supply System commanded the largest size in the Next Generation Military Power Supply Market in 2020. On the other hand, the Integrated Power Modules is expected to grow at the fastest CAGR during the forecast period. \"The Naval is projected to witness the highest growth during the forecast period\" Based on End Use, the Next Generation Military Power Supply Market studied across Aerial, Land, and Naval. The Aerial commanded the largest size in the Next Generation Military Power Supply Market in 2020. On the other hand, the Naval is expected to grow at the fastest CAGR during the forecast period. Cumulative Impact of COVID-19: COVID-19 is an incomparable global public health emergency that has affected almost every industry, so for and, the long-term effects projected to impact the industry growth during the forecast period. Our ongoing research amplifies our research framework to ensure the inclusion of underlaying COVID-19 issues and potential paths forward. The report is delivering insights on COVID-19 considering the changes in consumer behavior and demand, purchasing patterns, re-routing of the supply chain, dynamics of current market forces, and the significant interventions of governments. The updated study provides insights, analysis, estimations, and forecast, considering the COVID-19 impact on the market. 360iResearch\u2122 FPNV Positioning Matrix: The 360iResearch\u2122 FPNV Positioning Matrix evaluates and categorizes the vendors in the Next Generation Military Power Supply Market on the basis of Business Strategy (Business Growth, Industry Coverage, Financial Viability, and Channel Support) and Product Satisfaction (Value for Money, Ease of Use, Product Features, and Customer Support) that aids businesses in better decision making and understanding the competitive landscape. 360iResearch\u2122 Competitive Strategic Window: The 360iResearch\u2122 Competitive Strategic Window analyses the competitive landscape in terms of markets, applications, and geographies. The 360iResearch\u2122 Competitive Strategic Window helps the vendor define an alignment or fit between their capabilities and opportunities for future growth prospects. During a forecast period, it defines the optimal or favorable fit for the vendors to adopt successive merger and acquisition strategies, geography expansion, research & development, and new product introduction strategies to execute further business expansion and growth. The report provides insights on the following pointers: 1. Market Penetration: Provides comprehensive information on the market offered by the key players 2. Market Development: Provides in-depth information about lucrative emerging markets and analyzes the markets 3. Market Diversification: Provides detailed information about new product launches, untapped geographies, recent developments, and investments 4. Competitive Assessment & Intelligence: Provides an exhaustive assessment of market shares, strategies, products, and manufacturing capabilities of the leading players 5. Product Development & Innovation: Provides intelligent insights on future technologies, R&D activities, and new product developments The report answers questions such as: 1. What is the market size and forecast of the United States Next Generation Military Power Supply Market? 2. What are the inhibiting factors and impact of COVID-19 shaping the United States Next Generation Military Power Supply Market during the forecast period? 3. Which are the products/segments/applications/areas to invest in over the forecast period in the United States Next Generation Military Power Supply Market? 4. What is the competitive strategic window for opportunities in the United States Next Generation Military Power Supply Market? 5. What are the technology trends and regulatory frameworks in the United States Next Generation Military Power Supply Market? 6. What are the modes and strategic moves considered suitable for entering the United States Next Generation Military Power Supply Market? Read the full report:  https://www.reportlinker.com/p06014214/?utm_source=GNW About  Reportlinker ReportLinker is an award-winning market research solution. Reportlinker finds and organizes the latest industry data so you get all the market research you need - instantly, in one place. __________________________    "}, "210705_news_469555.txt": {"page_id": "210705_news_469555.txt", "text": "Jeff Hawkins gets full credit for getting me first interested in the idea that neuroscience might lead to artificial general intelligence\u2014an idea which gradually turned into an all-consuming hobby, and more recently a new job. I'm not alone in finding him inspiring. Andrew Ng claimed  here  that Hawkins helped convince him, as a young professor, that a simple scaled-up learning algorithm could reach Artificial General Intelligence (AGI). (Ironically, Hawkins scoffs at the deep neural nets built by Ng and others\u2014Hawkins would say: \"Yes yes, a simple scaled-up learning algorithm can reach AGI, but not  that  learning algorithm!!\") Hawkins's last book was  On Intelligence  in 2004. What's he been up to since then? Well, if you don't want to spend the time reading his journal articles or  watching his research meetings on YouTube , good news for you\u2014his new book,  A Thousand Brains , is out! There\u2019s a lot of fascinating stuff here. I'm going to pick and choose a couple topics that I find especially interesting and important, but do read the book for much more that I'm not mentioning. A grand vision of how the brain works Many expert neuroscientists think that the brain is horrifically complicated, and we are centuries away from understanding it well enough to build AGI (i.e., computer systems that have the same kind of common-sense and flexible understanding of the world and ability to solve problems that humans do). Not Jeff Hawkins! He thinks we  can  understand the brain well enough to copy its principles into an AGI. And he doesn't think that goal is centuries away. He thinks we're most of the way there! In  an interview last year  he guessed that we\u2019re within 20 years of finishing the job. The people arguing that the brain is horrifically complicated seem at first glance to have a strong case. The brain has a whopping\u00a0 10 11 \u00a0neurons with\u00a0 10 14 \u00a0synapses, packed full of intricate structure.  One study  found 180 distinct areas within the cerebral cortex. Neuroscience students pour over huge stacks of flashcards with terms like \u201cstriatum\u201d, \u201chabenula\u201d, \u201cstria medullaris\u201d, \u201cfregula\u201d, and \"interpeduncular nucleus\". (Quiz: Which of those are real brain regions, and which are types of pasta?) Every year we get another 50,000 or so new neuroscience papers dumped into our ever-deepening ocean of knowledge about the brain, with no end in sight. So the brain is indeed horrifically complicated. Right? Well, Jeff Hawkins and like-minded thinkers have a rebuttal, and it comes in two parts: 1. The horrific complexity of the \u201cold brain\u201d doesn\u2019t count, because we don\u2019t need it for AGI According to Hawkins, much of the brain\u2014including a disproportionate share of the brain's horrific complexity, like the interpeduncular nucleus I mentioned\u2014 just doesn\u2019t count . Yes it\u2019s complicated. But we don\u2019t care, because understanding it is not necessary for building AGI. In fact, understanding it is not even  helpful  for building AGI! I\u2019m talking here about the distinction between what Hawkins calls  \u201cold brain vs new brain\u201d . The \u201cnew brain\u201d is the mammalian neocortex, a wrinkly sheet on that is especially enlarged in humans, wrapping around the outside of the human brain, about 2.5 mm thick and the size of a large dinner napkin (if you unwrinkled it). The \u201cold brain\u201d is everything else in the brain, which (says Hawkins) is more similar between mammals, reptiles, and so on. \u201cThe neocortex is the organ of intelligence,\u201d writes Hawkins. \u201cAlmost all the capabilities we think of as intelligence\u2014such as vision, language, music, math, science, and engineering\u2014are created by the neocortex. When we think about something, it is mostly the neocortex doing the thinking\u2026. If we want to understand intelligence, then we have to understand what the neocortex does and how it does it. An animal doesn\u2019t need a neocortex to live a complex life. A crocodile\u2019s brain is roughly equivalent to our brain, but without a proper neocortex. A crocodile has sophisticated behaviors, cares for its young, and knows how to navigate its environment...but nothing close to human intelligence.\u201d I think Hawkins's  new brain / old brain discussion is bound to drive neuroscientist readers nuts . See, for example, the paper  Your Brain Is Not An Onion With A Tiny Reptile Inside  for this perspective, or see the current widespread dismissal of  \u201ctriune brain theory\u201d . The mammalian neocortex is in fact closely related to the \u201cpallium\u201d in other animals, particularly the well-developed pallium in birds and reptiles (including, yes, crocodiles!). One researcher (Tegan McCaslin) attempted a  head-to-head comparison between bird pallium and primate neocortex , and found that there was no obvious difference in intelligence, when you hold the number of neurons fixed. A  recent  paper  found suggestive evidence of similar neuron-level circuitry between the bird pallium and mammalian neocortex. Granted, the neurons have a different spatial arrangement in the bird pallium vs the mammal neocortex. But it\u2019s the neuron types and connectivity that define the algorithm, not the spatial arrangement.  Paul Cisek traces the origin of the pallium  all the way back to the earliest proto-brains. The human neocortex indeed massively expanded relative to chimpanzees, but then again, so did the \u201cold brain\u201d human cerebellum and thalamus. And what\u2019s more (these angry neuroscientists would likely continue), it\u2019s not like the neocortex works by itself. The \u201cold brain\u201d thalamus has just as much a claim to be involved in human intelligence, language, music, and so on as the neocortex does, and likewise with the \u201cold brain\u201d basal ganglia, cerebellum, and hippocampus. OK. All this is true. But I\u2019m going to stick my neck out and say that Hawkins is  \u201ccorrect in spirit\u201d  on this issue. And I\u2019ve tried (e.g.  here ) to stake out a more careful and defensible claim along the same lines. My version goes: Mammal (and lizard) brains have a  \u201clearning subsystem\u201d . It implements a learning algorithm that starts from scratch (analogous to random weights\u2014so it\u2019s utterly useless to the organism at birth\u2014see discussion of \"learning-from-scratch-ism\"  here ), but helps the organism more and more over time, as it learns. This subsystem involves the entire \"telencephalon\" region of the brain\u2014namely, the neocortex (or pallium), hippocampus, amygdala, part of the basal ganglia, and a few other things (again see  here )\u2014along with parts of the thalamus and cerebellum, but definitely  not , for example, the hypothalamus or brainstem. This subsystem is not particularly \u201cnew\u201d or peculiar to mammals; very simple versions of this subsystem date back to the earliest vertebrates, helping them learn to navigate their environment, remember where there's often food, etc. But the subsystem  is  unusually large and sophisticated in humans, and it  is  the home of human intelligence, and it does  primarily  revolve around the activities of the cortex / pallium. So far as I can tell, my version keeps all the good ideas of Hawkins (and like-minded thinkers) intact, while avoiding the problematic parts. I'm open to feedback, of course. 2. The horrific complexity of the neocortex is in the learned content, not the learning algorithm The second reason that making brain-like AGI is easier than it looks, according to Hawkins, is that \u201cthe neocortex looks similar everywhere\u201d. He writes, \"The complex circuitry of the neocortex looks remarkably alike in visual regions, language regions, and touch regions, [and even] across species.... There are differences. For example, some regions of the neocortex have more of certain cells and less of others, and there are some regions that have an extra cell type not found elsewhere...But overall, the variations between regions are relatively small compared to the similarities.\" How is it possible for one type of circuit to do so many things? Because it\u2019s a learning algorithm! Different parts of the neocortex receive different types of data, and correspondingly learn different types of patterns as they develop. Think of the  OpenAI Microscope  visualizations of different neurons in a deep neural net. There\u2019s so much complexity! But no human needed to design that complexity; it was automatically discovered by the learning algorithm. The learning algorithm itself is comparatively simple\u2014gradient descent and so on. By the same token, a cognitive psychologist could easily spend her entire career diving into the intricacies of how an adult neocortex processes phonemes. But on Hawkins's view, we can build brain-like AGI without doing any of that hard work. We just need to find the learning algorithm, and let 'er rip, and it will construct the phoneme-processing machinery on its own. Hawkins offers various pieces of evidence that the neocortex runs a single, massively-parallel, legible learning algorithm. First, as above, \"the detailed circuits seen everywhere in the neocortex are remarkably similar\u201d. Second, \u201cthe major expansion of the modern human neocortex relative to our hominid ancestors occurred rapidly in evolutionary time, just a few million years. This is probably not enough time for multiple new complex capabilities to be discovered by evolution, but it is plenty of time for evolution to make more copies of the same thing.\u201d Third is plasticity\u2014for example how blind people use their visual cortex for other purposes. Fourth, \u201cour brains did not evolve to program computers or make ice cream.\" There's a lot more evidence for and against, beyond what Hawkins talks about. (For example,  here's  a very clever argument in favor that I saw just a few days ago.) I\u2019ve written about cortical uniformity previously ( here ,  here ), and plan to do a more thorough and careful job in the future. For now I\u2019ll just say that this is certainly a hypothesis worth taking seriously, and even if it\u2019s not  universally  accepted in neuroscience, Hawkins is by no means the only one who believes it. 3. Put them together, and you get a vision for brain-like AGI on the horizon So if indeed we can get AGI by reverse-engineering just the neocortex (and its \u201chelper\u201d organs like the thalamus and hippocampus), and if the neocortex is a relatively simple, human-legible, learning algorithm, then all of the sudden it doesn\u2019t sound so crazy for Hawkins to say that brain-like AGI is feasible, and not centuries away, but rather already starting to crystallize into view on the horizon. I found this vision intriguing when I first heard it, and after quite a bit more research and exposure to other perspectives, I still more-or-less buy into it\u00a0(although as I mentioned, I'm not done studying it). By the way, an interesting aspect of cortical uniformity is that it's a giant puzzle piece into which we need to (and haven\u2019t yet) fit every other aspect of human nature and psychology. There should be whole books written on this. Instead,  nothing .\u00a0For example, I have all sorts of social instincts\u2014guilt, the desire to be popular, etc. How exactly does that work? The neocortex knows whether or not I\u2019m popular, but it doesn\u2019t care, because (on this view) it\u2019s just a generic learning algorithm. The old brain cares very much whether I'm popular, but it\u2019s too stupid to understand the world, so how would it know whether I\u2019m popular or not? I\u2019ve casually speculated on this a bit (e.g.  here ) but it seems like a gaping hole in our understanding of the brain, and you won\u2019t find any answers in Hawkins\u2019s book \u2026 or anywhere else as far as I know! I encourage anyone reading this to try to figure it out, or tell me if you know the answer. Thesis topic anyone? A grand vision of how the neocortex works For everything I've written so far, I could have written essentially the same thing about Hawkins\u2019s 2004 book. That's not new, although it remains as important and under-discussed as ever. A big  new  part of the book is that Hawkins and collaborators now have more refined ideas about exactly what learning algorithm the neocortex is running. (Hint: it\u2019s  not  a deep convolutional neural net trained by backpropagation. Hawkins  hates  those!) This is a big and important section of the book. I\u2019m going to skip it. My excuse is:  I wrote a summary of an interview he did a while back , and that post covered more-or-less similar ground. That said, this book describes it better, including a new and helpful (albeit still a bit sketchy) discussion of learning abstract concepts. To be clear, in case you're wondering, Hawkins does not have a complete ready-to-code algorithm for how the neocortex works. He claims to have a framework including essential ingredients that need to be present. But many details are yet to be filled in. Does machine intelligence pose any risk for humanity? Some people (cf.  Stuart Russell's book ) are concerned that the development of AGI poses a substantial risk of catastrophic accidents, up to and including human extinction. They therefore urge research into how to ensure that AIs robustly do what humans want them to do\u2014just as Enrico Fermi invented  nuclear reactor control rods   before  he built the first nuclear reactor. Jeff Hawkins is having none of it. \u201cWhen I read about these concerns,\u201d he says, \u201cI feel that the arguments are being made without any understanding of what intelligence is.\u201d Well, I\u2019m more-or-less fully on board with Hawkins\u2019s underlying framework for thinking about the brain and neocortex and intelligence. And I  do  think that developing a neocortex-like AGI poses a serious risk of catastrophic accidents, up to and including human extinction, if we don\u2019t spend some time and effort developing new good ideas analogous to Fermi\u2019s brilliant invention of control rods. So I guess I\u2019m in an unusually good position to make this case! Start with Hawkins\u2019s argument  against  machine intelligence being a risk I\u2019ll start by summarizing Hawkins\u2019s argument that neocortex-like AGI does  not  pose an existential threat of catastrophic accidents. Here are what I take to be his main and best arguments: First, Hawkins says that we\u2019ll build in safety features. Asimov\u2019s three laws of robotics were proposed in the context of science-fiction novels and don\u2019t necessarily apply to all forms of machine intelligence. But in any product design, there are safeguards that are worth considering. They can be quite simple. For example, my car has a built-in safety system to avoid accidents. Normally, the car follows my orders, which I communicate via the accelerator and brake pedals. However, if the car detects an obstacle that I am going to hit, it will ignore my orders and apply the brakes. You could say the car is following Asimov\u2019s first and second laws, or you could say that the engineers who designed my car built in some safety features. Intelligent machines will also have built-in behaviors for safety. Second, Hawkins says that goals and motivations are separate from intelligence. The neocortex makes a map of the world, he says. You can use a map to do good or ill, but \u201ca map has no motivations on its own. A map will not desire to go someplace, nor will it spontaneously develop goals or ambitions. The same is true for the neocortex.\u201d Third, Hawkins has specific disagreements with the idea of \u201cgoal misalignment\u201d. He correctly describes what that is: \u201cThis threat supposedly arises when an intelligent machine pursues a goal that is harmful to humans  and  we can\u2019t stop it. It is sometimes referred to as the \u201cSorcerer\u2019s Apprentice\u201d problem\u2026. The concern is that an intelligent machine might similarly do what we ask it to do, but when we ask the machine to stop, it sees that as an obstacle to completing the first request. The machine goes to any length to pursue the first goal\u2026. Again, he rejects this: The goal-misalignment threat depends on two improbabilities: first, although the intelligent machine accepts our first request, it ignores subsequent requests, and second, the intelligent machine is capable of commandeering sufficient resources to prevent all human efforts to stop it\u2026. Intelligence is the ability to learn a model of the world. Like a map, the model can tell you how to achieve something, but on its own it has no goals or drives. We, the designers of intelligent machines, have to go out of our way to design in motivations. Why would we design a machine that accepts our first request but ignores all others after that?...The second requirement of the goal-misalignment risk is that an intelligent machine can commandeer the Earth\u2019s resources to pursue its goals, or in other ways prevent us from stopping it...To do so would require the machine to be in control of the vast majority of the world\u2019s communications, production, and transportation\u2026. A possible way for an intelligent machine to prevent us from stopping it is blackmail. For example, if we put an intelligent machine in charge of nuclear weapons, then the machine could say \u201cIf you try to stop me, I will blow us all up.\u201d... We have similar concerns with humans. This is why no single human or entity can control the entire internet and why we require multiple people to launch a nuclear missile.\u201d The devil is in the details Now I don\u2019t think any of these arguments are particularly unreasonable. The common thread as I see it is, what Hawkins writes is the  start  of a plausible idea to avoid catastrophic AGI accidents. But when you think about those ideas a bit more carefully, and try to work out the details, it starts to seem much harder, and less like a slam-dunk and more like an open problem which might or might not even be solvable. 1. Goals and motivations are separate from intelligence (\"The Alignment Problem\") Hawkins writes that goals and motivations are separate from intelligence. Yes! I\u2019m totally on board with that. As stated above, I think that the neocortex (along with the thalamus etc.) is running a general-purpose learning algorithm, and the brainstem etc. is nudging it to hatch and execute plans that involve reproducing and winning allies, and nudging it to  not  hatch and execute plans that involve falling off cliffs and getting eaten by lions. By the same token, we want and expect our intelligent machines to have goals. As Hawkins says, \u201cWe wouldn\u2019t want to send a team of robotic construction workers to Mars, only to find them lying around in the sunlight all day\u201d! So how does that work? Here's Hawkins: To get a sense of how this works, imagine older brain areas conversing with the neocortex. Old brain says, \u201cI am hungry. I want food.\u201d The neocortex responds, \u201cI looked for food and found two places nearby that had food in the past. To reach one food location, we follow a river. To reach the other, we cross an open field where some tigers live.\u201d The neocortex says these things calmly and without value. However, the older brain area associates tigers with danger. Upon hearing the word \u201ctiger,\u201d the old brain jumps into action. It releases [cortisol]... and neuromodulators\u2026in essence, telling the neocortex \u201cWhatever you were just thinking, DON\u2019T do that.\u201d When I put that description into a diagram, I wind up with something like this: My attempt to depict goals and motivation, as described by Hawkins via his tiger example above. The box on the left has the learning algorithm (neocortex, thalamus, etc.) The box on the right is the Old Brain module that, for example, associates tigers with danger. (For my part,  I would draw the boundaries slightly differently, and put things into the terminology of reinforcement learning , but I'm trying to stick closely to the book here.) The neocortex proposes ideas, and the Judge (in the \"old brain\") judges those ideas to be good or bad. This is a good start. I can certainly imagine building an intelligent goal-seeking machine along these lines. But  the devil is in the details ! Specifically:  Exactly what algorithm do we put into the \u201cJudge\u201d box?  Let's think it through. First things first, we should not generally expect the \u201cJudge\u201d to be an intelligent machine that understands the world. Otherwise,  that  neocortex-like machine would need  its own  motivation, and we\u2019re right back to where we started! So I\u2019m going to suppose that the Judge box will house a relatively simple algorithm written by humans. So exactly what do you put in there to make the robot want to build the infrastructure for a Mars colony? That's an open question. Second, given that the Judge box is relatively stupid, it needs to do a lot of memorization of the form  \u201cthis meaningless pattern of neocortical activity is good, and this meaningless pattern of neocortical activity is bad\u201d , without having a  clue  what those patterns actually mean. Why? Because otherwise the neocortex would have an awfully hard time coming up with intelligent instrumental subgoals on its way to satisfying its actual goals. Let\u2019s say we have an intelligent robot trying to build the infrastructure for a Mars colony. It needs to build an oxygen-converting machine, which requires a gear, which requires a lubricant, and there isn't any, so it needs to brainstorm. As the robot's artificial neocortex brainstorms about the lubricant, its Judge needs to declare that some of the brainstormed plans are good (i.e., the ones that plausibly lead to finding a lubricant), while others are bad. But the Judge is too dumb to know what a lubricant is. The solution is a kind of back-chaining mechanism. The Judge starts out knowing that the Mars colony is good (How? I don't know! See above.). Then the neocortex envisages a plan where an oxygen machine helps enable the Mars colony, and the Judge sees this plan and memorizes that the \u201coxygen machine\u201d pattern in the neocortex is probably good too, and so on. The human brain has exactly this kind of mechanism, I believe, and I think that it\u2019s implemented in the basal ganglia. (Update: I now think it's not  just  the basal ganglia, see  here .) It seems like a necessary design feature, I\u2019ve never heard Hawkins say that there\u2019s anything problematic or risky about this mechanism, so I\u2019m going to assume that the Judge box will involve this kind of database mechanism. Modified version of the motivation installation system. The database\u2014which I believe is implemented in the basal ganglia\u2014is essential for the machine to pursue \u201cinstrumental subgoals\u201d, like \u201ctrying\u201d to design a lubricant without the machine needing to constantly have in mind the entire chain of logic for why it\u2019s doing so, i.e. that the lubricant is needed for the gear which is needed for the machine which is...etc. etc. Again, for my own purposes  I would draw it a bit differently and use reinforcement learning terminology , but I'm trying to stay close to what's in the book.\u00a0 Now given all that, we have two opportunities for \u201cgoal misalignment\u201d to happen: Outer misalignment:  The algorithm that we put into the Judge box might not exactly reflect the thing that we want the algorithm to do. For example, let\u2019s say I set up a machine intelligence to be the CEO of a company. This being America, my shareholders immediately launch a lawsuit that says that I am in violation of my fiduciary duty unless the Judge box is set to \u201cHigher share price is good, lower share price is bad,\u201d and nothing else. With lawyers breathing down my neck, I reluctantly do so. The machine is not  that  smart or powerful, what\u2019s the worst that could happen? The results are quite promising for a while, as the algorithm makes good business decisions. But meanwhile, over a year or two, the algorithm keeps learning and getting smarter, and behind my back it is  also  surreptitiously figuring out how to hack into the stock exchange to set its share price to infinity,  and  it's working to prevent anyone from restoring the computer systems after it does that, by secretly self-replicating around the internet, and earning money to hire freelancers for strange little jobs that involve receiving packages and mixing chemicals and mailing them off, unknowingly engineering a new pandemic virus, and meanwhile the algorithm is also quietly hacking into military robotics systems so that it will be ready to hunt down the few survivors of the plague, and spreading disinformation so that nobody knows what the heck is happening even as it's happening, etc. etc. I could go on all day but you get the idea. OK, maybe you'll say \"anyone could have seen that coming,  obviously  maximizing stock price is a dumb and dangerous goal\". So what goal should we use instead, and how do we write that code? Let's figure it out! And by the way, even if we have a concrete and non-problematic idea of what the goal is, remember that the Judge box is stupid and doesn't understand the world, and therefore the code that we write into the Judge box will presumably be a simplistic approximation of the goal we really want. And unfortunately,  seeking a simplistic approximation of a goal looks  very  different from seeking the actual goal . Inner misalignment:  The assigned values in the database of meaningless (to the Judge) memorized patterns could diverge from how the Judge algorithm would judge their consequences if it actually saw them implemented in the real world. I don\u2019t have to look far for an example of this:  Look at Hawkins himself!  He has a neocortex, and he has an \u201cold brain\u201d putting goals and motivations into his mind, and he just  hates  it!  His book has a whole subsection called \u201cHow the neocortex can thwart the old brain\u201d!  (And to be clear, thwarting the old brain is portrayed as a very good idea that he endorses.) I find it remarkable that Hawkins can gleefully plan to thwart his own \u201cold brain\u201d, while at the same time being baffled at the idea that anything might go wrong when we put old-brain-like motivation systems into our own intelligent machines. Not that things  definitely will  go wrong; it\u2019s just that avoiding these problems does not seem straightforward, and we shouldn\u2019t declare that this is a solvable problem until we have a better idea of what the solution is. So again,  exactly  what code do we put into the \u201cJudge\u201d box such that the resulting system is definitely motivated to build the infrastructure for a Mars colony (while not trying to manipulate its own motivation system, prevent its goals from being edited, and so on)? You won\u2019t find the answer in Hawkins\u2019s book. In fact, it\u2019s a wide-open problem, and I may well be literally the only person on Earth who is actively working on it in the specific context of neocortex-like machine intelligence. I sure don\u2019t know the answer, or indeed whether an answer exists. Reach out if you have ideas or want to collaborate! 2. \u201cSafety features\u201d Now let\u2019s move on to Hawkins\u2019s \u201csafety features\u201d, like Asimov\u2019s first two laws or something vaguely analogous. Great idea! I\u2019m all for it! We should definitely do that! But once again, the devil is in the details! Let\u2019s say we want to put in a safety interlock that prevents the AGI from self-replicating. How  exactly  do we do that? What code do we write? When  I  think about it, I immediately hit a similar problem as before. The safety interlock code is probably  not  going to be a neocortex-like intelligent submodule that understands the world\u2014because if it is, we\u2019re back to the problem of installing the right motivation. Instead maybe we\u2019ll go with human-written code, which implies it\u2019s a \u201cstupid\u201d module that does  not  understand the world. For example, to avoid self-replication, maybe we\u2019ll add a module that detects when a copy-paste operation is happening to the source code, and block it. Unfortunately, if an intelligent robot is motivated to self-replicate, and they notice that there is a module preventing them from doing so, then they will naturally start trying to undermine, outwit, or disable that module. And remember, the robot is a lot more intelligent than the module! By all means let\u2019s put in such a module anyway. It seems especially helpful in \"early childhood\" when the machine is not yet very intelligent, and still messing around, and we don't want it to do anything dangerous by accident. We should just recognize that it\u2019s unlikely to keep working when the machine becomes highly intelligent, unless we have  both  a safety interlock  and  a carefully-sculpted motivation system that makes the machine  like and endorse  that safety interlock. If we do it right, then the machine will even go out of its way to repair the safety interlock if it breaks! And how do we do that? Now we\u2019re back to the open problem of installing motivations, discussed above. The other option is to design a safety interlock that is  absolutely perfectly rock-solid air-tight , such that it cannot be broken even by a highly intelligent machine trying its best to break it. A fun example is  Appendix C of this paper  by Marcus Hutter and colleagues, where they propose to keep an intelligent machine from interacting with the world except through certain channels. They have a plan, and it\u2019s  hilariously awesome : it involves multiple stages of air-tight boxes, Faraday cages, laser interlocks, and so on, which could be (and absolutely should be) incorporated into a big-budget diamond heist movie starring Tom Cruise. OK sure, that could work! Let\u2019s keep brainstorming! But let\u2019s  not  talk about \u201csafety features\u201d for machine intelligence as if it\u2019s the same kind of thing as an automatic braking system. 3. Instrumental convergence Hawkins suggests that a machine will want to self-replicate if (and  only  if) we deliberately program it to want to self-replicate, and likewise that a machine will \u201caccept our first request but ignore all others after that\u201d if (and  only  if) we deliberately program it to accept our first request but ignore all others after that. (That would still leave the vexing problem of troublemakers deliberately putting dangerous motivations into AGIs, but let\u2019s optimistically set that aside.) ...If only it were that easy! \u201cInstrumental convergence\u201d  is the insight (generally credited to Steve Omohundro) that lots of seemingly-innocuous goals  incidentally  lead to dangerous motivations like self-preservation, self-replication, and goal-preservation. Stuart Russell\u2019s famous example is asking a robot to fetch some coffee. Let\u2019s say we solve the motivation problem (above) and actually get the robot to  want  to fetch the coffee, and to want absolutely nothing else in the world (for the sake of argument, but I\u2019ll get back to this). Well, what does that entail? What should we expect? Let\u2019s say I go to issue a new command to this robot (\u201cfetch the tea instead\u201d), before the robot has actually fetched the coffee. The robot sees me coming and knows what I'm going to do. Its neocortex module imagines the upcoming chain of events: it will receive my new command, and then all of the sudden it will only want to fetch tea, and it will never fetch the coffee. The Judge watches this imagined chain of events and\u2014just like the tiger example quoted above\u2014the judge will say \u201cWhatever you were just thinking, DON\u2019T do that!\u201d Remember, the Judge hasn\u2019t been reprogrammed  yet ! So  it  is still voting for neocortical plans-of-action based on whether the coffee winds up getting fetched. So that's no good. The neocortex goes right back to the drawing board. Hey, here's an idea, if I shut off my audio input, then I won't hear the new command, and I  will  fetch coffee. \"Hey, now  that's  a good plan,\" says the Judge. \"With  that  plan, the coffee will get fetched! Approved!\" And so that's what the robot does. Similar considerations show that intelligent machines may well try to stay alive, self-replicate, increase their intelligence, and so on, without anyone \u201cgoing out of their way\u201d to install those things as goals. A better perspective is that if we want our machines to have any goals at all, we have to \"go out of our way\" to  prevent  these problematic motivations\u2014and how to do so reliably is an open problem. Now, you ask, why would anyone do something so stupid as to give a robot a maniacal, all-encompassing, ultimate goal of fetching the coffee? Shouldn't we give it a more nuanced and inclusive goal, like \u201cfetch the coffee unless I tell you otherwise\u201d, \u201cfetch the coffee while respecting human values and following the law and so on\u201d or more simply \u201cAlways try to do the things that I, the programmer, want you to do\u201d? Yes! Yes they absolutely should! But yet again, the devil is in the details! As above, installing a motivation is in general an unsolved problem. It may not wind up being possible to install a complex motivation with  surgical precision ; installing a goal may wind up being a sloppy, gradual, error-prone process. If \u201cmost\u201d generic motivations lead to dangerous things like goal-preservation and self-replication, and if installing motivations into machine intelligence is a sloppy, gradual, error-prone process, then we should be awfully concerned that even skillful and well-intentioned people will sometimes wind up making a machine that will take actions to preserve its goals and self-replicate around the internet to prevent itself from being erased. How do we avoid that? Besides what I mentioned above (figure out a safe goal to install and a method for installing it with surgical precision), there is also interesting ongoing work searching for ways to generally prevent systems from developing these instrumental goals ( example ). It would be awesome to figure out how to apply those ideas to neocortex-like machine intelligence. Let\u2019s figure it out, hammer out the details, and  then  we can go build those intelligent machines with a clear conscience! Summary I found this book thought-provoking and well worth reading. Even when Hawkins is wrong in little details\u2014like whether the \u201cnew brain\u201d is \u201cnewer\u201d than the \u201cold brain\u201d, or whether a deep neural net image classifier can learn a new image class without being retrained from scratch (I guess he hasn\u2019t heard of fine-tuning?)\u2014I think he often winds up profoundly right about the big picture. Except for the \"risks of machine intelligence\" chapter, of course... Anyway, I for one thank Jeff Hawkins for inspiring me to do the research I\u2019m doing, and I hope that he spends more time applying his formidable intellect to the problem of how  exactly  to install goals and motivations in the intelligent machines he aims to create\u2014including complex motivations like \u201cbuild the infrastructure for a Mars colony\u201d. I encourage everyone else to think about it too! And reach out to me if you want to brainstorm together! Because  I  sure don\u2019t know the answers here, and if he's right, the clock is ticking..."}, "210705_news_469624.txt": {"page_id": "210705_news_469624.txt", "text": "Apple's event invite definitely has those spring colors, and the squiggles of an Apple Pencil. \n                                                    Apple\n                                                 This story is part of  Apple Event , our full coverage of the latest news from Apple headquarters.\n    \n                            \n                \n                                \n                                                            \n\n\n\n\n\n\n\n        \n\n                                         Apple has set the date for its next online-only event, sending out invites to the media for Tuesday, April 20, at 10 a.m. PT (1 p.m. ET). Apple is expected to introduce new iPads, with upgraded screens, and possibly the company's long-rumored AirTags trackers.   The new devices will be shown off during a stream on Apple's website. The tech giant has been holding online-only events amid the  coronavirus pandemic , which has spurred waves of lockdowns around the world. Apple's invite, sent in an email to reporters, shows an Apple logo rendered as a squiggle, a seeming reference to the iPad's popular Apple Pencil. The invite also included a phrase: Spring Loaded. Scant details like these often set tech industry watchers abuzz with attempts to decode what the company's hinting at. The invite also arrived a few hours after Apple's Siri  voice assistant appeared to spill the beans on the event before the official announcement . Close-watching Apple observers noticed that if you asked Siri when the next Apple event was, it responded, \"The special event is on Tuesday, April 20, at Apple Park in Cupertino, CA. You can get all the details on Apple.com.\" Later in the morning, Apple changed Siri to merely tell people that details about Apple events are on its website. The event marks another milestone for Apple as it aims to stay on track with its product launch schedule, which typically includes events in the spring, summer and fall. In 2020, as the COVID-19 pandemic took hold, Apple made  its Worldwide Developers Conference online-only, with slickly edited videos adding to the  typical stage presentation format  for its June event.\u00a0 \n                                                    Apple\n                                                 Apple didn't appear to let up on its device updates, either. Last year, it announced long-rumored changes to the technology powering its computers, moving away from the Intel processing brains it's used for more than a decade and switching to chips Apple designed in-house. The first of the new chips, called the M1, went on sale inside the MacBook Air, MacBook Pro and Mac Mini later in the year.\u00a0 See also: \u00a0 Apple's new iPad Pro in 2021: What to expect The company also  updated its iPhones  with new square-edged designs and 5G wireless technology, promising superfast mobile internet connections --  if you're in an area  that connects to 5G. CNET reviewer Patrick Holland called the iPhone 12 one of the highest-rated phones we've ever reviewed. \"5G support, a new striking design, improved cameras and four different models all add up to make the iPhone 12 an absolute unit,\" he wrote. The updated iPad will likely be a less dramatic shift, but it'll still be important. Students remain stuck at home, needing tablets and computers to do their schoolwork. Some people also use iPads to watch movies and TV shows. One change we know is coming will be iOS 14.5, a free software update that'll introduce major privacy changes to the iPad and iPhone software. A standout feature is  App Tracking Transparency , which Apple says will force companies and developers to be clear about how they're collecting user data and whether it's being used for advertising. Companies will also have to ask for permission from users  to more closely track them . Facebook has  pushed back on Apple's moves , saying they're unfair and could raise rates on small-business advertising. Effectively, Facebook said, Apple's privacy push would  make the internet more expensive .\u00a0 Apple CEO Tim Cook disagreed, saying he wants to give people the chance  to weigh in on how advertising works on their phones . \"We think that some number of people -- I don't know how many -- don't want to be tracked like that,\" Cook said. \"And they should be able to say they don't.\" The iOS 14.5 software update will also  feature new emoji , and an option to unlock your phone with your Apple Watch, making it much easier to  access information when wearing a mask .\u00a0 Apple hasn't said when the iOS 14.5 software update will arrive, but it's expected in the coming weeks.\u00a0 New tech In terms of technology built into the iPad, rumors swirling around the tech world suggest that Apple's biggest change will be to the screens. They're expected to use a technology called mini-LED, which screen makers say offers  improved power efficiency and better brightness . Apple is rumored to be transitioning its MacBook laptops and iMac desktop computers to the technology sometime later. Modern iPhones  use OLED screens , which are considered higher quality but typically  cost more money to make . iOS 15 rumors: \u00a0 Release date, buzzy new features, device compatibility and more The tech giant may also offer entirely new products,  rumored to be called AirTags . The devices are little trackers you can attach to backpacks, purses, toys or whatever else, so you can locate them using the iPhone's \"Find My\" app. Samsung beat Apple to market with its  $40 SmartTag devices , announced in January. Other companies make similar devices too,  including Tile  and  TrackR ."}, "210705_news_469660.txt": {"page_id": "210705_news_469660.txt", "text": "Die Praktikabilit\u00e4t und Effizienz des geplanten digitalen Nachweises f\u00fcr Impfungen gegen Covid-19, Testergebnisse und \u00fcberstandene Infektionen mit Sars-Cov-2 in Europa leuchten vielen EU-Abgeordneten noch nicht ganz ein. Mitglieder des Ausschusses f\u00fcr b\u00fcrgerliche Freiheiten, Justiz und Inneres des EU-Parlaments \u00fcberzogen Justizkommissar Didier Reynders und den Europ\u00e4ischen Datenschutzbeauftragten Wojciech Wiewi\u00f3rowski am Dienstag bei einer ersten Aussprache \u00fcber den Verordnungsentwurf mit so vielen Fragen, dass sie den Zeitrahmen der Sitzung sprengten und trotzdem nicht viele Antworten bekamen. Jeroen Lenaers von der konservativen Fraktion der Europ\u00e4ischen Volkspartei (EVP) begr\u00fc\u00dfte zwar den  Ansatz der Kommission, die Reisefreiheit zum Sommer hin wieder zu erleichtern . Die B\u00fcrger sollten damit leicht zeigen k\u00f6nnen, dass sie die erforderlichen Bedingungen erf\u00fcllen. Das vorgesehene \"digitale gr\u00fcne Zertifikat\" m\u00fcsse aber neue wissenschaftliche Erkenntnisse etwa zu Antik\u00f6rpern widerspiegeln und international interoperabel sein. Zudem d\u00fcrfe die Kommission nicht darauf warten, bis die Weltgesundheitsorganisation WHO das Ende der Corona-Pandemie erkl\u00e4re, um die volle Freiz\u00fcgigkeit wiederherzustellen. Unklar war dem Niederl\u00e4nder auch, wie der Nachweis hinreichend vor F\u00e4lschungen gesch\u00fctzt werden sollte. \"Mehr Flexibilit\u00e4t\" Die Sozialdemokratin Birgit Sippel wollte wissen, ob die Mitgliedsstaaten das Zertifikat nicht doch f\u00fcr andere Zwecke nutzen und pr\u00fcfende Beh\u00f6rden Profile \u00fcber die Nutzer anlegen k\u00f6nnten. Der Ministerrat mache bereits Druck auf \"mehr Flexibilit\u00e4t\" rund um den Einsatz des Instruments. Was mit den erhobenen Daten nach der Pandemie erfolge, gehe auch nicht eindeutig aus dem Entwurf hervor. Werde es wirklich so sein, dass B\u00fcrger an der Grenze durchgelassen w\u00fcrden, auch wenn sie kein Zertifikat h\u00e4tten? Diese Frage besch\u00e4ftigte die Liberale Sophie in \u2019t Veld genauso wie die Interoperabilit\u00e4t des Werkzeugs mit nationalen Systemen, auf deren Basis Interessierte wieder in Museen, Restaurants oder auf Konzerte gehen k\u00f6nnten. Die Vertreterin der Renew-Fraktion forderte von der Kommission einen Vorschlag f\u00fcr einfach handhabbare und kostenlose Tests. Wie beim Roaming m\u00fcssten dabei Preisobergrenzen der Anbieter festgelegt werden. Zudem sei ihr schleierhaft, wie alle Verkehrspunkte mit Leseger\u00e4ten ausgestattet werden sollten. \"Riesenstaus wie im Suez-Kanal\" F\u00fcr die Gr\u00fcnen signalisierte Tineke Strik, die Initiative prinzipiell zu unterst\u00fctzen. Der Datenschutz und das Prinzip der Nicht-Diskriminierung m\u00fcssten aber noch gest\u00e4rkt werden. F\u00fcr die Konservativen und Reformer unterstrich Nicola Procaccini, dass die EU-L\u00e4nder nicht weitere Ma\u00dfnahmen wie Zusatztests und Quarant\u00e4ne fordern d\u00fcrften. Die Linke Cornelia Ernst empfand es als irritierend, von einem digitalem Zertifikat zu sprechen, wenn auch analoge Nachweise gelten sollten. Unverst\u00e4ndlich sei, warum das Instrument an den Schengen-Raum gekn\u00fcpft werde, wenn es doch ohne Grenzkontrollen auskommen solle. Angesichts allein 12 Millionen t\u00e4glicher Pendler zwischen Mitgliedsstaaten und vielen Lkw-Fahrern bef\u00fcrchtet sie \"Riesenstaus wie im Suez-Kanal\", da Grenz\u00fcbertritte ja irgendwie \u00fcberpr\u00fcft werden m\u00fcssten. Die Deutsche sorgt sich ferner, dass etwa Personen durchs Raster fielen, die Covid-19 schon gehabt, aber keine Antik\u00f6rper h\u00e4tten.  Digitale Signatur im QR-Code Justizkommissar Reynders hielt dagegen, dass keine neuen nationalen Grenzkontrollen kommen sollten. Das kostenlose Zertifikat bette zus\u00e4tzliche Faktoren, aber weniger Daten ein als ein Impfausweis. Es werde etwa am Flughafen helfen, \"lange Diskussionen am Gate zu vermeiden\". F\u00fcr alle EU-B\u00fcrger sollten damit die gleichen Bedingungen rund um die Freiz\u00fcgigkeit gelten, sobald die Mitgliedsstaaten den Lockdown lockerten. F\u00fcr andere Nutzungszwecke als die Reisefreiheit br\u00e4uchten die Staaten ein eigenes Gesetz, das im vollen Einklang mit der Datenschutz-Grundverordnung ( DSGVO ) stehen m\u00fcsste. Reynders warnte davor, ein separates nationales System aufzubauen, da dies nur zu Fragmentierung und zus\u00e4tzlichen H\u00fcrden f\u00fcr die B\u00fcrger f\u00fchren w\u00fcrde. Es sollte auch ohne Impfung und Zertifikat m\u00f6glich sein zu reisen. Die Technik werde zusammen mit dem E-Health-Netzwerk der Mitgliedsstaaten entwickelt. Im Kern komme eine digitale Signatur zum Einsatz, die aus einem privaten Schl\u00fcssel einer Beh\u00f6rde beziehungsweise des Ausstellers und einem \u00f6ffentlichen bestehe. Letzterer k\u00f6nne zur Verifizierung der Authentizit\u00e4t des Zertifikats von vielen Stellen genutzt werden. Sonst gehe nur darum, einen QR-Code auszulesen und die elektronische Unterschrift dabei zu pr\u00fcfen. Falls erforderlich, solle daf\u00fcr noch eine spezielle Software entwickelt werden.  Kein falsches Sicherheitsgef\u00fchl schaffen Der Datenschutzbeauftragte Wiewi\u00f3rowski,  der in einer Stellungnahme bereits die Gefahr einer m\u00f6glichen Diskriminierung herausgearbeitet hatte , will das Zertifikat vor allem nicht als Immunit\u00e4tsausweis verstanden wissen. Dazu gebe es noch keine belegbaren empirischen Befunde. Selbst wenn das System aus Datenschutzgesichtspunkten tragf\u00e4hig w\u00e4re, d\u00fcrfe die Politik damit kein falsches Sicherheitsgef\u00fchl bei der Bev\u00f6lkerung schaffen. Im Kampf gegen Corona lasse sich mit dem Werkzeug kaum punkten, aber die B\u00fcrger k\u00f6nnten sich damit eher wieder als Europ\u00e4er f\u00fchlen. Eine Endklausel sei n\u00f6tig, hob Wiewi\u00f3rowskir hervor. Es m\u00fcsse klar kommuniziert werden, dass mit dem Ende der Pandemie kein Zugang mehr zu den Daten bestehe. Dieser Zeitpunkt sei bisher aber nicht absehbar, da das Virus sich hartn\u00e4ckig halten d\u00fcrfte. ( axk )"}, "210705_news_469669.txt": {"page_id": "210705_news_469669.txt", "text": "US health agencies have  recommended  states pause the administration of the Johnson & Johnson coronavirus vaccine, after reports of rare and severe blood clots emerged in six women. More than 6.8m doses have been administered nationally. The concerns  mirror those  of drugs agencies in Europe and Australia over the AstraZeneca vaccine, which has not been authorized in the US. There have been no significant safety concerns raised about the two other vaccines that make up the majority of US supply, from Pfizer-BioNTech and Moderna. The acting FDA chief, Janet Woodcock, said: \u201cWe\u2019re recommending this pause while we work together to full understand these events.\u201d The decision was taken in coordination with the CDC. Woodcock said: \u201cRight now, I\u2019d like to stress these events appear to be extremely rare. However, Covid-19 vaccine safety is a top priority for the federal government. We take all reports of adverse events related to the vaccine very seriously.\u201d The FDA and CDC said in a joint statement: \u201cPeople who have received the J&J vaccine who develop severe headache, abdominal pain, leg pain, or shortness of breath within three weeks after vaccination should contact their healthcare provider.\u201d Officials with both agencies said the events occurred between one and two weeks after the vaccine was administered, past the stage when many people might experience common, flu-like symptoms associated with the vaccine. Officials said people who received the vaccine more than one month ago should consider their risk \u201cvery low\u201d. Anne Schuchat, the deputy principal director of the CDC, said she understood many Americans who had already received the vaccine were \u201cprobably very concerned\u201d but emphasized the syndrome associated with the vaccine was exceedingly rare. Nevertheless, \u201cit was clear to us we needed to alert the public,\u201d Schuchat said. Officials are also expecting to discover more cases in the coming days, as clinicians realize the potential link to recent vaccination. Officials also warned clinicians to be on the lookout for the rare syndrome, as using a common treatment for blood clots \u2013 the blood thinner heparin \u2013 could be \u201cdangerous\u201d and worsen the syndrome. All six cases of the clotting disorder were women were between the ages of 18 and 48. One woman died, and a woman in Nebraska is hospitalized in critical care, according to the  New York Times . Johnson & Johnson\u2019s vaccine is also  under scrutiny  by the European Medicines Agency, which is investigating four cases of clotting. Health agencies said the blood clotting concern is \u201cextremely rare\u201d. Authorities are still investigating whether the clotting is indeed caused by the vaccine. The background rate of similar clotting disorders is between two and 14 people per million. However, the presence of low platelet count in conjunction with the clotting makes the syndrome remarkably similar to clotting seen with the AstraZeneca vaccine. As of  4 April , there have been 222 cases of blood clots among the more than 34 million globally who have received the AstraZeneca vaccine. The European Medicines Agency has examined 86 cases of blood clots, 18 of which have been fatal. US officials said they are \u201cin constant contact with regulators worldwide\u201d. J&J said in a statement: \u201cThe safety and wellbeing of the people who use our products is our No 1 priority. At present, no clear causal relationship has been established between these rare events and the Janssen Covid-19 vaccine.\u201d Janssen is the division of Johnson & Johnson that developed the vaccine. Johnson & Johnson also said it will \u201cproactively\u201d delay the rollout of the vaccine in Europe. US officials also said they are looking closely at whether the technology shared by the AstraZeneca and Johnson & Johnson vaccine could have a link to the syndrome. J&J and AstraZeneca use a vaccine technology called an adenovirus platform to prompt immunity. The technology uses a second, weakened cold virus to deliver the genetic payload of the coronavirus to the immune system, and spur the body to make antibodies. AstraZeneca\u2019s vaccine uses a virus derived from chimpanzees, where J&J\u2019s uses a human virus. Dr Peter Marks, director of the center for biologics evaluation and research at the FDA, said: \u201cIt\u2019s plainly obvious to us already that what we\u2019re seeing with the Janssen vaccines looks very similar to what was being seen with the AstraZeneca vaccine.\u201d However, a mechanism that may cause the potential connection between clotting and the adenovirus platform is not clear. Officials said they currently hypothesize the reaction may relate to an immune response involving platelets. Moderna and Pfizer use a different vaccine platform called messenger RNA, to provoke the body to build antibodies. Independent experts have said the \u201cpause\u201d in administration shows federal monitoring systems for potential adverse side effects of the vaccine are working. Scott Gottlieb, a former FDA commissioner, said on  CNBC\u2019s Squawk Box : \u201cFor most consumers, I wouldn\u2019t be concerned about this. Really this is an alert to doctors. That\u2019s how FDA framed it. It\u2019s advice to doctors to be monitoring more closely.\u201d The pause in administration of the Johnson & Johnson vaccine may also cause a hit to the American immunization campaign, which was set to administer hundreds of thousands of easy-to-administer doses provided by the company. The White House denied the pause would have a significant impact, and said vaccination appointments could be rescheduled. The J&J pause comes at a perilous time in the pandemic. Despite the strides made in immunizing all adult Americans, with 74 million people and  22% of the public  fully immunized, Covid-19 cases are  rising in several states , such as Minnesota, and remain high in others, such as Michigan. With a weary public and the return of warm weather in much of the country, politicians have had little appetite to renew restrictions on social life and many instead say the vaccination campaign can curb cases. The new concerns are also likely to inflame partisan tensions and the conspiracy theorists. Republicans,  especially men , remain the most hesitant to receive a coronavirus vaccine after national politicians and Donald Trump downplayed the seriousness of the virus. Vaccine hesitancy could play an increasingly significant role in whether the US is able to curb the pandemic, or if the viral spread is allowed to continue and circulate regionally. Although the exact level of immunity needed to squash viral spread is not known, experts believe between  70-90%  of the public will need to be vaccinated. Because children make up about a quarter of the US population, that level of immunity would require nearly every US adult to be vaccinated to stop spread."}, "210705_news_469674.txt": {"page_id": "210705_news_469674.txt", "text": "T he biggest surge-testing operation yet is  under way in south London , with all people over the age of 10 who live, work or travel through Wandsworth and Lambeth being urged to take a Covid PCR test on top of twice-weekly rapid testing. The measures are prompted by a cluster of cases of the variant first detected in South Africa, against which several vaccines have shown reduced efficacy. They cast a shadow over the UK\u2019s successful  inoculation campaign . The urgency is sensible. But it stands in striking contrast to the fact that the variant is being allowed to circulate largely unhindered in South Africa itself, and more broadly on the African continent, thanks to low levels of immunisation. Only about 300,000 of the country\u2019s one million health workers have been protected. While the government too has responsibility for the slow pace, it is hard to argue with the warning of the president, Cyril Ramaphosa, that we are watching \u201cvaccine apartheid\u201d. The 700m doses delivered worldwide have overwhelmingly gone to the rich. Low-income countries have received  just 0.2%  of them. This helps to explain the  exponential growth  of coronavirus that we are witnessing more than a year into the pandemic. Cases have been rising for seven straight weeks. India alone recorded 168,000 new cases on Monday. The  resurgence  has prompted it to halt exports from the Serum Institute of India, the world\u2019s largest manufacturer of Covid vaccines, to the dismay of waiting governments in Africa. Overall, Africa has so far fared much better than anticipated. Many governments were swift to close borders and impose restrictions as coronavirus emerged, compared to the complacency seen in Europe. Its relative youth may help to explain low death tolls; so too, almost certainly, does under-recording. But the World Health Organization has warned that a third wave of the virus  could overrun   struggling healthcare systems . Millions of doctors, nurses and others in sub-Saharan Africa are risking their lives, yet will wait  months or even years for protection . Though the vaccine pooling scheme Covax has begun delivering doses, it cannot acquire enough. Pharmaceutical firms should be pressed to follow AstraZeneca in providing vaccines at cost price. But the main issue is supply. Covax has already begun crucial work to help scale up manufacturing, but the complexities of production mean that may not pay off before next year. Wealthy countries promise to hand over spare doses, but will not say when. They are hoarding supplies to cover their entire populations, provide boosters, and have alternatives on hand in case of vaccine resistance. They should make a real commitment, instead of risking a worst-case scenario in which doses are squandered: stockpiled but unused until they prove redundant. The more the virus circulates, the greater its opportunities to mutate, increasing the risk to us all. The pandemic can only be brought under control by suitable public health measures \u2013 such as tracing, mask wearing and social distancing \u2013 and a more equitable distribution of vaccines. It is wise to keep a watch on new variants at home, but of limited use while the virus circulates unchecked elsewhere."}, "210705_news_469682.txt": {"page_id": "210705_news_469682.txt", "text": "Google has worked for years to position itself as a responsible steward of AI. Its research lab hires respected academics, publishes groundbreaking papers, and steers the agenda at the field\u2019s biggest conferences. But now its reputation has been badly, perhaps irreversibly damaged, just as the company is struggling to put a politically palatable face on its empire of data. The company\u2019s decision to fire Timnit Gebru and Margaret Mitchell \u2014 two of its top AI ethics researchers, who happened to be examining the downsides of technology integral to Google\u2019s search products \u2014 has triggered waves of protest. Academics have registered their discontent in various ways. Two  backed out  of a Google research workshop, a third  turned down  a $60,000 grant from the company, and a fourth  pledged  not to accept its funding in the future. Two engineers quit the company in  protest of Gebru\u2019s treatment  and just last week, one of Google\u2019s top AI employees, a research manager named Samy Bengio who oversaw hundreds of workers,  resigned . (Bengio did not mention the firings in an email announcing his resignation but earlier said he was \u201c stunned \u201d by what happened to Gebru.)  \u201cIt worries me that they\u2019ve shown a willingness to suppress science\u201d \u201cNot only does it make me deeply question the commitment to ethics and diversity inside the company,\u201d Scott Niekum, an assistant professor at the University of Texas at Austin who works on robotics and machine learning, told  The Verge.  \u201cBut it worries me that they\u2019ve shown a willingness to suppress science that doesn\u2019t align with their business interests. \u201cIt definitely hurts their credibility in the fairness and AI ethics space,\u201d says Deb Raji, a fellow at the Mozilla Foundation who works on AI accountability. \u201cI don\u2019t think the machine learning community has been very open about conflicts of interest due to industry participation in research.\u201d Niekum and Raji, along with many others inside and outside of Google, were shocked by what happened to Gebru and Mitchell, co-leads of the company\u2019s Ethical AI team. Gebru was fired  last December  after arguments with managers over a research paper she co-authored with Mitchell and others. (Google disputes this account and  says  Gebru resigned.) Mitchell was fired  in February  after searching her email for evidence of discrimination against Gebru. The paper in question  examined problems  in large-scale AI language models \u2014 technology that now underpins Google\u2019s lucrative search business \u2014 and the firings have led to protest as well as accusations that the company is suppressing research. After Gebru was ousted in December, a  Medium post  declaring solidarity with her and criticizing \u201cunprecedented research censorship\u201d by Google was signed by nearly 2,700 employees and more than 4,300 \u201cacademic, industry, and civil society supporters.\u201d  It\u2019s likely there will be more protest and more resignations, too. After Bengio left the company, Mitchell  tweeted , \u201cResignations coming now bc people started interviewing soon after we were fired,\u201d and that \u201cjob offers are just starting now; more resignations are likely.\u201d When asked for comment on these and other issues highlighted in this piece, Google offered only boilerplate responses. SHAKEN CONFIDENCE One of the employees who  quit the company  in protest earlier this year was David Baker. He started work at Google in 2004 and when he resigned in February, he was director of its Trust & Safety Engineering group. He tells  The Verge  that Google\u2019s treatment of Gebru (he left before Mitchell was fired) has seriously shaken his confidence in the company.  \u201cI was just blindsided to see and hear what happened to Timnit,\u201d Baker told  The Verge . \u201cIt broke my heart.\u201d He adds that he didn\u2019t take the decision to resign lightly: he loved his job and refers to his last couple of years at the company as \u201cthe happiest days of my life.\u201d But quitting was the least he could do to stand in solidarity with Gebru, he says. \u201cI spent a couple of weeks thinking and talking with my wife and ultimately decided I just couldn\u2019t bring myself to go back to work.\u201d  Baker is just one individual who feels let down by Google, but his response shows how the company has damaged its standing even with senior employees. The Trust & Safety team that Baker oversaw works on a range of important safety problems in Google, from tackling spam on Gmail to removing scams from the company\u2019s advertising platform. \u201cWe\u2019re behind the scenes on a whole bunch of applications,\u201d as Baker puts it. He adds that although he didn\u2019t work with Gebru or Mitchell personally, members of his team did, learning from them as part of what he calls the \u201cemerging discipline\u201d of AI safety.  \u201cGoogle\u2019s failure in diversity will lead to blindspots in its research\u201d AI safety will grow ever more important to Google as the company integrates machine learning methods ever deeper within its products. Probing the limitations of these systems \u2014 not just from a technical perspective but also a social one \u2014 was at the heart of Gebru and Mitchell\u2019s work. And while it\u2019s in Google\u2019s interests to find weaknesses in its own technology, it seems the company didn\u2019t want to hear everything its employees had to say. Baker says that although he was always reassured by Google\u2019s integrity within the Trust & Safety group (\u201cWe were very focused on what was right for the user, it was not about what was best for the brand\u201d) the treatment of Gebru has made him doubt whether the company is always able to live up to its best intentions.  \u201cI think it definitely calls into question whether Google can be trusted to honestly question the ethical applications of its technology,\u201d says Baker. \u201cAnd Google\u2019s failure in diversity will lead to blindspots in its research. The reality is that Google is not a place where folks from all backgrounds can thrive.\u201d SUPRESSING SCIENCE? Researchers and academics  The Verge  spoke to for this story highlighted two distinct but connected concerns with Google\u2019s behavior.  The first is the treatment of Gebru and Mitchell as individuals and what that says about the company\u2019s commitment to diversity and inclusion as an employer. Google has  well-documented problems  with hiring and retaining minority talent, and this is another example of its failures. The second touches on broader questions about the trustworthiness of the company\u2019s AI research and whether the company can fairly examine the potential harms of its technology. In the case of Gebru and Mitchell\u2019s work, that means the damage posed by large-scale language models.  All those interviewed for this story stressed that they didn\u2019t doubt the integrity of individual Google researchers, but were worried that the company\u2019s internal structures \u2014 including its review process of papers \u2014 were subtly warping their work. \u201cI trust that things they are publishing are correct but I don\u2019t trust that they\u2019re not censored,\u201d Hadas Kress-Gazit, a professor of robotics at Cornell who boycotted a Google workshop along with Scott, told  The Verge . \u201cIt\u2019ll be the truth but not the whole truth.\u201d  One of the ways Google\u2019s research is shaped to fit corporate interests is through the company\u2019s internal review process. Last December,  Reuters  reported  that Google had created a new level of review for \u201csensitive topics\u201d in 2020. If researchers are writing about topics like sentiment analysis, facial recognition, the categorization of gender, race, or politics, they have to consult with Google\u2019s PR team and legal advisors who will look over their work and suggest changes.  Internal correspondence cited by  Reuters  includes feedback in which a senior Google manager told a paper\u2019s author to \u201ctake great care to strike a positive tone.\u201d Another paper was edited to remove all references to Google\u2019s products, and another to remove mentions of legal risks associated with new research \u2014 including risks to users\u2019 personal data. In a statement to  The Verge , Google said: \u201cOur research review process engages a wide range of subject matter experts from across the Research org and Google overall, including social scientists, ethicists, policy and privacy advisors, and human rights specialists, and has helped improve many of our publications and research applications.\u201d \u201cwe\u2019re getting into a serious problem of censorship.\u201d But as Mitchell told  Reuters  last year (when she was still employed by Google): \u201cIf we are researching the appropriate thing given our expertise, and we are not permitted to publish that on grounds that are not in line with high-quality peer review, then we\u2019re getting into a serious problem of censorship.\u201d Mitchell\u2019s worries are substantiated by the nature of the paper that led to her and Gebru\u2019s departure. Far from offering a controversial or unexpected appraisal, the research gave a comprehensive overview of existing critiques. One marker of this (and of the research\u2019s thoroughness) is that the paper cited 128 previous publications in its original form \u2014 more than  six times the average  for papers published at AI conference NeurIPS. The paper says that, like many algorithms, AI language models have a tendency to regurgitate \u201cboth subtle biases and overtly abusive language patterns\u201d found in training data, and that because of the amount of computing power needed to create these models they come with environmental costs. These are not controversial observations, and even critiques of the paper have praised its general arguments. One widely shared evaluation of a finished version of the paper by computer scientist Yoav Goldberg  notes  that it \u201ctakes one-sided political views\u201d and is overly focused on questions of scale, but adds: \u201cI also agree with and endorse most of the content. This is important stuff, you should read it.\u201d This makes Google\u2019s objections to the paper unusual. The company\u2019s head of AI, Jeff Dean,  said  that the paper \u201cdidn\u2019t meet our bar for publication\u201d and \u201cignored too much relevant research\u201d about how the problems it highlighted might be mitigated. But for many, including employees at Google, these objections rang false. As one researcher at Google Brain Montreal, Nicolas Le Roux,  commented  on Twitter: \u201cMy submissions were always checked for disclosure of sensitive material, never for the quality of the literature review.\u201d Black and female researchers have made some of the most powerful critiques of AI Connected to Google\u2019s treatment of the paper itself is the treatment of Gebru as an individual, and what that says about the company\u2019s attitude toward Black and women researchers. \u201cIn environments where these people are dismissed, devalued, or discriminated against, their work \u2014 these valid critiques of the field \u2014 is discredited and dismissed, too,\u201d says Raji. \u201cMinoritized voices have a harder time vocalizing these critiques even though they\u2019re some of the most important contributions to the space.\u201d  This dynamic is not new. Raji gives the example of a 2018 paper called  Gender Shades  by researcher Joy Buolamwini \u2014 a paper now recognized as a landmark critique of gender and racial bias in facial recognition. \u201cFamously, the paper almost didn\u2019t get presented at conference because it was dismissed as too simple,\u201d says Raji. After it was published, Gender Shades had a huge effect on the industry and society at large. It sparked political debates about the utility of facial recognition, prompting companies like Microsoft to reevaluate the accuracy of their technology, and others, like IBM, to drop it altogether.  In other words: it significantly changed the political landscape and the priorities of big tech firms. This is the power and impact that the right paper at the right time can have, and for many people this explains why Google was so keen to shut down Gebru\u2019s criticism.  As Raji notes, much of this important work is done by groups who are not treated well by tech firms. She says this dynamic \u2014 dismissal of the individual leading to dismissal of their work \u2014 was at play with Google\u2019s treatment of Gebru. \u201cIt was really easy for them to fire her because they didn\u2019t value the work she was doing,\u201d she says. TURNING AWAY FROM TECH   Despite the anger and sadness articulated by many researchers  The Verge  spoke to, others were more ambivalent about recent incidents. They said it would not affect their willingness to work with Google in future, and noted that interference in research was the price of working in industry labs. Many said they thought the only lasting solution to this problem was better public funding.  One AI professor at an American university who\u2019s previously received money from Google to fund research and wished to be anonymous, told  The Verge  that he could understand why people wanted to protest the company but said that finding funding in academia would always force researchers to turn to potentially compromising sources. Industry labs will always be swayed by corporate interests, say many researchers \u201cI cannot really define a coherent moral or ethical position that says it is okay to accept money from the Department of Defense but not from Google,\u201d said the professor by email. \u201cPut another way: how can you accept (or avert your gaze from) the atrocities that the DoD commits (across the world and also in terms of HR matters involving its own people), but draw the line at the current case with Google?\u201d Another researcher, who also wished to be anonymous, noted that working in corporate labs would always come with trade-offs between academic freedom and other perks. They said that Google was not alone in treating research staff callously and pointed to  Microsoft\u2019s sudden decision in 2014  to shut down an entire Silicon Valley lab, firing more than 50 leading computer scientists with little warning. By some measures, though, Google is a special case and wields outsize influence in the field of AI in a way that other companies have not in the past. Firstly, Google happens to have in abundance the two resources that have powered AI\u2019s ascendance in recent years: abundant computing power and data. Secondly, the company has stated time and time again that AI is crucial to future profitability. This means it\u2019s directly invested in the field in a way that doesn\u2019t compare to its funding of, say, computational neuroscience. It\u2019s this combination of self-interest and technological advantage that gives it the ability and motivation to direct, to some degree, the parameters of academic research. \u201cThey have this massive influence because of the combination of money they\u2019re putting into research, [the] media influence they wield, and their enormous presence in terms of papers published and reviewers in the system,\u201d says Niekum. He adds, though, that this criticism could be applied to other big tech companies just easily. Whatever the context of Google\u2019s involvement in AI research, it\u2019s clear that the company has hurt its reputation significantly with its treatment of Gebru and Mitchell. Calculating what effect incidents like these will have on a company in the long run is impossible, but in the short term Google has eroded trust in its AI work and its ability to support minority voices. Accusations of self-censorship will also undermine claims that it can regulate its own technology. If Google can\u2019t be trusted to examine the shortcomings of its own AI tools, does the government need to take a closer look at their workings?  Google needs to do much more to win back many researchers\u2019 trust All the same, those boycotting Google workshops and refusing its money know that their actions are more symbolic than anything else. \u201cCompared to the number of people who are collaborating with Google and the number of academics who have part time appointments at Google, it\u2019s a drop in the ocean,\u201d says Kress-Gazit. They\u2019re still determined, though, to press the issue in the hope that Google will make amends. Since the firing of Gebru and Mitchell, the company has appointed a new employee, Marian Croak, to  oversee its Responsible AI initiatives . It\u2019s also  tweaked  its review process for papers (but offered no details about what has changed or why). For those angry with the firm, it needs to do much more, including offering real transparency for reviews and apologizing publicly to Gebru. And for others, it\u2019s too late altogether. Raji, who is close to Gebru, says that as a result of watching how Google treated her friend over last few months, she\u2019s changed her mind about going to work in industry and decided to pursue a career in academia instead. \u201cBefore this, I had a lot more faith in what could happen with industry research on these AI ethics issues,\u201d she says. \u201cThis whole situation shows that within industry there\u2019s a lot of cultural dynamics still at play and you\u2019re still beholden to leadership caring about these issues. As a minority woman, you\u2019re going to be disadvantaged and disrespected in certain ways. And I\u2019m just not ready for that.\u201d That\u2019s one talented researcher the tech industry has lost. It won\u2019t be the last. "}, "210705_news_469683.txt": {"page_id": "210705_news_469683.txt", "text": "Microsoft\u2019s hardware legacy stretches back more than four decades to the early days of personal computing. As Windows built and inspired new computing experiences, new Microsoft hardware accessories helped people interact with them. At this intersection of hardware and software, magical experiences are possible. This core belief is at the heart of every product we build at Microsoft, and it\u2019s especially true of our Surface products. For almost a decade, Surface has pioneered breakthrough experiences that immerse people in their creative flow, eliminating friction that often exists between people and technology. Surface allows people to adapt to new ways of working and creating through innovative and versatile devices that offer premium design and performance, quality typing and trackpads, exceptional cameras and mics, and immersive touchscreen display technologies. From the student learning on their Surface Go or Surface Laptop Go, to the knowledge worker on their Surface Pro 7+, to the creative professional working on a Surface Book 3, these products now serve as a lifeline to connections, content and work. It\u2019s inspiring to consider all that can be achieved on a Surface. For years, Surface Laptop has stood apart from the crowd with craftsmanship unlike any other. We\u2019ve spoken about how much we believe the details matter \u2013 and that conviction is rewarded when we see Surface Laptop consistently hold the highest customer satisfaction rating in its class.[1] When we explore what customers like most, we hear a unique mix of product capabilities and fine details. For some, it\u2019s all about the satisfying typing experience and Alcantara finishes. For others, it\u2019s the immersive capabilities of touch, video quality, our 3:2 displays and reliable app compatibility. We, too, believe that a device\u2019s performance goes beyond what can be measured in specifications. We hear from customers that Surface Laptop\u2019s performance stands out not only because of what it can do, but also because of how  it speaks to the senses and inspires people . We have been hard at work, continuing to innovate while preserving the elements customers love most about this product. We\u2019re pleased to introduce Surface Laptop 4. Surface Laptop 4  is, first and foremost, optimized for Microsoft experiences. It retains the iconic design, details and materials that our customers love. With our signature 3:2 PixelSense touchscreen displays in 13.5\u201d or 15\u201d models, customers can choose from Alcantara or metal finishes in a variety of bold colors you can\u2019t get anywhere else, including a new Ice Blue finish. Surface Laptop 4 offers a built-in HD front-facing camera with incredible low-light capability and a studio microphone array, ensuring you\u2019ll come through loud and clear as you connect with loved ones or participate in a crucial virtual work meeting. With the 201 PPI, high-contrast touchscreen display, and Dolby\u00ae Atmos\u2122 Omnisonic speakers, you\u2019ll get a cinematic experience that immerses you in your favorite movies and shows from the comfort of anywhere. Whether through the vibrant touchscreen display, large trackpad with gesture support, or its industry-leading typing experience, Surface Laptop can adjust to your preferred workflow. Surface Laptop 4 offers a choice between either the 11th Gen Intel\u00ae Core\u2122 processors, or AMD Ryzen\u2122 Mobile Processors with Radeon\u2122 Graphics Microsoft Surface\u00ae Edition (8 cores); in either case, you\u2019ll find a smooth experience that powers modern, multitasking demands. Across both options, we\u2019ve partnered to customize silicon in ways that boost performance while offering improved battery life. On the outside, you see style and sophistication. Effortlessly lift the screen, and this product will draw you into your flow. From the office, living room, coffee shop or classroom, nothing comes close to this product\u2019s look and feel. Surface Laptop 4 joins Surface Laptop Go to provide a variety of Laptop options. Choose your perfect color, size and price. Starting at $999.99 USD, Surface Laptop 4 is  available to order today  in the US, Canada, and Japan, with additional market availability in the coming weeks. Orders will begin shipping April 15. Surface fans in the US who purchase Surface Laptop 4 through  microsoft.com  or  bestbuy.com  before April 15 will receive Surface Earbuds as a special gift, on us.[2] \ud83d\ude09 Accessories to optimize your virtual meeting experience With deep roots in PC accessories, we\u2019re proud to offer a breadth of keyboards, mice and  other accessories  that feel tailored to individual needs \u2013 especially as we work, learn and play from home. In recent years, we\u2019ve added audio accessories like Surface Headphones 2 and Surface Earbuds that begin with incredible acoustic quality, comfort and functionality wrapped in Surface\u2019s iconic design ethos. Today, as people adapt to a spectrum of unique work environments, we\u2019re responding with a new assortment of audio and video accessories certified for Microsoft Teams and great across all your favorite video conferencing apps. Each of these products offers plug-and-play functionality for seamless setup, great audio or video to make you look and sound your best, LED indicators to know your Teams status, and on-device call controls to keep you in the flow. Additionally, most accessories feature a dedicated Teams button to bring Teams to the front of a crowded screen or quickly join your next meeting."}}